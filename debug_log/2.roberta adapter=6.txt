checkpoint_save_path: /home/zhujiangyi/prompt_learning/checkpoints/adapter/go_emotions/roberta-base/2022-10-17_02-26_bmtrain_float16_seed_78
Namespace(pattern='train', source='bmtrain', task='go_emotions', model_class='roberta-base', language='English', soft_layer=5, adapter_bottleneck_dim=6, adapter_init_std=0.0, epochs=20, learning_rate=0.0001, batch_size=32, seed=78, dtype='float16', lr_scheduler='Linear', soft_template_load_path='None', adapter_load_path='None', checkpoint_save_path='/home/zhujiangyi/prompt_learning/checkpoints')
roberta roberta-base
load from local file: /home/zhujiangyi/.cache/model_center/roberta-base
load from local file: /home/zhujiangyi/.cache/model_center/roberta-base
load from local file: /home/zhujiangyi/.cache/model_center/roberta-base
template_text: {"placeholder":"text_a"} It was {"mask"} {"soft": None, "duplicate":5}
len(train_dataloader) 1357
len(valid_dataloader) 170
len(test_dataloader) 170
train data truncate rate: 2.30361667818475e-05
==================
tensor([[-0.0083, -0.0109,  0.0041,  ..., -0.0063,  0.0151, -0.0129],
        [ 0.0039, -0.0075,  0.0213,  ..., -0.0004, -0.0071,  0.0240],
        [ 0.0029,  0.0118,  0.0090,  ...,  0.0070,  0.0040,  0.0017],
        [ 0.0020,  0.0171,  0.0100,  ...,  0.0026,  0.0034, -0.0074],
        [-0.0223, -0.0101, -0.0027,  ...,  0.0176, -0.0029, -0.0138],
        [ 0.0041, -0.0013, -0.0008,  ..., -0.0045,  0.0024,  0.0137]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>)
tensor([[-0.0070,  0.0030, -0.0211, -0.0035,  0.0046, -0.0013],
        [-0.0101, -0.0100,  0.0039,  0.0019, -0.0054, -0.0155],
        [-0.0125, -0.0002,  0.0092,  0.0085, -0.0076, -0.0029],
        ...,
        [ 0.0147, -0.0070, -0.0225, -0.0167, -0.0055,  0.0110],
        [-0.0191, -0.0028,  0.0116, -0.0056, -0.0162, -0.0096],
        [ 0.0012,  0.0109,  0.0027, -0.0029,  0.0052,  0.0139]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([[ 7.4463e-03,  7.3280e-03, -1.4465e-02,  ...,  2.2720e-02,
         -1.4259e-02,  7.5264e-03],
        [-1.2383e-02,  5.7869e-03, -3.2440e-02,  ..., -1.0315e-02,
          7.6714e-03,  1.9882e-02],
        [-6.9504e-03,  2.1896e-03,  9.1219e-04,  ..., -3.2024e-03,
         -3.1757e-03,  1.6556e-03],
        [-4.1275e-03,  5.8031e-04, -1.7120e-02,  ..., -6.0201e-05,
          3.5431e-02, -2.0199e-03],
        [-2.6611e-02, -1.2627e-02, -5.8591e-05,  ..., -3.8300e-03,
         -4.4098e-03, -2.7588e-02],
        [-5.9662e-03,  5.2977e-04, -2.7756e-02,  ..., -4.6844e-03,
         -1.2851e-04,  1.4601e-03]], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>)
tensor([[-0.0086,  0.0033, -0.0049, -0.0028,  0.0027,  0.0148],
        [ 0.0158,  0.0088, -0.0165, -0.0037,  0.0004, -0.0055],
        [ 0.0077,  0.0038,  0.0057, -0.0026,  0.0046, -0.0176],
        ...,
        [-0.0025, -0.0008, -0.0027, -0.0191, -0.0124,  0.0045],
        [ 0.0047,  0.0166, -0.0097, -0.0066,  0.0073,  0.0077],
        [-0.0043,  0.0057,  0.0067,  0.0016,  0.0017, -0.0027]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([[ 0.0159, -0.0002, -0.0156,  ..., -0.0054, -0.0054,  0.0121],
        [ 0.0200, -0.0069,  0.0156,  ..., -0.0096, -0.0115, -0.0127],
        [ 0.0022,  0.0024, -0.0121,  ...,  0.0119,  0.0011,  0.0122],
        [ 0.0019, -0.0018, -0.0044,  ...,  0.0003,  0.0067, -0.0003],
        [-0.0112, -0.0094,  0.0244,  ...,  0.0061,  0.0003,  0.0109],
        [-0.0111, -0.0035, -0.0171,  ...,  0.0107, -0.0152,  0.0113]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>)
tensor([[ 0.0257, -0.0078, -0.0025, -0.0119, -0.0034, -0.0154],
        [-0.0099,  0.0026,  0.0081, -0.0111,  0.0063, -0.0032],
        [ 0.0025, -0.0009,  0.0088, -0.0020,  0.0089, -0.0006],
        ...,
        [-0.0009,  0.0089,  0.0103,  0.0038, -0.0114, -0.0033],
        [ 0.0083,  0.0011,  0.0048,  0.0104,  0.0019,  0.0108],
        [-0.0078, -0.0141,  0.0090,  0.0019,  0.0022,  0.0035]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([[ 0.0317, -0.0047, -0.0225,  ...,  0.0074,  0.0090,  0.0039],
        [-0.0027,  0.0036, -0.0026,  ..., -0.0047,  0.0018,  0.0111],
        [ 0.0008, -0.0194, -0.0072,  ..., -0.0010, -0.0032, -0.0080],
        [-0.0160,  0.0005,  0.0132,  ...,  0.0087, -0.0080, -0.0225],
        [-0.0044,  0.0119,  0.0091,  ..., -0.0151,  0.0038, -0.0162],
        [-0.0086, -0.0016, -0.0004,  ..., -0.0094,  0.0093,  0.0155]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>)
tensor([[ 0.0026, -0.0117, -0.0037,  0.0099, -0.0066,  0.0094],
        [ 0.0043, -0.0061, -0.0213,  0.0178,  0.0062, -0.0087],
        [-0.0042,  0.0079, -0.0074, -0.0018, -0.0040, -0.0076],
        ...,
        [-0.0050,  0.0047,  0.0057, -0.0242, -0.0053,  0.0001],
        [-0.0084, -0.0174,  0.0028, -0.0024,  0.0040, -0.0019],
        [-0.0181, -0.0160, -0.0069,  0.0029, -0.0094,  0.0124]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([[ 0.0032,  0.0126,  0.0063,  ...,  0.0183,  0.0005, -0.0122],
        [ 0.0077, -0.0025, -0.0066,  ...,  0.0007,  0.0072, -0.0034],
        [ 0.0083,  0.0023, -0.0143,  ..., -0.0107, -0.0115, -0.0166],
        [-0.0249, -0.0094,  0.0145,  ..., -0.0037, -0.0029,  0.0066],
        [-0.0066, -0.0136, -0.0057,  ...,  0.0066, -0.0033,  0.0176],
        [-0.0017,  0.0075,  0.0144,  ...,  0.0066, -0.0153,  0.0033]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>)
tensor([[-0.0038, -0.0193, -0.0042, -0.0034,  0.0075, -0.0106],
        [-0.0224, -0.0096, -0.0123, -0.0131, -0.0027,  0.0010],
        [ 0.0029,  0.0093,  0.0012,  0.0026,  0.0008, -0.0016],
        ...,
        [ 0.0234, -0.0163, -0.0045, -0.0142, -0.0090, -0.0037],
        [ 0.0094, -0.0170, -0.0065,  0.0060,  0.0076,  0.0042],
        [ 0.0017, -0.0005, -0.0042, -0.0084, -0.0034, -0.0121]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([[ 0.0102, -0.0021,  0.0127,  ...,  0.0022,  0.0024, -0.0107],
        [ 0.0055, -0.0031,  0.0032,  ...,  0.0051,  0.0046,  0.0054],
        [ 0.0059, -0.0097,  0.0024,  ...,  0.0197, -0.0047,  0.0215],
        [-0.0044, -0.0114, -0.0013,  ..., -0.0067,  0.0186,  0.0017],
        [-0.0162, -0.0152,  0.0087,  ..., -0.0033, -0.0074,  0.0058],
        [-0.0117, -0.0002, -0.0094,  ...,  0.0079, -0.0054,  0.0158]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>)
tensor([[ 0.0023, -0.0008, -0.0049, -0.0152, -0.0008, -0.0101],
        [ 0.0111,  0.0034, -0.0019,  0.0116,  0.0070,  0.0213],
        [ 0.0024, -0.0067, -0.0336, -0.0067, -0.0008, -0.0063],
        ...,
        [-0.0071, -0.0029,  0.0002,  0.0118,  0.0119, -0.0118],
        [ 0.0049,  0.0019, -0.0089, -0.0049, -0.0057,  0.0006],
        [-0.0108,  0.0132, -0.0121,  0.0101,  0.0020, -0.0115]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([[ 0.0016, -0.0097, -0.0037,  ..., -0.0054,  0.0020,  0.0031],
        [-0.0072, -0.0294, -0.0059,  ..., -0.0172,  0.0265, -0.0088],
        [-0.0071, -0.0052, -0.0046,  ..., -0.0034,  0.0041,  0.0170],
        [ 0.0034, -0.0250,  0.0116,  ...,  0.0017,  0.0170, -0.0028],
        [-0.0014, -0.0011,  0.0008,  ...,  0.0194, -0.0047,  0.0077],
        [-0.0013, -0.0058, -0.0012,  ...,  0.0064,  0.0042,  0.0067]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>)
tensor([[-0.0155, -0.0176,  0.0065,  0.0066, -0.0064,  0.0072],
        [ 0.0016,  0.0006,  0.0096,  0.0106, -0.0077, -0.0028],
        [ 0.0021, -0.0110,  0.0061, -0.0168,  0.0117, -0.0080],
        ...,
        [-0.0069, -0.0072,  0.0137,  0.0089, -0.0098,  0.0040],
        [ 0.0044,  0.0092,  0.0011,  0.0074, -0.0039,  0.0122],
        [-0.0012, -0.0051,  0.0078, -0.0025, -0.0028,  0.0087]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([[ 0.0197,  0.0045,  0.0055,  ..., -0.0040,  0.0033,  0.0042],
        [ 0.0027,  0.0067,  0.0048,  ...,  0.0045,  0.0222,  0.0119],
        [-0.0072, -0.0131,  0.0125,  ..., -0.0033, -0.0010, -0.0027],
        [-0.0001, -0.0010,  0.0120,  ..., -0.0070,  0.0012, -0.0145],
        [-0.0032,  0.0028, -0.0014,  ..., -0.0014, -0.0018, -0.0173],
        [ 0.0067,  0.0114, -0.0028,  ..., -0.0135, -0.0040,  0.0071]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>)
tensor([[ 0.0174, -0.0054, -0.0016, -0.0025, -0.0051,  0.0004],
        [-0.0027,  0.0100,  0.0098,  0.0074,  0.0262, -0.0053],
        [-0.0005,  0.0040,  0.0046, -0.0170, -0.0032,  0.0190],
        ...,
        [-0.0047, -0.0168,  0.0060, -0.0158, -0.0019, -0.0168],
        [ 0.0026, -0.0073, -0.0182, -0.0083, -0.0129,  0.0095],
        [ 0.0100,  0.0093,  0.0178, -0.0056,  0.0012, -0.0063]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([[-0.0005,  0.0022, -0.0047,  ...,  0.0073,  0.0060,  0.0043],
        [-0.0098,  0.0067, -0.0084,  ..., -0.0038,  0.0014,  0.0098],
        [-0.0021,  0.0057,  0.0189,  ..., -0.0008,  0.0159, -0.0119],
        [ 0.0038, -0.0041,  0.0098,  ...,  0.0007,  0.0122,  0.0061],
        [ 0.0104, -0.0013,  0.0085,  ...,  0.0073, -0.0025, -0.0037],
        [-0.0012, -0.0006,  0.0062,  ...,  0.0059,  0.0147, -0.0073]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>)
tensor([[ 0.0099, -0.0020,  0.0017, -0.0149,  0.0033, -0.0123],
        [ 0.0047, -0.0006,  0.0031, -0.0065,  0.0018,  0.0168],
        [ 0.0055, -0.0314,  0.0093, -0.0052,  0.0008, -0.0109],
        ...,
        [-0.0009,  0.0048,  0.0091, -0.0042, -0.0060, -0.0165],
        [ 0.0116, -0.0167, -0.0021, -0.0140, -0.0014, -0.0216],
        [-0.0017, -0.0102, -0.0049,  0.0030,  0.0244, -0.0043]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([[-0.0062, -0.0125, -0.0126,  ..., -0.0167, -0.0103, -0.0006],
        [ 0.0015,  0.0021, -0.0225,  ...,  0.0090,  0.0092,  0.0100],
        [-0.0045, -0.0109, -0.0002,  ..., -0.0060, -0.0081, -0.0075],
        [ 0.0134,  0.0070,  0.0170,  ..., -0.0152, -0.0022, -0.0017],
        [ 0.0085, -0.0039,  0.0235,  ..., -0.0124,  0.0041,  0.0071],
        [ 0.0027, -0.0001,  0.0100,  ..., -0.0006,  0.0062, -0.0039]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>)
tensor([[ 1.3123e-02,  8.1024e-03, -6.9320e-05, -4.1461e-04,  4.6806e-03,
          7.2136e-03],
        [-1.2962e-02,  4.6463e-03, -9.2621e-03,  1.6800e-02, -7.9498e-03,
          3.3207e-03],
        [-1.5831e-03, -3.5191e-03, -8.7509e-03,  1.3512e-02, -2.0279e-02,
          9.6436e-03],
        ...,
        [ 8.6212e-03,  3.7785e-03,  2.8763e-02, -4.5433e-03, -1.4915e-02,
          1.0773e-02],
        [ 6.1913e-03, -1.6312e-02, -3.1400e-04, -3.2623e-02,  1.1196e-03,
         -1.5732e-02],
        [-1.6594e-03,  4.2343e-03, -9.5062e-03,  2.2113e-04, -6.6528e-03,
         -9.9106e-03]], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([[ 1.7441e-02,  7.7286e-03,  1.2894e-02,  ...,  9.9716e-03,
         -1.3245e-02, -1.0033e-02],
        [-1.5244e-02,  3.0403e-03,  2.7332e-03,  ...,  5.3024e-03,
         -2.4939e-04, -1.0483e-02],
        [-9.9719e-05, -2.9163e-03,  5.3101e-03,  ..., -7.8354e-03,
          1.5480e-02, -2.0111e-02],
        [ 1.1818e-02, -1.2444e-02,  6.9580e-03,  ...,  4.2953e-03,
          2.6520e-02,  1.4168e-02],
        [-1.9608e-02, -9.8495e-03,  6.0768e-03,  ...,  2.6073e-03,
          1.7181e-02,  8.6517e-03],
        [ 5.0011e-03, -5.3596e-03, -1.8250e-02,  ...,  7.8049e-03,
          7.4425e-03, -1.8768e-02]], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>)
tensor([[ 0.0100, -0.0062,  0.0072,  0.0127,  0.0035,  0.0020],
        [ 0.0066,  0.0002, -0.0004,  0.0017,  0.0055, -0.0036],
        [-0.0097,  0.0021,  0.0041,  0.0005,  0.0089, -0.0024],
        ...,
        [ 0.0087, -0.0083, -0.0055, -0.0008,  0.0013, -0.0047],
        [-0.0004, -0.0045,  0.0070, -0.0099,  0.0160,  0.0245],
        [-0.0154,  0.0039, -0.0073,  0.0047, -0.0183, -0.0130]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([[ 0.0035,  0.0150,  0.0066,  ..., -0.0096, -0.0141, -0.0074],
        [-0.0096,  0.0073, -0.0016,  ...,  0.0210,  0.0088,  0.0051],
        [ 0.0047,  0.0037, -0.0092,  ..., -0.0103,  0.0056, -0.0010],
        [ 0.0054,  0.0138, -0.0013,  ...,  0.0064, -0.0151, -0.0126],
        [-0.0007, -0.0009,  0.0113,  ...,  0.0013, -0.0084,  0.0134],
        [-0.0064, -0.0034, -0.0027,  ...,  0.0081, -0.0111,  0.0029]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>)
tensor([[-0.0065,  0.0237, -0.0075, -0.0031,  0.0022,  0.0024],
        [-0.0031,  0.0066,  0.0022,  0.0085,  0.0131, -0.0070],
        [ 0.0006,  0.0027,  0.0177, -0.0064, -0.0314, -0.0111],
        ...,
        [-0.0034, -0.0042, -0.0022, -0.0168,  0.0273, -0.0123],
        [-0.0044,  0.0044,  0.0188, -0.0007,  0.0078, -0.0062],
        [-0.0056,  0.0014,  0.0220,  0.0124,  0.0007,  0.0092]],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)
---------------------------------
root
 prompt_model (PromptModel)
    plm (Roberta)
       input_embedding (Embedding) weight:[38603520]
       position_embedding (Embedding) weight:[394752]
       token_type_embedding (Embedding) weight:[768]
       encoder (Encoder)
          layers (TransformerBlockList)
             0-11(CheckpointBlock)
                 self_att (SelfAttentionBlock)
                    layernorm_before_attention (LayerNorm) weight:[768] bias:[768]
                    self_attention (Attention)
                        project_q,project_k,project_v,attention_out(Linear) 
                           weight:[589824] bias:[768]
                 ffn (FFNBlock)
                     layernorm_before_ffn (LayerNorm) weight:[768] bias:[768]
                     ffn (FeedForward)
                         w_in (DenseACT)
                            w (Linear) weight:[2359296] bias:[3072]
                         w_out (Linear) weight:[2359296] bias:[768]
                         adapter (AdapterLayer)
                             modulelist (Sequential)
                                 down_proj (Linear) weight:[6, 768] bias:[6]
                                 up_proj (Linear) weight:[768, 6] bias:[768]
          output_layernorm (LayerNorm) weight:[768] bias:[768]
       lm_head (RoertaLMHead)
          dense (Linear) weight:[589824] bias:[768]
          layer_norm (LayerNorm) weight:[768] bias:[768]
          decoder (Linear) weight:[38603520] bias:[50265]
       pooler (RoertaPooler)
           dense (Linear) weight:[589824] bias:[768]
    template (MixedTemplate)
        raw_embedding (Embedding) weight:[38603520]
        soft_embedding (Embedding) weight:[6, 768]
 verbalizer (ManualVerbalizer) label_words_ids:[28, 1, 2] words_ids_mask:[28, 1, 2] 
    label_words_mask:[28, 1]

================ Epoch 0 ================
learning rate: 9.999631540162123e-05
Progress 0.07%, loss: 4.871312141418457, time 1.77s
Progress 1.03%, loss: 4.310363411903381, time 16.91s
Progress 2.06%, loss: 3.9931166768074036, time 17.85s
Progress 3.02%, loss: 3.793845490711491, time 17.55s
Progress 4.05%, loss: 3.6621180447665127, time 18.56s
Progress 5.01%, loss: 3.493228659910314, time 17.03s
Progress 30.07%, loss: 2.3883970040901032, time 501.06s
Progress 50.04%, loss: 2.1682059752572442, time 462.81s
Progress 75.02%, loss: 2.0307280188225105, time 569.86s
Epoch 0, average train loss: 1.9398441187318143, time 2201.12s
Epoch 0, average test loss: 1.5732217220699085
acc: 0.5412827128639882 
F1: 0.38690304033299683 
precision: 0.4854883812195522 
recall: 0.3659816217433899
lowest_loss updated

================ Epoch 1 ================
learning rate: 9.499631540162124e-05
Progress 0.07%, loss: 2.0941555500030518, time 1.74s
Progress 1.03%, loss: 1.6594535282679967, time 21.44s
Progress 2.06%, loss: 1.6186729243823461, time 23.51s
Progress 3.02%, loss: 1.5993471145629883, time 22.33s
Progress 4.05%, loss: 1.5998338482596657, time 23.86s
Progress 5.01%, loss: 1.6047336669529186, time 22.31s
Progress 30.07%, loss: 1.6289303705972784, time 576.53s
Progress 50.04%, loss: 1.6072007350085937, time 461.62s
Progress 75.02%, loss: 1.605907027812744, time 577.99s
Epoch 1, average train loss: 1.5923730652386772, time 2307.92s
Epoch 1, average test loss: 1.4711550526759203
acc: 0.5632141540729819 
F1: 0.41289320566147614 
precision: 0.5209200011015145 
recall: 0.38880728600165565
lowest_loss updated

================ Epoch 2 ================
learning rate: 8.999631540162124e-05
Progress 0.07%, loss: 1.5378292798995972, time 1.74s
Progress 1.03%, loss: 1.6067827599389213, time 22.1s
Progress 2.06%, loss: 1.6375458027635301, time 24.12s
Progress 3.02%, loss: 1.5949916926825918, time 21.35s
Progress 4.05%, loss: 1.5684644547375766, time 24.24s
Progress 5.01%, loss: 1.5581644475460052, time 21.91s
Progress 30.07%, loss: 1.5342765026817136, time 577.57s
Progress 50.04%, loss: 1.530904477228999, time 464.24s
Progress 75.02%, loss: 1.5263487201893962, time 578.32s
Epoch 2, average train loss: 1.520112385930783, time 2312.34s
Epoch 2, average test loss: 1.4302283599096186
acc: 0.5705860670844084 
F1: 0.4201608435518582 
precision: 0.532523974314553 
recall: 0.39296818963205554
lowest_loss updated

================ Epoch 3 ================
learning rate: 8.499631540162123e-05
Progress 0.07%, loss: 1.1847095489501953, time 1.8s
Progress 1.03%, loss: 1.3554341963359289, time 22.07s
Progress 2.06%, loss: 1.440353023154395, time 23.79s
Progress 3.02%, loss: 1.4757599452646768, time 21.65s
Progress 4.05%, loss: 1.474209557880055, time 24.33s
Progress 5.01%, loss: 1.4823958680910223, time 22.29s
Progress 30.07%, loss: 1.5043684030864752, time 578.38s
Progress 50.04%, loss: 1.5006682929831154, time 464.23s
Progress 75.02%, loss: 1.488066367117033, time 583.96s
Epoch 3, average train loss: 1.4850061297768078, time 2322.81s
Epoch 3, average test loss: 1.4097603443790885
acc: 0.5744563214154073 
F1: 0.44437396930500794 
precision: 0.5248348457738724 
recall: 0.42355032537926995
lowest_loss updated

================ Epoch 4 ================
learning rate: 7.999631540162123e-05
Progress 0.07%, loss: 1.1098830699920654, time 1.71s
Progress 1.03%, loss: 1.4572601658957345, time 22.52s
Progress 2.06%, loss: 1.475010437624795, time 23.9s
Progress 3.02%, loss: 1.4807928538903958, time 22.46s
Progress 4.05%, loss: 1.461180847341364, time 23.98s
Progress 5.01%, loss: 1.4549273147302515, time 22.05s
Progress 30.07%, loss: 1.4828906179058785, time 583.97s
Progress 50.04%, loss: 1.4687338982309615, time 465.18s
Progress 75.02%, loss: 1.4616095661414397, time 582.22s
Epoch 4, average train loss: 1.4597783245052935, time 2326.66s
Epoch 4, average test loss: 1.3889204712475047
acc: 0.57519351271655 
F1: 0.4381605274806711 
precision: 0.5270305715694967 
recall: 0.4118762807440082
lowest_loss updated

================ Epoch 5 ================
learning rate: 7.499631540162123e-05
Progress 0.07%, loss: 1.407193899154663, time 1.8s
Progress 1.03%, loss: 1.4282049238681793, time 22.16s
Progress 2.06%, loss: 1.4457322295222963, time 24.03s
Progress 3.02%, loss: 1.4644196455071612, time 22.26s
Progress 4.05%, loss: 1.4595346721735867, time 24.21s
Progress 5.01%, loss: 1.4896236964884926, time 22.13s
Progress 30.07%, loss: 1.4545740305500872, time 564.72s
Progress 50.04%, loss: 1.4475260774587144, time 461.26s
Progress 75.02%, loss: 1.4415945349250185, time 578.11s
Epoch 5, average train loss: 1.4354778381068651, time 2298.63s
Epoch 5, average test loss: 1.394243177946876
acc: 0.5737191301142647 
F1: 0.45783147937728735 
precision: 0.5203123628999047 
recall: 0.4456804116060732

================ Epoch 6 ================
learning rate: 6.999631540162123e-05
Progress 0.07%, loss: 1.3438341617584229, time 1.8s
Progress 1.03%, loss: 1.4371825882366724, time 21.93s
Progress 2.06%, loss: 1.3665428459644318, time 23.77s
Progress 3.02%, loss: 1.362397443957445, time 22.01s
Progress 4.05%, loss: 1.3723231705752286, time 24.33s
Progress 5.01%, loss: 1.380226556869114, time 21.97s
Progress 30.07%, loss: 1.4291015502576734, time 579.64s
Progress 50.04%, loss: 1.4299390986671503, time 456.42s
Progress 75.02%, loss: 1.4295277994836009, time 571.57s
Epoch 6, average train loss: 1.423663008713775, time 2298.2s
Epoch 6, average test loss: 1.3663354670300203
acc: 0.5818282344268337 
F1: 0.44753713643842535 
precision: 0.5360467632428296 
recall: 0.4200400007502733
lowest_loss updated

================ Epoch 7 ================
learning rate: 6.499631540162123e-05
Progress 0.07%, loss: 2.3443851470947266, time 1.67s
Progress 1.03%, loss: 1.4762637189456396, time 21.67s
Progress 2.06%, loss: 1.4055274384362357, time 23.66s
Progress 3.02%, loss: 1.403296493902439, time 22.42s
Progress 4.05%, loss: 1.4266426758332686, time 23.37s
Progress 5.01%, loss: 1.4066365273559795, time 21.98s
Progress 30.07%, loss: 1.3989050302143191, time 577.2s
Progress 50.04%, loss: 1.4023520418926965, time 460.02s
Progress 75.02%, loss: 1.4109228039654393, time 578.52s
Epoch 7, average train loss: 1.408420597619666, time 2307.21s
Epoch 7, average test loss: 1.363521829773398
acc: 0.5766678953188352 
F1: 0.44123367821371434 
precision: 0.5222415681638205 
recall: 0.4181765942427577
lowest_loss updated

================ Epoch 8 ================
learning rate: 5.999631540162123e-05
Progress 0.07%, loss: 1.617228388786316, time 1.8s
Progress 1.03%, loss: 1.3757175590310777, time 21.78s
Progress 2.06%, loss: 1.3437664955854416, time 23.86s
Progress 3.02%, loss: 1.351485460269742, time 22.07s
Progress 4.05%, loss: 1.344864628531716, time 23.81s
Progress 5.01%, loss: 1.3361920100801132, time 22.26s
Progress 30.07%, loss: 1.402638882690785, time 577.13s
Progress 50.04%, loss: 1.3929930105476211, time 459.67s
Progress 75.02%, loss: 1.4023201100250124, time 578.09s
Epoch 8, average train loss: 1.4004432919188818, time 2306.61s
Epoch 8, average test loss: 1.3557854834724874
acc: 0.5831183192038334 
F1: 0.4622817367867859 
precision: 0.5153055575308715 
recall: 0.44705037767206307
lowest_loss updated

================ Epoch 9 ================
learning rate: 5.499631540162123e-05
Progress 0.07%, loss: 1.503406047821045, time 1.81s
Progress 1.03%, loss: 1.476981256689344, time 22.07s
Progress 2.06%, loss: 1.3992227401052202, time 24.53s
Progress 3.02%, loss: 1.393750019189788, time 22.01s
Progress 4.05%, loss: 1.3846215876665982, time 24.09s
Progress 5.01%, loss: 1.3765746530364542, time 22.38s
Progress 30.07%, loss: 1.3913750021773226, time 578.32s
Progress 50.04%, loss: 1.3958388363314307, time 462.06s
Progress 75.02%, loss: 1.3860952356475276, time 574.08s
Epoch 9, average train loss: 1.3855368389131044, time 2298.9s
Epoch 9, average test loss: 1.3468792165026946
acc: 0.5812753409509768 
F1: 0.45950839250426956 
precision: 0.5137360652588232 
recall: 0.4385138104935145
lowest_loss updated

================ Epoch 10 ================
learning rate: 4.999631540162122e-05
Progress 0.07%, loss: 1.6735559701919556, time 1.79s
Progress 1.03%, loss: 1.3909004586083549, time 22.25s
Progress 2.06%, loss: 1.381768980196544, time 23.7s
Progress 3.02%, loss: 1.39740745904969, time 22.36s
Progress 4.05%, loss: 1.4096818295392124, time 22.5s
Progress 5.01%, loss: 1.4054469343493967, time 17.07s
Progress 30.07%, loss: 1.3781094029545784, time 576.74s
Progress 50.04%, loss: 1.375497760231962, time 462.05s
Progress 75.02%, loss: 1.3757772106669974, time 578.6s
Epoch 10, average train loss: 1.3768973028563187, time 2304.27s
Epoch 10, average test loss: 1.3445688990985647
acc: 0.5788794692222632 
F1: 0.46225621093209723 
precision: 0.5092727507649839 
recall: 0.44984538003093444
lowest_loss updated

================ Epoch 11 ================
learning rate: 4.499631540162122e-05
Progress 0.07%, loss: 1.5406708717346191, time 1.79s
Progress 1.03%, loss: 1.3575068712234497, time 22.4s
Progress 2.06%, loss: 1.3332944065332413, time 23.15s
Progress 3.02%, loss: 1.334844378436484, time 21.46s
Progress 4.05%, loss: 1.371471631526947, time 24.0s
Progress 5.01%, loss: 1.3596741215271109, time 21.94s
Progress 30.07%, loss: 1.3636962309771894, time 583.13s
Progress 50.04%, loss: 1.3621636996037478, time 461.16s
Progress 75.02%, loss: 1.3647602483308152, time 577.87s
Epoch 11, average train loss: 1.3655359066402377, time 2315.31s
Epoch 11, average test loss: 1.3343225209151997
acc: 0.584408403980833 
F1: 0.4669757343711249 
precision: 0.5215589260806756 
recall: 0.45067152838792285
lowest_loss updated

================ Epoch 12 ================
learning rate: 3.999631540162123e-05
Progress 0.07%, loss: 1.6798311471939087, time 1.83s
Progress 1.03%, loss: 1.4432304075786047, time 21.89s
Progress 2.06%, loss: 1.4281142098563058, time 22.94s
Progress 3.02%, loss: 1.399569553572957, time 22.68s
Progress 4.05%, loss: 1.4028807976029136, time 24.23s
Progress 5.01%, loss: 1.4147262003491907, time 22.46s
Progress 30.07%, loss: 1.3692444754289645, time 580.58s
Progress 50.04%, loss: 1.3621066755152942, time 461.57s
Progress 75.02%, loss: 1.3609754599499093, time 582.05s
Epoch 12, average train loss: 1.3602186357526982, time 2320.77s
Epoch 12, average test loss: 1.3423657809986789
acc: 0.5818282344268337 
F1: 0.4705263699367847 
precision: 0.5072035923526484 
recall: 0.46661682222271367

================ Epoch 13 ================
learning rate: 3.4996315401621224e-05
Progress 0.07%, loss: 1.0984885692596436, time 1.68s
Progress 1.03%, loss: 1.3708135528223855, time 22.79s
Progress 2.06%, loss: 1.335955594267164, time 24.04s
Progress 3.02%, loss: 1.3581556139922724, time 21.87s
Progress 4.05%, loss: 1.3610499056902798, time 23.62s
Progress 5.01%, loss: 1.3458586685797747, time 22.29s
Progress 30.07%, loss: 1.3557950684837266, time 581.99s
Progress 50.04%, loss: 1.3660917723652892, time 465.09s
Progress 75.02%, loss: 1.3663601537467454, time 580.28s
Epoch 13, average train loss: 1.3626269986283366, time 2322.04s
Epoch 13, average test loss: 1.3284024817102096
acc: 0.5840398083302617 
F1: 0.4684628748852355 
precision: 0.5170936828333773 
recall: 0.45459562766523715
lowest_loss updated

================ Epoch 14 ================
learning rate: 2.9996315401621225e-05
Progress 0.07%, loss: 0.9126893877983093, time 1.76s
Progress 1.03%, loss: 1.3058541544846125, time 22.02s
Progress 2.06%, loss: 1.3285085495029176, time 23.95s
Progress 3.02%, loss: 1.321787300633221, time 21.7s
Progress 4.05%, loss: 1.3115316856991162, time 24.04s
Progress 5.01%, loss: 1.3190959507928175, time 22.11s
Progress 30.07%, loss: 1.350957946011833, time 580.76s
Progress 50.04%, loss: 1.354436890278368, time 465.16s
Progress 75.02%, loss: 1.3535128252796669, time 571.95s
Epoch 14, average train loss: 1.348863484012615, time 2308.31s
Epoch 14, average test loss: 1.3316553070264705
acc: 0.5856984887578327 
F1: 0.4720586436258903 
precision: 0.5175912135633973 
recall: 0.46202985333215424

================ Epoch 15 ================
learning rate: 2.4996315401621222e-05
Progress 0.07%, loss: 1.2969605922698975, time 1.67s
Progress 1.03%, loss: 1.39896383030074, time 20.85s
Progress 2.06%, loss: 1.3991229342562812, time 24.21s
Progress 3.02%, loss: 1.3576878992522634, time 21.82s
Progress 4.05%, loss: 1.3405619848858226, time 24.58s
Progress 5.01%, loss: 1.320314003264203, time 21.95s
Progress 30.07%, loss: 1.346973482738523, time 581.53s
Progress 50.04%, loss: 1.3491873307501971, time 462.55s
Progress 75.02%, loss: 1.3442946924207722, time 578.83s
Epoch 15, average train loss: 1.3455775476327783, time 2316.95s
Epoch 15, average test loss: 1.3294106448397918
acc: 0.5869885735348322 
F1: 0.47074600762109275 
precision: 0.5188814844812437 
recall: 0.4601931766215504

================ Epoch 16 ================
learning rate: 1.9996315401621226e-05
Progress 0.07%, loss: 1.2796430587768555, time 1.72s
Progress 1.03%, loss: 1.241392799786159, time 22.19s
Progress 2.06%, loss: 1.3432545619351524, time 23.77s
Progress 3.02%, loss: 1.3692476124298283, time 22.04s
Progress 4.05%, loss: 1.3629654212431475, time 23.95s
Progress 5.01%, loss: 1.3725341803887312, time 22.39s
Progress 30.07%, loss: 1.3655582888453615, time 580.35s
Progress 50.04%, loss: 1.3469561830887211, time 463.37s
Progress 75.02%, loss: 1.3447144064313068, time 578.65s
Epoch 16, average train loss: 1.3464372017933501, time 2316.84s
Epoch 16, average test loss: 1.3197088157429415
acc: 0.5869885735348322 
F1: 0.4684139913256253 
precision: 0.5199779591016085 
recall: 0.4516705987539918
lowest_loss updated

================ Epoch 17 ================
learning rate: 1.4996315401621224e-05
Progress 0.07%, loss: 1.1263220310211182, time 1.97s
Progress 1.03%, loss: 1.4085114725998469, time 22.37s
Progress 2.06%, loss: 1.3373455298798425, time 23.89s
Progress 3.02%, loss: 1.3354452950198477, time 21.45s
Progress 4.05%, loss: 1.3414085518230092, time 23.66s
Progress 5.01%, loss: 1.350347885314156, time 22.2s
Progress 30.07%, loss: 1.3313975321019398, time 581.32s
Progress 50.04%, loss: 1.3400683471485864, time 464.36s
Progress 75.02%, loss: 1.340433414593661, time 580.94s
Epoch 17, average train loss: 1.343277390730583, time 2322.46s
Epoch 17, average test loss: 1.3237316531293533
acc: 0.58496129745669 
F1: 0.4698295286770411 
precision: 0.5157337817919128 
recall: 0.45821900814279465

================ Epoch 18 ================
learning rate: 9.996315401621223e-06
Progress 0.07%, loss: 1.4010541439056396, time 1.64s
Progress 1.03%, loss: 1.3086799894060408, time 22.08s
Progress 2.06%, loss: 1.2901549530880791, time 23.91s
Progress 3.02%, loss: 1.2930015092942773, time 22.65s
Progress 4.05%, loss: 1.3011964949694548, time 23.82s
Progress 5.01%, loss: 1.320902524625554, time 22.23s
Progress 30.07%, loss: 1.3566889698598898, time 579.64s
Progress 50.04%, loss: 1.34743108577335, time 461.82s
Progress 75.02%, loss: 1.3464160624210868, time 578.13s
Epoch 18, average train loss: 1.3421396649577817, time 2314.98s
Epoch 18, average test loss: 1.3235326384796815
acc: 0.585514190932547 
F1: 0.4685425682169623 
precision: 0.5177318819743425 
recall: 0.45504217026282934

================ Epoch 19 ================
learning rate: 4.9963154016212235e-06
Progress 0.07%, loss: 1.3314588069915771, time 1.78s
Progress 1.03%, loss: 1.255146290574755, time 22.66s
Progress 2.06%, loss: 1.272206608738218, time 23.6s
Progress 3.02%, loss: 1.303233041995909, time 22.18s
Progress 4.05%, loss: 1.3159441644495184, time 23.84s
Progress 5.01%, loss: 1.3190123526489033, time 22.36s
Progress 30.07%, loss: 1.3461954184606963, time 576.29s
Progress 50.04%, loss: 1.3392035164959477, time 464.1s
Progress 75.02%, loss: 1.340042791045719, time 582.81s
Epoch 19, average train loss: 1.3372287666349614, time 2312.61s
Epoch 19, average test loss: 1.3221830974606907
acc: 0.5834869148544047 
F1: 0.4668984052356838 
precision: 0.5135796805644981 
recall: 0.4534012210191341

================ inference on test dataset ================
acc: 0.5920398009950248 
F1: 0.4837629098255706 
precision: 0.5282680705689465 
recall: 0.47127779443266204
