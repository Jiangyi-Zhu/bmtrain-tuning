xlm 训练到epoch1（也就是4万多step） 报错gradient overflow

基础设置：Namespace(pattern='debug', source='bmtrain', task='go_emotions', model_class='xlm-roberta-base', language='English', soft_layer=5, adapter_bottleneck_dim=6, adapter_init_std=0.0, epochs=20, learning_rate=0.0001, batch_size=64, seed=78, dtype='float16', lr_scheduler='Linear', soft_template_load_path='None', adapter_load_path='None', checkpoint_save_path='/home/zhujiangyi/prompt_learning/checkpoints')
样本数量：train len: 43410 valid len: 5426 test len: 5427
分类数量：28

网络结构可视化：
root
├── prompt_model (PromptModel)
│   ├── plm (Roberta)
│   │   ├── input_embedding (Embedding) weight:[192001536]
│   │   ├── position_embedding (Embedding) weight:[394752]
│   │   ├── token_type_embedding (Embedding) weight:[768]
│   │   ├── encoder (Encoder)
│   │   │   ├── layers (TransformerBlockList)
│   │   │   │   └── 0-11(CheckpointBlock)
│   │   │   │       ├── self_att (SelfAttentionBlock)
│   │   │   │       │   ├── layernorm_before_attention (LayerNorm) weight:[768] bias:[768]
│   │   │   │       │   └── self_attention (Attention)
│   │   │   │       │       └── project_q,project_k,project_v,attention_out(Linear) weight:[589824]
│   │   │   │       │           bias:[768]
│   │   │   │       └── ffn (FFNBlock)
│   │   │   │           ├── layernorm_before_ffn (LayerNorm) weight:[768] bias:[768]
│   │   │   │           └── ffn (FeedForward)
│   │   │   │               ├── w_in (DenseACT)
│   │   │   │               │   └── w (Linear) weight:[2359296] bias:[3072]
│   │   │   │               ├── w_out (Linear) weight:[2359296] bias:[768]
│   │   │   │               └── adapter (AdapterLayer)
│   │   │   │                   └── modulelist (Sequential)
│   │   │   │                       ├── down_proj (Linear) weight:[6, 768] bias:[6]
│   │   │   │                       └── up_proj (Linear) weight:[768, 6] bias:[768]
│   │   │   └── output_layernorm (LayerNorm) weight:[768] bias:[768]
│   │   ├── lm_head (RoertaLMHead)
│   │   │   ├── dense (Linear) weight:[589824] bias:[768]
│   │   │   ├── layer_norm (LayerNorm) weight:[768] bias:[768]
│   │   │   └── decoder (Linear) weight:[192001536] bias:[250002]
│   │   └── pooler (RoertaPooler)
│   │       └── dense (Linear) weight:[589824] bias:[768]
│   └── template (MixedTemplate)
│       ├── raw_embedding (Embedding) weight:[192001536]
│       └── soft_embedding (Embedding) weight:[6, 768]
└── verbalizer (ManualVerbalizer) label_words_ids:[28, 1, 5] words_ids_mask:[28, 1, 5] 
    label_words_mask:[28, 1]

网络结构细节（size， 数据类型，require_grad）
prompt_model.plm.input_embedding.weight 
     torch.Size([192001536]) torch.float16     require_grad: False
prompt_model.plm.position_embedding.weight 
     torch.Size([394752]) torch.float16     require_grad: False
prompt_model.plm.token_type_embedding.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.self_att.layernorm_before_attention.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.self_att.layernorm_before_attention.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.self_att.self_attention.project_q.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.self_att.self_attention.project_q.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.self_att.self_attention.project_k.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.self_att.self_attention.project_k.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.self_att.self_attention.project_v.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.self_att.self_attention.project_v.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.self_att.self_attention.attention_out.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.self_att.self_attention.attention_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.ffn.layernorm_before_ffn.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.ffn.layernorm_before_ffn.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.ffn.ffn.w_in.w.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.ffn.ffn.w_in.w.bias 
     torch.Size([3072]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.ffn.ffn.w_out.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.ffn.ffn.w_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.0.ffn.ffn.adapter.modulelist.down_proj.weight 
     torch.Size([6, 768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.0.ffn.ffn.adapter.modulelist.down_proj.bias 
     torch.Size([6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.0.ffn.ffn.adapter.modulelist.up_proj.weight 
     torch.Size([768, 6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.0.ffn.ffn.adapter.modulelist.up_proj.bias 
     torch.Size([768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.1.self_att.layernorm_before_attention.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.1.self_att.layernorm_before_attention.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.1.self_att.self_attention.project_q.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.1.self_att.self_attention.project_q.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.1.self_att.self_attention.project_k.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.1.self_att.self_attention.project_k.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.1.self_att.self_attention.project_v.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.1.self_att.self_attention.project_v.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.1.self_att.self_attention.attention_out.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.1.self_att.self_attention.attention_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.1.ffn.layernorm_before_ffn.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.1.ffn.layernorm_before_ffn.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.1.ffn.ffn.w_in.w.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.1.ffn.ffn.w_in.w.bias 
     torch.Size([3072]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.1.ffn.ffn.w_out.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.1.ffn.ffn.w_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.1.ffn.ffn.adapter.modulelist.down_proj.weight 
     torch.Size([6, 768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.1.ffn.ffn.adapter.modulelist.down_proj.bias 
     torch.Size([6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.1.ffn.ffn.adapter.modulelist.up_proj.weight 
     torch.Size([768, 6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.1.ffn.ffn.adapter.modulelist.up_proj.bias 
     torch.Size([768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.2.self_att.layernorm_before_attention.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.2.self_att.layernorm_before_attention.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.2.self_att.self_attention.project_q.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.2.self_att.self_attention.project_q.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.2.self_att.self_attention.project_k.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.2.self_att.self_attention.project_k.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.2.self_att.self_attention.project_v.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.2.self_att.self_attention.project_v.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.2.self_att.self_attention.attention_out.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.2.self_att.self_attention.attention_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.2.ffn.layernorm_before_ffn.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.2.ffn.layernorm_before_ffn.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.2.ffn.ffn.w_in.w.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.2.ffn.ffn.w_in.w.bias 
     torch.Size([3072]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.2.ffn.ffn.w_out.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.2.ffn.ffn.w_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.2.ffn.ffn.adapter.modulelist.down_proj.weight 
     torch.Size([6, 768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.2.ffn.ffn.adapter.modulelist.down_proj.bias 
     torch.Size([6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.2.ffn.ffn.adapter.modulelist.up_proj.weight 
     torch.Size([768, 6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.2.ffn.ffn.adapter.modulelist.up_proj.bias 
     torch.Size([768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.3.self_att.layernorm_before_attention.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.3.self_att.layernorm_before_attention.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.3.self_att.self_attention.project_q.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.3.self_att.self_attention.project_q.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.3.self_att.self_attention.project_k.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.3.self_att.self_attention.project_k.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.3.self_att.self_attention.project_v.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.3.self_att.self_attention.project_v.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.3.self_att.self_attention.attention_out.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.3.self_att.self_attention.attention_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.3.ffn.layernorm_before_ffn.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.3.ffn.layernorm_before_ffn.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.3.ffn.ffn.w_in.w.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.3.ffn.ffn.w_in.w.bias 
     torch.Size([3072]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.3.ffn.ffn.w_out.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.3.ffn.ffn.w_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.3.ffn.ffn.adapter.modulelist.down_proj.weight 
     torch.Size([6, 768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.3.ffn.ffn.adapter.modulelist.down_proj.bias 
     torch.Size([6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.3.ffn.ffn.adapter.modulelist.up_proj.weight 
     torch.Size([768, 6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.3.ffn.ffn.adapter.modulelist.up_proj.bias 
     torch.Size([768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.4.self_att.layernorm_before_attention.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.4.self_att.layernorm_before_attention.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.4.self_att.self_attention.project_q.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.4.self_att.self_attention.project_q.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.4.self_att.self_attention.project_k.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.4.self_att.self_attention.project_k.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.4.self_att.self_attention.project_v.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.4.self_att.self_attention.project_v.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.4.self_att.self_attention.attention_out.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.4.self_att.self_attention.attention_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.4.ffn.layernorm_before_ffn.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.4.ffn.layernorm_before_ffn.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.4.ffn.ffn.w_in.w.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.4.ffn.ffn.w_in.w.bias 
     torch.Size([3072]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.4.ffn.ffn.w_out.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.4.ffn.ffn.w_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.4.ffn.ffn.adapter.modulelist.down_proj.weight 
     torch.Size([6, 768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.4.ffn.ffn.adapter.modulelist.down_proj.bias 
     torch.Size([6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.4.ffn.ffn.adapter.modulelist.up_proj.weight 
     torch.Size([768, 6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.4.ffn.ffn.adapter.modulelist.up_proj.bias 
     torch.Size([768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.5.self_att.layernorm_before_attention.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.5.self_att.layernorm_before_attention.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.5.self_att.self_attention.project_q.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.5.self_att.self_attention.project_q.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.5.self_att.self_attention.project_k.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.5.self_att.self_attention.project_k.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.5.self_att.self_attention.project_v.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.5.self_att.self_attention.project_v.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.5.self_att.self_attention.attention_out.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.5.self_att.self_attention.attention_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.5.ffn.layernorm_before_ffn.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.5.ffn.layernorm_before_ffn.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.5.ffn.ffn.w_in.w.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.5.ffn.ffn.w_in.w.bias 
     torch.Size([3072]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.5.ffn.ffn.w_out.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.5.ffn.ffn.w_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.5.ffn.ffn.adapter.modulelist.down_proj.weight 
     torch.Size([6, 768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.5.ffn.ffn.adapter.modulelist.down_proj.bias 
     torch.Size([6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.5.ffn.ffn.adapter.modulelist.up_proj.weight 
     torch.Size([768, 6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.5.ffn.ffn.adapter.modulelist.up_proj.bias 
     torch.Size([768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.6.self_att.layernorm_before_attention.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.6.self_att.layernorm_before_attention.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.6.self_att.self_attention.project_q.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.6.self_att.self_attention.project_q.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.6.self_att.self_attention.project_k.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.6.self_att.self_attention.project_k.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.6.self_att.self_attention.project_v.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.6.self_att.self_attention.project_v.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.6.self_att.self_attention.attention_out.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.6.self_att.self_attention.attention_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.6.ffn.layernorm_before_ffn.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.6.ffn.layernorm_before_ffn.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.6.ffn.ffn.w_in.w.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.6.ffn.ffn.w_in.w.bias 
     torch.Size([3072]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.6.ffn.ffn.w_out.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.6.ffn.ffn.w_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.6.ffn.ffn.adapter.modulelist.down_proj.weight 
     torch.Size([6, 768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.6.ffn.ffn.adapter.modulelist.down_proj.bias 
     torch.Size([6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.6.ffn.ffn.adapter.modulelist.up_proj.weight 
     torch.Size([768, 6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.6.ffn.ffn.adapter.modulelist.up_proj.bias 
     torch.Size([768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.7.self_att.layernorm_before_attention.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.7.self_att.layernorm_before_attention.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.7.self_att.self_attention.project_q.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.7.self_att.self_attention.project_q.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.7.self_att.self_attention.project_k.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.7.self_att.self_attention.project_k.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.7.self_att.self_attention.project_v.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.7.self_att.self_attention.project_v.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.7.self_att.self_attention.attention_out.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.7.self_att.self_attention.attention_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.7.ffn.layernorm_before_ffn.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.7.ffn.layernorm_before_ffn.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.7.ffn.ffn.w_in.w.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.7.ffn.ffn.w_in.w.bias 
     torch.Size([3072]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.7.ffn.ffn.w_out.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.7.ffn.ffn.w_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.7.ffn.ffn.adapter.modulelist.down_proj.weight 
     torch.Size([6, 768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.7.ffn.ffn.adapter.modulelist.down_proj.bias 
     torch.Size([6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.7.ffn.ffn.adapter.modulelist.up_proj.weight 
     torch.Size([768, 6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.7.ffn.ffn.adapter.modulelist.up_proj.bias 
     torch.Size([768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.8.self_att.layernorm_before_attention.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.8.self_att.layernorm_before_attention.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.8.self_att.self_attention.project_q.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.8.self_att.self_attention.project_q.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.8.self_att.self_attention.project_k.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.8.self_att.self_attention.project_k.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.8.self_att.self_attention.project_v.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.8.self_att.self_attention.project_v.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.8.self_att.self_attention.attention_out.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.8.self_att.self_attention.attention_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.8.ffn.layernorm_before_ffn.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.8.ffn.layernorm_before_ffn.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.8.ffn.ffn.w_in.w.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.8.ffn.ffn.w_in.w.bias 
     torch.Size([3072]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.8.ffn.ffn.w_out.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.8.ffn.ffn.w_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.8.ffn.ffn.adapter.modulelist.down_proj.weight 
     torch.Size([6, 768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.8.ffn.ffn.adapter.modulelist.down_proj.bias 
     torch.Size([6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.8.ffn.ffn.adapter.modulelist.up_proj.weight 
     torch.Size([768, 6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.8.ffn.ffn.adapter.modulelist.up_proj.bias 
     torch.Size([768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.9.self_att.layernorm_before_attention.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.9.self_att.layernorm_before_attention.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.9.self_att.self_attention.project_q.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.9.self_att.self_attention.project_q.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.9.self_att.self_attention.project_k.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.9.self_att.self_attention.project_k.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.9.self_att.self_attention.project_v.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.9.self_att.self_attention.project_v.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.9.self_att.self_attention.attention_out.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.9.self_att.self_attention.attention_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.9.ffn.layernorm_before_ffn.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.9.ffn.layernorm_before_ffn.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.9.ffn.ffn.w_in.w.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.9.ffn.ffn.w_in.w.bias 
     torch.Size([3072]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.9.ffn.ffn.w_out.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.9.ffn.ffn.w_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.9.ffn.ffn.adapter.modulelist.down_proj.weight 
     torch.Size([6, 768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.9.ffn.ffn.adapter.modulelist.down_proj.bias 
     torch.Size([6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.9.ffn.ffn.adapter.modulelist.up_proj.weight 
     torch.Size([768, 6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.9.ffn.ffn.adapter.modulelist.up_proj.bias 
     torch.Size([768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.10.self_att.layernorm_before_attention.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.10.self_att.layernorm_before_attention.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.10.self_att.self_attention.project_q.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.10.self_att.self_attention.project_q.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.10.self_att.self_attention.project_k.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.10.self_att.self_attention.project_k.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.10.self_att.self_attention.project_v.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.10.self_att.self_attention.project_v.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.10.self_att.self_attention.attention_out.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.10.self_att.self_attention.attention_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.10.ffn.layernorm_before_ffn.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.10.ffn.layernorm_before_ffn.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.10.ffn.ffn.w_in.w.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.10.ffn.ffn.w_in.w.bias 
     torch.Size([3072]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.10.ffn.ffn.w_out.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.10.ffn.ffn.w_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.10.ffn.ffn.adapter.modulelist.down_proj.weight 
     torch.Size([6, 768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.10.ffn.ffn.adapter.modulelist.down_proj.bias 
     torch.Size([6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.10.ffn.ffn.adapter.modulelist.up_proj.weight 
     torch.Size([768, 6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.10.ffn.ffn.adapter.modulelist.up_proj.bias 
     torch.Size([768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.11.self_att.layernorm_before_attention.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.11.self_att.layernorm_before_attention.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.11.self_att.self_attention.project_q.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.11.self_att.self_attention.project_q.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.11.self_att.self_attention.project_k.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.11.self_att.self_attention.project_k.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.11.self_att.self_attention.project_v.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.11.self_att.self_attention.project_v.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.11.self_att.self_attention.attention_out.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.11.self_att.self_attention.attention_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.11.ffn.layernorm_before_ffn.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.11.ffn.layernorm_before_ffn.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.11.ffn.ffn.w_in.w.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.11.ffn.ffn.w_in.w.bias 
     torch.Size([3072]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.11.ffn.ffn.w_out.weight 
     torch.Size([2359296]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.11.ffn.ffn.w_out.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.layers.11.ffn.ffn.adapter.modulelist.down_proj.weight 
     torch.Size([6, 768]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.11.ffn.ffn.adapter.modulelist.down_proj.bias 
     torch.Size([6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.11.ffn.ffn.adapter.modulelist.up_proj.weight 
     torch.Size([768, 6]) torch.float16     require_grad: True
prompt_model.plm.encoder.layers.11.ffn.ffn.adapter.modulelist.up_proj.bias 
     torch.Size([768]) torch.float16     require_grad: True
prompt_model.plm.encoder.output_layernorm.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.encoder.output_layernorm.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.lm_head.dense.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.lm_head.dense.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.lm_head.layer_norm.weight 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.lm_head.layer_norm.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.plm.lm_head.decoder.weight 
     torch.Size([192001536]) torch.float16     require_grad: False
prompt_model.plm.lm_head.decoder.bias 
     torch.Size([250002]) torch.float16     require_grad: False
prompt_model.plm.pooler.dense.weight 
     torch.Size([589824]) torch.float16     require_grad: False
prompt_model.plm.pooler.dense.bias 
     torch.Size([768]) torch.float16     require_grad: False
prompt_model.template.soft_embedding.weight 
     torch.Size([6, 768]) torch.float16     require_grad: True
verbalizer.label_words_ids 
     torch.Size([28, 1, 5]) torch.int64     require_grad: False
verbalizer.words_ids_mask 
     torch.Size([28, 1, 5]) torch.int64     require_grad: False
verbalizer.label_words_mask 
     torch.Size([28, 1]) torch.int64     require_grad: False

================ Epoch 0 ================
learning rate: 9.999263622974964e-05
loss: tensor(8.6740, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.2250e+01, -2.9739e-02, -8.9375e+00,  ..., -5.9062e+00,
         -7.9688e+00, -1.2180e+01],
        [-1.4688e+01, -3.4237e-03, -1.0781e+01,  ..., -7.7852e+00,
         -8.7812e+00, -1.0656e+01],
        [-1.2703e+01, -6.8588e-03, -9.4453e+00,  ..., -6.3516e+00,
         -7.7578e+00, -9.9141e+00],
        ...,
        [-1.3688e+01, -1.2283e-02, -8.2344e+00,  ..., -5.5742e+00,
         -6.1055e+00, -9.3281e+00],
        [-1.4070e+01, -8.8272e-03, -1.0320e+01,  ..., -6.1953e+00,
         -7.2266e+00, -1.0039e+01],
        [-1.3742e+01, -7.8430e-03, -9.4766e+00,  ..., -6.2578e+00,
         -7.7578e+00, -9.9141e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 27, 17,  0, 27,  7, 27,  4, 26,  6, 27, 20,  1, 18, 27, 25, 26, 27,
        10, 10, 10, 27, 18, 15,  1, 27,  3, 14,  1,  6, 27, 10,  7, 27, 27,  7,
        10, 27,  1, 27, 27,  2,  9,  4, 27, 27,  8, 27, 27,  0, 27, 17, 27, 10,
        27,  1, 14, 27,  7, 27, 14,  3, 27, 14], device='cuda:0')
step: 0
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2361, -0.6104,  ...,  0.4761, -0.0828,  1.5078],
        [ 1.2529, -1.8877, -0.2063,  ..., -2.3828,  0.6045, -0.8257],
        [ 1.9170, -0.2222,  0.7964,  ..., -1.9062, -1.3418, -0.5933],
        [-0.4075, -0.3035, -1.7168,  ..., -0.6543,  0.6509,  1.2676],
        [-1.1670,  2.2246, -0.0941,  ...,  0.6592,  0.7085, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0011,  0.0024,  0.0052,  ...,  0.0039, -0.0004, -0.0017],
        [ 0.0012,  0.0040, -0.0059,  ...,  0.0003,  0.0198,  0.0019],
        [ 0.0099, -0.0084, -0.0028,  ..., -0.0072,  0.0018,  0.0081],
        [ 0.0054, -0.0058, -0.0043,  ...,  0.0060,  0.0038, -0.0008],
        [ 0.0015, -0.0056,  0.0052,  ...,  0.0017,  0.0008,  0.0013]],
       device='cuda:0', dtype=torch.float16)
Progress 0.15%, loss: 8.674047470092773, time 0.98s
loss: tensor(8.5808, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.3805e+01, -7.3509e-03, -9.4453e+00,  ..., -6.4453e+00,
         -7.8203e+00, -9.5391e+00],
        [-1.3805e+01, -8.3389e-03, -8.2891e+00,  ..., -5.9453e+00,
         -8.1016e+00, -1.0414e+01],
        [-1.1359e+01, -1.5251e-02, -9.0469e+00,  ..., -6.0469e+00,
         -6.5781e+00, -9.5781e+00],
        ...,
        [-1.4070e+01, -5.8784e-03, -9.9766e+00,  ..., -7.5352e+00,
         -8.3828e+00, -8.8516e+00],
        [-1.4438e+01, -5.8784e-03, -1.0508e+01,  ..., -6.4766e+00,
         -9.3516e+00, -1.0789e+01],
        [-1.1797e+01, -1.6739e-02, -9.2031e+00,  ..., -5.8281e+00,
         -6.8594e+00, -1.1547e+01]], device='cuda:0', dtype=torch.float16,
       grad_fn=<DivBackward0>)
===============label===============
tensor([18, 13,  0,  0, 22, 17, 27, 27,  0, 25,  1, 27,  5, 15,  6, 27, 20,  1,
        15, 27, 15,  1, 20,  6, 18, 27,  2, 25, 27,  2,  6,  4,  0, 27, 10, 15,
         7, 27, 15, 27,  0, 25, 20, 27,  1, 17,  5, 26,  0, 15, 18, 27, 27,  0,
         0, 15,  0,  3,  3,  3, 27, 27, 24, 17], device='cuda:0')
step: 1
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2362, -0.6104,  ...,  0.4761, -0.0827,  1.5078],
        [ 1.2529, -1.8877, -0.2062,  ..., -2.3828,  0.6045, -0.8257],
        [ 1.9170, -0.2220,  0.7964,  ..., -1.9062, -1.3418, -0.5933],
        [-0.4075, -0.3035, -1.7168,  ..., -0.6543,  0.6509,  1.2676],
        [-1.1670,  2.2246, -0.0942,  ...,  0.6592,  0.7085, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.6146e-03,  2.3438e-02,  1.6632e-03,  ..., -2.3148e-02,
          2.8687e-03, -6.0844e-03],
        [ 6.1333e-05,  6.2027e-03, -1.0933e-02,  ..., -2.3346e-02,
          6.3782e-03,  5.8212e-03],
        [ 3.5534e-03, -7.3776e-03, -4.1618e-03,  ...,  3.1948e-03,
          5.8098e-03,  4.1008e-04],
        [ 3.7670e-03, -6.2332e-03, -3.2640e-04,  ...,  4.7913e-03,
          7.9346e-04, -1.9855e-03],
        [ 1.2760e-03, -5.0049e-03,  2.5635e-03,  ...,  1.5812e-03,
         -1.0195e-03,  5.2719e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(8.5658, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.3023e+01, -1.1292e-02, -9.2578e+00,  ..., -5.7305e+00,
         -6.2930e+00, -8.8828e+00],
        [-1.5023e+01, -1.4658e-03, -1.0500e+01,  ..., -9.8438e+00,
         -9.4688e+00, -1.1344e+01],
        [-1.4148e+01, -2.8732e-02, -8.8125e+00,  ..., -4.3711e+00,
         -7.6211e+00, -1.1094e+01],
        ...,
        [-1.1766e+01, -1.8723e-02, -8.6094e+00,  ..., -5.4258e+00,
         -6.8633e+00, -1.0828e+01],
        [-1.3461e+01, -6.8588e-03, -9.2266e+00,  ..., -6.3516e+00,
         -7.5078e+00, -9.0703e+00],
        [-1.0531e+01, -1.2463e-01, -9.2812e+00,  ..., -2.9375e+00,
         -5.4062e+00, -8.7188e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0, 13, 10, 25, 15, 22,  4,  5,  3, 10,  1, 17, 18,  9,  7, 27, 15,
        27,  7,  4, 17, 22, 27, 24, 25, 18, 20, 10, 18, 23, 27, 27, 27, 18, 20,
         7, 27, 27,  3, 20, 27,  4, 27,  4, 19, 14, 27, 27, 27,  4, 17, 24, 27,
        27, 13, 18, 25, 27,  1,  1, 26, 10, 15], device='cuda:0')
step: 2
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2362, -0.6104,  ...,  0.4761, -0.0828,  1.5078],
        [ 1.2529, -1.8877, -0.2061,  ..., -2.3828,  0.6045, -0.8257],
        [ 1.9170, -0.2219,  0.7964,  ..., -1.9062, -1.3418, -0.5933],
        [-0.4077, -0.3032, -1.7168,  ..., -0.6543,  0.6509,  1.2676],
        [-1.1670,  2.2246, -0.0942,  ...,  0.6592,  0.7085, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.9057e-03,  1.1421e-02,  5.0163e-03,  ..., -8.2550e-03,
          8.2932e-03, -8.3008e-03],
        [-2.3651e-03,  3.1185e-04, -1.3664e-02,  ..., -9.4376e-03,
          2.9099e-02,  1.7197e-02],
        [ 7.0989e-05, -7.0229e-03, -5.4741e-03,  ...,  3.8853e-03,
          6.1607e-03,  2.8610e-04],
        [ 7.9117e-03, -2.3804e-03, -8.8263e-04,  ...,  4.2267e-03,
         -7.4768e-04, -2.9526e-03],
        [-1.9407e-04, -7.1182e-03,  7.3957e-04,  ...,  4.4098e-03,
         -1.9045e-03,  2.7599e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(8.9039, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.1820e+01, -1.1292e-02, -8.7578e+00,  ..., -5.9805e+00,
         -6.3867e+00, -8.7266e+00],
        [-1.4070e+01, -2.4452e-03, -9.9375e+00,  ..., -7.4414e+00,
         -8.9375e+00, -9.8125e+00],
        [-1.0742e+01, -5.3650e-02, -8.0234e+00,  ..., -5.9297e+00,
         -5.8984e+00, -8.6797e+00],
        ...,
        [-1.0781e+01, -6.1951e-02, -7.9688e+00,  ..., -4.9062e+00,
         -4.3438e+00, -7.5625e+00],
        [-1.2047e+01, -2.3712e-02, -1.0242e+01,  ..., -4.7109e+00,
         -6.2109e+00, -9.3359e+00],
        [-1.2578e+01, -5.8784e-03, -9.4141e+00,  ..., -6.0391e+00,
         -7.5352e+00, -9.7578e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  0, 15, 10, 22,  4, 24,  1, 11,  4,  0, 24, 27,  1, 17, 17,  8,
        15, 27, 17, 18,  0, 27, 27, 27,  0, 20, 26, 22,  3, 17, 18, 15, 27, 15,
        27,  9,  4,  8, 20, 27,  0,  9, 17, 27,  6, 27, 27,  8,  1, 13, 15, 27,
        27, 15, 27, 10, 27, 27, 27, 22, 18, 10], device='cuda:0')
step: 3
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2363, -0.6108,  ...,  0.4761, -0.0829,  1.5078],
        [ 1.2529, -1.8877, -0.2061,  ..., -2.3828,  0.6040, -0.8262],
        [ 1.9170, -0.2219,  0.7969,  ..., -1.9062, -1.3418, -0.5933],
        [-0.4077, -0.3032, -1.7168,  ..., -0.6548,  0.6509,  1.2676],
        [-1.1670,  2.2246, -0.0943,  ...,  0.6587,  0.7085, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0046,  0.0456,  0.0088,  ..., -0.0194,  0.0100, -0.0035],
        [ 0.0012, -0.0033,  0.0006,  ...,  0.0258,  0.0208, -0.0110],
        [ 0.0052, -0.0112, -0.0054,  ..., -0.0003,  0.0045, -0.0022],
        [ 0.0056, -0.0041,  0.0008,  ...,  0.0067,  0.0044, -0.0004],
        [ 0.0020, -0.0033,  0.0003,  ...,  0.0040,  0.0007,  0.0009]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(8.1727, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-13.5469,  -0.0333,  -7.8789,  ...,  -4.7188,  -6.2227,  -9.0938],
        [-12.7266,  -0.1028,  -9.0391,  ...,  -3.7891,  -5.7578, -11.4453],
        [-12.8281,  -0.0222,  -8.3359,  ...,  -6.0234,  -6.5859,  -8.2422],
        ...,
        [-12.5078,  -0.0409,  -7.8828,  ...,  -5.7578,  -5.4453,  -7.0391],
        [-12.8984,  -0.0143,  -8.7656,  ...,  -6.0156,  -6.0156,  -8.9844],
        [ -9.2812,  -0.1578,  -8.2812,  ...,  -3.0957,  -4.5938,  -8.9688]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 18, 27, 27,  1,  4, 27, 14, 20, 15,  1, 17, 27, 15, 27,  4,  0,  3,
        27,  2, 25, 27, 20, 27, 26, 20, 27,  7, 27, 10, 18, 27, 16, 27, 14,  4,
        27,  9, 27,  1, 27,  9, 27, 20, 27,  1,  2, 14, 27, 20, 15, 27,  5, 27,
        27,  6, 15,  1,  0,  5, 10, 27, 27, 15], device='cuda:0')
step: 4
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2365, -0.6108,  ...,  0.4761, -0.0829,  1.5078],
        [ 1.2529, -1.8877, -0.2059,  ..., -2.3828,  0.6040, -0.8262],
        [ 1.9170, -0.2218,  0.7969,  ..., -1.9062, -1.3418, -0.5938],
        [-0.4080, -0.3030, -1.7168,  ..., -0.6548,  0.6504,  1.2676],
        [-1.1670,  2.2246, -0.0944,  ...,  0.6587,  0.7085, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0098, -0.0170,  0.0066,  ...,  0.0108,  0.0039,  0.0212],
        [-0.0045,  0.0242,  0.0074,  ..., -0.0565, -0.0437,  0.0089],
        [ 0.0049, -0.0118, -0.0041,  ...,  0.0009,  0.0037, -0.0011],
        [ 0.0035, -0.0069, -0.0112,  ...,  0.0075,  0.0054, -0.0086],
        [-0.0020, -0.0075,  0.0045,  ..., -0.0020, -0.0003,  0.0047]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(8.6210, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.3930e+01, -1.0803e-02, -9.9766e+00,  ..., -5.3867e+00,
         -8.0391e+00, -9.8516e+00],
        [-1.3078e+01, -1.1292e-02, -9.0078e+00,  ..., -6.7305e+00,
         -6.9180e+00, -9.4141e+00],
        [-1.3375e+01, -2.0218e-02, -8.3359e+00,  ..., -5.7070e+00,
         -7.1758e+00, -8.4297e+00],
        ...,
        [-1.1508e+01, -7.4463e-02, -8.1094e+00,  ..., -3.2930e+00,
         -5.6992e+00, -9.3906e+00],
        [-1.4438e+01, -4.8943e-03, -1.0570e+01,  ..., -7.5977e+00,
         -7.5977e+00, -9.6641e+00],
        [-1.2328e+01, -2.0721e-02, -8.0547e+00,  ..., -6.1758e+00,
         -6.7383e+00, -7.5508e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<DivBackward0>)
===============label===============
tensor([25, 22,  2, 13, 15,  7,  3,  4,  9, 27, 27,  7, 27, 27, 10, 15,  3, 27,
         6,  4, 15, 26, 27, 27, 18,  1, 27,  3, 10, 25, 15,  4, 27,  5, 27,  2,
        18,  9, 18,  9, 27,  6, 27,  0, 27, 22, 27,  0, 27, 20,  5, 27, 18, 25,
        27,  0, 10, 21,  0, 11, 27, 27, 10, 27], device='cuda:0')
step: 5
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2365, -0.6108,  ...,  0.4763, -0.0830,  1.5078],
        [ 1.2529, -1.8877, -0.2059,  ..., -2.3828,  0.6040, -0.8262],
        [ 1.9170, -0.2217,  0.7969,  ..., -1.9062, -1.3418, -0.5938],
        [-0.4080, -0.3030, -1.7168,  ..., -0.6548,  0.6504,  1.2676],
        [-1.1670,  2.2246, -0.0945,  ...,  0.6587,  0.7085, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0016, -0.0001,  0.0045,  ...,  0.0056,  0.0018,  0.0031],
        [-0.0011, -0.0020,  0.0015,  ...,  0.0133,  0.0064, -0.0121],
        [ 0.0011, -0.0091,  0.0021,  ...,  0.0110, -0.0013, -0.0027],
        [ 0.0059, -0.0068, -0.0021,  ...,  0.0088,  0.0044, -0.0018],
        [-0.0004, -0.0034,  0.0046,  ...,  0.0013,  0.0019,  0.0033]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(8.8048, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.5250e+01, -6.8588e-03, -1.0008e+01,  ..., -5.6641e+00,
         -8.5078e+00, -1.0164e+01],
        [-1.3742e+01, -5.3864e-03, -1.0320e+01,  ..., -7.0039e+00,
         -7.4414e+00, -8.8828e+00],
        [-1.3172e+01, -2.5711e-02, -9.3359e+00,  ..., -4.1211e+00,
         -7.6836e+00, -9.1172e+00],
        ...,
        [-1.3461e+01, -9.8114e-03, -9.1953e+00,  ..., -6.1641e+00,
         -7.7930e+00, -9.3203e+00],
        [-1.4844e+01, -3.4237e-03, -1.0539e+01,  ..., -6.2227e+00,
         -9.5703e+00, -9.7266e+00],
        [-1.2328e+01, -1.3763e-02, -1.0109e+01,  ..., -4.9219e+00,
         -7.1719e+00, -9.2344e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  3,  5, 27, 18, 18, 27,  9,  9, 27,  3, 15,  6,  0, 27, 11,  2,
         2,  0,  8, 27, 27,  2,  1,  4, 27, 13, 27, 27, 27, 20, 27, 27,  4, 27,
        27, 27, 27, 27, 27, 27,  3, 10, 14, 26, 22, 27, 17, 27, 27,  2,  8, 27,
         7, 17,  4, 27, 17, 27,  4, 20, 25, 27], device='cuda:0')
step: 6
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2366, -0.6108,  ...,  0.4763, -0.0831,  1.5078],
        [ 1.2529, -1.8877, -0.2058,  ..., -2.3828,  0.6040, -0.8262],
        [ 1.9170, -0.2216,  0.7969,  ..., -1.9062, -1.3428, -0.5938],
        [-0.4080, -0.3030, -1.7168,  ..., -0.6548,  0.6504,  1.2686],
        [-1.1670,  2.2246, -0.0945,  ...,  0.6587,  0.7085, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0064,  0.0073, -0.0025,  ..., -0.0086,  0.0044, -0.0141],
        [ 0.0067, -0.0189, -0.0303,  ...,  0.0018,  0.0272,  0.0074],
        [ 0.0078, -0.0110,  0.0002,  ..., -0.0078, -0.0002,  0.0045],
        [ 0.0052, -0.0088, -0.0044,  ...,  0.0088,  0.0037, -0.0016],
        [ 0.0009, -0.0073,  0.0053,  ..., -0.0015,  0.0019,  0.0046]],
       device='cuda:0', dtype=torch.float16)
Progress 1.03%, loss: 8.617591176714216, time 4.96s
loss: tensor(7.9305, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.2414e+01, -1.6739e-02, -9.2969e+00,  ..., -4.7969e+00,
         -8.1406e+00, -1.0266e+01],
        [-1.1664e+01, -3.7323e-02, -8.1328e+00,  ..., -4.4141e+00,
         -5.5391e+00, -7.2891e+00],
        [-1.3414e+01, -2.3712e-02, -9.6172e+00,  ..., -4.1797e+00,
         -6.0234e+00, -8.8672e+00],
        ...,
        [-1.5539e+01, -5.3864e-03, -1.0258e+01,  ..., -6.5352e+00,
         -9.6953e+00, -8.7891e+00],
        [-1.1867e+01, -2.1713e-02, -8.6484e+00,  ..., -4.4297e+00,
         -7.1172e+00, -8.6797e+00],
        [-1.1898e+01, -2.1713e-02, -7.5859e+00,  ..., -5.6445e+00,
         -6.5195e+00, -8.5234e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<DivBackward0>)
===============label===============
tensor([10, 10, 27,  4, 18, 10,  6, 18,  6, 22,  3,  6, 26,  1, 18, 27, 15, 27,
         3, 10,  2, 27, 27, 27, 17, 18, 26, 27, 27, 27,  6,  4,  1, 22, 18, 15,
        27, 27, 27, 27, 26,  4, 25, 15,  2,  1,  9,  9, 27, 21,  0,  0,  4,  7,
        27,  3,  0, 27,  4, 27, 11, 27, 27,  3], device='cuda:0')
step: 7
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2366, -0.6108,  ...,  0.4763, -0.0832,  1.5078],
        [ 1.2529, -1.8877, -0.2058,  ..., -2.3828,  0.6040, -0.8262],
        [ 1.9160, -0.2214,  0.7969,  ..., -1.9062, -1.3428, -0.5938],
        [-0.4082, -0.3027, -1.7168,  ..., -0.6548,  0.6504,  1.2686],
        [-1.1670,  2.2246, -0.0947,  ...,  0.6587,  0.7085, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.8599e-03, -3.3588e-03, -8.9188e-03,  ..., -1.4435e-02,
          4.9820e-03, -1.2032e-02],
        [-6.3419e-05, -4.7455e-03, -1.0048e-02,  ..., -1.6876e-02,
          9.0866e-03,  1.8435e-03],
        [ 1.7328e-03, -7.1030e-03, -3.0384e-03,  ...,  3.0537e-03,
          4.6349e-03, -1.8320e-03],
        [ 6.5231e-03, -5.9814e-03, -1.0529e-02,  ...,  2.4223e-03,
          9.4833e-03, -1.0345e-02],
        [ 1.2178e-03, -7.2479e-03,  4.9477e-03,  ...,  3.1471e-03,
          3.8891e-03,  3.1319e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(7.9872, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.0633e+01, -7.3425e-02, -8.3203e+00,  ..., -4.1367e+00,
         -6.3867e+00, -7.9492e+00],
        [-1.0617e+01, -5.1605e-02, -6.8945e+00,  ..., -6.2383e+00,
         -4.8633e+00, -7.5508e+00],
        [-1.2875e+01, -2.4719e-02, -8.3047e+00,  ..., -5.7734e+00,
         -6.7734e+00, -9.1484e+00],
        ...,
        [-1.2320e+01, -3.1250e-02, -7.9375e+00,  ..., -4.6250e+00,
         -6.4375e+00, -8.1250e+00],
        [-1.2508e+01, -7.8430e-03, -9.1016e+00,  ..., -5.7891e+00,
         -8.3828e+00, -1.0070e+01],
        [-1.1852e+01, -3.4271e-02, -8.0938e+00,  ..., -5.5977e+00,
         -5.4414e+00, -6.8164e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<DivBackward0>)
===============label===============
tensor([15,  0, 10, 17,  3, 27, 27, 27, 27,  1, 27,  5, 18, 24, 10, 12, 27, 17,
        13, 12,  3, 15,  5,  0,  3, 18, 27,  4,  1,  0, 27,  9,  7,  6, 27, 18,
        27,  2, 27, 27, 26, 27, 17, 23, 17, 27, 27,  4,  1,  5, 18, 27, 27, 18,
        22, 20,  1, 10,  4,  1,  9,  2, 11, 15], device='cuda:0')
step: 8
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2366, -0.6108,  ...,  0.4763, -0.0833,  1.5078],
        [ 1.2529, -1.8877, -0.2057,  ..., -2.3828,  0.6040, -0.8262],
        [ 1.9160, -0.2214,  0.7969,  ..., -1.9062, -1.3428, -0.5938],
        [-0.4082, -0.3027, -1.7158,  ..., -0.6553,  0.6504,  1.2686],
        [-1.1670,  2.2246, -0.0947,  ...,  0.6587,  0.7085, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0032,  0.0199,  0.0046,  ..., -0.0160,  0.0005,  0.0096],
        [-0.0035,  0.0002, -0.0123,  ...,  0.0235,  0.0312, -0.0260],
        [ 0.0092, -0.0126,  0.0004,  ..., -0.0105,  0.0013,  0.0065],
        [ 0.0107, -0.0063, -0.0022,  ...,  0.0090,  0.0081, -0.0110],
        [ 0.0031, -0.0028, -0.0021,  ..., -0.0001, -0.0008,  0.0005]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(8.0902, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.2445e+01, -7.8430e-03, -9.7266e+00,  ..., -7.2578e+00,
         -7.6016e+00, -1.0102e+01],
        [-1.2203e+01, -1.5251e-02, -9.2656e+00,  ..., -5.1094e+00,
         -7.6406e+00, -8.9219e+00],
        [-1.2000e+01, -6.7688e-02, -7.8789e+00,  ..., -3.0996e+00,
         -5.1289e+00, -7.6602e+00],
        ...,
        [-1.1680e+01, -2.1713e-02, -8.9922e+00,  ..., -5.3672e+00,
         -6.3047e+00, -8.7734e+00],
        [-1.1531e+01, -6.0394e-02, -7.9336e+00,  ..., -3.7480e+00,
         -5.1836e+00, -8.9688e+00],
        [-1.1344e+01, -2.9739e-02, -9.1562e+00,  ..., -4.5938e+00,
         -6.9375e+00, -9.5625e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<DivBackward0>)
===============label===============
tensor([ 9, 27,  7, 25, 27, 27, 26, 27,  0,  2,  4, 24, 15, 27,  5,  5, 18, 14,
        26, 27, 20, 26,  0,  7, 27, 25, 20,  0,  8, 25, 17, 27, 27,  0, 25, 27,
         1, 27, 25,  4, 27, 17, 27, 27, 27, 27, 27, 27,  4, 27, 10,  9,  1,  7,
        27, 17,  0,  7, 14, 27, 27,  5, 27, 27], device='cuda:0')
step: 9
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2367, -0.6108,  ...,  0.4763, -0.0834,  1.5078],
        [ 1.2529, -1.8877, -0.2057,  ..., -2.3828,  0.6040, -0.8262],
        [ 1.9160, -0.2213,  0.7974,  ..., -1.9062, -1.3428, -0.5938],
        [-0.4084, -0.3025, -1.7158,  ..., -0.6553,  0.6499,  1.2686],
        [-1.1680,  2.2246, -0.0948,  ...,  0.6587,  0.7085, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.8060e-03, -1.1177e-03,  4.9896e-03,  ..., -3.8490e-03,
          2.0294e-03, -5.3711e-03],
        [-4.4823e-03, -4.6768e-03, -1.5778e-02,  ..., -2.7161e-03,
          2.4475e-02, -4.2796e-04],
        [ 4.3869e-03, -9.9487e-03, -1.4286e-03,  ..., -1.3094e-03,
          3.3550e-03,  1.4858e-03],
        [ 1.0002e-02, -1.3971e-04, -5.0278e-03,  ...,  5.5742e-04,
          5.1727e-03, -3.7594e-03],
        [ 3.1796e-03, -4.6577e-03,  1.5192e-03,  ..., -1.6689e-06,
          1.1625e-03,  9.9897e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(8.6253, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.1484e+01, -1.2283e-02, -9.2031e+00,  ..., -5.2930e+00,
         -7.6992e+00, -1.0570e+01],
        [-1.0633e+01, -9.8450e-02, -8.0703e+00,  ..., -4.1289e+00,
         -5.3164e+00, -8.1328e+00],
        [-9.5859e+00, -8.3984e-02, -7.3008e+00,  ..., -4.3359e+00,
         -5.3320e+00, -8.2734e+00],
        ...,
        [-1.1461e+01, -2.1130e-01, -6.1484e+00,  ..., -4.2422e+00,
         -4.3359e+00, -6.0234e+00],
        [-1.3234e+01, -7.3509e-03, -9.1328e+00,  ..., -6.5391e+00,
         -7.1328e+00, -8.2266e+00],
        [-1.3500e+01, -1.4755e-02, -1.0141e+01,  ..., -5.1406e+00,
         -8.5781e+00, -9.4531e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 27,  8, 27,  7, 27, 27, 14, 12, 20, 22, 20, 20, 17,  1,  7,  0, 15,
        27, 24, 11,  0, 27, 27, 22, 10, 22, 20, 24,  9,  0, 27, 20, 13,  5,  3,
         2, 27,  0, 17,  4, 18,  9, 17, 15,  3, 25,  0, 25, 20,  0,  2, 27, 18,
        23, 27, 27, 10,  5,  2, 27,  5,  3, 20], device='cuda:0')
step: 10
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2367, -0.6113,  ...,  0.4766, -0.0834,  1.5078],
        [ 1.2529, -1.8877, -0.2056,  ..., -2.3828,  0.6040, -0.8262],
        [ 1.9160, -0.2212,  0.7974,  ..., -1.9062, -1.3428, -0.5938],
        [-0.4084, -0.3025, -1.7158,  ..., -0.6553,  0.6499,  1.2686],
        [-1.1680,  2.2246, -0.0948,  ...,  0.6587,  0.7085, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0048,  0.0271,  0.0025,  ..., -0.0393,  0.0093, -0.0070],
        [-0.0025, -0.0256, -0.0361,  ..., -0.0181,  0.0339,  0.0025],
        [ 0.0092, -0.0152,  0.0047,  ..., -0.0093, -0.0075,  0.0077],
        [ 0.0112, -0.0080, -0.0016,  ...,  0.0050,  0.0123, -0.0074],
        [ 0.0033, -0.0074,  0.0079,  ..., -0.0025,  0.0013,  0.0019]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(8.0139, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.1305e+01, -2.0218e-02, -8.4609e+00,  ..., -5.0508e+00,
         -6.3008e+00, -1.0117e+01],
        [-1.2180e+01, -5.8838e-02, -8.6562e+00,  ..., -4.2461e+00,
         -5.5586e+00, -7.8086e+00],
        [-1.3375e+01, -1.3268e-02, -9.3906e+00,  ..., -5.8242e+00,
         -6.7930e+00, -6.8867e+00],
        ...,
        [-1.2344e+01, -3.1250e-02, -8.3125e+00,  ..., -4.8438e+00,
         -7.0312e+00, -9.2500e+00],
        [-1.1461e+01, -1.9226e-02, -9.5469e+00,  ..., -4.5508e+00,
         -6.6758e+00, -9.8906e+00],
        [-1.0273e+01, -2.0218e-02, -8.3672e+00,  ..., -5.6133e+00,
         -7.1445e+00, -7.8320e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 17, 27,  7,  1, 18, 17,  5,  5, 17,  3,  0, 27, 27, 27,  4, 10, 15,
         0, 27,  7, 27, 14, 27, 15,  8,  2,  1,  4, 27, 27, 22, 22, 20,  7,  1,
         0, 22, 17, 18,  9, 26, 22,  7,  6,  5, 15, 18,  0, 12, 18,  4, 27, 27,
        10,  4, 25,  7, 27, 15, 13,  5, 27, 18], device='cuda:0')
step: 11
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2367, -0.6113,  ...,  0.4766, -0.0835,  1.5078],
        [ 1.2529, -1.8877, -0.2054,  ..., -2.3828,  0.6040, -0.8262],
        [ 1.9160, -0.2211,  0.7974,  ..., -1.9062, -1.3428, -0.5938],
        [-0.4084, -0.3025, -1.7158,  ..., -0.6553,  0.6499,  1.2686],
        [-1.1680,  2.2266, -0.0950,  ...,  0.6587,  0.7085, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0022, -0.0053,  0.0027,  ..., -0.0039, -0.0033, -0.0048],
        [-0.0011,  0.0054, -0.0046,  ..., -0.0030,  0.0107, -0.0045],
        [ 0.0013, -0.0073, -0.0050,  ...,  0.0030,  0.0038, -0.0029],
        [ 0.0019, -0.0032, -0.0075,  ...,  0.0035,  0.0007, -0.0032],
        [-0.0007, -0.0039,  0.0068,  ..., -0.0013,  0.0036,  0.0071]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(7.6794, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.2133e+01, -1.0307e-02, -9.4141e+00,  ..., -8.4141e+00,
         -7.2930e+00, -9.5078e+00],
        [-1.2266e+01, -7.4463e-02, -8.1719e+00,  ..., -3.5117e+00,
         -5.5117e+00, -9.0469e+00],
        [-1.2250e+01, -9.3628e-02, -7.7500e+00,  ..., -2.9062e+00,
         -5.1562e+00, -7.5000e+00],
        ...,
        [-1.1211e+01, -2.4719e-02, -7.6172e+00,  ..., -4.7109e+00,
         -6.7734e+00, -8.3047e+00],
        [-1.2664e+01, -1.0803e-02, -1.0039e+01,  ..., -5.2930e+00,
         -7.0742e+00, -8.0078e+00],
        [-1.2117e+01, -1.0980e-01, -7.6406e+00,  ..., -3.3906e+00,
         -6.0156e+00, -7.7031e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 27, 27, 27, 27,  9, 27, 15, 27, 17, 26, 15, 27, 27, 27,  4, 26, 27,
         9, 27,  1,  5, 25,  2, 18, 27, 27, 27, 27,  5, 27,  4,  4,  2, 27,  0,
        27,  4, 27, 12,  2, 18, 27, 27, 27, 27, 10, 14,  3,  4,  1, 27,  7,  8,
        10, 27,  9,  9,  3, 27, 15, 27, 27, 22], device='cuda:0')
step: 12
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2368, -0.6113,  ...,  0.4766, -0.0836,  1.5078],
        [ 1.2529, -1.8877, -0.2054,  ..., -2.3828,  0.6040, -0.8262],
        [ 1.9160, -0.2209,  0.7974,  ..., -1.9062, -1.3428, -0.5938],
        [-0.4087, -0.3022, -1.7158,  ..., -0.6553,  0.6499,  1.2686],
        [-1.1680,  2.2266, -0.0950,  ...,  0.6582,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0011, -0.0014,  0.0037,  ...,  0.0122,  0.0057, -0.0012],
        [ 0.0055,  0.0232,  0.0162,  ..., -0.0371, -0.0309,  0.0148],
        [ 0.0048, -0.0110, -0.0048,  ...,  0.0009,  0.0030,  0.0013],
        [ 0.0011, -0.0065, -0.0046,  ...,  0.0091,  0.0049, -0.0046],
        [ 0.0002, -0.0087,  0.0057,  ...,  0.0006,  0.0015,  0.0026]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(7.4183, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-11.4922,  -0.0883,  -7.9961,  ...,  -4.0898,  -5.4336, -10.3359],
        [-11.2422,  -0.0531,  -7.6797,  ...,  -4.4297,  -5.1797,  -8.4922],
        [-11.7812,  -0.0307,  -8.9688,  ...,  -4.6250,  -6.1562,  -8.0625],
        ...,
        [-11.2734,  -0.0506,  -8.2656,  ...,  -4.1758,  -5.8945,  -8.7344],
        [-10.9219,  -0.3303,  -8.7344,  ...,  -1.8926,  -5.1445,  -7.0195],
        [-11.0781,  -0.0888,  -7.8711,  ...,  -3.4941,  -6.5586,  -9.5234]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  4, 27, 26, 15, 17,  6,  5,  0,  3, 27, 27, 25,  9, 27, 27, 27,  3,
         1, 27, 27, 26, 15, 18, 27, 27, 10, 27, 27, 27,  8,  7,  5, 27, 25, 11,
         4,  3, 20, 27,  3, 10, 15,  2, 26,  0, 27, 19, 17, 18, 23, 27, 27,  6,
         8, 20,  3, 18, 18, 27,  7,  4, 18, 27], device='cuda:0')
step: 13
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2368, -0.6113,  ...,  0.4766, -0.0836,  1.5078],
        [ 1.2529, -1.8877, -0.2053,  ..., -2.3828,  0.6035, -0.8262],
        [ 1.9160, -0.2209,  0.7974,  ..., -1.9062, -1.3428, -0.5938],
        [-0.4087, -0.3022, -1.7158,  ..., -0.6558,  0.6499,  1.2686],
        [-1.1680,  2.2266, -0.0951,  ...,  0.6582,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0072,  0.0276,  0.0076,  ..., -0.0175,  0.0063, -0.0124],
        [ 0.0020,  0.0119,  0.0066,  ...,  0.0066,  0.0092, -0.0130],
        [ 0.0041, -0.0068, -0.0036,  ..., -0.0013,  0.0057, -0.0010],
        [-0.0002, -0.0080, -0.0033,  ...,  0.0140,  0.0033,  0.0049],
        [ 0.0024, -0.0076,  0.0029,  ...,  0.0015,  0.0022,  0.0025]],
       device='cuda:0', dtype=torch.float16)
Progress 2.06%, loss: 8.29057162148612, time 5.94s
loss: tensor(7.1662, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-11.4531,  -0.0333,  -6.2227,  ...,  -5.0352,  -5.7227,  -7.3477],
        [-10.0469,  -0.2683,  -6.4570,  ...,  -2.2051,  -4.5195,  -8.1719],
        [-12.5547,  -0.0257,  -8.2734,  ...,  -5.2148,  -7.2773,  -6.9961],
        ...,
        [-12.0625,  -0.0792,  -7.9844,  ...,  -3.1113,  -6.1094,  -8.7344],
        [-12.1484,  -0.0542,  -8.2734,  ...,  -4.2422,  -5.6484,  -9.1172],
        [-10.9688,  -0.1235,  -7.5586,  ...,  -2.5918,  -6.3750,  -7.2500]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([24, 25, 27, 27, 15,  1, 27, 27, 18,  1, 24, 15,  4, 15, 27, 27, 20,  5,
        27,  6,  0, 27, 27, 10, 27,  5, 27, 27,  6, 17,  7, 15,  0,  9, 14,  7,
        10,  2, 18,  4, 25, 25, 27, 18,  0, 17, 15, 27, 27,  5, 27, 13, 27,  4,
         3, 27,  2, 27, 27, 27, 15, 27,  1,  9], device='cuda:0')
step: 14
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2368, -0.6113,  ...,  0.4768, -0.0837,  1.5088],
        [ 1.2529, -1.8877, -0.2053,  ..., -2.3828,  0.6035, -0.8262],
        [ 1.9160, -0.2208,  0.7974,  ..., -1.9062, -1.3428, -0.5938],
        [-0.4087, -0.3022, -1.7158,  ..., -0.6558,  0.6499,  1.2686],
        [-1.1680,  2.2266, -0.0952,  ...,  0.6582,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.2673e-03,  1.7975e-02, -3.8071e-03,  ..., -2.7985e-02,
          5.7487e-03, -1.5556e-02],
        [ 1.8606e-03,  8.7814e-03, -5.5432e-06,  ...,  1.3161e-03,
          1.3573e-02, -7.1907e-03],
        [ 6.7558e-03, -1.1940e-02, -4.2877e-03,  ..., -8.2016e-03,
          9.7275e-04,  2.3727e-03],
        [ 6.2981e-03, -9.1324e-03, -8.6288e-03,  ...,  3.7479e-03,
          1.9417e-03, -8.9569e-03],
        [ 1.7633e-03, -7.6256e-03,  4.1771e-03,  ...,  2.0866e-03,
          3.8414e-03,  1.8272e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(7.2126, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-12.8281,  -0.0163,  -8.7031,  ...,  -5.9531,  -6.6094,  -7.9219],
        [-10.6641,  -0.0363,  -7.0977,  ...,  -5.6602,  -7.2227,  -7.7227],
        [-11.5547,  -0.0247,  -9.4297,  ...,  -6.2109,  -5.1172,  -6.4609],
        ...,
        [ -9.3516,  -0.2903,  -7.8203,  ...,  -3.4785,  -3.5723,  -7.1328],
        [-11.0391,  -0.2289,  -6.9805,  ...,  -2.4785,  -3.8848,  -6.1992],
        [-10.2812,  -0.0656,  -7.0664,  ...,  -4.0664,  -5.5977,  -6.9414]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6,  9,  7,  0, 26, 27,  0, 27,  2, 27, 27,  4, 27,  3, 10, 22,  1,  3,
         6, 10,  4, 27, 18,  3,  4, 27, 27, 10, 17, 24, 27, 27, 27, 27, 20, 17,
        27, 27, 25,  1, 27,  9, 24, 27, 15, 15, 25, 17, 27, 27, 18, 27,  7, 27,
         5,  2, 15, 27,  0, 25, 25,  1, 15, 27], device='cuda:0')
step: 15
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2369, -0.6113,  ...,  0.4768, -0.0837,  1.5088],
        [ 1.2529, -1.8877, -0.2053,  ..., -2.3828,  0.6035, -0.8262],
        [ 1.9160, -0.2207,  0.7974,  ..., -1.9062, -1.3428, -0.5938],
        [-0.4089, -0.3020, -1.7158,  ..., -0.6558,  0.6494,  1.2686],
        [-1.1680,  2.2266, -0.0953,  ...,  0.6582,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0040, -0.0192, -0.0030,  ..., -0.0065, -0.0035, -0.0093],
        [ 0.0009, -0.0039, -0.0102,  ...,  0.0114,  0.0223, -0.0140],
        [ 0.0049, -0.0133, -0.0028,  ..., -0.0011,  0.0011,  0.0039],
        [ 0.0078, -0.0053, -0.0018,  ...,  0.0058,  0.0055, -0.0052],
        [ 0.0009, -0.0047,  0.0080,  ..., -0.0006,  0.0058,  0.0020]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(7.7908, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-11.1562,  -0.1915,  -9.4062,  ...,  -1.9727,  -5.3477,  -8.6016],
        [-11.1016,  -0.0724,  -4.7578,  ...,  -7.6367,  -8.6641,  -8.9453],
        [-10.0625,  -0.1224,  -8.4062,  ...,  -2.4355,  -5.3711,  -9.2500],
        ...,
        [-10.5078,  -0.0388,  -8.4141,  ...,  -5.5703,  -6.2266,  -7.3828],
        [-11.7969,  -0.0192,  -8.3594,  ...,  -5.1133,  -7.3633,  -9.1719],
        [-11.4219,  -0.0163,  -9.7656,  ...,  -5.4844,  -6.4844,  -8.3281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 11,  0,  3,  6,  2,  3, 27, 27, 22, 27,  1, 27, 27, 27, 27, 27, 10,
        10, 27, 10,  0, 27, 27, 18,  3,  2, 14, 10, 27, 27, 27, 27, 15,  3, 27,
        26, 26, 27, 27,  9, 20, 20,  2, 18, 26,  7, 21, 22, 27,  6,  5,  4, 27,
        20, 27,  7,  4,  3, 27, 27, 27, 20, 27], device='cuda:0')
step: 16
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7646, -0.2369, -0.6113,  ...,  0.4768, -0.0838,  1.5088],
        [ 1.2529, -1.8877, -0.2052,  ..., -2.3828,  0.6035, -0.8262],
        [ 1.9160, -0.2206,  0.7974,  ..., -1.9062, -1.3428, -0.5938],
        [-0.4089, -0.3020, -1.7158,  ..., -0.6558,  0.6494,  1.2686],
        [-1.1680,  2.2266, -0.0953,  ...,  0.6582,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0099, -0.0540,  0.0015,  ...,  0.0372, -0.0104,  0.0088],
        [ 0.0034,  0.0078,  0.0111,  ..., -0.0090,  0.0038, -0.0025],
        [ 0.0011,  0.0013, -0.0080,  ...,  0.0028,  0.0072, -0.0015],
        [ 0.0081, -0.0003, -0.0088,  ...,  0.0021,  0.0051, -0.0162],
        [-0.0005, -0.0028,  0.0093,  ...,  0.0016,  0.0066,  0.0039]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(7.7366, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-11.9453,  -0.0399,  -6.5078,  ...,  -5.0078,  -5.6641,  -7.1641],
        [-11.8203,  -0.0419,  -7.3867,  ...,  -5.0117,  -6.1367,  -4.8555],
        [ -9.8828,  -0.2910,  -9.4453,  ...,  -1.6025,  -4.8828,  -4.9453],
        ...,
        [-11.6719,  -0.0153,  -9.5781,  ...,  -5.3281,  -6.6719,  -8.4844],
        [-10.7188,  -0.0317,  -7.9062,  ...,  -5.1562,  -7.3750,  -8.1875],
        [-10.2734,  -0.0573,  -8.3984,  ...,  -4.2773,  -4.9648,  -7.4648]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 9,  4, 15, 27,  3, 27, 10,  6,  4,  0, 27, 18, 27,  0, 27, 20, 18, 10,
         4,  2, 27, 27, 27,  3, 18,  8,  5,  4, 10, 20,  0,  7, 27, 27, 17, 27,
        10, 18, 27, 10, 27, 20,  7,  3,  4,  7,  4, 18, 27, 27,  7, 14, 20, 27,
        27, 15, 27, 27,  0, 27,  7, 26,  4, 20], device='cuda:0')
step: 17
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2369, -0.6113,  ...,  0.4768, -0.0839,  1.5088],
        [ 1.2529, -1.8877, -0.2052,  ..., -2.3828,  0.6035, -0.8262],
        [ 1.9160, -0.2206,  0.7974,  ..., -1.9062, -1.3428, -0.5938],
        [-0.4089, -0.3020, -1.7158,  ..., -0.6558,  0.6494,  1.2686],
        [-1.1680,  2.2266, -0.0955,  ...,  0.6582,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0057,  0.0260,  0.0056,  ..., -0.0150,  0.0043, -0.0071],
        [ 0.0018,  0.0018, -0.0078,  ...,  0.0017,  0.0151, -0.0081],
        [ 0.0050, -0.0050, -0.0020,  ...,  0.0038,  0.0059,  0.0044],
        [ 0.0012, -0.0082, -0.0078,  ...,  0.0085,  0.0021, -0.0012],
        [-0.0021, -0.0071,  0.0070,  ...,  0.0037,  0.0054,  0.0034]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(7.5560, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-11.1953,  -0.0409,  -7.0078,  ...,  -6.6055,  -6.1328,  -6.6055],
        [-11.5078,  -0.0424,  -7.2930,  ...,  -4.3555,  -6.6367,  -7.2305],
        [-12.0234,  -0.0202,  -7.4883,  ...,  -6.5195,  -6.5195,  -5.5820],
        ...,
        [ -9.8906,  -0.1403,  -7.3281,  ...,  -3.0469,  -4.1094,  -6.4844],
        [-10.8516,  -0.2532,  -7.4727,  ...,  -1.9414,  -5.7227,  -8.6250],
        [-10.2266,  -0.1011,  -7.6953,  ...,  -3.4766,  -5.6953,  -7.8828]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  6,  2,  5, 27, 27, 17, 27, 15, 18,  6, 13, 27, 17, 27, 20, 13,
        27, 18,  5, 27, 27,  1,  0, 27,  0,  9, 20, 27,  6,  2,  4, 11, 10, 27,
        27, 17, 20, 13, 27, 25, 10, 20,  0, 27, 15, 13, 10, 10, 15,  5, 27, 20,
        10, 19,  7, 27, 15, 25,  0,  4, 27,  5], device='cuda:0')
step: 18
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2369, -0.6113,  ...,  0.4768, -0.0839,  1.5088],
        [ 1.2529, -1.8877, -0.2052,  ..., -2.3828,  0.6035, -0.8262],
        [ 1.9160, -0.2205,  0.7979,  ..., -1.9062, -1.3428, -0.5938],
        [-0.4092, -0.3018, -1.7158,  ..., -0.6558,  0.6494,  1.2695],
        [-1.1680,  2.2266, -0.0955,  ...,  0.6582,  0.7080, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0016, -0.0035,  0.0004,  ..., -0.0128, -0.0028, -0.0066],
        [ 0.0074, -0.0026, -0.0067,  ...,  0.0106,  0.0168, -0.0121],
        [ 0.0043, -0.0043, -0.0038,  ..., -0.0085,  0.0020,  0.0048],
        [ 0.0056, -0.0033, -0.0044,  ...,  0.0032,  0.0047, -0.0016],
        [ 0.0037, -0.0056,  0.0023,  ..., -0.0001,  0.0024,  0.0022]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(7.0478, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-12.9688,  -0.0343,  -8.4062,  ...,  -5.4727,  -4.7852,  -8.7812],
        [ -9.6328,  -0.1676,  -7.3555,  ...,  -3.6367,  -3.9492,  -6.6367],
        [-11.4297,  -0.0267,  -8.9609,  ...,  -4.8086,  -5.9023,  -9.2422],
        ...,
        [ -8.8516,  -0.2607,  -5.6680,  ...,  -2.9805,  -4.4805,  -6.6055],
        [ -8.5391,  -0.3501,  -7.1328,  ...,  -3.2559,  -4.3828,  -7.6953],
        [-11.6641,  -0.0692,  -7.2578,  ...,  -3.5684,  -5.8828,  -7.6641]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  4, 27, 27, 13,  4, 18, 17, 20,  0,  1,  0, 27,  2, 20,  3,  6, 20,
        25, 27,  8,  3, 22,  4, 27,  3, 26, 27, 10, 27, 25,  7, 27,  2, 27, 27,
        27, 27,  9,  4, 27,  1, 10,  3, 24, 27, 15,  3, 26, 27,  0, 26,  0, 15,
        20, 20, 18, 25, 22, 15, 27, 17, 18,  3], device='cuda:0')
step: 19
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2371, -0.6113,  ...,  0.4768, -0.0839,  1.5088],
        [ 1.2529, -1.8877, -0.2052,  ..., -2.3828,  0.6035, -0.8257],
        [ 1.9160, -0.2203,  0.7979,  ..., -1.9062, -1.3428, -0.5938],
        [-0.4092, -0.3018, -1.7158,  ..., -0.6562,  0.6494,  1.2695],
        [-1.1680,  2.2266, -0.0956,  ...,  0.6582,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.0414e-03,  5.3024e-03, -1.0635e-02,  ..., -1.4305e-02,
          9.7733e-03, -1.1787e-02],
        [ 4.5681e-04, -1.8415e-03, -1.7838e-02,  ..., -5.9738e-03,
          1.7899e-02,  1.8060e-04],
        [ 3.4084e-03, -6.4735e-03, -4.2038e-03,  ...,  2.9850e-04,
          5.7161e-05, -3.0060e-03],
        [ 4.8256e-03, -2.7237e-03, -9.4070e-03,  ..., -1.0176e-03,
         -6.3944e-04, -7.8812e-03],
        [ 1.3828e-03, -4.5700e-03, -3.8624e-03,  ...,  1.4839e-03,
         -1.2517e-04, -6.6805e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(7.0941, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-11.2734,  -0.0568,  -7.7461,  ...,  -4.5234,  -6.7148,  -6.9297],
        [ -9.1328,  -0.8823,  -7.6016,  ...,  -0.6953,  -4.8828,  -7.5078],
        [ -9.9844,  -0.0776,  -6.0156,  ...,  -5.7031,  -5.7031,  -5.3906],
        ...,
        [-11.0312,  -0.0614,  -6.9375,  ...,  -3.8730,  -7.0312,  -8.2812],
        [-11.6484,  -0.0197,  -8.1172,  ...,  -4.9883,  -7.5195,  -7.8945],
        [ -9.7266,  -0.1313,  -7.0703,  ...,  -2.8809,  -4.9766,  -5.9141]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0, 26,  7,  0, 17, 27,  0, 18, 22, 27, 20, 27, 22,  9, 27, 10, 20,
        27, 27,  3,  0, 18, 20, 18, 22, 27,  9, 10, 27,  1,  1,  9, 27, 11, 20,
         0, 27, 13, 18, 27,  7, 10,  0,  6, 15, 10, 25, 10,  4, 18,  5, 20, 27,
        15, 18, 27, 15,  1,  0,  0, 19,  2,  4], device='cuda:0')
step: 20
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2371, -0.6113,  ...,  0.4771, -0.0840,  1.5088],
        [ 1.2529, -1.8887, -0.2051,  ..., -2.3828,  0.6035, -0.8257],
        [ 1.9150, -0.2203,  0.7979,  ..., -1.9062, -1.3428, -0.5938],
        [-0.4092, -0.3018, -1.7148,  ..., -0.6562,  0.6494,  1.2695],
        [-1.1680,  2.2266, -0.0957,  ...,  0.6582,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0083, -0.0135,  0.0008,  ...,  0.0081,  0.0061, -0.0002],
        [ 0.0128, -0.0258, -0.0075,  ...,  0.0111, -0.0573, -0.0257],
        [ 0.0018, -0.0091, -0.0050,  ..., -0.0002,  0.0019, -0.0010],
        [ 0.0002, -0.0070,  0.0023,  ...,  0.0145,  0.0055, -0.0082],
        [ 0.0006, -0.0076,  0.0084,  ..., -0.0033,  0.0015,  0.0045]],
       device='cuda:0', dtype=torch.float16)
Progress 3.09%, loss: 7.98438903263637, time 5.95s
loss: tensor(7.2999, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-10.3438,  -0.0630,  -8.7812,  ...,  -5.2812,  -4.6562,  -6.0312],
        [-10.0312,  -0.5903,  -6.2148,  ...,  -2.2773,  -5.0586,  -5.7773],
        [-10.6641,  -0.2234,  -6.6289,  ...,  -3.0039,  -3.4727,  -6.5039],
        ...,
        [ -9.6641,  -0.1364,  -7.4180,  ...,  -3.1055,  -4.0117,  -6.0742],
        [ -9.8203,  -0.1653,  -7.5078,  ...,  -3.7285,  -4.3516,  -6.5078],
        [ -9.4219,  -0.3584,  -6.7969,  ...,  -2.0781,  -5.0156,  -6.7344]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 10,  3, 27, 27, 18, 18, 20, 15, 27, 16, 24, 15, 27,  5, 27, 15, 27,
        27, 27,  0, 25,  7,  0, 27,  0, 15,  4,  7,  0, 27, 22, 27, 27, 22,  0,
         4, 27,  1,  6,  3,  7, 27, 27, 27, 10, 27, 26,  7, 27, 10,  4, 25, 27,
        20, 24, 27, 22,  4, 25, 17, 27,  0, 22], device='cuda:0')
step: 21
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2371, -0.6113,  ...,  0.4771, -0.0840,  1.5088],
        [ 1.2529, -1.8877, -0.2051,  ..., -2.3828,  0.6035, -0.8257],
        [ 1.9150, -0.2202,  0.7979,  ..., -1.9062, -1.3428, -0.5942],
        [-0.4094, -0.3015, -1.7148,  ..., -0.6562,  0.6494,  1.2695],
        [-1.1680,  2.2266, -0.0958,  ...,  0.6582,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0026, -0.0124, -0.0062,  ..., -0.0070, -0.0060, -0.0012],
        [ 0.0030,  0.0083,  0.0053,  ...,  0.0331,  0.0080, -0.0276],
        [ 0.0037, -0.0063, -0.0052,  ..., -0.0050,  0.0055,  0.0016],
        [ 0.0063, -0.0069, -0.0066,  ...,  0.0039,  0.0035, -0.0067],
        [ 0.0022, -0.0052,  0.0030,  ...,  0.0012,  0.0026,  0.0021]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(6.2533, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-13.0000,  -0.0511,  -8.7109,  ...,  -3.9883,  -6.2070,  -5.7383],
        [-10.7422,  -0.0500,  -8.1406,  ...,  -4.7070,  -5.3633,  -7.5195],
        [-10.4375,  -0.0630,  -7.5312,  ...,  -3.7500,  -6.2500,  -7.8750],
        ...,
        [ -9.2266,  -0.0708,  -7.4453,  ...,  -4.3516,  -5.7266,  -6.1953],
        [ -9.0234,  -0.5195,  -6.3477,  ...,  -1.7695,  -3.8633,  -6.3789],
        [ -9.4844,  -0.2382,  -7.1445,  ...,  -2.3320,  -4.9883,  -7.0508]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3,  7, 18,  0, 27, 13, 26,  9, 10, 20,  2,  3, 26, 11,  0, 18,  0, 20,
        25, 17,  0,  1, 17,  0, 10, 18, 27, 27, 27, 21, 27,  6,  4, 25, 27, 27,
        27, 27, 27, 27, 11, 27, 27,  6,  1, 10, 27, 27, 25,  3, 27, 27,  5,  1,
        22, 12, 27, 13, 26,  9, 24,  7, 14, 18], device='cuda:0')
step: 22
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2371, -0.6113,  ...,  0.4771, -0.0840,  1.5088],
        [ 1.2529, -1.8877, -0.2051,  ..., -2.3828,  0.6035, -0.8257],
        [ 1.9150, -0.2202,  0.7979,  ..., -1.9062, -1.3428, -0.5942],
        [-0.4094, -0.3015, -1.7148,  ..., -0.6562,  0.6489,  1.2695],
        [-1.1680,  2.2266, -0.0958,  ...,  0.6582,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0125,  0.0169, -0.0021,  ..., -0.0192, -0.0013, -0.0189],
        [ 0.0145, -0.0076, -0.0039,  ...,  0.0972,  0.0618, -0.0730],
        [ 0.0051, -0.0034, -0.0021,  ..., -0.0019,  0.0032, -0.0004],
        [ 0.0012, -0.0029,  0.0004,  ...,  0.0048,  0.0034,  0.0101],
        [ 0.0036, -0.0023,  0.0005,  ...,  0.0020,  0.0030, -0.0027]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(6.3324, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -9.6094,  -0.6689,  -6.1055,  ...,  -1.7939,  -3.6699,  -3.8867],
        [ -9.4844,  -0.6699,  -6.2031,  ...,  -1.9824,  -4.2656,  -4.2969],
        [-10.8359,  -0.0526,  -7.9570,  ...,  -4.9258,  -6.7109,  -6.6133],
        ...,
        [-10.2969,  -0.1682,  -5.7930,  ...,  -3.6992,  -4.4492,  -6.7930],
        [ -9.8125,  -0.1230,  -6.4648,  ...,  -2.8418,  -4.6250,  -6.8086],
        [-10.4531,  -0.1716,  -5.6406,  ...,  -3.1406,  -5.2656,  -6.2656]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  3, 27, 27, 18, 27,  0, 18,  4, 17, 27,  3, 27, 27, 27, 11,  0,  7,
        27, 27,  5,  4, 15, 27, 24, 27, 27,  9,  1, 25, 27, 22,  9,  0, 27, 27,
         0, 22,  4, 23, 25,  2, 25, 27,  0, 22, 27,  7, 10, 25,  3, 15,  2,  6,
        15,  5, 27,  9, 25,  0, 27, 25, 14,  9], device='cuda:0')
step: 23
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2371, -0.6113,  ...,  0.4771, -0.0840,  1.5088],
        [ 1.2529, -1.8877, -0.2050,  ..., -2.3828,  0.6035, -0.8257],
        [ 1.9150, -0.2201,  0.7979,  ..., -1.9062, -1.3438, -0.5942],
        [-0.4094, -0.3015, -1.7148,  ..., -0.6562,  0.6489,  1.2695],
        [-1.1680,  2.2266, -0.0959,  ...,  0.6582,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.3940e-03,  3.1250e-02, -8.3084e-03,  ..., -4.2152e-03,
          1.5945e-02, -5.6686e-03],
        [ 9.3699e-04,  1.5854e-02,  2.0237e-03,  ..., -5.2338e-03,
         -4.1008e-03, -3.0472e-02],
        [ 5.4321e-03, -1.0468e-02, -3.9215e-03,  ..., -1.0948e-03,
          1.1368e-03, -3.9597e-03],
        [ 1.3952e-03, -7.1411e-03, -6.4507e-03,  ...,  4.1122e-03,
          3.1769e-05, -4.6654e-03],
        [ 4.3106e-03, -6.2866e-03, -1.8349e-03,  ...,  4.7827e-04,
         -1.1754e-04,  2.6722e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(6.4150, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -9.3672,  -0.1499,  -6.9609,  ...,  -3.8066,  -4.1172,  -4.5234],
        [ -9.6484,  -0.1833,  -7.2773,  ...,  -2.7148,  -5.3711,  -6.3398],
        [ -9.2500,  -0.2179,  -5.8750,  ...,  -2.4062,  -4.1875,  -7.5625],
        ...,
        [-10.4141,  -0.4492,  -7.7305,  ...,  -1.3242,  -3.6680,  -6.5742],
        [ -9.0703,  -0.4763,  -6.4453,  ...,  -3.6328,  -2.8203,  -5.1016],
        [-12.8516,  -0.0630,  -6.5938,  ...,  -5.8438,  -7.5312,  -7.5312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 22,  1,  7, 20, 14, 27, 27, 27, 27, 15, 22, 20,  3, 27,  4,  3, 10,
        17, 27, 18, 26, 27, 15,  4, 20, 27,  7, 26, 26, 26, 22,  0, 19, 18, 17,
         2, 18, 27, 27,  0, 27, 20, 27, 27, 25, 17, 27, 27,  0, 27,  0, 27, 26,
        27, 27, 18, 27, 27, 11, 13, 19, 27, 15], device='cuda:0')
step: 24
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2371, -0.6113,  ...,  0.4771, -0.0841,  1.5088],
        [ 1.2529, -1.8877, -0.2050,  ..., -2.3828,  0.6035, -0.8257],
        [ 1.9150, -0.2200,  0.7979,  ..., -1.9062, -1.3438, -0.5942],
        [-0.4094, -0.3013, -1.7148,  ..., -0.6562,  0.6489,  1.2695],
        [-1.1680,  2.2266, -0.0959,  ...,  0.6582,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0005,  0.0187,  0.0103,  ..., -0.0052,  0.0045, -0.0018],
        [-0.0012, -0.0039, -0.0062,  ...,  0.0026,  0.0072, -0.0137],
        [ 0.0025, -0.0082,  0.0025,  ..., -0.0044, -0.0039,  0.0030],
        [ 0.0045, -0.0069,  0.0021,  ...,  0.0101,  0.0067, -0.0050],
        [-0.0009, -0.0045,  0.0072,  ...,  0.0009,  0.0051,  0.0014]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(5.9167, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -9.7734,  -0.0516,  -7.3945,  ...,  -5.2383,  -5.2070,  -5.0508],
        [-11.1562,  -0.0904,  -7.9961,  ...,  -5.4648,  -4.7773,  -5.7461],
        [ -9.4297,  -0.2761,  -7.1523,  ...,  -3.8691,  -3.9316,  -5.6211],
        ...,
        [-11.9609,  -0.4978,  -3.8105,  ..., -10.9062,  -9.9219,  -7.8086],
        [ -9.7031,  -0.2040,  -6.2344,  ...,  -2.8594,  -4.4531,  -5.0469],
        [ -9.0938,  -1.9395,  -7.6289,  ...,  -0.5957,  -3.0332,  -5.2188]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27, 27,  6, 27, 17,  0,  3, 27, 27,  9, 27, 27, 27, 27, 13, 27,
        15,  7, 22, 15,  4, 10,  0,  3,  7, 27, 27,  5,  0,  5, 18, 10,  1,  1,
        27, 27, 27, 27, 11, 18, 27,  7, 27, 27, 27,  6, 27,  9, 27, 27, 27, 20,
        27, 20, 26, 13, 27, 10, 27, 27,  9, 20], device='cuda:0')
step: 25
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2371, -0.6113,  ...,  0.4771, -0.0842,  1.5088],
        [ 1.2529, -1.8877, -0.2050,  ..., -2.3828,  0.6035, -0.8257],
        [ 1.9150, -0.2200,  0.7983,  ..., -1.9062, -1.3438, -0.5942],
        [-0.4097, -0.3013, -1.7148,  ..., -0.6562,  0.6489,  1.2695],
        [-1.1680,  2.2266, -0.0960,  ...,  0.6582,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.5234e-03,  1.7227e-02, -1.2760e-03,  ..., -1.4496e-02,
          7.4310e-03,  1.4477e-03],
        [ 6.9082e-05,  4.3716e-03,  4.4441e-04,  ..., -8.1360e-05,
          3.0403e-03, -4.9324e-03],
        [ 4.8599e-03, -8.4381e-03, -1.3084e-03,  ...,  3.2663e-04,
         -2.0275e-03, -4.2419e-03],
        [ 4.1695e-03, -3.4428e-03, -7.4921e-03,  ...,  1.8435e-03,
          3.1528e-03, -9.3842e-03],
        [ 1.2941e-03, -2.5635e-03, -5.9795e-04,  ..., -1.5068e-03,
          3.0212e-03, -6.6376e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(6.3271, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -8.9922,  -0.4431,  -6.5820,  ...,  -2.8184,  -3.5059,  -4.7695],
        [ -9.0625,  -0.2222,  -5.6602,  ...,  -5.4414,  -4.8789,  -4.8477],
        [-12.2031,  -0.0197,  -8.6172,  ...,  -5.3008,  -5.8633,  -8.0234],
        ...,
        [ -9.4531,  -0.1396,  -7.5469,  ...,  -3.1387,  -4.2656,  -5.4531],
        [-10.4609,  -0.3342,  -7.3359,  ...,  -3.2402,  -2.5527,  -6.4609],
        [-11.2891,  -0.0212,  -7.8945,  ...,  -6.4883,  -6.6758,  -6.4258]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 27, 10, 27, 13, 10,  4,  6, 22, 27, 15, 20, 27, 27,  7,  0,  9,  1,
         7,  4,  1, 15, 27, 20, 27, 27,  3, 12, 13,  0,  0, 13, 12,  6,  6,  0,
        27, 25, 27, 27, 27, 25,  7,  3,  4, 27, 14, 27, 27, 27, 15,  4, 27, 17,
        25, 26,  1, 17, 22, 26,  4,  7,  0,  4], device='cuda:0')
step: 26
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2371, -0.6113,  ...,  0.4773, -0.0842,  1.5088],
        [ 1.2529, -1.8887, -0.2050,  ..., -2.3828,  0.6035, -0.8257],
        [ 1.9150, -0.2198,  0.7983,  ..., -1.9062, -1.3438, -0.5942],
        [-0.4097, -0.3013, -1.7148,  ..., -0.6562,  0.6489,  1.2695],
        [-1.1689,  2.2266, -0.0961,  ...,  0.6582,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.3902e-03,  2.1378e-02,  3.2187e-06,  ..., -6.1264e-03,
          8.5526e-03,  7.2861e-03],
        [ 2.1759e-02, -1.9653e-02,  5.1537e-03,  ...,  6.6101e-02,
         -4.5959e-02, -1.0565e-01],
        [ 6.8550e-03, -1.1261e-02, -1.9417e-03,  ...,  2.0103e-03,
          1.2989e-03, -2.8267e-03],
        [ 4.9248e-03, -2.2717e-03, -5.2261e-03,  ...,  5.1155e-03,
          7.6637e-03, -3.4428e-03],
        [ 8.6975e-04, -4.2305e-03,  8.1711e-03,  ..., -3.3417e-03,
          3.1414e-03,  3.3627e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(6.2199, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-10.9375,  -0.2468,  -7.5273,  ...,  -2.2773,  -3.9961,  -6.8711],
        [ -9.1875,  -0.6558,  -6.7188,  ...,  -1.3115,  -3.6875,  -5.5938],
        [-10.0625,  -0.4661,  -6.8438,  ...,  -1.5605,  -4.3398,  -4.9023],
        ...,
        [ -9.2031,  -1.3262,  -6.8281,  ...,  -0.5454,  -4.8594,  -7.4531],
        [-10.2578,  -0.1653,  -6.2266,  ...,  -4.3828,  -5.7266,  -3.9141],
        [ -9.8438,  -0.2161,  -6.7461,  ...,  -4.2148,  -4.1523,  -5.5898]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  2, 15, 26, 27,  5, 15, 27, 27,  7, 27, 22, 27, 12, 15, 22,  3,
        27, 15, 15, 10,  4,  3,  1,  7, 15,  0, 15, 27,  2, 20, 10,  0, 27, 20,
        18, 10,  1, 27, 27, 27,  8, 27, 27, 27, 15, 27, 22, 27, 13,  5,  6, 18,
        20,  4, 22, 27, 27, 27,  3,  0,  4, 18], device='cuda:0')
step: 27
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2372, -0.6113,  ...,  0.4773, -0.0842,  1.5088],
        [ 1.2529, -1.8877, -0.2048,  ..., -2.3828,  0.6030, -0.8252],
        [ 1.9150, -0.2198,  0.7983,  ..., -1.9062, -1.3438, -0.5942],
        [-0.4097, -0.3013, -1.7148,  ..., -0.6567,  0.6489,  1.2695],
        [-1.1689,  2.2266, -0.0961,  ...,  0.6582,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.9695e-03, -1.3519e-02, -4.1237e-03,  ..., -3.6945e-03,
          4.6310e-03, -1.1917e-02],
        [ 4.9095e-03,  9.5367e-04,  4.6616e-03,  ...,  3.8055e-02,
          2.1255e-02, -3.2318e-02],
        [ 3.8853e-03, -7.9498e-03, -4.5896e-06,  ..., -3.8834e-03,
         -1.0090e-03,  4.7636e-04],
        [ 4.5700e-03, -8.6823e-03,  7.2241e-04,  ...,  8.1558e-03,
          1.6373e-02, -9.9869e-03],
        [ 1.7147e-03, -6.2981e-03,  7.6561e-03,  ..., -3.6469e-03,
          2.8305e-03, -1.1921e-03]], device='cuda:0', dtype=torch.float16)
Progress 4.12%, loss: 7.587018745286124, time 5.94s
loss: tensor(5.2954, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-10.8750,  -0.1868,  -7.7188,  ...,  -2.8125,  -4.9062,  -5.3750],
        [ -8.1016,  -0.6025,  -6.5391,  ...,  -1.2900,  -3.7578,  -4.0391],
        [ -9.1250,  -1.9971,  -6.2773,  ...,  -0.9336,  -3.2461,  -5.0273],
        ...,
        [ -7.9609,  -0.8984,  -7.1484,  ...,  -1.1172,  -3.3047,  -4.3984],
        [-11.1094,  -0.7363,  -7.3945,  ...,  -1.7363,  -3.0801,  -5.6406],
        [ -9.7031,  -0.2969,  -7.1094,  ...,  -2.0156,  -5.7344,  -4.7031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 20, 20, 27, 17, 27, 15, 26, 27, 18, 23, 27, 13, 20, 10, 14, 24, 27,
        15, 27,  3, 21,  7, 27, 27, 27,  3, 18, 27,  5,  0, 27,  7, 10, 27,  4,
         3, 20,  7, 15, 15, 10, 27,  1,  2, 27, 13,  1,  1, 27, 27, 15, 27,  3,
        15, 26, 27,  5, 27, 27, 11, 27,  7, 27], device='cuda:0')
step: 28
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2372, -0.6113,  ...,  0.4773, -0.0843,  1.5088],
        [ 1.2520, -1.8877, -0.2048,  ..., -2.3828,  0.6030, -0.8252],
        [ 1.9150, -0.2197,  0.7983,  ..., -1.9062, -1.3438, -0.5942],
        [-0.4097, -0.3010, -1.7148,  ..., -0.6567,  0.6489,  1.2695],
        [-1.1689,  2.2266, -0.0962,  ...,  0.6582,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.6736e-03,  5.0087e-03, -1.1606e-03,  ..., -1.3275e-02,
          6.7558e-03, -1.2421e-02],
        [ 3.5954e-03,  6.7902e-03,  1.4359e-02,  ..., -1.3664e-02,
         -1.0475e-02,  5.5504e-03],
        [ 1.7672e-03, -4.2801e-03,  1.2274e-03,  ..., -7.5111e-03,
          6.3467e-04,  5.2147e-03],
        [ 3.8853e-03, -2.5916e-04,  5.4073e-04,  ..., -6.6376e-04,
          1.2344e-02, -4.6501e-03],
        [-1.1444e-05, -8.0719e-03,  8.6975e-03,  ..., -3.1605e-03,
          6.6986e-03, -1.9398e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(6.0570, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -9.4062,  -0.7188,  -6.2188,  ...,  -2.9375,  -2.6250,  -4.5625],
        [-11.7656,  -0.0470,  -7.9531,  ...,  -5.3594,  -4.7031,  -6.9844],
        [-10.2891,  -0.9155,  -6.5703,  ...,  -0.8218,  -5.1641,  -6.4141],
        ...,
        [ -9.5625,  -0.2532,  -7.2227,  ...,  -3.0977,  -2.7227,  -4.3164],
        [ -9.5547,  -0.2124,  -7.3047,  ...,  -2.5566,  -4.2422,  -5.2109],
        [-11.0625,  -0.0599,  -7.2461,  ...,  -4.6523,  -6.1523,  -6.0898]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  7,  4,  0, 27, 27,  0, 13, 20, 27,  0,  1,  4,  0,  6,  4, 27, 20,
        13, 27, 10, 27, 22,  1, 20, 25, 20, 27, 15, 15, 18, 10,  4, 18,  4,  3,
        22,  4, 20, 27, 11, 27,  1, 10,  6, 27, 26, 26, 27, 10,  3,  3, 22, 14,
        27, 27, 25, 17,  0, 18,  0, 26, 27, 27], device='cuda:0')
step: 29
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2372, -0.6113,  ...,  0.4773, -0.0844,  1.5088],
        [ 1.2520, -1.8877, -0.2048,  ..., -2.3828,  0.6030, -0.8252],
        [ 1.9150, -0.2197,  0.7983,  ..., -1.9062, -1.3438, -0.5942],
        [-0.4097, -0.3010, -1.7148,  ..., -0.6567,  0.6484,  1.2695],
        [-1.1689,  2.2266, -0.0963,  ...,  0.6582,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0028,  0.0039,  0.0010,  ...,  0.0101,  0.0020, -0.0001],
        [ 0.0047, -0.0082, -0.0065,  ...,  0.0250,  0.0160, -0.0132],
        [ 0.0042, -0.0044, -0.0011,  ..., -0.0036, -0.0002,  0.0015],
        [-0.0033, -0.0039, -0.0054,  ...,  0.0021,  0.0028,  0.0053],
        [ 0.0017, -0.0023,  0.0034,  ..., -0.0011,  0.0025,  0.0019]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(5.5257, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-10.1484,  -0.3015,  -6.1133,  ...,  -3.4883,  -3.5508,  -3.8320],
        [ -8.7500,  -0.2532,  -8.2188,  ...,  -2.5664,  -3.1602,  -4.7539],
        [ -9.2969,  -0.3933,  -6.5820,  ...,  -2.2988,  -3.8320,  -4.8320],
        ...,
        [ -9.3125,  -0.4692,  -6.3750,  ...,  -2.0312,  -3.3438,  -5.4375],
        [ -9.7734,  -0.5386,  -5.5391,  ...,  -2.4141,  -3.5703,  -6.6953],
        [ -9.2266,  -0.9414,  -6.9414,  ...,  -1.9414,  -3.4102,  -6.9727]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  7,  1, 15, 27,  3, 10, 27, 25, 27, 27, 14, 27, 10, 13,  9, 27, 15,
        18, 24,  2, 27, 27, 27,  0, 13, 27,  2, 17, 27, 27,  8, 15, 13, 27,  2,
        11, 27,  2, 24, 13, 15, 27,  0, 27,  2, 22,  0, 10, 27, 27, 27, 27,  0,
        27, 26, 20, 27,  7, 17,  2, 27, 17, 27], device='cuda:0')
step: 30
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2372, -0.6113,  ...,  0.4773, -0.0844,  1.5088],
        [ 1.2520, -1.8877, -0.2048,  ..., -2.3828,  0.6030, -0.8252],
        [ 1.9150, -0.2196,  0.7983,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4099, -0.3010, -1.7148,  ..., -0.6567,  0.6484,  1.2695],
        [-1.1689,  2.2266, -0.0963,  ...,  0.6582,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.3400e-03, -4.0131e-03, -1.3062e-02,  ...,  3.1185e-03,
          1.1597e-02, -7.0915e-03],
        [ 6.3324e-04, -1.6298e-03, -1.7242e-03,  ..., -5.7907e-03,
         -3.2005e-03,  3.1395e-03],
        [ 2.1820e-03, -8.7509e-03, -1.7052e-03,  ...,  3.8147e-03,
         -3.5610e-03, -6.8016e-03],
        [ 2.4815e-03, -4.9515e-03, -3.1185e-03,  ...,  9.3317e-04,
         -4.6825e-04, -3.1395e-03],
        [ 4.9257e-04, -1.3819e-03, -1.4782e-03,  ...,  2.4796e-05,
          4.8971e-04, -4.6921e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(5.6012, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.8359, -2.2715, -6.3984,  ..., -3.2109, -2.5234, -4.6484],
        [-9.4062, -1.0029, -6.7539,  ..., -1.0977, -3.0352, -4.1914],
        [-9.7656, -1.9814, -7.0742,  ..., -0.7627, -4.0742, -4.2305],
        ...,
        [-9.0781, -0.1115, -7.2031,  ..., -3.2051, -5.6719, -7.1758],
        [-9.9219, -0.4233, -6.1094,  ..., -2.6426, -3.3613, -4.0156],
        [-9.0859, -0.4756, -6.1641,  ..., -2.0391, -3.6641, -4.7266]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15,  9, 27,  4,  2, 18, 27,  4, 15,  6, 27,  0, 18, 15, 10,  7,  2,  8,
        17, 27,  4, 27,  4,  6, 27, 10,  3,  7, 15,  3,  0, 27, 17, 27, 17, 27,
        24, 17, 20, 27,  0,  9, 15, 27, 27, 10, 27, 17, 27,  4,  5, 15, 27, 15,
        27,  1, 27, 24,  4, 27, 27, 27,  6, 27], device='cuda:0')
step: 31
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2372, -0.6113,  ...,  0.4773, -0.0844,  1.5088],
        [ 1.2520, -1.8877, -0.2048,  ..., -2.3828,  0.6030, -0.8252],
        [ 1.9150, -0.2196,  0.7983,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4099, -0.3010, -1.7148,  ..., -0.6567,  0.6484,  1.2695],
        [-1.1689,  2.2266, -0.0964,  ...,  0.6582,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0002, -0.0153, -0.0069,  ..., -0.0016,  0.0013, -0.0032],
        [ 0.0032,  0.0157,  0.0171,  ...,  0.0071, -0.0017, -0.0110],
        [ 0.0033, -0.0029, -0.0057,  ..., -0.0037,  0.0079, -0.0019],
        [ 0.0010, -0.0081, -0.0054,  ..., -0.0006, -0.0028, -0.0020],
        [-0.0001, -0.0060,  0.0036,  ...,  0.0013,  0.0011,  0.0013]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(5.0240, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-8.4766, -0.4622, -5.1953,  ..., -2.2910, -3.7754, -5.5391],
        [-9.3047, -1.5537, -7.5547,  ..., -1.7715, -2.9277, -4.4922],
        [-7.1797, -0.4905, -5.4609,  ..., -2.6465, -3.3652, -3.2090],
        ...,
        [-8.7188, -0.2832, -6.5039,  ..., -3.4395, -4.7812, -2.7520],
        [-9.4531, -0.4211, -5.8594,  ..., -3.7344, -4.3594, -2.7031],
        [-8.2422, -0.3350, -6.6484,  ..., -2.4902, -4.2734, -5.4609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 18, 27, 27, 27, 25, 27,  2, 17, 18, 20,  6, 21,  7,  5,  3, 27,  4,
        27,  8, 17,  9,  7,  5, 15, 20, 27,  3, 27, 27, 17, 27, 27,  7, 27, 27,
        11, 18,  9, 26, 27, 27,  4, 27, 27, 27,  6,  2, 15, 20,  3, 27, 15, 20,
         1,  7,  1,  0, 27, 27, 27, 27, 27,  5], device='cuda:0')
step: 32
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2372, -0.6113,  ...,  0.4773, -0.0845,  1.5088],
        [ 1.2520, -1.8877, -0.2048,  ..., -2.3828,  0.6030, -0.8252],
        [ 1.9150, -0.2196,  0.7983,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4099, -0.3008, -1.7148,  ..., -0.6567,  0.6484,  1.2695],
        [-1.1689,  2.2266, -0.0964,  ...,  0.6582,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0049,  0.0581, -0.0134,  ..., -0.0480,  0.0056, -0.0147],
        [ 0.0046,  0.0083,  0.0065,  ..., -0.0078,  0.0018, -0.0102],
        [ 0.0029, -0.0053, -0.0017,  ..., -0.0013,  0.0045, -0.0065],
        [-0.0024, -0.0089, -0.0091,  ...,  0.0052, -0.0050,  0.0037],
        [ 0.0017, -0.0067,  0.0010,  ..., -0.0005,  0.0066, -0.0022]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(4.9215, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -8.3438,  -0.8452,  -5.2812,  ...,  -2.0645,  -3.0332,  -3.5957],
        [ -8.4141,  -1.9951,  -5.9961,  ...,  -1.1504,  -3.5566,  -4.9023],
        [-10.1406,  -0.2690,  -7.7695,  ...,  -2.5820,  -3.9883,  -4.2383],
        ...,
        [ -9.6250,  -1.1602,  -6.1289,  ...,  -3.2539,  -2.7852,  -1.0654],
        [ -9.2500,  -0.1224,  -7.0586,  ...,  -3.7168,  -4.5273,  -5.4961],
        [ -8.6250,  -0.4568,  -5.2695,  ...,  -2.3008,  -3.8008,  -4.1758]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1,  1,  9, 27, 27,  4, 27, 27, 22,  2, 27,  5,  6,  7,  4, 27, 27, 15,
         4,  4, 10, 17, 27,  0,  1,  1,  5, 25, 27, 27, 27, 27, 27,  7, 27, 10,
         0, 15, 17, 15,  0, 17, 10, 27, 13,  1, 20, 27, 24, 27, 15, 10, 27,  9,
        24,  1, 20,  1, 15, 27, 27,  7, 10, 27], device='cuda:0')
step: 33
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2373, -0.6113,  ...,  0.4773, -0.0845,  1.5088],
        [ 1.2520, -1.8877, -0.2048,  ..., -2.3828,  0.6030, -0.8252],
        [ 1.9150, -0.2195,  0.7983,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4099, -0.3008, -1.7148,  ..., -0.6567,  0.6484,  1.2695],
        [-1.1689,  2.2266, -0.0965,  ...,  0.6582,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0072, -0.0106, -0.0090,  ..., -0.0038,  0.0004, -0.0093],
        [ 0.0035, -0.0007,  0.0009,  ...,  0.0037, -0.0007, -0.0052],
        [ 0.0028, -0.0059, -0.0028,  ...,  0.0038,  0.0013, -0.0040],
        [ 0.0071, -0.0045, -0.0041,  ...,  0.0014, -0.0032, -0.0068],
        [ 0.0050, -0.0059, -0.0017,  ..., -0.0002,  0.0013, -0.0014]],
       device='cuda:0', dtype=torch.float16)
Progress 5.01%, loss: 7.201803614111507, time 5.09s
loss: tensor(4.6448, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -8.1875,  -1.6260,  -6.6562,  ...,  -2.2832,  -2.7520,  -3.9395],
        [-11.5391,  -0.3826,  -6.6328,  ...,  -2.5391,  -4.6328,  -3.6328],
        [ -9.5547,  -1.3027,  -6.4297,  ...,  -1.1465,  -2.3027,  -4.5547],
        ...,
        [ -9.1172,  -0.4131,  -6.1641,  ...,  -3.1953,  -2.1953,  -4.7266],
        [ -6.9062,  -0.5635,  -7.9688,  ...,  -2.3438,  -2.5957,  -5.1875],
        [ -9.9922,  -0.1809,  -7.3672,  ...,  -2.7129,  -4.8984,  -5.9922]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 24, 26, 27,  1,  6, 27,  0, 10, 27,  4, 24, 27, 18, 27,  7, 27, 13,
        27, 10, 27, 27, 27,  4,  0,  0, 17, 27, 27, 27,  0,  3,  0, 27,  6, 14,
        27, 12,  3, 25, 27,  7,  9, 27,  1, 25, 27, 25, 26, 17, 27, 27,  3,  5,
        10, 22, 10, 27, 15, 27, 15, 13, 27, 27], device='cuda:0')
step: 34
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2373, -0.6113,  ...,  0.4775, -0.0846,  1.5088],
        [ 1.2520, -1.8877, -0.2048,  ..., -2.3828,  0.6030, -0.8252],
        [ 1.9150, -0.2195,  0.7983,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4099, -0.3008, -1.7148,  ..., -0.6567,  0.6484,  1.2695],
        [-1.1689,  2.2266, -0.0966,  ...,  0.6582,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0060, -0.0206, -0.0038,  ...,  0.0066,  0.0018, -0.0086],
        [ 0.0028,  0.0052,  0.0018,  ..., -0.0118, -0.0002,  0.0025],
        [ 0.0047, -0.0054, -0.0022,  ..., -0.0021,  0.0034,  0.0008],
        [ 0.0012, -0.0095, -0.0047,  ...,  0.0067, -0.0046, -0.0045],
        [ 0.0006, -0.0040,  0.0025,  ...,  0.0002,  0.0028,  0.0004]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(4.0799, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-10.1172,  -0.2295,  -6.0117,  ...,  -5.2305,  -5.5742,  -2.5723],
        [ -7.5352,  -1.4102,  -6.0039,  ...,  -1.6602,  -2.1289,  -3.3789],
        [ -7.8906,  -0.6401,  -6.2031,  ...,  -4.3906,  -3.0781,  -4.9531],
        ...,
        [ -8.4844,  -0.7954,  -6.6719,  ...,  -2.8574,  -3.3574,  -4.0469],
        [-11.5391,  -0.7271,  -7.8203,  ...,  -1.5391,  -3.4766,  -5.3828],
        [-10.3516,  -2.7734,  -7.2578,  ...,  -1.2412,  -4.2109,  -6.9766]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15,  4, 18, 10,  2, 15, 15,  0, 26, 18, 27, 26, 27, 27,  0, 18, 25,  3,
        14, 27, 27, 27, 27, 27, 27,  1, 27, 15, 27,  7,  0, 15, 26,  3, 27, 27,
         3, 22, 15, 18, 27, 26,  4, 27,  5, 27,  0, 27, 26,  2,  3, 27, 20, 27,
        15, 18, 27, 10, 27, 18, 15,  7, 17, 13], device='cuda:0')
step: 35
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2373, -0.6113,  ...,  0.4775, -0.0847,  1.5088],
        [ 1.2520, -1.8877, -0.2048,  ..., -2.3828,  0.6030, -0.8247],
        [ 1.9150, -0.2194,  0.7983,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4099, -0.3008, -1.7148,  ..., -0.6567,  0.6484,  1.2695],
        [-1.1689,  2.2266, -0.0966,  ...,  0.6582,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.0870e-05, -2.5101e-03, -4.4632e-03,  ...,  8.7967e-03,
          4.2534e-03, -2.1439e-03],
        [ 4.7455e-03,  1.2627e-03,  1.6508e-03,  ...,  1.5569e-04,
          3.2973e-04,  4.8876e-04],
        [ 4.4327e-03, -4.4289e-03, -2.6455e-03,  ..., -4.7455e-03,
          4.4274e-04,  1.7576e-03],
        [ 2.0638e-03, -6.6223e-03, -2.4509e-04,  ...,  4.1275e-03,
         -3.6860e-04, -1.7672e-03],
        [ 1.2505e-04, -4.2877e-03,  6.2332e-03,  ..., -2.5654e-03,
          1.9341e-03,  1.0576e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(4.2990, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-8.4531, -0.3303, -4.9258,  ..., -2.9551, -4.2695, -3.6738],
        [-7.8828, -1.3203, -4.4453,  ..., -1.7578, -4.1328, -4.6953],
        [-8.7969, -1.0498, -6.5820,  ..., -2.2070, -3.0820, -2.2070],
        ...,
        [-7.8398, -3.1855, -7.2773,  ..., -0.6538, -2.8730, -4.4336],
        [-9.1797, -0.7422, -5.3984,  ..., -2.3672, -2.2422, -2.9922],
        [-8.1875, -1.8096, -7.4336,  ..., -0.9971, -1.8418, -2.6543]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  2, 20, 27, 25, 17, 25,  1,  7,  2, 17,  9,  0,  5, 18, 27,  7,  3,
        15, 22, 18, 27, 27,  4, 13,  0,  7,  9, 27,  3, 27,  6,  3, 26, 27, 27,
         3, 26, 27, 20,  6, 26, 27, 27, 17, 10, 27, 18,  0,  4, 27,  1, 11, 27,
        15, 27,  3, 27, 27,  1, 22, 18, 27, 15], device='cuda:0')
step: 36
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2373, -0.6113,  ...,  0.4775, -0.0847,  1.5088],
        [ 1.2520, -1.8887, -0.2048,  ..., -2.3828,  0.6030, -0.8247],
        [ 1.9150, -0.2194,  0.7983,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4099, -0.3005, -1.7148,  ..., -0.6572,  0.6484,  1.2695],
        [-1.1689,  2.2266, -0.0966,  ...,  0.6582,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0028, -0.0166, -0.0116,  ..., -0.0026, -0.0040, -0.0018],
        [-0.0002,  0.0010,  0.0020,  ..., -0.0077,  0.0001, -0.0020],
        [ 0.0016, -0.0015, -0.0009,  ..., -0.0008,  0.0014,  0.0001],
        [ 0.0036, -0.0014, -0.0103,  ..., -0.0039,  0.0027, -0.0071],
        [ 0.0036, -0.0036, -0.0012,  ..., -0.0018,  0.0001, -0.0024]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(4.4249, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-8.6641, -0.5693, -5.2266,  ..., -1.7266, -3.1621, -3.4746],
        [-7.6328, -3.1777, -6.1484,  ..., -2.2422, -3.3965, -4.6953],
        [-8.5469, -3.3594, -7.3281,  ..., -0.6079, -1.9521, -4.9219],
        ...,
        [-7.5156, -1.0449, -5.8867,  ..., -1.2949, -3.7324, -2.4199],
        [-6.5469, -1.1416, -5.1094,  ..., -2.9219, -2.0781, -4.2656],
        [-7.4062, -0.7197, -5.9375,  ..., -4.1875, -3.6582, -3.5645]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([12,  0, 27, 27,  9, 15, 17, 27, 25, 18, 20, 18,  5, 18, 27, 26, 27,  5,
         3,  8, 27,  0,  6,  7,  8,  6, 27, 27, 15,  7, 27, 15,  3, 27, 27,  1,
        17, 27, 15,  0, 27,  0,  3, 10, 15,  3, 10, 20,  7, 27,  3, 27, 10, 27,
         5,  5,  5,  2, 27, 27, 17,  3,  8,  0], device='cuda:0')
step: 37
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2373, -0.6113,  ...,  0.4775, -0.0847,  1.5088],
        [ 1.2520, -1.8887, -0.2048,  ..., -2.3828,  0.6030, -0.8247],
        [ 1.9141, -0.2192,  0.7988,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4102, -0.3005, -1.7148,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1689,  2.2285, -0.0967,  ...,  0.6582,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.6760e-03,  5.7640e-03, -6.6910e-03,  ...,  8.3847e-03,
          3.8929e-03, -2.3770e-04],
        [-5.1003e-03,  5.5389e-03, -9.7046e-03,  ..., -1.8723e-02,
         -6.5308e-03,  7.8049e-03],
        [ 4.2992e-03,  1.3256e-03, -7.3929e-03,  ...,  5.5771e-03,
          5.0049e-03, -6.5460e-03],
        [-3.6278e-03,  2.6655e-04, -7.8888e-03,  ...,  5.9724e-05,
         -4.7264e-03,  1.0443e-03],
        [ 1.6079e-03, -4.2496e-03,  8.3160e-04,  ...,  2.8515e-04,
         -8.6164e-04,  1.9627e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(5.0477, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -9.5078,  -1.0723,  -7.0391,  ...,  -2.6973,  -2.8223,  -1.3848],
        [ -9.5234,  -0.4265,  -5.1445,  ...,  -2.6445,  -4.5508,  -2.7070],
        [ -8.9766,  -0.6973,  -5.1328,  ...,  -4.1328,  -2.6035,  -3.3535],
        ...,
        [ -9.6562,  -0.5947,  -6.7500,  ...,  -2.6250,  -3.1582,  -2.8438],
        [ -8.6641,  -0.8525,  -6.7266,  ...,  -1.9775,  -2.7910,  -3.1035],
        [-10.5312,  -0.3740,  -3.5918,  ...,  -7.9219,  -8.3750,  -6.6875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  6, 22, 15, 27,  0, 15, 27,  6, 22, 27, 27,  4, 17, 27, 27,  1,  1,
         6, 10, 27,  0, 22, 22, 27, 18, 27, 27, 27,  0, 27,  0,  6,  6,  7, 27,
        13, 27, 26,  1, 20,  7,  3,  0, 15,  3, 22,  3,  6, 27, 23, 27, 27, 17,
        18,  4,  6,  1,  1, 27, 27, 27,  2, 27], device='cuda:0')
step: 38
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2373, -0.6113,  ...,  0.4775, -0.0848,  1.5088],
        [ 1.2520, -1.8887, -0.2048,  ..., -2.3828,  0.6030, -0.8247],
        [ 1.9141, -0.2192,  0.7988,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4102, -0.3005, -1.7139,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1689,  2.2285, -0.0967,  ...,  0.6582,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.6260e-03,  4.8676e-03, -4.6310e-03,  ..., -7.4883e-03,
          5.6610e-03, -8.7051e-03],
        [ 4.6082e-03,  1.1993e-02, -3.5133e-03,  ...,  1.2293e-03,
          3.7594e-03, -1.5579e-02],
        [ 5.8098e-03, -2.9850e-03, -1.8120e-03,  ..., -2.6016e-03,
          2.7637e-03,  1.5259e-03],
        [ 2.8362e-03, -4.7112e-03, -1.5249e-03,  ...,  1.6956e-03,
          3.3569e-03, -3.9291e-03],
        [ 5.8632e-03, -3.8910e-03, -1.0204e-03,  ..., -5.1422e-03,
         -8.8930e-05,  6.9618e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(4.4077, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -8.8125,  -1.4375,  -5.0938,  ...,  -2.2812,  -2.0938,  -2.3750],
        [ -9.4141,  -1.2881,  -4.1328,  ...,  -1.4434,  -3.9434,  -3.2871],
        [ -6.2969,  -1.2051,  -6.1406,  ...,  -2.6426,  -2.0488,  -2.5488],
        ...,
        [-10.5625,  -0.3289,  -5.9375,  ...,  -5.4062,  -2.5469,  -2.7969],
        [ -9.3906,  -0.6406,  -6.6406,  ...,  -3.2344,  -2.6406,  -2.1719],
        [ -9.4297,  -2.3320,  -6.8633,  ...,  -1.1133,  -2.3633,  -2.5820]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26, 25, 15, 10, 27, 27, 27, 24, 27, 25,  6,  3, 27,  3, 27, 27,  9, 19,
        27, 27,  8, 22, 27,  8,  0, 10,  0,  6, 27, 15, 15,  1, 17, 20, 20,  6,
        27,  4, 26, 27, 10, 27, 27, 10,  1, 25, 26,  4,  3,  0,  1,  4,  9, 11,
         7, 27, 15,  8,  7, 20, 22, 27, 23, 20], device='cuda:0')
step: 39
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2373, -0.6113,  ...,  0.4775, -0.0848,  1.5098],
        [ 1.2520, -1.8887, -0.2048,  ..., -2.3828,  0.6030, -0.8247],
        [ 1.9141, -0.2192,  0.7988,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4102, -0.3005, -1.7139,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1689,  2.2285, -0.0967,  ...,  0.6582,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0015, -0.0131, -0.0058,  ..., -0.0036,  0.0008, -0.0053],
        [ 0.0015,  0.0007,  0.0012,  ..., -0.0056, -0.0016, -0.0033],
        [ 0.0015, -0.0018,  0.0002,  ..., -0.0009,  0.0001, -0.0019],
        [ 0.0019, -0.0044,  0.0042,  ...,  0.0033,  0.0001, -0.0017],
        [-0.0005, -0.0030,  0.0050,  ...,  0.0010,  0.0016, -0.0005]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(3.9579, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-8.0312, -0.8135, -4.5625,  ..., -3.8125, -2.7188, -2.2832],
        [-9.0234, -2.0547, -4.9297,  ..., -1.5850, -1.6787, -2.8047],
        [-9.4609, -1.9326, -5.4023,  ..., -2.5879, -2.0566, -3.7129],
        ...,
        [-7.2227, -1.3486, -7.2227,  ..., -1.3486, -2.1934, -3.7871],
        [-8.8359, -1.0557, -5.8047,  ..., -2.3691, -1.8682, -1.7119],
        [-9.3125, -0.2216, -6.7852,  ..., -3.3164, -4.0977, -3.9707]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 27, 19, 27, 27,  3,  4, 27, 25, 10, 27, 18,  9, 14,  1, 27, 22, 15,
         2,  3, 27, 27, 10, 18,  2,  4, 27, 26, 27,  7, 27, 27,  3, 12, 10, 21,
        27, 18,  1, 17, 10, 27,  1,  9, 27, 27, 26, 27, 27,  4, 27, 26, 27, 15,
        11, 10, 15, 15, 27, 27, 27, 26,  6,  1], device='cuda:0')
step: 40
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2373, -0.6113,  ...,  0.4775, -0.0848,  1.5098],
        [ 1.2520, -1.8887, -0.2048,  ..., -2.3828,  0.6030, -0.8247],
        [ 1.9141, -0.2192,  0.7988,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4102, -0.3003, -1.7139,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1689,  2.2285, -0.0968,  ...,  0.6582,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.7967e-03, -2.0508e-02, -1.5039e-03,  ...,  3.2444e-03,
         -1.9474e-03, -1.2802e-02],
        [ 1.9646e-03, -3.1071e-03,  2.0050e-02,  ...,  1.2115e-02,
         -6.5498e-03, -8.1635e-03],
        [ 5.4703e-03,  4.4346e-04, -3.7937e-03,  ..., -3.4447e-03,
          8.0643e-03,  1.4763e-03],
        [ 1.4315e-03, -9.4557e-04,  2.9564e-03,  ...,  5.0783e-04,
         -2.3041e-03, -2.8324e-03],
        [ 4.2381e-03, -2.6703e-03, -2.7537e-04,  ..., -2.4776e-03,
         -3.8981e-05, -1.4153e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(4.4044, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-7.7266, -1.9131, -4.7266,  ..., -1.9443, -2.5059, -1.2568],
        [-8.3438, -0.8140, -5.7812,  ..., -2.7832, -3.2520, -2.6582],
        [-8.4609, -1.3994, -4.6172,  ..., -1.9619, -4.7734, -2.3047],
        ...,
        [-8.1406, -1.2852, -5.2539,  ..., -3.6914, -1.7852, -1.5039],
        [-7.6758, -0.5190, -5.8945,  ..., -4.0820, -3.4258, -1.8623],
        [-7.4922, -2.0547, -5.2422,  ..., -1.8984, -1.7422, -2.8359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 5, 27,  9, 14,  3,  6,  4, 27,  7,  4, 27, 21,  0,  2, 27,  4, 27, 23,
        27,  4,  0, 25, 13, 27,  5, 27, 27,  2, 13, 27,  3,  6, 27,  4,  2, 18,
         5, 27, 15,  7, 18,  7, 24,  1, 27, 20, 17,  0, 14, 25,  7,  4, 27,  9,
        27, 18, 27, 27, 18,  0,  8,  7, 27, 27], device='cuda:0')
step: 41
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2373, -0.6113,  ...,  0.4775, -0.0848,  1.5098],
        [ 1.2520, -1.8887, -0.2048,  ..., -2.3828,  0.6030, -0.8247],
        [ 1.9141, -0.2191,  0.7988,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4102, -0.3003, -1.7139,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1689,  2.2285, -0.0968,  ...,  0.6582,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0008, -0.0404, -0.0162,  ...,  0.0287,  0.0055, -0.0004],
        [ 0.0002,  0.0004,  0.0033,  ..., -0.0076, -0.0077,  0.0093],
        [ 0.0027, -0.0012, -0.0018,  ..., -0.0029,  0.0002,  0.0016],
        [ 0.0042, -0.0068, -0.0044,  ..., -0.0013,  0.0016, -0.0128],
        [ 0.0026, -0.0025,  0.0006,  ..., -0.0034,  0.0003, -0.0001]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(3.9147, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-8.3906, -1.3916, -4.4219,  ..., -2.3906, -2.4219, -3.0488],
        [-8.9531, -0.2017, -5.8906,  ..., -4.0781, -4.5469, -2.7012],
        [-7.9375, -2.0312, -6.1250,  ..., -2.8750, -2.7188, -1.7822],
        ...,
        [-8.9688, -1.3164, -6.4102,  ..., -1.5986, -3.1289, -1.5664],
        [-7.6914, -2.0059, -6.1328,  ..., -2.1621, -1.4746, -2.2246],
        [-7.3281, -1.0469, -4.2812,  ..., -2.8906, -3.3594, -2.7656]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  0,  3,  9, 27,  3, 15, 20,  0,  4, 18,  4, 27, 18,  7,  8, 27,
        27, 22, 27, 26,  3, 22, 27, 27,  5, 27, 27, 15,  8, 27, 27, 25, 26,  1,
        21, 15, 18,  9, 15, 27,  8, 27, 26,  6, 27,  9, 15,  7, 15,  0,  4,  2,
         9, 25,  4,  7, 24, 25,  8, 27, 27, 27], device='cuda:0')
step: 42
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2373, -0.6108,  ...,  0.4775, -0.0849,  1.5098],
        [ 1.2520, -1.8887, -0.2048,  ..., -2.3828,  0.6030, -0.8247],
        [ 1.9141, -0.2191,  0.7988,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4102, -0.3003, -1.7139,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1689,  2.2285, -0.0969,  ...,  0.6582,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0017,  0.0070, -0.0052,  ..., -0.0021,  0.0009, -0.0040],
        [ 0.0047,  0.0041, -0.0049,  ..., -0.0081, -0.0034, -0.0050],
        [ 0.0034, -0.0085,  0.0005,  ..., -0.0059, -0.0076, -0.0021],
        [ 0.0037, -0.0039,  0.0013,  ..., -0.0015, -0.0014, -0.0011],
        [ 0.0016, -0.0021,  0.0004,  ..., -0.0014,  0.0013,  0.0001]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(4.3104, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-7.7031, -0.9229, -5.0156,  ..., -3.3594, -3.5781, -2.9531],
        [-7.4180, -1.3564, -5.8867,  ..., -2.6074, -3.2305, -1.4814],
        [-6.8828, -0.7275, -5.8203,  ..., -1.5088, -3.1348, -3.4766],
        ...,
        [-7.1523, -1.5605, -5.7461,  ..., -3.1543, -2.9355, -1.4355],
        [-7.4180, -1.1826, -4.9961,  ..., -2.6504, -3.1211, -1.6836],
        [-8.1875, -2.0312, -5.2500,  ..., -3.2188, -4.0938, -0.5625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 8, 10,  3,  3, 15, 27, 27,  4, 10,  0, 27, 12, 17,  4,  7,  5, 27,  7,
        26, 27,  8, 18, 11,  1,  9, 20,  0, 18,  5,  9, 27,  7, 27, 20, 27,  4,
        27,  4,  1, 17,  3, 27, 15, 17,  0,  9,  1, 10,  5, 27, 25,  1, 10, 21,
        13,  7,  9,  5, 17,  3, 24,  0, 15, 10], device='cuda:0')
step: 43
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2373, -0.6108,  ...,  0.4775, -0.0849,  1.5098],
        [ 1.2520, -1.8887, -0.2048,  ..., -2.3828,  0.6030, -0.8247],
        [ 1.9141, -0.2191,  0.7988,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4102, -0.3003, -1.7139,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1689,  2.2285, -0.0969,  ...,  0.6582,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0023,  0.0044, -0.0015,  ..., -0.0051,  0.0039, -0.0049],
        [ 0.0014, -0.0039, -0.0008,  ..., -0.0071, -0.0089, -0.0066],
        [ 0.0095, -0.0184, -0.0064,  ..., -0.0084, -0.0093,  0.0005],
        [ 0.0012, -0.0063,  0.0012,  ...,  0.0012,  0.0015, -0.0005],
        [ 0.0011, -0.0015, -0.0009,  ..., -0.0007, -0.0007, -0.0006]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(3.6537, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-10.8984,  -1.1484,  -5.5234,  ...,  -1.4297,  -5.9297,  -2.8672],
        [ -8.9766,  -2.8203,  -6.8203,  ...,  -0.8516,  -3.3203,  -3.0391],
        [ -7.6875,  -2.3438,  -8.0625,  ...,  -0.8442,  -1.5947,  -1.8760],
        ...,
        [ -6.0938,  -2.9688,  -5.5938,  ...,  -3.1250,  -2.0312,  -2.4688],
        [ -9.1562,  -1.4248,  -5.9570,  ...,  -4.1445,  -2.7051,  -0.6436],
        [ -8.5703,  -0.8525,  -5.3203,  ...,  -2.0723,  -3.1660,  -4.2266]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25,  2, 27, 20, 27, 27, 24, 15, 20, 27, 15,  7, 27, 25,  6, 27, 27,  6,
        27, 18, 10, 27,  4,  3,  3, 12, 27, 27, 27,  0, 27, 15, 18, 17, 15, 15,
        27,  7, 27, 27, 15, 27, 27, 10, 27, 25, 10, 27, 15,  4,  4, 27,  4, 13,
        27, 13, 27,  7,  6,  7, 12,  1, 27, 27], device='cuda:0')
step: 44
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2373, -0.6108,  ...,  0.4775, -0.0850,  1.5098],
        [ 1.2520, -1.8887, -0.2048,  ..., -2.3828,  0.6030, -0.8247],
        [ 1.9141, -0.2190,  0.7988,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4102, -0.3003, -1.7139,  ..., -0.6572,  0.6479,  1.2705],
        [-1.1699,  2.2285, -0.0969,  ...,  0.6582,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0081,  0.0360, -0.0004,  ..., -0.0330,  0.0121, -0.0146],
        [-0.0020, -0.0062, -0.0080,  ..., -0.0111,  0.0008, -0.0080],
        [ 0.0081, -0.0062,  0.0030,  ..., -0.0149, -0.0041,  0.0045],
        [ 0.0043, -0.0084, -0.0002,  ...,  0.0017,  0.0125, -0.0088],
        [ 0.0018, -0.0047,  0.0028,  ..., -0.0030,  0.0045, -0.0068]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(3.9172, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-8.7266, -1.6914, -5.2227,  ..., -1.8789, -3.0352, -1.8789],
        [-7.5703, -2.3496, -6.0703,  ..., -2.6621, -2.4434, -0.7568],
        [-8.6797, -1.3066, -6.0898,  ..., -2.1504, -2.4004, -1.6504],
        ...,
        [-9.4531, -2.0508, -4.8789,  ..., -4.1133, -4.5039, -0.3000],
        [-6.7773, -4.0273, -6.7773,  ..., -0.9971, -2.5605, -3.6836],
        [-8.8984, -2.6445, -7.0195,  ..., -5.2070, -4.5820, -0.1757]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 17, 27, 15, 20, 18,  5,  7, 14, 27, 27,  6, 15,  1, 27, 17,  1,  0,
        27, 11, 27,  1, 27, 27,  8, 17, 21,  4, 10, 18, 18, 27,  9, 18, 13,  4,
        27,  5, 15,  4,  5,  7, 13,  0, 15,  0,  9, 27, 27, 27,  6,  0,  9,  1,
        27,  2, 27,  5, 27,  2, 27, 25, 18,  4], device='cuda:0')
step: 45
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2373, -0.6108,  ...,  0.4775, -0.0850,  1.5098],
        [ 1.2520, -1.8887, -0.2048,  ..., -2.3828,  0.6030, -0.8247],
        [ 1.9141, -0.2190,  0.7988,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4104, -0.3000, -1.7139,  ..., -0.6572,  0.6479,  1.2705],
        [-1.1699,  2.2285, -0.0969,  ...,  0.6582,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0028, -0.0115, -0.0004,  ...,  0.0091, -0.0016, -0.0027],
        [ 0.0026,  0.0025, -0.0021,  ...,  0.0045,  0.0040, -0.0058],
        [ 0.0010,  0.0039, -0.0022,  ..., -0.0011,  0.0044, -0.0004],
        [ 0.0030, -0.0028,  0.0008,  ...,  0.0016, -0.0028,  0.0048],
        [-0.0010, -0.0023,  0.0026,  ..., -0.0008, -0.0008,  0.0045]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(4.0567, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -8.8359,  -2.3652,  -6.0195,  ...,  -1.4277,  -3.3652,  -3.5215],
        [-10.1250,  -2.9062,  -7.2500,  ...,  -2.5312,  -3.8125,  -0.2812],
        [ -9.1953,  -1.3818,  -6.4141,  ...,  -2.9434,  -2.6309,  -1.1631],
        ...,
        [ -9.7266,  -3.2090,  -5.9570,  ...,  -0.8330,  -3.7246,  -3.5352],
        [ -5.9570,  -2.9746,  -5.6484,  ...,  -2.8965,  -3.0371,  -3.4434],
        [ -6.6406,  -0.8271,  -5.7656,  ...,  -2.2324,  -4.0781,  -2.8281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 10,  6, 27,  3,  3,  3, 27,  9, 25,  6,  0, 21, 27,  8, 10,  7, 15,
         5, 27, 27, 23,  3,  0,  4, 27,  2,  7, 27,  8,  0, 22,  3, 27,  5,  4,
        17,  0, 27, 27,  0,  7, 27,  0, 27,  3, 18, 20, 27, 27, 25, 25, 20, 27,
        27, 10, 18, 27, 26, 10, 20, 27, 15, 11], device='cuda:0')
step: 46
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2373, -0.6108,  ...,  0.4775, -0.0850,  1.5098],
        [ 1.2520, -1.8887, -0.2048,  ..., -2.3828,  0.6030, -0.8247],
        [ 1.9141, -0.2190,  0.7988,  ..., -1.9053, -1.3438, -0.5938],
        [-0.4104, -0.3000, -1.7139,  ..., -0.6572,  0.6479,  1.2705],
        [-1.1699,  2.2285, -0.0970,  ...,  0.6582,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0003,  0.0170,  0.0015,  ..., -0.0081,  0.0051, -0.0061],
        [-0.0028,  0.0579,  0.0701,  ...,  0.1091, -0.0066, -0.0771],
        [ 0.0045,  0.0018, -0.0023,  ..., -0.0032,  0.0027, -0.0039],
        [ 0.0043, -0.0004,  0.0065,  ...,  0.0004, -0.0061, -0.0049],
        [ 0.0048, -0.0024,  0.0016,  ..., -0.0015, -0.0002, -0.0004]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(3.6484, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-7.6445, -1.5039, -5.8633,  ..., -2.6602, -4.0352, -0.7534],
        [-8.5859, -2.7773, -6.6836,  ..., -2.6211, -2.4023, -0.9648],
        [-8.6016, -2.9805, -6.9180,  ..., -0.4485, -3.2598, -2.6035],
        ...,
        [-8.2344, -1.9697, -5.5938,  ..., -1.8447, -3.0938, -1.8447],
        [-5.9961, -1.5410, -4.0430,  ..., -2.9473, -2.4316, -1.6035],
        [-6.7891, -1.3838, -6.2578,  ..., -1.6025, -2.6660, -2.9766]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6, 15,  4, 25, 27, 27,  6, 14,  1, 24, 27, 27,  2, 18,  3, 27,  9, 27,
        22,  6, 25,  0,  7, 18, 15, 13, 27, 11,  7, 27, 27, 20, 27, 27,  7, 27,
        27, 11, 27, 27, 25, 27, 27, 15, 27, 18, 27, 11, 27, 27, 27, 22, 27, 20,
        17, 10, 27,  2, 27, 22, 27, 27, 27, 27], device='cuda:0')
step: 47
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2373, -0.6108,  ...,  0.4775, -0.0851,  1.5098],
        [ 1.2520, -1.8887, -0.2048,  ..., -2.3828,  0.6030, -0.8247],
        [ 1.9141, -0.2189,  0.7988,  ..., -1.9053, -1.3438, -0.5938],
        [-0.4104, -0.3000, -1.7139,  ..., -0.6572,  0.6479,  1.2705],
        [-1.1699,  2.2285, -0.0970,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0012, -0.0033,  0.0016,  ..., -0.0018,  0.0023, -0.0019],
        [-0.0056,  0.0165,  0.0040,  ..., -0.0249, -0.0042,  0.0101],
        [ 0.0032, -0.0013, -0.0021,  ..., -0.0002,  0.0037,  0.0026],
        [-0.0015,  0.0035, -0.0061,  ..., -0.0042, -0.0046, -0.0054],
        [ 0.0019,  0.0006, -0.0019,  ...,  0.0009,  0.0007, -0.0012]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(3.7402, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-7.9219, -2.0137, -5.7656,  ..., -1.7637, -3.8262, -1.0137],
        [-6.4570, -2.7715, -5.2227,  ..., -3.2559, -2.3477, -1.2080],
        [-8.5781, -2.1543, -5.5586,  ..., -5.3711, -5.4023, -0.3101],
        ...,
        [-8.4844, -1.8135, -4.5625,  ..., -2.2520, -2.4707, -1.0322],
        [-9.1484, -2.4961, -5.3711,  ..., -1.9639, -4.1211, -0.9326],
        [-7.0430, -1.5586, -4.8398,  ..., -2.7461, -3.1211, -1.0898]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20, 15,  4, 25, 18, 15, 13,  1, 10, 10, 27,  0, 11, 10,  9, 10, 27, 27,
        26,  1,  1, 27,  0, 27, 27, 27, 26, 27, 27,  9, 17,  1,  0,  0, 15, 20,
         9, 18, 14, 27, 15,  3, 27, 27, 13, 20,  9, 13, 27, 20, 13, 18,  4, 25,
         5, 27, 27, 27, 27,  4, 27, 27, 22, 27], device='cuda:0')
step: 48
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2373, -0.6108,  ...,  0.4775, -0.0851,  1.5098],
        [ 1.2520, -1.8887, -0.2050,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9141, -0.2189,  0.7993,  ..., -1.9053, -1.3438, -0.5938],
        [-0.4104, -0.3000, -1.7139,  ..., -0.6572,  0.6479,  1.2705],
        [-1.1699,  2.2285, -0.0970,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0056, -0.0043, -0.0074,  ..., -0.0057,  0.0031,  0.0023],
        [ 0.0013, -0.0213, -0.0282,  ..., -0.0242,  0.0102,  0.0259],
        [-0.0012,  0.0015, -0.0024,  ..., -0.0036,  0.0024,  0.0045],
        [ 0.0017, -0.0025, -0.0018,  ..., -0.0054, -0.0010, -0.0017],
        [ 0.0026, -0.0019, -0.0040,  ..., -0.0005, -0.0007, -0.0003]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(3.5140, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-8.5625, -1.6523, -5.2148,  ..., -3.8086, -2.8086, -0.7461],
        [-6.3359, -3.6504, -5.3984,  ..., -2.9316, -3.1816, -0.9629],
        [-8.0625, -2.0664, -5.7852,  ..., -4.4414, -2.6602, -0.6289],
        ...,
        [-5.8203, -2.6641, -3.8672,  ..., -2.5703, -2.5234, -1.6797],
        [-8.1250, -2.2949, -6.2305,  ..., -1.9502, -2.3105, -1.5908],
        [-7.1641, -1.7432, -4.8359,  ..., -3.5254, -3.5566, -1.4316]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 17, 20,  4,  7,  0,  2, 27, 27, 18, 27, 25, 25, 17,  0,  0, 27, 18,
        27,  1,  4, 27, 15,  1,  0, 27, 18, 25, 27, 27, 27, 27, 27, 27,  0,  3,
         3, 15,  7,  2, 18,  0, 11, 27,  7, 27,  7, 27, 27, 27,  4,  9,  3, 27,
        20,  6,  7,  1,  4,  4,  5, 18, 27, 17], device='cuda:0')
step: 49
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2373, -0.6108,  ...,  0.4775, -0.0852,  1.5098],
        [ 1.2520, -1.8887, -0.2050,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9141, -0.2189,  0.7993,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4104, -0.2998, -1.7139,  ..., -0.6572,  0.6479,  1.2705],
        [-1.1699,  2.2285, -0.0970,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0010,  0.0135,  0.0035,  ..., -0.0011,  0.0007, -0.0006],
        [-0.0020,  0.0007, -0.0052,  ..., -0.0049, -0.0041,  0.0006],
        [ 0.0007,  0.0019, -0.0031,  ..., -0.0013,  0.0015,  0.0015],
        [ 0.0009,  0.0041, -0.0002,  ..., -0.0038, -0.0044,  0.0046],
        [ 0.0004,  0.0005, -0.0012,  ...,  0.0015, -0.0006, -0.0027]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(3.7408, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -6.9102,  -3.0684,  -6.2539,  ...,  -1.6611,  -2.6602,  -0.8486],
        [ -9.1406,  -1.1104,  -2.8594,  ..., -10.1406,  -8.4844,  -4.6094],
        [ -5.9102,  -1.5059,  -5.0977,  ...,  -2.7246,  -2.5371,  -1.7871],
        ...,
        [ -7.8672,  -2.3984,  -4.3203,  ...,  -4.3047,  -4.6016,  -0.5547],
        [ -8.6875,  -2.1211,  -3.8398,  ...,  -1.7148,  -3.3711,  -1.1533],
        [ -7.3984,  -2.2285,  -4.9609,  ...,  -2.8848,  -3.3223,  -0.6650]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 18, 20,  2, 27,  2,  0, 18, 18,  4,  7, 22, 26,  0, 27, 15, 20,  1,
        27, 27, 27,  0,  0, 26, 27, 27, 27, 13,  0,  2,  1, 18, 27,  5, 27, 27,
        27,  7,  9,  0,  4, 25, 27,  0,  0,  0,  7,  7, 25,  1, 10, 15, 14, 27,
         0,  0, 27, 10,  4, 27, 27,  7,  7, 27], device='cuda:0')
step: 50
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2373, -0.6108,  ...,  0.4778, -0.0852,  1.5098],
        [ 1.2520, -1.8887, -0.2050,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9141, -0.2189,  0.7993,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4104, -0.2998, -1.7139,  ..., -0.6572,  0.6479,  1.2705],
        [-1.1699,  2.2285, -0.0970,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.8964e-03, -1.1375e-02,  1.0214e-03,  ..., -5.5389e-03,
         -1.2268e-02,  9.5367e-03],
        [-1.1940e-03, -1.0805e-03,  1.9064e-03,  ..., -4.0283e-03,
         -2.3060e-03,  2.1076e-04],
        [ 4.6062e-04,  1.7262e-04,  2.3613e-03,  ..., -2.1801e-03,
         -4.3559e-04,  1.8549e-03],
        [-2.3365e-03, -5.7316e-04,  5.7907e-03,  ...,  2.2049e-03,
         -8.4221e-05,  3.4180e-03],
        [ 1.7872e-03, -1.8730e-03,  1.3933e-03,  ..., -1.6432e-03,
          2.2602e-04, -3.5357e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(3.4990, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-9.2188, -3.2832, -6.0312,  ..., -2.3770, -3.2520, -0.5957],
        [-7.8086, -3.0586, -5.6680,  ..., -4.9180, -4.6211, -0.5269],
        [-7.6523, -2.4023, -4.9648,  ..., -2.5586, -2.9648, -0.9019],
        ...,
        [-7.0273, -2.9043, -4.5586,  ..., -1.6064, -2.6992, -2.1543],
        [-7.0859, -2.0059, -5.2734,  ..., -2.0684, -2.7559, -1.7246],
        [-7.0312, -3.5332, -5.8477,  ..., -4.0664, -3.6582, -0.2520]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20,  1, 12,  4, 27, 27, 27, 27, 11, 27, 27, 27, 27,  0,  4, 15, 27, 20,
        27,  7,  4,  6,  3,  1, 10, 27, 20, 27, 18,  1, 14, 27, 25, 25, 10, 10,
         9,  3, 27,  3, 15,  4, 18,  2,  3,  6,  6, 27, 18, 27, 26, 20, 25,  9,
        20, 27, 24,  8,  2,  1, 10,  5, 27,  1], device='cuda:0')
step: 51
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2373, -0.6108,  ...,  0.4778, -0.0852,  1.5098],
        [ 1.2520, -1.8887, -0.2050,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9141, -0.2188,  0.7993,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4104, -0.2998, -1.7139,  ..., -0.6572,  0.6479,  1.2705],
        [-1.1699,  2.2285, -0.0970,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.6069e-03, -2.4689e-02, -4.4584e-04,  ...,  9.3460e-03,
         -6.2675e-03,  4.6086e-04],
        [-2.3556e-03,  5.9128e-03,  7.3929e-03,  ...,  9.3536e-03,
          2.9354e-03,  2.2125e-04],
        [ 1.0319e-03,  2.8038e-03, -4.5395e-04,  ..., -4.8866e-03,
          4.4365e-03,  1.1055e-02],
        [-2.4724e-04,  5.3482e-03,  3.3951e-04,  ..., -5.1537e-03,
         -5.7106e-03,  6.6528e-03],
        [-7.1466e-05,  1.2732e-03, -4.4084e-04,  ...,  4.4136e-03,
         -1.0366e-03, -5.2214e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(3.4983, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-7.4141, -2.2871, -5.8203,  ..., -2.3496, -2.0059, -1.1621],
        [-8.6641, -3.1621, -4.8477,  ..., -1.8184, -1.2236, -2.0684],
        [-7.3750, -2.7520, -5.1250,  ..., -4.6250, -4.7188, -1.0645],
        ...,
        [-8.2500, -3.0586, -4.9961,  ..., -3.7930, -2.3398, -2.9336],
        [-6.1133, -3.0176, -4.1094,  ..., -5.0977, -2.7363, -0.3613],
        [-7.6055, -2.3398, -4.2148,  ..., -2.6523, -3.3711, -1.1836]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 25, 27,  4, 17, 27,  0, 27, 17, 15, 27, 20, 17, 20, 27,  2, 18, 27,
        27, 27, 10,  8,  3, 27,  1, 27,  3, 20, 10, 22, 15, 18, 27, 27, 27,  0,
        27, 27, 11, 27, 10, 15, 27,  0, 10,  3,  1,  7, 27,  2, 17,  0, 15, 27,
         0, 27, 27,  3, 17, 27, 27,  0, 18, 14], device='cuda:0')
step: 52
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2373, -0.6104,  ...,  0.4778, -0.0852,  1.5098],
        [ 1.2520, -1.8887, -0.2050,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9141, -0.2188,  0.7993,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4104, -0.2998, -1.7139,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1699,  2.2285, -0.0970,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.3967e-03,  1.4183e-02, -5.5161e-03,  ..., -1.8784e-02,
          1.8787e-03, -4.8714e-03],
        [ 6.5279e-04,  9.3699e-04, -1.1330e-03,  ...,  4.6692e-03,
          4.7836e-03, -4.8790e-03],
        [ 4.4022e-03, -1.2970e-02,  6.1874e-03,  ..., -1.2482e-02,
         -1.1940e-02,  6.0005e-03],
        [ 2.9802e-04, -5.2338e-03,  3.2997e-03,  ..., -3.4976e-04,
         -8.2588e-04,  1.1969e-03],
        [ 4.3297e-03, -1.5306e-03, -2.8000e-03,  ..., -3.1013e-03,
         -2.7955e-05, -1.8873e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(3.2544, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-7.2266, -3.6641, -5.3203,  ..., -2.6016, -3.1953, -1.3525],
        [-6.6445, -2.8320, -5.6133,  ..., -2.6758, -2.3008, -1.9258],
        [-8.1016, -4.0352, -5.5352,  ..., -4.4727, -4.3164, -0.3167],
        ...,
        [-6.6211, -2.5723, -4.5742,  ..., -3.7285, -3.7910, -0.6660],
        [-5.7578, -2.3516, -4.7891,  ..., -2.8203, -3.1641, -1.6641],
        [-9.1641, -4.2734, -7.6328,  ..., -5.0547, -4.4297, -0.1482]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 18, 27, 27,  1, 27,  9,  1, 26, 27, 25, 26, 27,  9, 25, 27,  9, 11,
        27, 27,  7, 27,  2,  1, 26, 13,  3,  3, 27, 18,  6, 27, 27,  0, 27,  0,
         0, 27, 26, 27, 27, 12, 27, 15, 24, 18,  4, 27, 22,  1,  9, 20, 17,  2,
        15,  0, 10, 27,  7, 27, 27, 27, 24, 27], device='cuda:0')
step: 53
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2372, -0.6104,  ...,  0.4778, -0.0852,  1.5098],
        [ 1.2520, -1.8887, -0.2050,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9141, -0.2188,  0.7993,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4106, -0.2998, -1.7139,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1699,  2.2285, -0.0970,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0022,  0.0067,  0.0085,  ..., -0.0041,  0.0004,  0.0043],
        [-0.0026,  0.0015,  0.0022,  ..., -0.0055, -0.0043, -0.0002],
        [ 0.0006, -0.0009,  0.0015,  ..., -0.0020, -0.0005,  0.0025],
        [ 0.0026,  0.0013,  0.0065,  ...,  0.0012,  0.0042, -0.0042],
        [ 0.0011,  0.0008, -0.0011,  ..., -0.0004, -0.0016, -0.0025]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(4.2259, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.8125, -3.1582, -4.8750,  ..., -2.6582, -2.4707, -1.7510],
        [-8.7656, -4.7344, -7.7656,  ..., -5.5156, -5.0469, -0.1093],
        [-6.8750, -3.3457, -5.1875,  ..., -1.9395, -2.0332, -1.0020],
        ...,
        [-6.5742, -1.9355, -4.4336,  ..., -2.9512, -3.2793, -1.1543],
        [-6.7617, -2.3574, -3.7949,  ..., -2.4824, -2.8574, -0.7319],
        [-7.2188, -3.1719, -4.9219,  ..., -2.0469, -3.5469, -1.2344]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([14,  7, 22,  7,  9, 27,  3,  8, 26, 11,  7,  0, 25, 27, 15, 11,  3,  0,
         4, 16, 10, 26, 17, 15, 26, 25,  0,  3, 10, 27, 27, 27, 19,  7,  8, 27,
        25, 27, 27,  6, 26, 22, 27, 18, 20, 27,  0, 17, 17, 22,  0, 10, 24,  4,
         0, 27, 15,  3, 18,  2,  0, 15,  8, 25], device='cuda:0')
step: 54
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2372, -0.6104,  ...,  0.4778, -0.0851,  1.5098],
        [ 1.2520, -1.8887, -0.2050,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9141, -0.2188,  0.7993,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4106, -0.2998, -1.7139,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1699,  2.2285, -0.0970,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1845e-03, -3.0060e-03,  1.9817e-03,  ...,  4.3297e-04,
          2.0683e-05,  2.7561e-03],
        [ 4.1199e-03, -5.4054e-03,  2.9869e-03,  ...,  1.0170e-02,
         -3.7384e-03, -1.2138e-02],
        [-7.6485e-04,  8.9407e-04,  2.4433e-03,  ..., -3.2520e-03,
         -3.2177e-03,  3.2787e-03],
        [ 1.7977e-04, -1.9341e-03,  8.5602e-03,  ...,  6.8903e-04,
          4.6082e-03, -7.0839e-03],
        [ 5.4502e-04,  5.3549e-04,  3.5439e-03,  ...,  5.0306e-04,
          1.5392e-03, -7.9823e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(3.2772, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.5430, -3.4961, -4.9180,  ..., -3.6523, -3.9805, -0.8232],
        [-8.5000, -3.2188, -5.3750,  ..., -0.9370, -3.7812, -1.8428],
        [-6.6914, -2.0684, -5.4727,  ..., -2.2559, -2.9746, -2.0059],
        ...,
        [-8.7734, -3.8066, -5.3047,  ..., -4.7734, -3.7129, -1.6494],
        [-8.6406, -2.8887, -6.4531,  ..., -2.4824, -2.3887, -0.7324],
        [-5.8867, -4.2617, -5.6992,  ..., -4.4492, -2.6680, -0.4485]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 27,  7, 27,  7, 15, 15,  2, 27,  4, 27, 27, 27,  0,  1, 15, 27, 20,
        20, 25,  1,  6,  3, 27, 25, 27, 27, 27,  7, 27,  9,  4,  1,  7, 27,  4,
        27, 27, 27, 20, 27, 27, 27, 25,  9, 27,  1, 25, 22, 13, 27, 27, 10,  7,
        27, 27, 27, 12, 17, 11, 22, 27, 11,  0], device='cuda:0')
step: 55
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2372, -0.6104,  ...,  0.4778, -0.0851,  1.5098],
        [ 1.2520, -1.8887, -0.2050,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9141, -0.2188,  0.7993,  ..., -1.9053, -1.3438, -0.5942],
        [-0.4106, -0.2998, -1.7139,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1699,  2.2285, -0.0970,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0026,  0.0318, -0.0022,  ..., -0.0386, -0.0023, -0.0116],
        [ 0.0025,  0.0085, -0.0055,  ..., -0.0011,  0.0069, -0.0028],
        [ 0.0030,  0.0027, -0.0034,  ..., -0.0002,  0.0086,  0.0038],
        [-0.0019, -0.0032,  0.0024,  ...,  0.0004, -0.0117,  0.0064],
        [ 0.0029, -0.0016, -0.0087,  ...,  0.0019, -0.0044, -0.0019]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(3.0848, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.4961, -1.7451, -4.9961,  ..., -4.5273, -2.3066, -1.0879],
        [-7.0391, -2.1348, -5.0742,  ..., -3.9160, -2.7910, -0.6187],
        [-7.3086, -2.3086, -6.4961,  ..., -3.4961, -2.6855, -0.6528],
        ...,
        [-7.4258, -3.3027, -6.2734,  ..., -5.9258, -4.0508, -0.2406],
        [-7.8008, -3.0488, -6.0469,  ..., -2.5488, -2.3613, -0.8926],
        [-7.7070, -2.4902, -5.7852,  ..., -3.5996, -1.8027, -0.6455]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 27, 27,  0, 18,  1, 27,  5, 22, 27, 27, 27, 27, 18, 27, 27, 25, 27,
         2, 17, 10,  6, 26, 27,  4,  6, 27, 25, 15, 27, 26, 18,  4, 27,  0, 27,
         2,  4, 27, 18, 27, 14, 27, 26,  7, 27,  1, 27, 11, 24,  3, 14, 27,  4,
         3,  1, 27,  8, 27,  5, 27,  7,  9,  0], device='cuda:0')
step: 56
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2372, -0.6104,  ...,  0.4778, -0.0851,  1.5098],
        [ 1.2520, -1.8887, -0.2050,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9131, -0.2188,  0.7993,  ..., -1.9043, -1.3438, -0.5942],
        [-0.4106, -0.2998, -1.7139,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1699,  2.2285, -0.0970,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.7676e-03,  6.0577e-03, -4.3488e-03,  ..., -1.0376e-02,
          4.7264e-03, -9.8419e-03],
        [ 2.4390e-04,  3.5000e-03,  1.4114e-03,  ..., -7.3204e-03,
         -8.0185e-03,  2.7103e-03],
        [-3.7289e-04,  2.9354e-03, -1.5440e-03,  ...,  6.8426e-05,
          3.2921e-03, -8.5688e-04],
        [-2.9640e-03, -1.6451e-03, -1.9302e-03,  ...,  1.2045e-03,
         -2.1915e-03,  3.7060e-03],
        [ 4.8470e-04, -1.0214e-03, -2.8725e-03,  ...,  2.7351e-03,
         -3.5810e-04, -2.2526e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(3.0042, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-7.6836, -2.6816, -5.3711,  ..., -3.0586, -2.5566, -0.8389],
        [-7.6367, -3.5137, -5.3867,  ..., -1.2314, -1.9805, -2.6680],
        [-4.6016, -1.7285, -5.0586,  ..., -3.5723, -2.7598, -1.7910],
        ...,
        [-5.6289, -1.5674, -4.7852,  ..., -3.3184, -3.5996, -1.3184],
        [-6.7422, -1.4004, -4.5547,  ..., -4.1797, -3.2754, -0.9312],
        [-6.5430, -2.7930, -5.3555,  ..., -5.2305, -3.6680, -0.5430]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 18,  3,  7, 27, 27,  9, 27, 27, 25, 26, 27, 27,  0, 27, 27, 18,
         0, 13, 17, 25, 15, 27, 17, 27, 27, 25,  1, 27, 15, 27, 10, 27, 20, 25,
         3,  0,  2, 10, 27,  2, 27, 15, 27,  7,  0, 17, 15, 22, 27,  6,  4, 27,
         2, 22, 27,  4, 27,  7, 18, 27, 14, 27], device='cuda:0')
step: 57
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2372, -0.6104,  ...,  0.4778, -0.0851,  1.5098],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9131, -0.2186,  0.7993,  ..., -1.9043, -1.3438, -0.5947],
        [-0.4106, -0.2998, -1.7139,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1709,  2.2285, -0.0970,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.6623e-03, -1.1005e-03, -4.8332e-03,  ..., -5.4321e-03,
          1.3113e-03, -3.4447e-03],
        [ 1.5097e-03, -6.2227e-04, -9.1195e-06,  ...,  1.5259e-03,
          3.4065e-03, -1.7986e-03],
        [-8.2779e-04,  1.1425e-03, -9.9123e-05,  ...,  5.8126e-04,
          1.5984e-03, -4.8351e-04],
        [-4.3440e-04, -2.2564e-03,  1.5855e-04,  ...,  4.2057e-04,
          2.4700e-04, -2.2259e-03],
        [ 1.7567e-03, -1.3723e-03, -1.5554e-03,  ..., -1.7490e-03,
          2.3067e-04, -8.8978e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(3.5633, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.0508, -1.8955, -4.3789,  ..., -4.8945, -3.7383, -0.6768],
        [-7.1367, -3.2480, -4.4023,  ..., -2.4824, -2.9043, -0.8413],
        [-7.7109, -3.6348, -4.3242,  ..., -4.6211, -4.9961, -1.2129],
        ...,
        [-4.9023, -4.6836, -6.1523,  ..., -2.0605, -2.0918, -1.8730],
        [-6.5859, -2.7422, -4.7422,  ..., -3.9004, -5.4609, -0.2432],
        [-6.4531, -2.9238, -5.3281,  ..., -3.1738, -2.1426, -1.2666]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 27, 27, 27,  4, 11,  4,  3,  4,  5,  3, 20, 27, 27, 25, 26, 10, 10,
         0, 15,  4,  5, 17, 27, 27,  2, 27,  6, 25,  7, 20,  4, 11, 27, 27, 15,
         5,  7, 18, 18,  3,  3,  9, 27,  6, 10, 27,  4, 24, 27, 27,  2, 22, 27,
        17, 27,  4, 10,  7, 27, 10,  0, 27, 20], device='cuda:0')
step: 58
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2373, -0.6104,  ...,  0.4780, -0.0851,  1.5098],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9131, -0.2186,  0.7993,  ..., -1.9043, -1.3438, -0.5947],
        [-0.4106, -0.2998, -1.7139,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1709,  2.2285, -0.0969,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.1175e-04,  2.3529e-02,  1.0300e-04,  ..., -1.1414e-02,
          7.9346e-03, -4.6234e-03],
        [ 3.7193e-04,  1.0582e-02,  1.0605e-02,  ...,  1.2482e-02,
         -2.0981e-03, -9.7656e-03],
        [ 1.7948e-03, -8.5783e-04,  4.5662e-03,  ..., -7.2765e-04,
         -7.9453e-05,  4.1885e-03],
        [-6.2294e-03, -2.2182e-03, -7.1383e-04,  ...,  6.0463e-04,
         -3.0861e-03,  3.5172e-03],
        [ 1.2054e-03, -7.5293e-04, -2.2507e-03,  ..., -1.4925e-03,
         -1.1396e-04, -1.0948e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(3.2805, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.9805, -3.4648, -4.8398,  ..., -3.5898, -4.0898, -0.4968],
        [-6.0352, -2.3613, -4.6914,  ..., -2.8301, -3.1133, -0.9556],
        [-4.6836, -2.7461, -3.9336,  ..., -3.8867, -3.2461, -0.7773],
        ...,
        [-6.5586, -2.8105, -4.6055,  ..., -1.6543, -3.1855, -1.2168],
        [-6.1016, -1.6787, -3.0527,  ..., -7.7578, -6.9297, -2.3652],
        [-7.0898, -2.5586, -5.2148,  ..., -3.2148, -4.7773, -0.7769]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 5, 11,  7,  4,  0,  9, 12, 27, 27,  0, 15, 27, 27,  9, 18, 27,  0, 13,
        27, 27, 27,  0, 27, 27,  6, 13, 10, 18, 10,  0, 27, 26,  8,  3, 27, 13,
         6, 27, 22, 27, 27, 10,  6, 27, 27, 26, 27,  0, 15, 27, 15, 27,  4, 10,
        27, 27,  0, 27, 18,  7, 27,  3,  4,  4], device='cuda:0')
step: 59
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2373, -0.6104,  ...,  0.4780, -0.0852,  1.5098],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9131, -0.2186,  0.7993,  ..., -1.9043, -1.3438, -0.5947],
        [-0.4106, -0.2998, -1.7139,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1709,  2.2285, -0.0969,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.4049e-03,  2.0172e-02,  1.2520e-02,  ..., -1.0063e-02,
          8.8215e-04,  7.8430e-03],
        [-2.4967e-03, -3.8567e-03, -7.5102e-06,  ..., -2.0657e-03,
          6.5088e-04,  4.5657e-04],
        [-1.6594e-03,  1.0014e-04,  4.3564e-03,  ..., -2.2469e-03,
         -2.1191e-03,  2.3975e-03],
        [ 3.5019e-03, -1.2169e-03,  7.0724e-03,  ...,  3.0060e-03,
          7.8201e-03, -3.8891e-03],
        [ 8.3148e-05, -1.0014e-03,  4.5090e-03,  ..., -2.1420e-03,
          1.6785e-03, -1.6632e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(3.0357, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2773, -2.9824, -4.1836,  ..., -5.4023, -4.1680, -0.7627],
        [-5.8164, -2.9102, -4.8633,  ..., -2.4414, -2.0664, -1.4717],
        [-6.7617, -2.2305, -5.0273,  ..., -3.8711, -4.1367, -1.2305],
        ...,
        [-6.2617, -3.0117, -4.6367,  ..., -2.4805, -2.0430, -2.0742],
        [-6.3750, -2.8438, -5.6562,  ..., -2.3418, -2.8105, -0.8115],
        [-8.2500, -3.5020, -5.3750,  ..., -1.3916, -3.7500, -2.2656]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  3,  3,  4, 27,  9, 25, 27, 27, 13, 27, 27, 11,  8, 27, 22, 11, 17,
        15, 27, 27, 10,  5,  5, 27, 27, 26, 27, 27, 20,  3, 27, 27,  9, 17,  9,
         0, 27, 27, 19, 17, 17, 27, 27,  7, 27, 15, 27, 18, 20,  9,  9, 27, 15,
        11, 27, 26, 12, 27,  4,  0, 20, 22, 27], device='cuda:0')
step: 60
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2373, -0.6104,  ...,  0.4780, -0.0852,  1.5098],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9131, -0.2186,  0.7993,  ..., -1.9043, -1.3438, -0.5947],
        [-0.4106, -0.2996, -1.7139,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1709,  2.2285, -0.0968,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.7061e-03, -1.8295e-02, -5.4512e-03,  ...,  1.2383e-02,
         -3.8528e-03,  7.6866e-03],
        [-1.2779e-03, -1.1673e-03, -1.7061e-03,  ...,  7.5531e-03,
          6.0577e-03,  7.5161e-05],
        [ 2.6131e-03,  1.9503e-03, -2.9106e-03,  ..., -2.6941e-04,
          1.4381e-03, -2.0447e-03],
        [-1.0653e-03, -1.0777e-03, -7.5760e-03,  ..., -5.1346e-03,
         -9.0790e-03, -3.4637e-03],
        [ 2.1420e-03,  1.4915e-03, -4.9553e-03,  ..., -1.5068e-03,
         -2.0866e-03,  1.1998e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(3.7959, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2773, -2.9160, -4.6328,  ..., -3.6504, -2.4316, -1.9473],
        [-5.0312, -2.7812, -3.9844,  ..., -2.8125, -3.3906, -1.3438],
        [-6.2266, -3.5371, -4.9141,  ..., -3.0059, -5.2266, -0.9121],
        ...,
        [-9.0703, -4.9727, -7.5039,  ..., -5.3477, -4.2227, -0.1921],
        [-5.8906, -2.8125, -3.8281,  ..., -2.8281, -2.3125, -1.1562],
        [-6.3594, -1.4512, -4.9492,  ..., -2.1387, -2.4199, -2.2637]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17,  1, 22, 22, 27, 20, 27,  4,  3, 16, 11,  3, 27, 25,  3, 27, 27,  0,
        27, 27, 25, 20,  9, 27, 27, 27, 10, 27, 27, 24, 22,  9,  6, 17, 18,  4,
         9, 27, 27, 26, 15,  4,  7,  4,  0, 27,  0, 27, 26,  4, 25, 27, 13,  6,
        14, 20, 14,  0,  0,  0,  9, 27,  2,  1], device='cuda:0')
step: 61
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2374, -0.6104,  ...,  0.4783, -0.0853,  1.5098],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9131, -0.2186,  0.7993,  ..., -1.9043, -1.3438, -0.5947],
        [-0.4106, -0.2996, -1.7139,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1709,  2.2285, -0.0968,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0024,  0.0114, -0.0028,  ..., -0.0111,  0.0007, -0.0066],
        [ 0.0046, -0.0032, -0.0003,  ...,  0.0258,  0.0213, -0.0190],
        [ 0.0035, -0.0051,  0.0050,  ..., -0.0042, -0.0006,  0.0065],
        [-0.0016, -0.0051,  0.0051,  ...,  0.0031,  0.0014,  0.0036],
        [ 0.0002, -0.0016,  0.0012,  ..., -0.0007,  0.0007,  0.0021]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.8120, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.6289, -1.8154, -4.6602,  ..., -4.9414, -4.3789, -0.8462],
        [-5.9414, -2.2207, -4.6445,  ..., -5.4883, -2.8770, -1.1582],
        [-5.8203, -3.4004, -4.1953,  ..., -4.1172, -3.2910, -0.5405],
        ...,
        [-4.7617, -3.0586, -4.3398,  ..., -4.0117, -3.6211, -1.6523],
        [-4.9609, -3.2266, -4.6016,  ..., -2.7285, -2.5859, -2.2109],
        [-8.0000, -4.0938, -6.8438,  ..., -5.6562, -4.9375, -0.2179]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7,  7, 27,  7, 27,  6, 27, 10,  8, 27,  3, 15, 27, 27, 25, 26, 27,  4,
         1, 17, 14, 24, 27, 12,  3,  0, 15, 18,  4,  1, 27, 27, 27, 27, 25,  3,
        27,  1,  5, 27, 10, 27,  4, 25, 27, 25,  4, 18,  4, 10, 18, 27,  7,  3,
         1,  2,  2, 10, 27, 11, 27, 26, 27, 27], device='cuda:0')
step: 62
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2374, -0.6104,  ...,  0.4783, -0.0853,  1.5098],
        [ 1.2510, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9131, -0.2186,  0.7993,  ..., -1.9043, -1.3438, -0.5947],
        [-0.4104, -0.2996, -1.7148,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1709,  2.2285, -0.0967,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0712e-02,  5.4810e-02, -6.4621e-03,  ..., -2.9114e-02,
          2.0828e-02, -1.6677e-04],
        [-4.7088e-04,  5.2452e-06, -3.2883e-03,  ...,  1.3275e-03,
          1.9855e-03,  1.4515e-03],
        [ 5.2071e-04,  3.1509e-03,  1.1719e-02,  ...,  2.6382e-02,
         -8.0490e-03, -2.4242e-03],
        [-2.3708e-03, -2.5101e-03,  6.5041e-04,  ..., -1.8663e-03,
         -8.0414e-03,  4.4479e-03],
        [ 2.7394e-04, -3.0289e-03, -7.1182e-03,  ...,  3.6411e-03,
         -3.9787e-03, -1.9493e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(3.6184, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.9883, -3.7695, -3.5195,  ..., -2.0195, -3.4570, -1.1436],
        [-7.4141, -3.2402, -5.9570,  ..., -4.8672, -3.4902, -0.3655],
        [-5.9258, -3.6152, -4.3789,  ..., -3.0215, -2.2715, -1.0830],
        ...,
        [-6.2500, -3.1426, -5.1406,  ..., -2.6426, -2.5332, -2.4219],
        [-7.3398, -0.9946, -3.1504,  ..., -7.3711, -6.6680, -3.3691],
        [-6.7383, -3.2402, -4.4297,  ..., -5.4727, -3.9590, -0.3342]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 5,  6,  7,  3,  7, 13,  5, 27, 13, 27, 27,  4, 20, 27,  1, 27, 11, 25,
        10, 27,  8,  4, 10,  0,  3, 27, 22,  9, 25,  3,  6, 17, 18, 27, 10, 10,
         6,  1, 15, 27, 27,  4, 27,  3,  0,  1,  6, 17, 15,  9, 13, 27,  3, 15,
        25, 14, 11,  7, 27,  7,  7,  0, 26, 10], device='cuda:0')
step: 63
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2374, -0.6104,  ...,  0.4783, -0.0853,  1.5098],
        [ 1.2510, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9131, -0.2186,  0.7993,  ..., -1.9043, -1.3438, -0.5947],
        [-0.4104, -0.2996, -1.7148,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1709,  2.2285, -0.0967,  ...,  0.6587,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0011, -0.0024, -0.0022,  ...,  0.0055, -0.0015,  0.0028],
        [-0.0003, -0.0074, -0.0048,  ...,  0.0085,  0.0100, -0.0056],
        [ 0.0015,  0.0057,  0.0287,  ..., -0.0139, -0.0061,  0.0089],
        [-0.0021,  0.0014,  0.0150,  ...,  0.0067, -0.0045,  0.0056],
        [ 0.0013, -0.0028, -0.0062,  ...,  0.0048, -0.0013, -0.0081]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(3.3122, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.5879, -3.0879, -4.7617,  ..., -3.1836, -3.1191, -1.5889],
        [-6.2852, -3.1914, -5.3477,  ..., -4.9727, -3.3477, -0.5972],
        [-6.4961, -2.6367, -3.3887,  ..., -3.0762, -4.9492, -1.5752],
        ...,
        [-5.0156, -2.0625, -4.0156,  ..., -3.2656, -3.7812, -1.2188],
        [-4.7070, -2.3926, -4.2383,  ..., -3.9570, -2.2695, -1.4873],
        [-9.1875, -5.4336, -7.7461,  ..., -7.3086, -5.2148, -0.1213]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 10, 27, 27, 27,  3,  1, 22,  4,  3, 27, 15,  0,  0,  3, 27, 27, 27,
         4, 10, 27,  7, 27, 22, 25, 25, 17, 20, 27,  9, 27, 22, 20, 15,  1,  1,
         7,  1, 27, 27,  9,  3, 24, 17,  0, 15,  3, 15, 14, 26, 27, 11, 20,  4,
        27,  0,  0, 13,  4,  4,  1, 27,  2, 27], device='cuda:0')
step: 64
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6104,  ...,  0.4783, -0.0854,  1.5098],
        [ 1.2510, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9131, -0.2186,  0.7988,  ..., -1.9043, -1.3438, -0.5952],
        [-0.4104, -0.2996, -1.7148,  ..., -0.6572,  0.6484,  1.2705],
        [-1.1709,  2.2285, -0.0966,  ...,  0.6582,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0002, -0.0042, -0.0030,  ...,  0.0076, -0.0018, -0.0010],
        [-0.0043,  0.0005, -0.0013,  ...,  0.0046,  0.0011, -0.0031],
        [ 0.0034, -0.0033,  0.0008,  ..., -0.0106, -0.0029,  0.0057],
        [-0.0002,  0.0002, -0.0022,  ..., -0.0025, -0.0033,  0.0020],
        [-0.0003, -0.0003, -0.0026,  ...,  0.0015, -0.0008, -0.0025]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(3.1210, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.1641, -2.7266, -5.1328,  ..., -4.0703, -3.1328, -0.7578],
        [-5.0977, -2.4102, -4.4258,  ..., -4.5508, -4.4883, -0.6914],
        [-3.9414, -2.3164, -4.9414,  ..., -4.0977, -4.1602, -0.8159],
        ...,
        [-5.3672, -1.8047, -3.8047,  ..., -3.5078, -2.9922, -1.4297],
        [-3.3008, -3.8945, -5.3320,  ..., -3.6758, -2.4570, -4.3945],
        [-5.3555, -3.2754, -4.7773,  ..., -4.8242, -4.8867, -0.9634]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  3, 13,  2,  9,  1, 26, 22, 27, 26,  3,  2,  4, 18, 21, 27, 27, 10,
        27,  6, 14, 17, 27, 27, 27, 27, 27, 27, 27, 27,  3, 27, 27, 27,  7, 27,
         2,  2, 27, 26, 15, 10, 27, 24,  7, 15,  0, 18, 25, 27, 22, 17,  8, 17,
         1, 27,  0,  4,  1, 27,  6,  2, 18, 11], device='cuda:0')
step: 65
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6104,  ...,  0.4785, -0.0854,  1.5098],
        [ 1.2510, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2186,  0.7988,  ..., -1.9043, -1.3438, -0.5952],
        [-0.4104, -0.2993, -1.7148,  ..., -0.6572,  0.6489,  1.2705],
        [-1.1709,  2.2285, -0.0966,  ...,  0.6582,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.9162e-04, -6.8932e-03, -3.5629e-03,  ...,  4.3144e-03,
         -4.0221e-04,  2.5368e-04],
        [-2.1029e-04, -9.9850e-04, -6.3095e-03,  ...,  1.1749e-02,
          6.5460e-03, -1.0162e-02],
        [-1.5049e-03,  2.6417e-03, -9.2506e-04,  ..., -3.1948e-04,
          1.1978e-03,  1.3125e-04],
        [ 7.5042e-05,  5.4169e-04, -5.2834e-03,  ..., -3.7727e-03,
          2.5034e-06, -4.5776e-03],
        [ 1.3762e-03, -8.3804e-05, -5.7507e-04,  ..., -1.5612e-03,
         -2.4056e-04,  6.0976e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(3.1042, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.6406, -3.5488, -5.4883,  ..., -4.9570, -4.0508, -0.3926],
        [-5.1719, -3.4531, -3.6719,  ..., -3.5781, -3.2324, -1.2959],
        [-2.9727, -1.2227, -4.7383,  ..., -4.8320, -3.4570, -2.9570],
        ...,
        [-2.8184, -3.6777, -5.1758,  ..., -5.0352, -2.2695, -0.8643],
        [-6.1328, -1.3496, -4.9102,  ..., -4.1914, -3.4746, -1.5684],
        [-4.3672, -3.0879, -5.4336,  ..., -5.0273, -2.9941, -1.6191]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1,  9,  0, 22,  0, 27,  0,  1, 15, 22, 27, 27, 18,  9,  8, 17, 27,  4,
         6,  7, 17, 27, 27, 27, 13,  9, 18, 15, 27,  1, 15, 27, 22,  7, 27,  2,
        27, 23, 27, 27, 27, 27, 27, 27,  0, 27,  6, 27,  4, 18, 15,  1, 27,  6,
        12, 27, 27, 27, 14,  9,  4, 15, 22,  8], device='cuda:0')
step: 66
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6104,  ...,  0.4785, -0.0855,  1.5098],
        [ 1.2510, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2186,  0.7988,  ..., -1.9043, -1.3438, -0.5952],
        [-0.4104, -0.2993, -1.7148,  ..., -0.6572,  0.6489,  1.2705],
        [-1.1709,  2.2285, -0.0964,  ...,  0.6582,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0104,  0.0117, -0.0037,  ..., -0.0156,  0.0072, -0.0087],
        [ 0.0003,  0.0006, -0.0010,  ...,  0.0057,  0.0053, -0.0047],
        [ 0.0060, -0.0062,  0.0009,  ..., -0.0065, -0.0009, -0.0010],
        [ 0.0011, -0.0029, -0.0019,  ...,  0.0036,  0.0022, -0.0058],
        [ 0.0035, -0.0022, -0.0002,  ..., -0.0032,  0.0014, -0.0033]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(3.2722, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.2969, -2.4707, -3.8926,  ..., -2.6875, -3.6113, -1.9854],
        [-4.8867, -3.2617, -4.8711,  ..., -4.9961, -4.1367, -1.8398],
        [-5.4023, -2.6230, -5.0430,  ..., -5.7930, -3.5605, -1.9668],
        ...,
        [-4.0703, -3.0078, -3.6328,  ..., -4.5703, -2.9453, -2.6641],
        [-4.4141, -2.5078, -4.4141,  ..., -3.4141, -3.3203, -1.6016],
        [-5.6406, -2.3770, -4.0000,  ..., -4.7344, -4.0312, -1.6885]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 27,  8,  9,  2,  7,  2,  2,  3,  6, 26, 15, 10, 18, 27,  7, 23, 27,
        22,  0, 27, 18,  0, 17, 27, 27,  2, 27, 13, 10, 27,  1, 25, 10,  0, 18,
         7,  3, 27, 22, 27, 27,  3,  1, 27,  0, 27, 22,  0, 15,  9, 27, 17, 22,
        14, 25, 14, 27, 27, 27, 27, 20,  8, 27], device='cuda:0')
step: 67
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6104,  ...,  0.4785, -0.0855,  1.5098],
        [ 1.2510, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2186,  0.7988,  ..., -1.9043, -1.3438, -0.5952],
        [-0.4104, -0.2993, -1.7148,  ..., -0.6572,  0.6489,  1.2705],
        [-1.1709,  2.2285, -0.0964,  ...,  0.6582,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.2002e-05,  6.7472e-04, -9.0361e-04,  ...,  3.4199e-03,
          1.3084e-03,  2.3670e-03],
        [ 4.6968e-05, -3.9062e-03, -7.0229e-03,  ..., -3.1555e-02,
         -1.3214e-02,  1.1635e-02],
        [-9.2411e-04, -2.2182e-03, -1.7395e-03,  ...,  1.7281e-03,
          8.0109e-04, -9.6750e-04],
        [-2.7046e-03,  2.7132e-04,  3.1066e-04,  ...,  1.7047e-04,
         -1.7672e-03,  3.7899e-03],
        [-8.2779e-04,  8.1158e-04, -1.5140e-04,  ...,  1.9407e-03,
         -1.3018e-04,  1.2589e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(3.3869, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.1035, -3.1191, -3.3535,  ..., -3.3223, -2.9473, -1.6973],
        [-5.4883, -2.1133, -4.6133,  ..., -4.9414, -5.6914, -1.0830],
        [-6.5039, -4.9414, -4.4102,  ..., -5.3477, -3.8477, -3.0664],
        ...,
        [-5.4844, -2.9551, -5.1094,  ..., -3.9219, -4.1719, -0.4849],
        [-3.8066, -2.9004, -4.5078,  ..., -4.0391, -3.0410, -1.2588],
        [-4.3750, -3.1543, -4.8906,  ..., -3.5605, -2.9980, -1.6543]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22,  6, 20,  4,  0,  8,  3, 20,  1, 10, 27, 27, 27,  5,  3,  5, 27, 27,
         0,  0, 14, 13, 27, 27,  0,  2, 27, 15,  3,  0, 20, 15,  4,  2, 21, 27,
         3, 12,  9, 17,  7, 27,  4, 27,  7,  1,  0, 13,  1,  9, 25, 27,  4, 18,
        27, 26,  4, 27,  5, 27,  6,  2, 27, 27], device='cuda:0')
step: 68
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6104,  ...,  0.4785, -0.0856,  1.5098],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2186,  0.7988,  ..., -1.9043, -1.3438, -0.5952],
        [-0.4104, -0.2993, -1.7148,  ..., -0.6572,  0.6489,  1.2705],
        [-1.1719,  2.2285, -0.0963,  ...,  0.6582,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0052, -0.0108, -0.0005,  ...,  0.0076, -0.0029,  0.0057],
        [ 0.0022,  0.0031,  0.0139,  ...,  0.0046, -0.0180, -0.0098],
        [-0.0029, -0.0027,  0.0003,  ...,  0.0047, -0.0014, -0.0018],
        [-0.0012,  0.0007,  0.0018,  ..., -0.0002,  0.0026, -0.0002],
        [-0.0010,  0.0019,  0.0022,  ..., -0.0016,  0.0003, -0.0002]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(3.3086, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.8984, -2.9473, -3.3848,  ..., -1.9473, -4.0078, -0.8218],
        [-4.4844, -3.5000, -5.2812,  ..., -3.9375, -3.4688, -0.7192],
        [-5.3398, -3.6211, -4.7461,  ..., -5.2617, -2.6523, -0.8408],
        ...,
        [-5.0234, -3.0859, -5.0234,  ..., -4.1172, -2.1191, -1.2119],
        [-4.9570, -2.7363, -4.7070,  ..., -4.6094, -4.5820, -1.4238],
        [-4.4336, -2.7793, -3.9668,  ..., -3.4980, -4.0586, -1.7480]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  8, 27, 20, 22, 20, 27, 27, 18, 27, 10, 27,  7, 10, 27, 17, 27,
        26, 15, 26, 27, 18,  0, 15,  2, 27,  1, 27,  9, 15, 27, 20,  6, 15, 27,
        27, 14, 22,  7, 18, 22,  6, 27,  0,  1, 25, 13,  4, 27, 10, 10,  1, 17,
        10, 15, 15, 27,  4, 26, 27,  4, 26,  1], device='cuda:0')
step: 69
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6099,  ...,  0.4785, -0.0856,  1.5098],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2186,  0.7988,  ..., -1.9043, -1.3438, -0.5952],
        [-0.4102, -0.2993, -1.7148,  ..., -0.6572,  0.6489,  1.2705],
        [-1.1719,  2.2285, -0.0963,  ...,  0.6582,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0025,  0.0003,  0.0010,  ..., -0.0016, -0.0016, -0.0029],
        [ 0.0007, -0.0018, -0.0014,  ..., -0.0036,  0.0035,  0.0152],
        [ 0.0020,  0.0034, -0.0017,  ..., -0.0026,  0.0011,  0.0077],
        [ 0.0007, -0.0018,  0.0005,  ...,  0.0007, -0.0035,  0.0015],
        [ 0.0008,  0.0006, -0.0009,  ...,  0.0018, -0.0002,  0.0009]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.7983, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-7.9648, -5.5273, -5.6680,  ..., -0.3557, -3.2930, -4.4336],
        [-4.0664, -2.0645, -4.2852,  ..., -3.1895, -2.5020, -2.0020],
        [-3.6367, -2.4961, -3.8867,  ..., -2.9805, -3.2305, -2.3398],
        ...,
        [-6.4219, -4.0156, -4.5156,  ..., -4.7969, -5.1406, -0.3599],
        [-5.4766, -3.4453, -5.0234,  ..., -5.4922, -4.3203, -0.4763],
        [-3.5957, -3.3281, -4.3281,  ..., -4.3750, -2.6113, -1.2197]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25, 27, 27,  6, 27, 27,  2, 27, 24,  3,  0, 10, 27,  0, 27, 24, 27,  3,
        26,  4, 27, 27, 26, 27, 11, 17, 27, 27, 22, 27,  9,  0,  0, 27,  0,  0,
        17, 27,  7, 13,  3,  0, 18, 27, 15, 18, 15, 26, 10, 17,  1, 27, 17,  1,
         7,  9, 27,  7,  9,  5, 18, 27, 10, 27], device='cuda:0')
step: 70
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6099,  ...,  0.4785, -0.0856,  1.5107],
        [ 1.2510, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2186,  0.7983,  ..., -1.9043, -1.3438, -0.5952],
        [-0.4102, -0.2993, -1.7148,  ..., -0.6572,  0.6489,  1.2705],
        [-1.1719,  2.2285, -0.0963,  ...,  0.6582,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0020,  0.0022,  0.0018,  ...,  0.0027, -0.0022,  0.0035],
        [ 0.0008,  0.0052,  0.0078,  ..., -0.0115, -0.0116,  0.0004],
        [-0.0026,  0.0040,  0.0001,  ...,  0.0030,  0.0033, -0.0011],
        [ 0.0011, -0.0015,  0.0033,  ...,  0.0023, -0.0012,  0.0021],
        [ 0.0002,  0.0004,  0.0003,  ...,  0.0013, -0.0004,  0.0010]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.9181, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.3047, -3.3340, -6.5547,  ..., -1.4277, -2.4902, -1.4277],
        [-3.6543, -3.1543, -5.1211,  ..., -4.7500, -3.6230, -0.8418],
        [-6.7930, -4.9180, -5.4492,  ..., -5.7305, -5.9648, -0.1368],
        ...,
        [-3.9629, -2.6973, -4.3984,  ..., -4.4141, -2.5410, -1.1963],
        [-2.2207, -2.8770, -4.0000,  ..., -4.6602, -3.0332, -1.5020],
        [-4.7148, -3.3086, -5.3398,  ..., -5.4492, -2.7773, -0.7153]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10,  0, 25,  3, 25, 27,  5,  4, 25, 27, 27, 27, 27, 18, 27, 27, 18, 15,
        25, 27,  3,  4,  0, 27, 27, 22, 27, 27,  1,  5, 27,  9, 27, 27, 15, 11,
        27, 27,  2,  1,  0, 27, 27, 22, 10, 27, 27, 27,  4, 22, 27, 14, 15, 20,
         1,  2, 22, 27, 14, 27,  3, 27, 18,  7], device='cuda:0')
step: 71
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6099,  ...,  0.4788, -0.0856,  1.5107],
        [ 1.2510, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2186,  0.7983,  ..., -1.9043, -1.3438, -0.5957],
        [-0.4102, -0.2993, -1.7148,  ..., -0.6572,  0.6489,  1.2705],
        [-1.1719,  2.2285, -0.0962,  ...,  0.6582,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0027, -0.0095, -0.0034,  ..., -0.0003, -0.0043, -0.0039],
        [-0.0075, -0.0145, -0.0162,  ..., -0.0004,  0.0187,  0.0233],
        [-0.0005,  0.0030,  0.0018,  ...,  0.0031,  0.0002,  0.0007],
        [-0.0011,  0.0013, -0.0020,  ..., -0.0028, -0.0047,  0.0036],
        [ 0.0004,  0.0004, -0.0028,  ...,  0.0015, -0.0011, -0.0003]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(3.3034, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.5332, -3.4238, -4.0000,  ..., -4.2344, -3.3457, -0.7832],
        [-5.0898, -3.2129, -4.8398,  ..., -1.9482, -2.7441, -1.6191],
        [-3.4531, -2.4219, -4.9219,  ..., -3.6094, -2.5781, -2.7656],
        ...,
        [-4.9570, -3.1152, -4.8633,  ..., -4.2539, -3.8008, -1.6455],
        [-4.6602, -2.0352, -4.5352,  ..., -5.3320, -4.1289, -2.1602],
        [-6.4453, -1.4160, -2.0410,  ..., -8.5078, -9.0859, -3.4160]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  4, 20, 17,  4,  2, 15, 10, 27,  3,  9, 27,  1,  7,  1, 18,  8, 24,
        27, 20, 27, 15, 11,  6, 27,  1, 27,  2,  2, 13, 18, 11, 25, 27, 27, 22,
         6, 27, 14, 26, 22, 15, 10, 26, 27, 22, 15, 27,  2,  1, 27, 27, 27,  0,
        20,  6,  4, 27,  9, 27, 17,  7, 20, 27], device='cuda:0')
step: 72
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6099,  ...,  0.4788, -0.0857,  1.5107],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2186,  0.7983,  ..., -1.9043, -1.3438, -0.5957],
        [-0.4102, -0.2993, -1.7148,  ..., -0.6572,  0.6489,  1.2705],
        [-1.1719,  2.2285, -0.0961,  ...,  0.6582,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.7026e-04, -5.3787e-03, -8.2169e-03,  ...,  4.6768e-03,
          2.4567e-03, -1.2903e-03],
        [-2.0123e-04,  4.3297e-03,  5.1003e-03,  ..., -1.2085e-02,
         -1.4687e-02,  5.3253e-03],
        [-8.7929e-04,  4.3793e-03, -4.3221e-03,  ...,  1.7595e-03,
          2.6989e-03, -7.8812e-03],
        [-3.7169e-04, -4.5109e-04, -9.6512e-04,  ..., -2.2373e-03,
         -2.7466e-03, -3.2272e-03],
        [ 2.3484e-05, -1.6565e-03, -1.8969e-03,  ...,  1.1749e-03,
         -4.1819e-04, -3.3593e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(3.0188, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.8633, -4.0039, -4.3945,  ..., -2.9727, -2.5664, -2.0664],
        [-2.6914, -3.9883, -3.9883,  ..., -4.0664, -3.3789, -1.5039],
        [-5.3398, -4.4336, -5.3242,  ..., -4.9492, -3.6504, -0.2915],
        ...,
        [-3.8633, -2.6289, -4.5508,  ..., -4.7695, -3.5508, -1.2842],
        [-2.6758, -3.1445, -5.3945,  ..., -4.5039, -3.0820, -1.3008],
        [-3.7617, -3.2930, -4.3711,  ..., -5.0586, -3.4180, -1.7627]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20, 27, 10,  0,  0, 17, 14, 18, 25,  4,  1, 18, 27,  6,  6,  3, 27, 15,
         7, 27,  0,  9, 22, 27,  7, 27, 18, 27,  5,  0, 27,  6,  4,  9,  1,  2,
         4, 27, 27, 25, 20, 22, 26, 27, 22,  0, 27, 27, 10, 27,  2, 27,  5, 20,
        25,  0, 27, 27, 27,  0, 19, 11, 27, 27], device='cuda:0')
step: 73
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6099,  ...,  0.4788, -0.0857,  1.5107],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2186,  0.7983,  ..., -1.9043, -1.3438, -0.5957],
        [-0.4102, -0.2991, -1.7148,  ..., -0.6572,  0.6489,  1.2705],
        [-1.1719,  2.2285, -0.0961,  ...,  0.6582,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.4400e-03,  3.2635e-03, -3.6812e-03,  ..., -7.2403e-03,
          5.2547e-04, -9.5654e-04],
        [ 1.1349e-03,  7.7057e-04, -5.0974e-04,  ..., -2.8305e-03,
         -2.2831e-03,  1.9741e-03],
        [-2.1439e-03,  3.8815e-03, -2.4433e-03,  ...,  4.7302e-03,
          4.7264e-03,  2.4147e-03],
        [ 1.2207e-03, -2.1601e-04,  1.7357e-04,  ...,  2.3823e-03,
          1.3828e-04, -3.7155e-03],
        [ 6.2656e-04, -1.3428e-03, -1.8167e-03,  ...,  4.8518e-05,
         -3.5191e-04, -7.8535e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.8729, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.7891, -3.2891, -4.7578,  ..., -3.3496, -2.8203, -1.0068],
        [-2.6738, -2.8301, -3.7988,  ..., -6.6094, -3.4863, -2.0488],
        [-3.1406, -4.0312, -4.8125,  ..., -4.7656, -3.0312, -0.7026],
        ...,
        [-2.5840, -4.2578, -4.7695,  ..., -5.2891, -1.5996, -1.8809],
        [-3.7344, -2.6719, -3.6406,  ..., -4.5781, -3.6719, -1.2656],
        [-4.4805, -3.2773, -4.8398,  ..., -3.6055, -2.9805, -0.9170]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3,  0,  4, 10,  7, 17, 17, 17, 10, 15, 27,  0,  0, 27, 20, 15, 24, 27,
        27, 27, 27, 11, 24, 26, 27,  1, 10, 27, 17, 14, 27,  3, 27, 15, 22,  2,
        10, 17, 27, 27,  1,  4, 15, 27, 27, 13,  7,  0,  7, 27, 20, 18, 25, 27,
        20, 10, 14, 27, 27,  3, 26, 13, 27, 27], device='cuda:0')
step: 74
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6099,  ...,  0.4788, -0.0857,  1.5107],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2188,  0.7983,  ..., -1.9043, -1.3438, -0.5957],
        [-0.4102, -0.2991, -1.7148,  ..., -0.6572,  0.6489,  1.2705],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6582,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.0542e-03,  3.2864e-03, -1.1101e-03,  ..., -2.6836e-03,
          2.3575e-03,  3.3677e-05],
        [ 1.9875e-03,  9.4557e-04, -1.1368e-03,  ..., -7.7133e-03,
         -5.2299e-03, -3.0684e-04],
        [ 1.7281e-03, -4.0321e-03, -1.6823e-03,  ..., -1.6584e-03,
         -7.1347e-05, -1.1425e-03],
        [ 1.1581e-04, -6.3467e-04, -2.4872e-03,  ..., -5.9795e-04,
          1.5001e-03, -3.2501e-03],
        [-2.0397e-04, -1.2100e-04,  2.5129e-04,  ..., -1.1177e-03,
          4.0054e-04, -8.2016e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5556, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.6445, -3.4883, -4.0039,  ..., -2.3320, -3.2227, -1.5195],
        [-2.6875, -2.9219, -4.2344,  ..., -4.1250, -3.0156, -1.3125],
        [-3.2598, -4.0703, -4.0234,  ..., -3.7754, -1.8379, -0.9941],
        ...,
        [-1.2930, -4.1055, -4.1523,  ..., -4.9961, -2.6055, -2.9648],
        [-4.3555, -2.8535, -4.3555,  ..., -2.8223, -2.7910, -1.0420],
        [-3.0059, -2.7871, -3.9902,  ..., -4.2578, -2.7246, -1.7246]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 9, 27,  5,  2, 20, 27,  4,  1, 10, 27,  5, 17, 11,  4, 27,  9,  0, 11,
         0, 20, 27,  1, 10, 26, 27,  9, 27,  7, 27,  0, 27, 15,  5, 10,  0, 12,
         2, 15, 26,  1,  0,  1,  5, 22, 13, 27, 27,  4, 15,  5, 12, 26, 27, 25,
        27, 27, 27,  0, 27, 26, 27, 17, 22, 27], device='cuda:0')
step: 75
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6099,  ...,  0.4788, -0.0857,  1.5107],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2188,  0.7983,  ..., -1.9043, -1.3438, -0.5957],
        [-0.4102, -0.2991, -1.7148,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6582,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.3004e-04,  3.8605e-03,  3.7217e-04,  ..., -1.7195e-03,
          1.4782e-04,  4.0779e-03],
        [ 2.6584e-04,  2.0294e-03,  2.5368e-03,  ..., -3.3073e-03,
         -1.2388e-03,  2.0266e-06],
        [-8.5545e-04,  2.7180e-03,  1.3089e-04,  ..., -2.9945e-03,
         -2.7132e-04,  5.5161e-03],
        [-2.7752e-04, -5.0783e-04,  1.6241e-03,  ..., -6.2370e-04,
          1.1511e-03, -9.1028e-04],
        [ 8.5783e-04,  1.3173e-04,  2.5630e-06,  ..., -6.5184e-04,
          2.6751e-04, -5.5599e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.9917, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.2578, -2.8828, -4.9297,  ..., -6.0234, -4.1328, -1.4141],
        [-3.7871, -3.8184, -5.0039,  ..., -5.6484, -5.5703, -0.4124],
        [-3.0176, -2.7676, -4.5156,  ..., -3.4707, -3.1582, -1.4551],
        ...,
        [-2.8594, -2.9531, -4.1562,  ..., -6.1094, -4.4062, -1.1104],
        [-3.4688, -3.1250, -5.6250,  ..., -4.5000, -3.3125, -0.8755],
        [-2.3477, -3.0820, -3.2852,  ..., -4.4570, -4.1758, -2.1914]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6,  6,  3, 25, 18,  6, 17,  0, 27, 27, 23,  1,  1, 10, 27, 27, 20,  2,
        15, 22,  0, 27, 25,  3,  1,  7, 27, 17, 25, 22,  4, 27, 27, 27, 11, 10,
         0, 27, 27, 13,  9,  2, 27, 27,  0, 27,  4, 25, 20, 14, 10,  3, 10, 18,
        18, 18, 19, 10, 27,  2, 27, 27, 27, 27], device='cuda:0')
step: 76
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6099,  ...,  0.4788, -0.0857,  1.5107],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2188,  0.7983,  ..., -1.9043, -1.3438, -0.5957],
        [-0.4102, -0.2991, -1.7148,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6582,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.4257e-04,  7.5197e-04,  2.0828e-03,  ...,  1.7321e-04,
          7.8440e-04,  2.1515e-03],
        [ 2.4452e-03, -1.4925e-03,  1.8015e-03,  ...,  5.1422e-03,
          1.1950e-03, -5.1079e-03],
        [ 6.8378e-04, -9.5558e-04,  6.3324e-04,  ...,  1.2856e-03,
         -2.2964e-03, -1.0282e-04],
        [-3.6776e-05, -4.1962e-05,  3.3245e-03,  ..., -9.4509e-04,
          1.1101e-03, -1.8966e-04],
        [-3.6061e-05, -1.3857e-03,  1.5736e-03,  ..., -1.0309e-03,
          1.2302e-04,  9.4509e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.8151, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.2676, -3.2207, -3.1895,  ..., -4.0000, -3.3457, -1.5947],
        [-4.6055, -3.7012, -4.6719,  ..., -4.4688, -3.9355, -0.6069],
        [-5.6641, -4.3828, -6.1016,  ..., -5.7578, -5.1016, -0.1324],
        ...,
        [-1.9375, -3.5938, -5.1250,  ..., -4.2812, -3.7500, -1.9697],
        [-4.6992, -3.6348, -4.7930,  ..., -2.9961, -1.9639, -1.9326],
        [-3.0391, -2.9141, -5.1641,  ..., -4.9141, -4.0078, -0.8823]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 14,  4, 27, 27, 18, 27,  1,  9, 20,  4, 27, 20, 27, 27,  4, 18, 27,
        27, 27,  2, 27, 27, 23, 18, 17, 27,  4, 27, 27, 15,  7, 25, 27, 27, 25,
        26, 18,  4, 27, 22, 10, 20, 18, 20, 25, 27,  5,  6, 18, 20, 10, 26,  3,
        27, 22, 25, 15, 18,  0,  9,  7, 25, 15], device='cuda:0')
step: 77
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6099,  ...,  0.4788, -0.0858,  1.5107],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2188,  0.7983,  ..., -1.9043, -1.3438, -0.5957],
        [-0.4102, -0.2991, -1.7148,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6582,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.5869e-03,  8.9722e-03,  6.9160e-03,  ..., -2.6093e-03,
         -9.7942e-04,  5.6839e-03],
        [ 3.3498e-04,  1.3959e-04, -8.4686e-04,  ..., -8.9264e-03,
         -4.3488e-03,  6.8092e-03],
        [ 2.6822e-06, -7.9775e-04,  3.9625e-04,  ...,  8.3113e-04,
         -7.8249e-04,  7.8487e-04],
        [-6.4754e-04,  8.2970e-04,  3.7766e-03,  ...,  2.5215e-03,
          6.0177e-04,  1.7300e-03],
        [-7.8154e-04,  4.0293e-04,  1.6680e-03,  ..., -1.0319e-03,
         -1.3895e-03,  1.6642e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4753, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.7891, -3.8203, -5.2266,  ..., -3.0703, -4.0703, -0.8193],
        [-2.2246, -3.0371, -4.6445,  ..., -5.4414, -4.3516, -1.0996],
        [-4.0352, -4.2539, -4.3320,  ..., -2.6133, -2.7383, -2.9102],
        ...,
        [-4.2578, -3.4941, -4.7266,  ..., -2.5859, -1.6816, -2.8848],
        [-3.4688, -3.0938, -4.4062,  ..., -4.6094, -2.6250, -1.1572],
        [-5.2109, -2.9277, -4.7422,  ..., -3.1777, -2.3809, -1.2881]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  4, 27, 27, 27, 10, 15, 27,  1, 27, 27, 27, 17,  6, 14, 27, 27, 27,
         4,  7, 27, 27, 17, 15, 10,  3,  5, 27, 23, 27,  2,  1, 27, 27, 27, 27,
         4,  0, 27, 27, 27,  5, 10, 20, 27, 13, 27, 25, 27,  7,  9, 27, 10, 15,
        18,  4,  1,  1,  6, 10, 27, 13, 27, 26], device='cuda:0')
step: 78
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2379, -0.6099,  ...,  0.4788, -0.0858,  1.5107],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2188,  0.7983,  ..., -1.9043, -1.3438, -0.5957],
        [-0.4102, -0.2991, -1.7148,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6582,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.3117e-03,  3.1281e-04, -1.7357e-04,  ...,  1.2426e-03,
         -3.1710e-05, -8.4543e-04],
        [-9.0742e-04,  1.4009e-03,  3.8948e-03,  ...,  6.7368e-03,
          2.4929e-03,  2.0313e-03],
        [ 7.2670e-04, -1.7242e-03,  7.3671e-04,  ...,  1.1120e-03,
         -3.4809e-05,  1.6136e-03],
        [ 3.6120e-04,  4.5490e-04, -1.7300e-03,  ...,  2.5415e-04,
         -9.8407e-05, -1.0405e-03],
        [ 1.2398e-05,  6.7091e-04, -1.0853e-03,  ...,  8.0061e-04,
         -6.1083e-04, -1.3361e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5273, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.1094, -3.3594, -3.7500,  ..., -3.1562, -2.8594, -3.6094],
        [-2.7441, -3.2129, -4.6055,  ..., -5.9180, -4.4180, -0.6045],
        [-2.6309, -3.6309, -3.5371,  ..., -3.5371, -2.8496, -2.0996],
        ...,
        [-3.2988, -2.7363, -3.4707,  ..., -3.6270, -2.0645, -1.6279],
        [-2.9238, -3.3594, -4.0156,  ..., -4.2188, -4.2969, -2.1738],
        [-1.6953, -4.1016, -5.2734,  ..., -5.4141, -3.0078, -1.7578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 13, 26,  0, 27, 20,  4,  9,  3, 27, 27, 15,  4,  7,  4, 11, 27,
         6, 27, 27, 10, 24,  7,  4,  0, 27,  7, 27,  6, 27, 25, 20, 18, 27, 18,
         0, 12, 10, 10, 27, 27,  7, 27,  1, 17, 27, 14, 27, 27, 15, 27,  4,  1,
         0, 27, 20,  9, 11, 27,  4, 27, 27, 15], device='cuda:0')
step: 79
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2379, -0.6099,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2188,  0.7983,  ..., -1.9043, -1.3438, -0.5957],
        [-0.4102, -0.2991, -1.7148,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6582,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.5664e-04, -3.7384e-03, -6.9904e-04,  ...,  3.7766e-03,
         -7.0477e-04, -5.7268e-04],
        [ 4.3821e-04,  5.6763e-03,  2.6684e-03,  ..., -5.3883e-04,
          9.8515e-04, -2.2316e-03],
        [-4.6158e-04,  1.6794e-03, -1.7881e-03,  ...,  1.0014e-03,
          1.4830e-03,  5.9080e-04],
        [-3.1471e-04,  5.3740e-04,  1.6670e-03,  ...,  1.1053e-03,
         -7.2598e-05,  1.7910e-03],
        [-3.2306e-04,  9.5129e-04,  9.1195e-05,  ...,  1.1597e-03,
          5.5695e-04, -1.3411e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.7053, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.9707, -3.1426, -4.4531,  ..., -4.5156, -4.0469, -2.1113],
        [-2.9141, -3.7891, -5.5391,  ..., -4.9766, -3.2109, -0.6016],
        [-3.7168, -2.7637, -4.1211,  ..., -4.4023, -3.7949, -1.6074],
        ...,
        [-3.1758, -2.3789, -3.6445,  ..., -5.9258, -5.5820, -1.4727],
        [-1.6309, -3.9434, -4.3203,  ..., -4.8516, -3.0996, -3.5371],
        [-2.8301, -3.4707, -4.9258,  ..., -4.5039, -3.8770, -1.2676]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  6,  5, 26, 27, 15, 27, 27, 27, 27, 21,  1, 19, 19,  4, 15, 27,
        18, 27, 17, 27, 27,  4, 27,  3, 27, 18,  0,  3, 10, 27,  3, 27,  9, 27,
        17, 27, 18,  7,  1,  9,  8,  9,  3, 24, 15,  1, 27, 22,  4, 18, 27, 27,
        27, 27, 27,  0, 27, 27, 26,  0, 20,  3], device='cuda:0')
step: 80
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6099,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2520, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2188,  0.7983,  ..., -1.9043, -1.3438, -0.5957],
        [-0.4102, -0.2991, -1.7148,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6582,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.7336e-04,  4.5478e-05, -1.0815e-03,  ...,  1.9836e-03,
          7.7009e-04, -2.3079e-03],
        [ 9.8801e-04,  1.7605e-03,  1.1826e-03,  ..., -1.8425e-03,
         -9.9564e-04, -1.1940e-03],
        [ 6.6280e-05,  1.4095e-03,  1.6129e-04,  ..., -2.6298e-04,
         -1.3247e-03, -1.4410e-03],
        [ 1.5202e-03,  7.4577e-04, -2.0542e-03,  ..., -1.0366e-03,
         -6.5994e-04, -1.9932e-03],
        [-8.0645e-05,  5.0783e-04,  1.2226e-03,  ..., -3.9124e-04,
          3.7980e-04,  8.5688e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6379, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.7168, -4.3125, -4.3281,  ..., -3.9824, -3.2949, -0.9512],
        [-4.1328, -3.2246, -3.7109,  ..., -5.9141, -5.4766, -0.8823],
        [-3.7715, -3.1152, -4.2227,  ..., -5.4570, -5.0664, -0.5205],
        ...,
        [-2.2246, -4.1602, -5.2227,  ..., -4.4414, -3.6934, -0.6626],
        [-2.5879, -3.1504, -4.6484,  ..., -3.8379, -4.1797, -1.0879],
        [-3.4746, -2.5684, -4.6914,  ..., -5.3320, -4.5508, -1.1299]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27, 27, 27,  3,  3,  9, 15,  0,  0,  1,  0, 27,  7, 10, 10, 14,
        13, 26,  7, 27, 27, 27, 27, 27, 27, 17, 27, 17, 22,  6,  9, 27, 20,  4,
        17, 15, 27, 15,  5, 27,  1,  9,  7,  7, 27, 27, 26, 27, 25,  3,  4,  9,
         9, 18,  4, 27,  1, 27, 20,  0, 27,  8], device='cuda:0')
step: 81
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6099,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2510, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2189,  0.7983,  ..., -1.9043, -1.3438, -0.5957],
        [-0.4102, -0.2991, -1.7148,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6582,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.8702e-03,  1.9913e-03,  2.2221e-03,  ..., -1.2484e-03,
         -1.9274e-03,  4.7531e-03],
        [-3.2961e-05,  1.5726e-03, -5.3358e-04,  ..., -1.2884e-03,
         -9.8228e-04,  2.2125e-03],
        [-1.7700e-03,  1.9140e-03, -2.5959e-03,  ...,  7.1049e-04,
          2.7847e-04,  7.7772e-04],
        [ 1.5087e-03, -2.1625e-04,  1.8492e-03,  ..., -4.3869e-04,
         -1.1740e-03,  2.4662e-03],
        [-3.9101e-04, -1.9932e-04, -4.7588e-04,  ...,  1.2388e-03,
         -1.1816e-03,  1.5993e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.7636, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.6602, -4.3945, -5.5195,  ..., -2.1602, -2.5020, -0.9717],
        [-3.7500, -3.3125, -3.9863,  ..., -5.0469, -4.5938, -0.8135],
        [-2.3633, -2.8633, -4.1445,  ..., -3.5195, -3.3633, -2.4883],
        ...,
        [-2.4766, -3.4297, -3.9297,  ..., -6.4609, -4.8359, -1.1328],
        [-3.7344, -3.3281, -3.7969,  ..., -2.6250, -3.0312, -1.4990],
        [-1.8350, -3.5078, -4.0547,  ..., -5.8828, -4.9297, -1.3350]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10,  4,  8, 27, 27,  0, 27, 27, 11, 15, 26, 27,  4,  1, 20, 13,  2, 18,
        11, 27,  9, 15, 17, 17, 20, 27, 18, 11, 22, 27, 27, 27,  7, 23, 13, 18,
        17,  0,  9, 15, 15, 18, 10, 27, 27, 27, 10, 24, 10, 27,  7,  7,  7, 18,
         9,  2, 27, 10, 27, 27, 20, 27,  4, 27], device='cuda:0')
step: 82
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6099,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2510, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2189,  0.7983,  ..., -1.9043, -1.3438, -0.5957],
        [-0.4102, -0.2991, -1.7148,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6582,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.5406e-03, -5.4626e-03,  2.3174e-04,  ...,  2.1248e-03,
          4.6158e-04, -1.8740e-03],
        [ 5.3024e-04,  3.0079e-03, -1.4687e-03,  ..., -1.0193e-02,
          9.6893e-04,  1.0834e-03],
        [-2.3675e-04,  1.8845e-03, -4.0698e-04,  ...,  4.5891e-03,
          1.6046e-04, -3.1052e-03],
        [-2.3925e-04, -1.7338e-03,  4.9706e-03,  ...,  3.8414e-03,
         -1.2550e-03,  2.0580e-03],
        [ 5.6171e-04, -6.8331e-04, -1.5497e-04,  ..., -5.8842e-04,
          4.7088e-06,  3.4022e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5174, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.5156, -2.4844, -4.1719,  ..., -5.3125, -2.5469, -1.5469],
        [-2.4473, -3.6660, -4.3984,  ..., -4.6641, -3.8535, -1.4775],
        [-3.8281, -3.8438, -5.0000,  ..., -5.1094, -5.5000, -0.3120],
        ...,
        [-3.9688, -2.7656, -3.9375,  ..., -5.3281, -4.1094, -0.6250],
        [-2.4961, -3.8711, -4.7305,  ..., -3.8398, -4.6992, -0.6519],
        [-2.8203, -3.5547, -4.1953,  ..., -3.7891, -3.2891, -1.2256]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  3, 27,  7, 27,  0, 27,  1, 10, 27, 22,  4, 13, 27, 21, 27, 17, 27,
        27, 27, 15,  0, 15, 17,  2,  4, 27, 25, 18, 20, 27, 27,  5,  0, 14,  0,
        27,  4, 27, 20, 27, 15, 27, 27, 27, 25, 27, 20, 27,  3, 10, 27, 27, 18,
         6,  1,  1, 27, 27,  0, 11,  5, 15,  6], device='cuda:0')
step: 83
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6099,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2510, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2189,  0.7983,  ..., -1.9043, -1.3438, -0.5962],
        [-0.4102, -0.2991, -1.7148,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6582,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.2030e-03,  7.4148e-04,  1.7653e-03,  ...,  8.4972e-04,
          8.7023e-04,  1.2522e-03],
        [ 2.8458e-03, -4.8935e-05,  3.2067e-04,  ..., -2.8458e-03,
         -4.2839e-03, -2.7013e-04],
        [ 1.7424e-03, -2.1057e-03,  1.3838e-03,  ..., -1.0414e-03,
         -1.0180e-04,  1.9002e-04],
        [ 1.6527e-03,  3.6168e-04,  2.3270e-03,  ...,  7.8201e-04,
          1.1247e-04, -1.0401e-04],
        [-3.0494e-04,  6.1274e-04, -1.6737e-04,  ...,  6.1417e-04,
         -2.9397e-04, -4.1795e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5136, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.6543, -3.9980, -4.1367,  ..., -3.8730, -4.4648, -1.4658],
        [-1.9980, -4.2148, -3.8887,  ..., -3.0293, -2.9980, -1.9355],
        [-2.0508, -4.8008, -4.7227,  ..., -4.1445, -3.4258, -1.2061],
        ...,
        [-0.7969, -3.5156, -5.5469,  ..., -5.2344, -3.8906, -1.9219],
        [-3.2070, -3.8633, -3.4258,  ..., -5.6445, -2.8633, -0.9878],
        [-0.6362, -4.9805, -4.6836,  ..., -5.4648, -3.4805, -2.3223]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15, 15,  1,  4, 27, 22, 18, 27, 17, 18, 17, 27, 25,  9,  6,  7,  5, 15,
        15, 24,  0, 27,  3, 19, 27, 27,  0,  0, 27, 27,  0,  4, 27, 27, 27,  3,
        15, 25, 27, 25, 24, 20,  0,  6, 27, 27,  0, 14, 27, 27,  4, 27,  6, 27,
        27, 27, 22,  7, 27, 27,  3, 15, 10,  0], device='cuda:0')
step: 84
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6099,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2510, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9131, -0.2189,  0.7983,  ..., -1.9043, -1.3438, -0.5962],
        [-0.4102, -0.2991, -1.7148,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6582,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.0466e-05, -3.0594e-03,  6.5470e-04,  ...,  2.9354e-03,
          2.5702e-04, -2.0444e-04],
        [-2.9850e-03, -5.7678e-03, -5.1041e-03,  ..., -2.6512e-03,
          7.8857e-05,  4.2000e-03],
        [ 2.4185e-03, -2.9373e-03,  2.1267e-03,  ..., -1.7967e-03,
         -2.2984e-03,  8.0013e-04],
        [ 1.7166e-03, -3.2091e-04, -6.3944e-04,  ...,  2.0714e-03,
          9.2173e-04, -7.4768e-04],
        [ 2.1422e-04, -8.2731e-04,  7.8857e-05,  ...,  5.5695e-04,
          7.3910e-04, -4.6730e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4505, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.6992, -5.5742, -6.4180,  ..., -6.9336, -5.4805, -0.1527],
        [-3.7715, -2.9590, -4.9570,  ..., -5.8320, -3.6777, -2.1777],
        [-2.6719, -3.5625, -4.1250,  ..., -4.1875, -3.6406, -1.4688],
        ...,
        [-3.2168, -4.0312, -4.3750,  ..., -5.7031, -4.4688, -0.5928],
        [-2.6641, -3.5234, -3.8047,  ..., -3.9453, -3.8203, -1.1006],
        [-4.6445, -3.8320, -4.9727,  ..., -4.9883, -3.5352, -0.3782]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 27, 27, 27, 27, 27, 23, 26, 10, 27, 27,  3,  4, 10, 27, 27,  0, 15,
        27,  4, 27,  7, 25,  0, 27, 25, 27, 15, 14, 12,  9,  0, 10, 15, 15, 27,
        27, 25,  7, 27, 27,  1,  3, 27,  4,  0,  0,  0, 27,  4,  0,  8, 27, 27,
        11, 27, 27, 11, 27, 10, 15, 27, 18, 25], device='cuda:0')
step: 85
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6099,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2510, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9131, -0.2189,  0.7983,  ..., -1.9043, -1.3438, -0.5962],
        [-0.4102, -0.2991, -1.7148,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6582,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0002,  0.0003,  0.0026,  ...,  0.0002, -0.0015, -0.0002],
        [-0.0030, -0.0009, -0.0034,  ...,  0.0100,  0.0062, -0.0070],
        [ 0.0006, -0.0013,  0.0018,  ..., -0.0013,  0.0040,  0.0027],
        [ 0.0010,  0.0008,  0.0060,  ..., -0.0002, -0.0019,  0.0010],
        [ 0.0008, -0.0003,  0.0014,  ...,  0.0003, -0.0001,  0.0006]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.4995, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.5156, -3.2969, -4.1406,  ..., -4.7188, -4.6094, -0.6558],
        [-3.6230, -4.1992, -4.2617,  ..., -2.9805, -3.0449, -1.3730],
        [-1.8701, -3.4160, -4.4961,  ..., -3.6660, -2.1660, -1.4473],
        ...,
        [-2.3848, -3.3223, -4.3867,  ..., -5.4805, -4.0742, -0.8857],
        [-3.3379, -4.9609, -5.5703,  ..., -6.9297, -5.1797, -0.2903],
        [-1.6562, -3.6250, -3.9375,  ..., -3.9062, -3.7812, -2.1875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6, 27, 27,  0, 27, 27,  4, 27, 27, 27,  0,  7, 27,  2,  4, 27, 27,  4,
        27, 10, 17, 20,  1, 27,  6,  0,  7, 27, 18, 27, 20,  2,  2, 27,  3, 27,
        27, 27,  3, 15, 27, 18, 27,  2,  7, 26,  0, 20, 20, 27,  1,  7,  3, 20,
         0, 27, 27, 27, 15,  3,  0, 27,  0,  9], device='cuda:0')
step: 86
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6099,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2510, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9131, -0.2189,  0.7983,  ..., -1.9043, -1.3438, -0.5962],
        [-0.4102, -0.2991, -1.7158,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6582,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.7588e-04, -2.7580e-03, -1.2903e-03,  ...,  2.2335e-03,
         -5.1212e-04, -8.4496e-04],
        [ 2.3937e-03,  1.0017e-02,  7.9727e-03,  ...,  7.0305e-03,
         -1.8702e-03, -9.8267e-03],
        [ 1.7405e-05, -3.5095e-04, -2.2583e-03,  ...,  2.1005e-04,
          1.4601e-03, -2.7294e-03],
        [ 2.2101e-04, -1.0115e-04, -3.3617e-04,  ...,  1.0691e-03,
          1.5497e-04, -2.5806e-03],
        [-4.1068e-05,  4.5395e-04,  1.0777e-04,  ...,  5.6314e-04,
          1.7190e-04, -1.6522e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.8365, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.6367, -2.3555, -3.3242,  ..., -5.1680, -4.1680, -1.2939],
        [-1.4180, -2.6367, -4.2617,  ..., -3.2305, -4.2148, -2.2617],
        [-1.1611, -3.3789, -4.4727,  ..., -3.5996, -2.9746, -1.4424],
        ...,
        [-3.1855, -4.4219, -4.9062,  ..., -5.9844, -5.0312, -0.6709],
        [-3.1992, -3.8711, -5.4805,  ..., -4.8555, -4.3242, -1.1523],
        [-4.0117, -2.9492, -3.6836,  ..., -3.2305, -3.7617, -0.9810]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10,  5,  0,  6, 27, 20, 17, 17,  0,  9, 27, 27,  7,  1, 27, 20,  2, 15,
        27, 11,  6, 10,  4,  7, 15, 20, 24,  1, 27,  7, 27,  1, 27, 26, 15, 24,
        27,  6, 27, 17,  0, 22,  2, 22,  6,  1, 27, 27, 27, 24, 15,  5, 10, 10,
        18,  0, 15, 27, 27, 27, 11, 27,  7,  2], device='cuda:0')
step: 87
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6099,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2510, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9131, -0.2189,  0.7983,  ..., -1.9043, -1.3438, -0.5962],
        [-0.4102, -0.2991, -1.7158,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6582,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.1655e-04,  5.1270e-03,  1.6870e-03,  ...,  3.8862e-05,
          1.1244e-03,  3.2406e-03],
        [-3.0117e-03,  8.2779e-03,  8.0109e-03,  ..., -5.1537e-03,
         -9.4223e-03, -4.6387e-03],
        [-1.9684e-03, -3.6259e-03,  3.2139e-04,  ..., -9.5701e-04,
         -1.7750e-04, -8.2397e-04],
        [-1.0300e-03,  7.2300e-05,  1.7481e-03,  ..., -3.9291e-04,
          4.3154e-04,  3.2830e-04],
        [-8.2016e-04, -7.5769e-04, -1.0691e-03,  ..., -8.3780e-04,
         -2.6989e-04, -1.9035e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6372, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.5117, -3.9492, -4.8398,  ..., -7.2617, -6.1367, -0.3242],
        [-3.4023, -3.3555, -2.7461,  ..., -3.4648, -2.0742, -1.9346],
        [-6.4180, -5.5273, -5.7930,  ..., -6.6836, -6.6680, -0.0729],
        ...,
        [-3.4473, -3.7578, -4.8203,  ..., -2.9922, -3.4004, -0.6333],
        [-1.1260, -4.4375, -3.9863,  ..., -4.9219, -2.5156, -2.0000],
        [-3.6230, -4.9375, -4.8125,  ..., -6.1875, -6.1719, -0.2798]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15, 22, 27, 27, 11, 26, 27, 10, 15, 27, 27, 27, 27,  3, 27,  6, 10, 26,
        27, 24, 27, 18, 25, 27,  5, 27,  3, 19, 27, 27, 24, 27,  0,  3, 27,  3,
         9, 27, 27, 27, 10, 27, 22, 27, 24, 27, 18, 27,  0,  6, 10, 27, 26,  0,
        27,  1,  6,  1,  8, 27, 18,  1,  0, 10], device='cuda:0')
step: 88
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6099,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2510, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9131, -0.2189,  0.7983,  ..., -1.9043, -1.3438, -0.5962],
        [-0.4102, -0.2991, -1.7158,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6582,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.6207e-03, -7.5245e-04,  2.0905e-03,  ..., -4.6272e-03,
         -1.6813e-03, -3.2806e-03],
        [-5.4855e-03, -4.0817e-03, -1.1765e-02,  ..., -3.2578e-03,
          1.5541e-02,  2.4719e-02],
        [ 2.0714e-03, -1.9360e-03,  4.1885e-03,  ..., -3.4332e-03,
          6.3801e-04,  1.0292e-02],
        [ 1.3123e-03, -9.7370e-04,  3.5839e-03,  ..., -1.5230e-03,
         -2.1591e-03,  7.0047e-04],
        [ 6.1369e-04, -1.7345e-05, -2.4185e-03,  ...,  8.8453e-04,
          7.6592e-05, -2.2106e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.8648, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.4023, -4.6211, -4.4336,  ..., -6.1211, -7.2930, -1.3721],
        [-2.3984, -4.0547, -4.0859,  ..., -2.9766, -3.8828, -1.8359],
        [-1.7832, -3.7676, -3.9707,  ..., -5.1914, -4.1914, -0.9238],
        ...,
        [-2.5664, -3.7070, -3.2852,  ..., -5.5664, -4.6445, -1.2852],
        [-3.2383, -3.7695, -4.4414,  ..., -5.2227, -3.3320, -0.9565],
        [-2.8281, -3.6406, -4.1406,  ..., -4.8594, -3.6562, -0.5464]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  6, 27,  7,  2,  4, 27,  3, 20, 14,  0, 25,  4, 26, 27,  6, 15, 15,
        10, 27,  4, 24, 25,  6, 27,  0, 27, 22, 15, 15, 10, 27,  0,  3, 27, 13,
         4,  4, 27, 10,  6,  5, 10, 26, 27, 15, 11,  3, 15, 27, 27, 13,  2,  0,
        27, 15, 18, 11, 15,  2, 14,  0,  5, 27], device='cuda:0')
step: 89
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6099,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2510, -1.8887, -0.2052,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9121, -0.2188,  0.7983,  ..., -1.9043, -1.3438, -0.5962],
        [-0.4104, -0.2991, -1.7158,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6582,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0548e-03,  2.0618e-03,  4.4966e-04,  ..., -1.9646e-04,
          7.4911e-04,  1.5135e-03],
        [-6.8009e-05, -4.8327e-04, -5.0697e-03,  ..., -6.5994e-03,
          2.5806e-03, -3.1452e-03],
        [ 1.2512e-03, -3.8738e-03,  1.9255e-03,  ..., -3.6507e-03,
         -1.0796e-03,  3.0017e-04],
        [-2.2626e-04,  1.2022e-04,  1.9875e-03,  ..., -1.1501e-03,
         -2.3484e-04, -4.7183e-04],
        [-1.5593e-04,  4.2987e-04, -2.8181e-04,  ...,  9.5129e-04,
         -2.3377e-04, -6.0558e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5364, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.8965, -2.5527, -3.6777,  ..., -5.4453, -2.9590, -1.7402],
        [-2.1289, -4.3320, -3.8633,  ..., -3.9570, -3.7539, -0.7217],
        [-3.7695, -3.3164, -3.8320,  ..., -3.8789, -4.2539, -0.9419],
        ...,
        [-3.0293, -4.7969, -4.7969,  ..., -6.0469, -7.0625, -0.6079],
        [-1.7412, -3.2734, -3.8809,  ..., -4.2578, -3.4590, -1.1787],
        [-2.1426, -3.2520, -3.6113,  ..., -5.8633, -4.8320, -1.6123]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 17, 27,  3,  6, 27,  1, 15,  4,  4, 26, 20, 27,  7, 27, 27,  7, 10,
        27,  7, 26, 27, 25,  0, 18, 25,  0,  4, 26,  4, 15,  0, 18, 27, 18, 15,
         3,  4, 27, 27,  0, 10, 27, 22,  1, 18, 27, 25,  4, 27, 27, 27,  0, 27,
        10,  8, 27,  3, 27, 27,  4, 27,  0, 26], device='cuda:0')
step: 90
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6099,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2510, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9121, -0.2188,  0.7983,  ..., -1.9043, -1.3438, -0.5962],
        [-0.4104, -0.2991, -1.7158,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6582,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.4107e-06,  4.8141e-03,  9.4557e-04,  ..., -4.9438e-03,
         -1.3914e-03,  8.6212e-04],
        [-2.2564e-03, -7.6523e-03, -1.1742e-02,  ..., -1.2188e-03,
          6.1531e-03,  8.2703e-03],
        [-2.0218e-04,  3.5334e-04,  2.0146e-04,  ...,  1.7605e-03,
          3.4595e-04, -3.8052e-04],
        [-1.7154e-04, -8.8501e-04,  3.2101e-03,  ...,  2.2163e-03,
         -1.5230e-03,  1.6336e-03],
        [-3.0351e-04, -2.6321e-04, -3.9291e-04,  ...,  1.0109e-03,
         -7.0858e-04,  7.3147e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.8952, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.7402, -3.1621, -3.0527,  ..., -4.5039, -4.7383, -0.6777],
        [-2.2812, -4.3594, -4.1406,  ..., -4.1406, -4.1406, -0.8755],
        [-4.4258, -3.8164, -3.7852,  ..., -5.8789, -4.7070, -0.7539],
        ...,
        [-0.9551, -4.3008, -4.6875,  ..., -4.4727, -4.1406, -1.6426],
        [-2.9043, -2.8574, -4.8398,  ..., -5.5742, -5.0742, -1.4189],
        [-4.7969, -5.1719, -5.4844,  ..., -5.9531, -4.6406, -0.1729]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 27,  6, 17, 27,  9, 14, 27, 18,  1,  3,  5,  4, 13,  3, 15, 22, 27,
         3, 24,  4, 24,  3,  5, 18, 14, 27, 27, 22, 27,  7, 27, 27,  0, 27, 27,
        27, 27, 26, 26, 27, 27,  2,  1, 17, 27, 13, 27,  7, 27, 20, 18, 18,  4,
         0, 27, 17, 27, 27,  3, 17, 15, 27,  4], device='cuda:0')
step: 91
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2188,  0.7979,  ..., -1.9043, -1.3438, -0.5962],
        [-0.4104, -0.2991, -1.7158,  ..., -0.6572,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6582,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0014,  0.0017,  0.0008,  ...,  0.0004,  0.0023, -0.0008],
        [ 0.0027, -0.0009,  0.0054,  ..., -0.0090, -0.0044,  0.0062],
        [ 0.0021, -0.0013,  0.0003,  ..., -0.0048, -0.0011,  0.0039],
        [-0.0008, -0.0004, -0.0029,  ...,  0.0007,  0.0003, -0.0008],
        [-0.0006,  0.0014,  0.0016,  ..., -0.0009,  0.0005,  0.0008]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.8305, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9590, -3.7715, -4.9297,  ..., -3.5840, -4.5547, -0.4905],
        [-1.7051, -4.4727, -4.9570,  ..., -5.0312, -3.8457, -0.8613],
        [-4.0391, -5.2891, -5.1641,  ..., -6.0391, -5.4922, -1.2275],
        ...,
        [-2.6523, -3.6230, -3.2637,  ..., -4.0273, -4.1055, -1.3877],
        [-2.9902, -3.9141, -4.4297,  ..., -3.1016, -4.1484, -1.2881],
        [-2.4961, -4.3086, -4.8398,  ..., -5.3086, -3.8691, -0.5420]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  2, 27, 27, 27, 27, 27, 25, 10,  2, 17, 25,  8,  3,  4, 27, 10,
         0,  0, 27,  8, 27,  1,  0,  4, 10,  0, 27,  3,  7,  8, 27,  7,  8, 10,
        20,  3, 25,  3, 27, 25,  4,  7, 22, 27, 15, 26, 27, 26, 27, 25,  7,  1,
         8, 27, 27, 27,  6, 20, 27,  3,  9, 26], device='cuda:0')
step: 92
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2382, -0.6104,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2186,  0.7979,  ..., -1.9043, -1.3438, -0.5962],
        [-0.4104, -0.2991, -1.7158,  ..., -0.6577,  0.6494,  1.2705],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.4643e-03,  1.0843e-03,  1.3218e-03,  ..., -1.9331e-03,
         -1.3170e-03,  1.4372e-03],
        [ 9.2697e-04,  4.6310e-03,  1.7462e-03,  ...,  1.1808e-04,
          1.1396e-03, -2.0742e-04],
        [-9.7334e-05,  3.2234e-03, -1.2856e-03,  ..., -5.3692e-04,
          6.4888e-03,  4.1695e-03],
        [-3.1490e-03, -4.3845e-04, -6.8331e-04,  ..., -2.1267e-03,
         -8.9455e-04,  3.9053e-04],
        [-6.4802e-04,  5.6267e-04, -1.5516e-03,  ...,  5.5218e-04,
         -6.5565e-07, -1.6251e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.8264, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.3379, -4.2266, -4.8203,  ..., -3.7129, -3.4160, -0.4629],
        [-3.8125, -4.4844, -4.7812,  ..., -5.1562, -6.2656, -0.5312],
        [-1.5859, -3.8984, -5.0234,  ..., -4.4297, -3.0234, -1.6172],
        ...,
        [-2.5664, -3.9570, -3.4727,  ..., -4.9102, -3.4883, -0.8784],
        [-4.5781, -5.1719, -4.6719,  ..., -0.4858, -4.6719, -4.7031],
        [-2.1875, -3.4199, -4.2344,  ..., -5.0469, -3.6094, -1.4053]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 12, 18, 27, 11, 27, 27, 27,  8,  2, 13, 10, 27,  7,  4, 22, 15, 15,
         7, 27, 27, 10, 27, 11,  1, 27,  0, 11, 27, 27,  5,  5, 27, 11, 15,  3,
        20,  3,  4, 27, 25, 27, 27, 17, 27,  5,  3,  1, 18, 15,  2, 25,  6,  4,
        27, 18,  4, 10,  4, 18, 12, 13, 25, 10], device='cuda:0')
step: 93
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2382, -0.6104,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2186,  0.7979,  ..., -1.9043, -1.3438, -0.5962],
        [-0.4104, -0.2988, -1.7158,  ..., -0.6577,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.2131e-03, -3.1548e-03, -9.6941e-04,  ..., -6.7997e-04,
         -1.0862e-03, -3.6449e-03],
        [ 9.0408e-04,  2.8973e-03, -2.0237e-03,  ...,  1.0513e-02,
          9.2010e-03, -4.4212e-03],
        [-2.0771e-03,  2.9125e-03, -3.4847e-03,  ...,  5.5923e-03,
          3.6087e-03, -3.1204e-03],
        [-7.2193e-04, -2.4939e-04, -2.1136e-04,  ...,  5.5981e-04,
         -7.8011e-04,  2.6798e-03],
        [-1.1015e-04,  1.5440e-03,  2.7061e-05,  ...,  9.3269e-04,
          4.6062e-04,  7.1239e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6947, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.0332, -4.5195, -4.2852,  ..., -4.7070, -4.2227, -0.4399],
        [-3.4336, -4.9336, -5.6992,  ..., -4.1055, -3.6367, -0.6362],
        [-1.1924, -3.7852, -4.9570,  ..., -4.7227, -3.0215, -1.6768],
        ...,
        [-2.1895, -3.1113, -4.0781,  ..., -4.4531, -4.0938, -1.6582],
        [-1.8926, -3.3613, -5.0820,  ..., -6.7539, -5.6133, -1.0811],
        [-2.9004, -2.8379, -4.1992,  ..., -4.4180, -4.1055, -1.8232]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 17, 15, 27,  1,  0,  4, 22,  0, 27,  4,  3, 17, 27, 20,  7, 27, 25,
        20,  7,  8, 18, 27,  3, 17, 27, 27, 18,  3, 20,  4, 27,  1, 27,  7, 21,
        27,  0,  3, 18,  7,  1, 27, 27,  1,  4, 20, 27, 27, 12, 22, 15,  9, 27,
        20, 10, 24,  5,  4, 27, 27, 15,  0, 18], device='cuda:0')
step: 94
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2382, -0.6104,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2520, -1.8887, -0.2050,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2186,  0.7979,  ..., -1.9043, -1.3438, -0.5962],
        [-0.4104, -0.2988, -1.7158,  ..., -0.6577,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.5554e-03, -8.7662e-03, -8.6594e-04,  ..., -2.6016e-03,
         -3.0403e-03, -5.8479e-03],
        [-1.4973e-03, -4.6501e-03, -8.7509e-03,  ...,  2.5146e-02,
          1.9119e-02, -1.2917e-02],
        [ 1.0166e-03, -2.3997e-04, -8.4698e-05,  ..., -3.0212e-03,
         -6.1083e-04,  3.7694e-04],
        [ 2.5501e-03, -4.6921e-04,  1.6232e-03,  ...,  4.6301e-04,
          7.3147e-04, -3.7098e-04],
        [ 1.3199e-03,  8.6021e-04, -1.1706e-04,  ...,  6.7353e-05,
          6.2883e-05, -7.6199e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5560, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.5918, -6.4062, -5.5312,  ..., -5.4844, -7.1875, -2.6074],
        [-4.2070, -3.8789, -4.5352,  ..., -5.3945, -5.4883, -0.4109],
        [-2.3965, -2.6465, -3.8184,  ..., -5.3945, -5.0039, -1.8652],
        ...,
        [-1.9102, -3.5664, -3.8477,  ..., -5.1602, -4.4102, -1.9102],
        [-2.8633, -3.2383, -3.1133,  ..., -4.1445, -4.4102, -1.7686],
        [-3.7129, -3.8223, -4.2891,  ..., -3.1348, -3.4941, -0.7749]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 10, 27,  3, 27, 27, 20, 27,  0, 18,  1, 27,  0,  9, 14, 27, 10, 19,
        27, 27, 27, 17, 27, 27,  4, 10, 20,  6, 27, 27,  6, 27, 27,  2, 17,  4,
        27, 27, 18, 27, 17,  0, 11, 21, 27, 27, 27,  0, 27, 10, 27, 22,  2, 27,
         3, 27, 27, 27, 17, 22, 15,  0, 11, 10], device='cuda:0')
step: 95
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2382, -0.6104,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2520, -1.8887, -0.2050,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2186,  0.7979,  ..., -1.9043, -1.3438, -0.5962],
        [-0.4104, -0.2988, -1.7158,  ..., -0.6577,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.9855e-03,  2.0313e-04,  8.7118e-04,  ..., -5.9605e-04,
         -4.8113e-04, -7.2289e-04],
        [ 6.2895e-04, -1.7233e-03, -8.7643e-04,  ..., -1.7176e-03,
         -4.1223e-04,  1.0624e-03],
        [-2.9736e-03,  1.7948e-03,  2.0103e-03,  ...,  5.5885e-03,
          4.7073e-03,  5.6725e-03],
        [ 3.5477e-04, -8.8871e-05,  1.2894e-03,  ...,  4.0245e-04,
         -5.6505e-04, -1.5888e-03],
        [-3.9697e-04,  1.0853e-03, -2.7490e-04,  ...,  1.7118e-04,
          2.0385e-04, -5.3453e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5497, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.9893, -3.5352, -4.5664,  ..., -3.4102, -2.6445, -1.8789],
        [-4.3320, -3.3945, -4.9102,  ..., -5.2383, -4.1758, -0.4731],
        [-1.6797, -2.6484, -2.6172,  ..., -4.0391, -4.7109, -1.9453],
        ...,
        [-2.7559, -2.4590, -4.3516,  ..., -4.9453, -3.7559, -1.7715],
        [-1.9756, -4.1484, -4.6953,  ..., -5.7734, -4.0547, -1.1787],
        [-3.3535, -3.5723, -4.6836,  ..., -3.1973, -4.5078, -0.7441]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 11, 14,  0,  1, 27, 27, 27, 27, 27,  4, 27, 27, 18,  4, 27, 27, 18,
        27, 20, 23,  6, 10, 27, 17, 15, 18, 10,  9,  2, 18, 18, 27, 27,  7,  7,
        26,  6, 25, 27,  1,  4, 19,  1,  1, 15, 27,  7, 10, 27, 27, 18, 11,  9,
        27, 27, 27,  1,  7, 20,  9, 27,  0,  9], device='cuda:0')
step: 96
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2382, -0.6104,  ...,  0.4788, -0.0858,  1.5098],
        [ 1.2520, -1.8887, -0.2050,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2186,  0.7979,  ..., -1.9043, -1.3438, -0.5967],
        [-0.4104, -0.2988, -1.7158,  ..., -0.6577,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.8556e-04,  2.8477e-03,  1.5640e-03,  ..., -6.2561e-04,
         -1.2569e-03, -2.2304e-04],
        [-1.7524e-04, -9.0122e-05, -1.3084e-03,  ..., -6.8569e-04,
          1.6689e-03,  2.4533e-04],
        [-8.6212e-04,  1.8635e-03, -1.3180e-03,  ..., -3.1137e-04,
          2.4891e-04,  1.5163e-03],
        [-1.8063e-03,  1.1034e-03,  3.4809e-03,  ...,  2.6560e-04,
         -4.1676e-04,  4.6921e-03],
        [ 2.2006e-04,  7.6723e-04,  1.0748e-03,  ...,  3.7432e-04,
          1.7047e-04,  2.9254e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5429, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.1133, -3.0977, -3.2852,  ..., -5.2070, -2.9258, -0.7690],
        [-3.6465, -3.6152, -5.1445,  ..., -4.4102, -4.8633, -0.3962],
        [-2.5449, -3.3887, -3.9180,  ..., -3.5898, -2.6211, -1.9180],
        ...,
        [-2.5723, -3.3086, -4.2305,  ..., -3.4180, -3.6680, -2.3086],
        [-2.5547, -3.6172, -3.5391,  ..., -3.4922, -3.3516, -1.2266],
        [-2.6953, -3.2891, -4.3359,  ..., -4.9766, -3.4297, -1.2432]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  3, 18,  2, 27, 27,  1,  5, 27, 17, 27, 25, 13, 17, 27, 14,  7, 27,
        27,  7, 22, 27, 27,  0,  3,  7,  5,  0, 22, 27,  2, 20, 27, 27,  1, 17,
         4,  9, 24,  4,  2, 27, 25, 18,  0,  0, 11, 27,  5, 24, 13, 27, 27, 27,
        27, 27, 27, 27,  4,  5, 13,  0,  1, 27], device='cuda:0')
step: 97
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2382, -0.6104,  ...,  0.4790, -0.0858,  1.5098],
        [ 1.2520, -1.8887, -0.2048,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2186,  0.7979,  ..., -1.9043, -1.3438, -0.5967],
        [-0.4104, -0.2988, -1.7158,  ..., -0.6577,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.9427e-04,  3.0403e-03,  2.8515e-04,  ..., -6.5660e-04,
          8.6927e-04,  8.5258e-04],
        [ 2.0275e-03, -1.3199e-03,  1.0353e-02,  ..., -8.0729e-04,
         -4.5624e-03,  9.6588e-03],
        [ 4.5052e-03, -4.7188e-03,  3.7003e-03,  ..., -1.0445e-02,
         -7.1945e-03,  1.8311e-02],
        [ 9.2685e-05, -1.1263e-03,  2.8095e-03,  ...,  2.5120e-03,
         -6.8140e-04,  8.1062e-04],
        [ 8.1444e-04, -1.6904e-04,  3.4392e-05,  ...,  5.0402e-04,
         -3.6597e-04,  3.8981e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.7289, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.1582, -3.7988, -4.7969,  ..., -5.1562, -4.6406, -0.5322],
        [-2.5430, -4.1055, -3.9492,  ..., -3.6523, -4.1211, -1.1045],
        [-2.7266, -4.2422, -4.8203,  ..., -5.8359, -5.9766, -0.6323],
        ...,
        [-3.6855, -4.2148, -4.5898,  ..., -5.6211, -6.3711, -0.3254],
        [-2.8809, -3.3184, -4.4258,  ..., -5.8633, -3.7559, -0.8960],
        [-3.5703, -3.0703, -3.3984,  ..., -3.7891, -4.5391, -1.2725]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 15,  7, 27, 27, 27, 27, 22,  5, 27, 17, 27, 27, 10, 17,  3, 27,  1,
        27, 27, 27, 27, 10, 27,  7, 11, 27,  4, 14, 18, 27, 27, 13,  3, 27,  2,
        25, 17,  6, 27, 27,  3, 17, 27,  0,  1,  8,  3, 10, 20,  7, 27, 27, 11,
        26, 11,  5,  4, 17, 27, 15,  4, 13, 27], device='cuda:0')
step: 98
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2382, -0.6104,  ...,  0.4790, -0.0858,  1.5098],
        [ 1.2520, -1.8887, -0.2048,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2186,  0.7979,  ..., -1.9043, -1.3438, -0.5967],
        [-0.4104, -0.2988, -1.7158,  ..., -0.6577,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.3174e-04,  2.9984e-03,  1.5640e-03,  ..., -8.2684e-04,
         -7.5817e-04,  7.5579e-04],
        [-4.7088e-04,  4.9095e-03,  6.9466e-03,  ...,  5.5237e-03,
          1.5080e-05, -3.5496e-03],
        [ 6.9571e-04, -4.9114e-04,  7.3814e-04,  ..., -1.0023e-03,
         -2.6846e-04, -2.1720e-04],
        [-8.3637e-04, -2.5749e-05, -3.9220e-04,  ...,  3.8958e-04,
         -5.8317e-04, -5.9485e-05],
        [ 1.7881e-07,  9.8884e-05,  2.2304e-04,  ..., -1.6999e-04,
          7.8559e-05,  2.4915e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.9912, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.9199, -3.8574, -3.8105,  ..., -4.9219, -3.6855, -1.1074],
        [-1.6260, -3.1582, -3.3281,  ..., -5.5000, -4.5625, -1.4229],
        [-2.4570, -4.1602, -4.8633,  ..., -5.0664, -4.1602, -1.2217],
        ...,
        [-2.8105, -3.6387, -3.8574,  ..., -5.5430, -6.0273, -0.7944],
        [-2.8145, -3.4082, -3.6582,  ..., -4.4219, -3.6426, -1.2051],
        [-6.5352, -3.5352, -6.5195,  ..., -5.0039, -2.2852, -0.5972]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 27, 27, 27, 22, 15, 15, 14,  3, 27, 22, 26, 12, 27, 27, 27, 15,  3,
         4,  9,  3, 27, 27, 27, 22, 27, 27,  6,  0, 27,  0,  6, 26, 22, 15, 25,
        15, 27, 26,  4,  3, 21, 27,  5,  1,  9, 15,  5, 27, 10,  5, 14, 10, 22,
        27, 17, 15,  7, 22, 25,  1, 18, 17, 27], device='cuda:0')
step: 99
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2382, -0.6104,  ...,  0.4790, -0.0857,  1.5098],
        [ 1.2520, -1.8887, -0.2050,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2185,  0.7979,  ..., -1.9043, -1.3438, -0.5967],
        [-0.4104, -0.2988, -1.7158,  ..., -0.6577,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.6088e-05, -1.9836e-03,  1.7405e-03,  ...,  2.0237e-03,
         -4.0269e-04, -2.1982e-04],
        [ 1.0281e-03,  4.8065e-03,  4.2992e-03,  ..., -3.9597e-03,
         -5.1308e-03, -3.4313e-03],
        [ 2.7990e-04, -1.2712e-03, -4.9353e-04,  ..., -3.0136e-03,
         -6.3777e-05,  1.5659e-03],
        [ 2.7990e-04, -6.4135e-04, -1.2569e-03,  ..., -8.0299e-04,
          8.4209e-04,  8.0943e-05],
        [-1.4734e-04, -2.4438e-06,  9.0170e-04,  ..., -5.6148e-05,
          7.4005e-04,  5.7411e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6350, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.3457, -4.1758, -3.9082,  ..., -2.9863, -2.9238, -2.7676],
        [-2.3164, -3.5977, -3.8633,  ..., -4.7539, -4.5820, -1.3154],
        [-1.6807, -3.6973, -3.2109,  ..., -4.3984, -3.7422, -1.3369],
        ...,
        [-1.3955, -3.7090, -4.1133,  ..., -3.8965, -5.6133, -1.5830],
        [-1.9805, -3.7617, -2.9648,  ..., -1.4023, -4.7305, -2.2305],
        [-4.2344, -4.0469, -4.2031,  ..., -2.0156, -3.8594, -1.2041]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  5, 26,  3,  0, 25, 22,  0, 15,  0, 11, 11, 12, 17,  3, 22, 27,
         9,  5, 27, 18, 22, 27, 27, 26, 27, 15,  3, 10,  1, 27, 27, 27, 17, 18,
         9, 27, 13, 27, 27, 27, 17, 24, 25, 15,  5,  4,  9,  2, 15,  2,  4,  7,
        15, 27, 18, 27, 27, 27, 18, 17, 25, 25], device='cuda:0')
step: 100
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6104,  ...,  0.4790, -0.0857,  1.5098],
        [ 1.2520, -1.8887, -0.2050,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2185,  0.7979,  ..., -1.9043, -1.3438, -0.5967],
        [-0.4104, -0.2988, -1.7158,  ..., -0.6577,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.2670e-04,  7.6771e-04, -1.5841e-03,  ...,  1.4143e-03,
          4.2748e-04, -1.2779e-03],
        [ 1.5984e-03, -2.7180e-04, -6.8903e-04,  ...,  1.9159e-03,
          1.3170e-03, -1.0548e-03],
        [-3.0637e-04,  2.4080e-04, -2.3508e-04,  ...,  1.0920e-03,
          8.1897e-05, -8.3685e-04],
        [ 1.2884e-03,  5.9032e-04,  4.1413e-04,  ...,  1.9169e-03,
         -1.3435e-04, -1.0691e-03],
        [ 5.1975e-04,  3.7050e-04, -4.4966e-04,  ...,  2.0504e-05,
          1.8215e-04, -5.0068e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.7215, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.9473, -3.1973, -3.2910,  ..., -3.7754, -2.9316, -1.6504],
        [-3.3594, -3.8438, -4.6875,  ..., -4.5469, -5.3125, -0.4849],
        [-2.0312, -2.9062, -3.7188,  ..., -4.3281, -3.9219, -1.2646],
        ...,
        [-2.7578, -3.3516, -3.6641,  ..., -3.0391, -2.3359, -2.7734],
        [-3.7988, -4.3438, -4.9062,  ..., -4.3906, -4.6250, -0.4080],
        [-4.1289, -3.2383, -4.1289,  ..., -4.3633, -5.5508, -1.0654]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27, 15, 27, 20, 10,  2,  3, 27, 27,  6, 27, 22, 17,  1, 18,  3,
        25, 27,  7, 27, 20, 27, 10, 25, 20, 22, 27, 27, 11, 18, 10, 27, 10, 27,
         9,  4, 18, 11,  5, 27,  7,  0,  1,  1,  4,  5, 10, 20, 18, 15,  1,  0,
         3,  1, 17,  0, 25, 27,  0, 13,  4, 19], device='cuda:0')
step: 101
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6104,  ...,  0.4790, -0.0857,  1.5098],
        [ 1.2520, -1.8887, -0.2050,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2185,  0.7979,  ..., -1.9043, -1.3438, -0.5967],
        [-0.4104, -0.2988, -1.7168,  ..., -0.6577,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.7459e-04, -1.4858e-03,  7.8487e-04,  ..., -1.5378e-04,
          1.9240e-04,  9.0647e-04],
        [ 6.8569e-04,  2.2316e-03,  1.0223e-03,  ...,  4.9858e-03,
          3.2806e-03, -1.9350e-03],
        [ 5.7030e-04, -1.6749e-04, -7.3814e-04,  ..., -1.9493e-03,
         -1.4677e-03, -6.7711e-04],
        [-1.0242e-03,  4.8018e-04,  1.1854e-03,  ...,  4.8399e-04,
          4.1533e-04, -1.4706e-03],
        [-6.7282e-04,  1.0567e-03,  8.5402e-04,  ...,  3.7849e-05,
         -2.4056e-04, -7.6652e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.7563, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.5039, -3.8633, -4.4102,  ..., -4.6445, -4.7383, -1.9414],
        [-4.0938, -4.6094, -5.1250,  ..., -5.2656, -5.3125, -0.4707],
        [-2.6191, -3.3223, -4.0430,  ..., -3.9785, -4.4336, -1.1035],
        ...,
        [-4.7344, -3.5020, -4.7344,  ..., -5.2188, -5.1875, -0.5479],
        [-2.9746, -2.8496, -3.1934,  ..., -3.2246, -4.6016, -1.1943],
        [-0.9185, -3.5430, -3.4180,  ..., -5.1992, -4.2773, -2.9180]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10,  3, 27, 27, 23,  4, 27,  7,  7,  5,  4, 27, 27, 27, 18, 13, 25, 25,
        27, 27, 27,  1,  7, 27,  0,  5,  9,  7, 27,  3,  3,  7,  2, 21, 27, 27,
        27,  7, 27, 27, 27, 17,  0, 27, 27,  3,  9, 27, 27, 25, 20, 12, 20, 18,
        22, 27, 10, 25,  0, 25, 25, 22, 11,  0], device='cuda:0')
step: 102
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6104,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2520, -1.8887, -0.2050,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2185,  0.7979,  ..., -1.9033, -1.3438, -0.5972],
        [-0.4104, -0.2988, -1.7168,  ..., -0.6577,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.1052e-03,  1.8864e-03,  3.4027e-03,  ...,  1.0624e-03,
         -7.7152e-04,  2.6932e-03],
        [-1.5392e-03,  2.3060e-03,  1.5411e-03,  ..., -9.8801e-03,
         -1.0254e-02,  3.9558e-03],
        [-6.7890e-05,  1.7796e-03, -9.9003e-05,  ..., -2.1725e-03,
          1.3981e-03,  4.7493e-03],
        [-2.5063e-03,  8.2016e-05,  1.5602e-03,  ...,  1.8053e-03,
         -5.4693e-04,  2.8229e-03],
        [-9.6178e-04,  9.6416e-04,  2.9254e-04,  ..., -2.9683e-05,
         -5.9223e-04, -8.7261e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.7359, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.2891, -3.0859, -3.7422,  ..., -5.4922, -6.4922, -0.8989],
        [-2.7871, -3.3027, -4.3320,  ..., -3.7559, -2.9590, -0.7866],
        [-3.8340, -2.8027, -3.7715,  ..., -1.9287, -3.3027, -2.0684],
        ...,
        [-2.1387, -3.3262, -3.8887,  ..., -3.4355, -5.0898, -1.7168],
        [-3.3633, -3.4258, -3.6133,  ..., -3.5195, -4.3164, -1.5664],
        [-3.7910, -3.4785, -4.4453,  ..., -3.3066, -3.1191, -1.8379]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 22,  2,  0,  5, 25, 27,  9, 24,  0, 13, 11,  3,  4, 15, 27,  9, 26,
        17, 18, 17,  0,  1,  7, 27,  1, 18, 15, 20,  3, 20, 12, 27, 18, 27, 27,
        27, 27, 17,  4, 25, 27, 27, 22,  1, 27, 15, 27,  7, 18, 27,  7,  2,  7,
         2, 10, 27,  3, 27, 27, 27,  1, 18, 27], device='cuda:0')
step: 103
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6108,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2185,  0.7979,  ..., -1.9033, -1.3438, -0.5972],
        [-0.4104, -0.2988, -1.7168,  ..., -0.6577,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0017,  0.0006, -0.0023,  ..., -0.0028,  0.0019, -0.0042],
        [ 0.0008, -0.0015, -0.0028,  ..., -0.0016,  0.0007, -0.0008],
        [ 0.0030, -0.0023, -0.0017,  ..., -0.0057, -0.0010,  0.0026],
        [ 0.0001, -0.0014, -0.0004,  ...,  0.0008, -0.0006, -0.0014],
        [ 0.0007, -0.0002, -0.0008,  ...,  0.0002,  0.0003, -0.0006]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.2286, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.4941, -4.1484, -4.6641,  ..., -4.6484, -4.6797, -0.4622],
        [-2.2227, -3.0977, -3.1445,  ..., -3.8164, -3.7520, -1.6123],
        [-2.6992, -3.3867, -3.3555,  ..., -3.6680, -3.2773, -1.2930],
        ...,
        [-2.2031, -3.9062, -3.0469,  ..., -3.2500, -3.7031, -0.9688],
        [-3.7109, -3.6016, -5.2422,  ..., -5.1797, -5.4453, -0.6011],
        [-2.4668, -3.4531, -3.0000,  ..., -4.2344, -4.6719, -1.0303]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 18,  3, 18,  2, 18, 27, 27, 27,  1, 17, 18,  4, 27, 27, 27,  1,  3,
        18,  2, 22, 25, 27,  2, 27, 11, 27, 17, 27, 27,  4, 11,  0, 11, 27, 15,
        27, 27, 20, 27, 20, 27, 27, 14,  4, 27, 15,  0, 20, 18,  6, 27, 17, 27,
        12, 10, 18, 15,  7, 27, 15, 27,  6, 27], device='cuda:0')
step: 104
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6108,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2185,  0.7979,  ..., -1.9033, -1.3438, -0.5972],
        [-0.4104, -0.2988, -1.7168,  ..., -0.6577,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.0060e-03,  3.1719e-03,  5.3167e-04,  ..., -1.1206e-03,
          1.5182e-03, -1.8854e-03],
        [ 1.5059e-03, -1.8396e-03, -2.4357e-03,  ..., -8.6927e-04,
          4.9400e-04, -5.3406e-04],
        [ 1.3275e-03, -2.3441e-03,  1.8196e-03,  ..., -7.4673e-04,
         -2.3708e-03,  2.1210e-03],
        [ 1.1482e-03, -4.8256e-04,  7.1335e-04,  ..., -6.3777e-06,
          1.4544e-03, -1.0214e-03],
        [ 5.7161e-05, -1.2118e-04,  7.7295e-04,  ...,  4.2295e-04,
          7.4911e-04, -3.4261e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3483, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.4043, -4.1992, -4.3555,  ..., -3.1855, -4.1367, -1.1387],
        [-3.2246, -3.1953, -5.1484,  ..., -5.0234, -4.8672, -0.8818],
        [-4.3047, -3.9141, -4.5859,  ..., -2.6641, -2.9453, -1.0400],
        ...,
        [-5.2109, -3.3516, -5.4453,  ..., -3.6016, -3.7891, -0.5083],
        [-4.2969, -4.1875, -4.2031,  ..., -5.8281, -5.6094, -0.5322],
        [-2.5938, -2.8750, -3.5312,  ..., -4.0156, -3.1562, -2.0312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 15, 17, 18,  6, 27, 27, 27,  5,  4, 27, 17, 21, 27,  7, 11, 20, 27,
        27,  0, 20,  0, 21, 27, 11, 26, 27, 27, 27, 27, 27, 27, 26, 27, 25, 27,
         1, 27,  7, 27, 27,  2, 13, 27, 17, 27, 11, 27,  4, 25, 27,  0, 18, 27,
        27, 10, 14, 27, 27, 25, 18,  6,  3,  6], device='cuda:0')
step: 105
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2383, -0.6108,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2184,  0.7979,  ..., -1.9033, -1.3438, -0.5972],
        [-0.4104, -0.2988, -1.7168,  ..., -0.6577,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.1114e-03, -8.9216e-04,  6.5994e-04,  ..., -3.6144e-04,
         -1.0729e-03,  1.5821e-03],
        [ 1.0318e-04,  5.4436e-03,  5.1956e-03,  ...,  5.8060e-03,
          7.8583e-04, -7.2861e-03],
        [-1.4162e-03,  2.3460e-03,  7.9513e-05,  ...,  1.7729e-03,
          2.6379e-03,  8.4162e-04],
        [ 7.4148e-04, -8.5735e-04,  4.5729e-04,  ...,  4.5514e-04,
         -1.2302e-04, -9.0170e-04],
        [-4.2272e-04,  1.1075e-04,  1.6928e-03,  ..., -6.1607e-04,
         -2.8849e-04,  1.9875e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3363, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.7129, -3.2148, -4.1523,  ..., -4.9805, -4.2148, -2.6816],
        [-4.8086, -3.1992, -4.2148,  ..., -4.4492, -4.4648, -1.1992],
        [-4.6875, -3.6543, -4.4688,  ..., -4.6523, -5.5312, -0.6392],
        ...,
        [-2.6934, -3.1465, -3.2871,  ..., -3.5059, -4.6758, -1.5215],
        [-2.8691, -3.8535, -3.5410,  ..., -3.8848, -4.4453, -0.8057],
        [-2.9434, -2.6934, -4.5195,  ..., -4.8203, -4.2383, -1.5205]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15,  1,  3, 10, 27, 27, 27, 27,  2, 27,  0, 26, 17,  7, 20,  7, 27, 15,
         2,  0, 15,  2, 27, 27,  4, 10,  0, 15, 27, 27,  3, 27, 15, 11, 18, 20,
        27, 27,  7, 27, 24,  6, 18,  0,  9, 27, 13,  0, 27, 11, 27, 27, 27, 27,
         0, 10,  6, 27, 27, 27, 27, 27, 27, 27], device='cuda:0')
step: 106
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2383, -0.6108,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2184,  0.7979,  ..., -1.9033, -1.3438, -0.5972],
        [-0.4104, -0.2988, -1.7168,  ..., -0.6582,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.5885e-04,  4.4155e-04, -7.7152e-04,  ...,  3.4642e-04,
         -4.6325e-04, -1.0977e-03],
        [-2.3913e-04,  1.3895e-03,  1.3132e-03,  ...,  3.5191e-03,
         -1.0748e-03, -1.5984e-03],
        [ 9.4318e-04, -7.3290e-04, -1.6222e-03,  ...,  6.6328e-04,
         -6.1703e-04,  1.5469e-03],
        [ 3.1614e-04, -8.6069e-04, -5.7268e-04,  ...,  3.3975e-05,
         -2.1191e-03, -8.1348e-04],
        [ 4.9877e-04, -8.8978e-04, -8.9931e-04,  ..., -2.0051e-04,
         -9.7692e-05, -3.8528e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6849, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.1289, -2.6289, -4.0820,  ..., -4.4883, -5.0508, -0.9258],
        [-3.2266, -2.7578, -3.3516,  ..., -3.3672, -3.9766, -1.5078],
        [-2.4863, -3.5801, -3.7520,  ..., -3.6582, -3.6113, -1.2832],
        ...,
        [-3.1348, -4.0117, -5.9961,  ..., -5.3242, -3.7910, -0.8540],
        [-2.0879, -3.4316, -2.9473,  ..., -3.6660, -4.0859, -1.8535],
        [-4.2969, -3.3125, -3.7344,  ..., -2.8906, -4.1406, -0.8755]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  3, 15, 17, 27,  0,  0,  6,  0,  3, 27, 20,  4,  7, 27, 18,  7, 27,
        13,  3, 27,  1, 27, 15,  1, 18,  9, 14, 17, 15, 27,  3, 17, 15, 15, 18,
        27, 27,  4, 27, 27, 27, 27, 22, 12, 15, 27,  9, 27,  6,  1, 24, 27,  1,
         1, 26, 10, 27, 10, 17,  4, 15, 18,  1], device='cuda:0')
step: 107
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2383, -0.6108,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2520, -1.8887, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2184,  0.7979,  ..., -1.9033, -1.3438, -0.5972],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6582,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.5764e-03,  2.0063e-04,  4.5547e-03,  ...,  4.9324e-03,
         -2.0447e-03,  4.5357e-03],
        [ 9.3102e-05,  4.9400e-04, -7.0667e-04,  ...,  2.1667e-03,
          2.3975e-03,  6.7949e-04],
        [-1.3876e-04,  4.5729e-04, -1.4091e-04,  ...,  1.1778e-03,
         -5.9271e-04,  1.6527e-03],
        [ 1.1873e-03, -7.6354e-05,  2.0008e-03,  ...,  1.2484e-03,
          4.4584e-04,  6.2370e-04],
        [ 3.2544e-04,  1.7595e-03,  4.4608e-04,  ...,  3.8445e-05,
          9.7942e-04, -1.3857e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4529, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9902, -4.0547, -4.8828,  ..., -5.8359, -5.2422, -0.5850],
        [-3.7012, -3.1074, -3.7168,  ..., -3.5605, -2.9980, -1.1689],
        [-3.0332, -2.2832, -2.0020,  ..., -3.0801, -4.7695, -1.9863],
        ...,
        [-3.0781, -3.0469, -3.7500,  ..., -3.5469, -2.7812, -1.7969],
        [-3.2148, -3.4492, -4.0586,  ..., -3.4492, -4.4336, -0.9331],
        [-2.3867, -3.0742, -3.8555,  ..., -4.7461, -5.2305, -1.6680]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27, 27,  0, 15,  3, 27, 27, 27,  3, 17, 27, 27,  1, 27, 17,  1,
         5, 10, 27,  3, 15, 17, 27, 27, 27, 27, 15, 27,  1, 27, 26,  1, 27, 27,
         7, 25, 27, 10, 27, 15, 13,  1, 27, 11, 27,  2, 18, 20, 10, 18,  5,  0,
         0,  1, 12, 27, 27, 27, 27, 27, 10, 27], device='cuda:0')
step: 108
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2383, -0.6108,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2520, -1.8896, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2184,  0.7979,  ..., -1.9033, -1.3438, -0.5977],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6582,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0036,  0.0035, -0.0023,  ..., -0.0026,  0.0021, -0.0053],
        [ 0.0010, -0.0054, -0.0080,  ...,  0.0128,  0.0114, -0.0066],
        [ 0.0022, -0.0023, -0.0009,  ..., -0.0035, -0.0022, -0.0020],
        [ 0.0023, -0.0017,  0.0013,  ...,  0.0020,  0.0006, -0.0024],
        [ 0.0009, -0.0004, -0.0005,  ..., -0.0009,  0.0008, -0.0007]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.4580, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.4102, -3.2695, -3.2383,  ..., -3.3477, -5.4414, -0.7065],
        [-3.9375, -4.2656, -5.2500,  ..., -3.7812, -4.5156, -0.9692],
        [-3.9863, -2.5000, -3.9531,  ..., -3.5469, -4.6094, -1.2812],
        ...,
        [-3.0254, -3.1523, -3.2598,  ..., -3.8223, -3.9336, -1.0576],
        [-3.4141, -3.3496, -4.0703,  ..., -2.8516, -3.4453, -1.1953],
        [-2.0469, -3.6113, -2.9551,  ..., -3.3770, -4.8750, -1.4854]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  9, 10, 26,  1, 15, 27, 17,  7, 27, 10,  3,  1, 27, 14, 27, 27, 27,
        27,  3, 15, 27,  9, 27, 24, 26, 27, 27, 20, 27, 17, 27,  7, 10, 15,  2,
        27, 27, 25, 27, 18, 18, 27, 27, 27, 27,  6, 27, 27, 27,  0,  6,  2, 27,
        26, 27,  7,  1, 27, 26, 20, 27, 26,  8], device='cuda:0')
step: 109
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2383, -0.6108,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2510, -1.8896, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2183,  0.7979,  ..., -1.9033, -1.3438, -0.5977],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6582,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.6785e-03, -1.8530e-03, -1.4420e-03,  ...,  1.5402e-03,
         -5.8413e-05, -4.8904e-03],
        [ 8.6069e-04,  5.1260e-04,  3.6061e-05,  ...,  1.0738e-03,
          6.0463e-04, -1.7996e-03],
        [ 6.9141e-05,  9.2030e-04, -8.7619e-05,  ...,  1.3828e-03,
          1.2469e-04,  1.0281e-03],
        [ 2.6202e-04, -1.1712e-04, -2.5749e-03,  ...,  4.9067e-04,
          1.3256e-04, -6.3372e-04],
        [-1.4901e-04, -4.9973e-04, -5.4264e-04,  ..., -6.7329e-04,
          2.9302e-04,  2.1696e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.8222, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.9922, -4.7734, -7.0547,  ..., -2.9316, -4.0547, -0.2747],
        [-2.2441, -4.2891, -5.4453,  ..., -4.6797, -4.2422, -0.7432],
        [-4.3594, -3.7637, -4.9219,  ..., -5.4062, -4.3750, -0.6865],
        ...,
        [-4.9258, -3.2695, -3.8945,  ..., -5.1445, -4.2227, -0.7227],
        [-5.4453, -3.5410, -4.6797,  ..., -4.9453, -6.0078, -0.7905],
        [-2.8828, -3.6797, -2.6953,  ..., -3.0703, -4.5547, -1.3984]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 18, 27,  1, 24, 22, 27,  1, 25, 14,  0, 27, 19,  0, 10, 27,  4,  0,
        15, 27, 27, 17,  4, 27, 14, 14,  1,  9, 20,  3,  7, 26, 27,  6,  3, 27,
        20, 18, 15,  0, 26, 27,  4, 27,  1, 11, 27,  7,  1, 27, 22, 14, 26, 27,
         1, 27, 27, 11,  7,  2,  0,  6, 27, 27], device='cuda:0')
step: 110
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2383, -0.6108,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2510, -1.8896, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9121, -0.2183,  0.7979,  ..., -1.9033, -1.3438, -0.5977],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6582,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0043,  0.0073,  0.0060,  ...,  0.0015, -0.0009,  0.0041],
        [-0.0014, -0.0008, -0.0017,  ..., -0.0020, -0.0036,  0.0050],
        [ 0.0004, -0.0003, -0.0004,  ..., -0.0003, -0.0007,  0.0005],
        [-0.0008,  0.0018,  0.0032,  ...,  0.0015,  0.0007,  0.0011],
        [-0.0012,  0.0019,  0.0008,  ...,  0.0007, -0.0008,  0.0001]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.8322, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.9785, -2.8691, -4.8398,  ..., -2.9941, -4.3555, -1.0107],
        [-4.2422, -4.0859, -4.8203,  ..., -3.8516, -5.5547, -0.5547],
        [-1.3643, -2.8789, -3.2559,  ..., -4.6758, -2.6309, -2.2383],
        ...,
        [-3.8340, -3.3027, -2.5684,  ..., -3.3652, -5.5703, -1.0527],
        [-2.4648, -3.5898, -4.1211,  ..., -3.1211, -4.4492, -1.4023],
        [-4.4258, -3.5039, -3.9102,  ..., -3.1914, -4.4727, -0.5513]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15,  5,  7, 10, 27, 27, 27, 13, 27, 19, 18, 25,  7, 17, 17, 20,  7, 27,
         0,  5, 12, 15, 27,  7,  8,  3, 11, 15, 26, 26, 27,  7, 15,  3, 27, 27,
         4,  1, 22,  0, 27, 27, 27, 26, 27, 27, 27, 27,  4, 17, 18,  4, 11,  1,
        11, 13, 27, 26, 10, 27, 18,  2, 18,  9], device='cuda:0')
step: 111
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2383, -0.6108,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2510, -1.8896, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9111, -0.2183,  0.7979,  ..., -1.9033, -1.3438, -0.5977],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6582,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0071e-03,  7.3853e-03,  3.6488e-03,  ..., -9.0361e-04,
         -1.8950e-03,  1.3771e-03],
        [-4.0221e-04,  2.9221e-03,  4.0741e-03,  ...,  3.1891e-03,
         -1.2522e-03, -1.9312e-03],
        [-6.6376e-04, -4.4727e-04,  1.3552e-03,  ...,  6.7568e-04,
         -1.8511e-03, -1.1673e-03],
        [-1.9550e-03,  1.9293e-03,  2.3975e-03,  ..., -1.0653e-03,
         -2.5482e-03,  3.3875e-03],
        [-5.3835e-04,  5.6124e-04, -4.6015e-04,  ...,  7.7069e-05,
         -7.7772e-04,  4.1246e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3959, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.6621, -2.5527, -3.8965,  ..., -5.0703, -5.4297, -1.0381],
        [-4.6758, -3.1152, -3.5039,  ..., -2.5820, -3.4258, -1.3330],
        [-4.4766, -2.7871, -3.0684,  ..., -2.9121, -4.1641, -0.9438],
        ...,
        [-4.4688, -3.5938, -4.7812,  ..., -3.5625, -3.5312, -0.5928],
        [-4.1914, -3.4883, -3.3320,  ..., -1.4883, -3.6914, -1.6758],
        [-3.5078, -3.4609, -3.5078,  ..., -3.9297, -3.1641, -1.6484]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22, 27, 27,  5, 10, 18, 27, 22, 27,  7, 27,  9, 27,  4, 27,  1, 26,  1,
        22, 27, 27, 27,  4, 27, 20, 27, 27, 27, 27, 27, 27,  9,  8, 20, 13, 25,
         0,  3, 27,  0, 20,  0, 27, 24,  6,  2, 10, 15, 27,  4,  3,  4,  1, 27,
        27, 17, 27, 27, 27,  4,  1,  4, 18,  7], device='cuda:0')
step: 112
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2384, -0.6108,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2510, -1.8896, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9111, -0.2183,  0.7979,  ..., -1.9033, -1.3438, -0.5977],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6582,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.4264e-04, -4.2076e-03,  1.1444e-03,  ...,  2.8229e-03,
         -4.7112e-04,  1.8291e-03],
        [-2.1248e-03, -1.2980e-03, -2.1782e-03,  ..., -2.4891e-03,
          2.2354e-03,  4.9248e-03],
        [ 3.5343e-03, -2.9373e-03,  3.6888e-03,  ..., -1.0471e-03,
         -2.2068e-03,  6.1874e-03],
        [ 5.7888e-04, -2.4414e-04,  1.6432e-03,  ...,  6.8760e-04,
         -3.3140e-04, -1.5755e-03],
        [ 9.0122e-04, -4.1819e-04, -3.5501e-04,  ..., -1.1170e-04,
          3.2842e-05, -1.0986e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6548, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.8809, -4.5391, -4.4570,  ..., -3.8184, -3.9902, -1.0674],
        [-3.2852, -2.6289, -4.9414,  ..., -4.5977, -4.6602, -1.2373],
        [-2.1250, -3.5156, -4.2188,  ..., -3.8125, -2.9688, -1.7656],
        ...,
        [-2.4375, -3.2656, -3.9219,  ..., -4.0469, -3.7812, -0.7969],
        [-3.2129, -3.4941, -2.7617,  ..., -3.0586, -3.8848, -0.9946],
        [-3.3203, -2.7734, -2.9766,  ..., -4.3672, -4.8828, -0.8823]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 18, 13, 15, 25, 27, 15, 11, 18, 27, 22,  4,  0, 11,  3,  5, 27, 10,
         2, 17, 22, 25,  4, 17, 27,  7, 26, 10,  5, 27, 18, 18,  2, 27, 26, 22,
        18, 27, 18, 27, 23, 27, 15,  0, 18, 27, 20, 27, 27, 27, 27,  7,  6, 18,
        27,  0, 15, 27,  0, 27, 18,  4, 19,  2], device='cuda:0')
step: 113
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2384, -0.6108,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2510, -1.8896, -0.2051,  ..., -2.3828,  0.6030, -0.8242],
        [ 1.9111, -0.2183,  0.7979,  ..., -1.9033, -1.3438, -0.5977],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6582,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.9798e-03,  9.9087e-04,  1.4791e-03,  ..., -1.4029e-03,
         -1.1425e-03,  1.3018e-03],
        [ 3.1233e-04,  7.3814e-04,  3.2735e-04,  ...,  2.3460e-03,
          4.7588e-04, -1.1358e-03],
        [-1.1292e-03,  5.9128e-04, -7.5960e-04,  ...,  2.3575e-03,
          6.5470e-04, -2.6913e-03],
        [ 6.2847e-04,  2.1458e-05,  2.7065e-03,  ...,  1.5688e-03,
          5.9414e-04, -4.7541e-04],
        [-7.7820e-04,  5.4073e-04, -1.1343e-04,  ...,  7.6056e-04,
         -5.0640e-04, -2.2399e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6577, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.6348, -3.4941, -4.4297,  ..., -4.0234, -4.6016, -0.7593],
        [-2.6445, -2.8164, -3.8633,  ..., -4.8633, -4.2539, -1.2539],
        [-4.3555, -3.4668, -4.4492,  ..., -4.0742, -5.2617, -0.5132],
        ...,
        [-4.5352, -3.4414, -5.2852,  ..., -4.1602, -5.6445, -0.3633],
        [-2.7637, -3.3887, -3.7480,  ..., -3.8887, -3.5605, -1.6230],
        [-3.0000, -3.9082, -4.1250,  ..., -3.6406, -3.6582, -1.0322]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  7, 27, 27, 22, 27, 17,  4,  1, 27,  4, 27, 17, 18, 27, 20, 19, 27,
        18, 27,  9,  4,  7, 25,  3,  0, 27, 27, 18,  3,  9, 17, 12, 24,  4,  4,
        27,  0,  5, 25, 27,  4,  8, 27, 27,  7, 10,  0, 20, 26,  4, 27, 15,  4,
        27, 11, 27,  0, 26, 27, 18, 27, 15, 23], device='cuda:0')
step: 114
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2384, -0.6108,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2510, -1.8896, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2183,  0.7979,  ..., -1.9033, -1.3438, -0.5977],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6582,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.7061e-04, -3.9139e-03, -2.0752e-03,  ...,  2.6321e-03,
          4.7684e-05, -6.2037e-04],
        [ 4.3869e-04,  6.7329e-04,  7.0286e-04,  ..., -3.1300e-03,
         -4.7836e-03, -1.8334e-04],
        [-1.7080e-03,  5.3864e-03, -1.3952e-03,  ...,  1.6603e-03,
          6.3782e-03,  8.8692e-04],
        [ 1.4448e-04,  3.8671e-04, -1.9646e-03,  ..., -1.4277e-03,
         -9.7942e-04, -3.0231e-03],
        [ 7.8487e-04, -1.1816e-03, -7.6914e-04,  ..., -2.3127e-04,
         -4.9782e-04,  8.9931e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5155, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.5508, -3.4102, -4.2852,  ..., -3.4258, -2.7695, -0.7222],
        [-4.0938, -3.5020, -2.7832,  ..., -2.7207, -3.7832, -1.0010],
        [-4.5547, -3.0566, -3.3066,  ..., -2.9473, -3.6348, -1.0410],
        ...,
        [-3.8789, -2.8164, -3.7363,  ..., -3.5195, -5.1914, -1.2061],
        [-3.0840, -3.2871, -3.1309,  ..., -4.5234, -2.8184, -1.0068],
        [-2.4590, -2.9590, -4.8633,  ..., -4.9102, -4.0664, -0.8018]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  3,  1, 27,  1, 15, 27,  0, 18, 15, 27, 27, 15,  6, 20, 25,  0,  2,
        20, 26, 22, 21,  4,  4,  7, 27, 27, 27,  9,  0, 27, 27, 10, 15,  0, 27,
        27, 10, 27,  8, 27, 15,  0, 26,  1,  3, 27, 27, 18, 22, 27, 27, 27,  9,
        27,  9,  4,  8, 27,  3,  8, 27,  7,  0], device='cuda:0')
step: 115
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2384, -0.6108,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2510, -1.8896, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2183,  0.7979,  ..., -1.9033, -1.3438, -0.5977],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6582,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.7084e-04, -6.6102e-05,  1.3371e-03,  ...,  7.8321e-05,
         -1.2903e-03,  3.6030e-03],
        [-7.2622e-04, -3.8862e-04,  3.7575e-04,  ..., -5.7907e-03,
         -5.9128e-03,  2.0046e-03],
        [-2.6989e-04, -2.8074e-05,  1.9264e-04,  ..., -1.0033e-03,
         -4.0889e-04, -4.8351e-04],
        [-1.2980e-03,  4.9710e-05,  3.1223e-03,  ..., -8.0013e-04,
         -4.5609e-04,  2.1667e-03],
        [ 6.6328e-04, -9.4557e-04,  8.2970e-04,  ...,  2.5797e-04,
         -2.7418e-04, -3.8743e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3046, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.1211, -3.7773, -4.2305,  ..., -2.5273, -3.3398, -0.7148],
        [-3.2930, -3.1367, -3.6523,  ..., -3.3398, -4.7773, -1.6211],
        [-2.7852, -3.1133, -3.3320,  ..., -3.4238, -4.9570, -1.2842],
        ...,
        [-6.4258, -3.1445, -4.5977,  ..., -4.8945, -5.8320, -0.6299],
        [-3.0078, -3.5840, -3.3984,  ..., -4.8516, -4.3828, -0.8350],
        [-4.3242, -3.8379, -3.2910,  ..., -3.7910, -4.3867, -0.8228]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 20, 15, 15, 14,  7,  6, 20, 15, 27, 27, 27,  0, 11, 18, 22, 27, 15,
         8, 27, 27, 27, 27,  9, 27, 15, 18, 15, 27,  6, 26,  6,  4, 27, 15, 27,
        20, 27, 24, 27, 27,  0, 27, 15,  8, 27, 27, 27, 27, 10, 18, 15, 15, 27,
        26, 15, 12, 27, 24,  1, 27, 27, 27, 27], device='cuda:0')
step: 116
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2384, -0.6113,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2510, -1.8896, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2183,  0.7979,  ..., -1.9033, -1.3438, -0.5977],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6582,  0.6499,  1.2695],
        [-1.1719,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.0866e-03, -2.7924e-02, -5.3253e-03,  ..., -8.9417e-03,
          3.7384e-04, -8.1940e-03],
        [ 5.2166e-04, -4.9286e-03, -1.3123e-03,  ...,  1.2660e-04,
         -3.7122e-04,  2.0943e-03],
        [ 2.6379e-03, -2.3918e-03,  8.4591e-04,  ..., -1.3361e-03,
         -1.0223e-03,  2.5826e-03],
        [ 2.2907e-03,  1.9360e-04, -4.0588e-03,  ..., -1.2770e-03,
         -9.5320e-04, -2.2602e-03],
        [ 1.5726e-03, -1.1253e-03,  2.3425e-05,  ..., -3.4046e-04,
          1.9264e-03,  8.6784e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.7686, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3477, -2.7539, -4.1602,  ..., -6.0664, -4.9570, -0.9253],
        [-2.0547, -4.0078, -4.6328,  ..., -3.6641, -3.7578, -2.2109],
        [-6.0391, -3.8340, -5.6797,  ..., -5.1641, -5.7109, -0.3188],
        ...,
        [-5.4336, -3.3887, -4.1211,  ..., -4.6367, -4.6992, -0.4653],
        [-4.6406, -3.5469, -3.5469,  ..., -4.3281, -4.7812, -0.5010],
        [-3.7656, -3.3770, -4.1250,  ..., -2.8438, -3.6582, -0.9380]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 15, 10, 27,  7, 27, 27,  0, 27, 10, 27, 15, 11, 27, 27,  6, 27, 24,
        27, 27,  4, 20, 27, 27, 27,  4,  4, 10,  4,  3,  7,  4, 25,  0,  2,  9,
        22,  1, 27, 20, 27, 22, 10, 24,  7,  3, 27, 20, 15,  8, 24, 27, 27, 24,
         2,  0, 17,  3, 27,  1, 27, 27,  3,  4], device='cuda:0')
step: 117
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2384, -0.6113,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2510, -1.8896, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2183,  0.7979,  ..., -1.9033, -1.3438, -0.5981],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6582,  0.6499,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.3590e-03, -9.7656e-03, -3.2234e-03,  ...,  5.9967e-03,
          1.0328e-03, -2.6779e-03],
        [ 3.6430e-04, -4.9782e-04, -2.9385e-05,  ..., -2.4700e-03,
          2.4617e-05,  6.5374e-04],
        [ 8.6546e-04,  1.2331e-03,  5.3596e-04,  ...,  6.3467e-04,
          3.2349e-03,  2.1381e-03],
        [ 1.9341e-03, -5.6267e-04, -2.5177e-03,  ..., -9.7322e-04,
         -8.3447e-04, -2.6436e-03],
        [ 2.0370e-03,  8.2684e-04, -1.0853e-03,  ...,  5.2500e-04,
          3.9864e-04, -8.8513e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.7260, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.9883, -3.4883, -3.7363,  ..., -3.9883, -3.7539, -0.8472],
        [-1.1992, -3.6367, -4.3086,  ..., -4.3398, -4.9961, -2.3242],
        [-5.1016, -3.6484, -4.3359,  ..., -3.8672, -4.2266, -0.4771],
        ...,
        [-4.6758, -3.7383, -3.8945,  ..., -4.7539, -4.6758, -0.5034],
        [-3.3359, -3.4297, -3.8203,  ..., -3.0859, -4.7891, -0.9927],
        [-2.8926, -3.3926, -2.8770,  ..., -3.7363, -3.4863, -1.2051]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  5,  7, 27, 27,  6,  8, 15,  0, 27, 27, 18,  5,  3,  0,  1, 26, 20,
        27,  1, 18,  7,  9, 25,  4,  2, 11,  9, 27,  1, 23,  4, 15,  9,  4, 13,
        27, 27, 27,  1, 27, 26,  0, 27,  0,  0, 27,  4, 10, 27, 10, 27, 25,  0,
        27,  4, 20,  2, 15, 25,  3,  4,  5, 27], device='cuda:0')
step: 118
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2383, -0.6113,  ...,  0.4790, -0.0856,  1.5098],
        [ 1.2510, -1.8896, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2183,  0.7979,  ..., -1.9033, -1.3438, -0.5981],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6582,  0.6499,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0004, -0.0056, -0.0040,  ...,  0.0039, -0.0012, -0.0017],
        [-0.0004,  0.0028,  0.0005,  ..., -0.0008,  0.0003, -0.0001],
        [-0.0005,  0.0024, -0.0008,  ..., -0.0015,  0.0020,  0.0017],
        [-0.0016, -0.0004, -0.0023,  ...,  0.0008, -0.0015,  0.0020],
        [ 0.0006, -0.0011, -0.0006,  ...,  0.0003, -0.0004,  0.0004]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.7177, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0117, -3.2793, -4.2148,  ..., -4.4180, -5.4961, -0.8882],
        [-3.2012, -2.6387, -2.8574,  ..., -3.3418, -2.9512, -1.9209],
        [-5.5000, -3.9375, -4.1562,  ..., -5.0625, -5.6875, -0.5312],
        ...,
        [-2.7656, -3.6562, -3.3281,  ..., -3.6875, -4.3281, -0.8276],
        [-3.8770, -3.1113, -4.8945,  ..., -4.5195, -4.0352, -1.0332],
        [-4.4297, -3.6641, -5.3984,  ..., -4.1172, -4.4141, -0.4912]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 27, 10, 20, 20, 27,  5, 27, 20,  0, 19, 23, 27, 26, 27, 19,  8, 27,
        18, 27, 27,  6, 27, 27, 17,  3,  4, 27,  6, 26,  5,  3, 26, 22, 22,  0,
         4, 27, 23, 27,  0, 20, 17,  0,  0, 27, 27, 27,  3, 27, 27, 27,  3, 17,
         4, 26, 25, 22, 16, 14,  6, 27, 27, 27], device='cuda:0')
step: 119
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2383, -0.6108,  ...,  0.4790, -0.0856,  1.5107],
        [ 1.2510, -1.8896, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2183,  0.7979,  ..., -1.9033, -1.3438, -0.5981],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6587,  0.6499,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.0103e-03, -2.1763e-03, -1.4377e-04,  ..., -8.7929e-04,
         -7.9536e-04, -1.3390e-03],
        [ 6.0654e-04,  3.1204e-03,  2.5845e-03,  ...,  3.8261e-03,
         -3.3112e-03, -4.9858e-03],
        [-1.4801e-03,  1.1930e-03,  7.0143e-04,  ...,  8.4114e-04,
          3.2246e-05,  1.5903e-04],
        [ 1.1597e-03,  4.0865e-04, -4.4394e-04,  ...,  1.0508e-04,
          1.4601e-03, -2.7332e-03],
        [ 3.7622e-04,  2.2388e-04,  6.8903e-04,  ..., -3.5715e-04,
          5.6124e-04, -3.8671e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5811, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.8379, -3.1504, -3.9629,  ..., -2.9316, -4.1797, -1.0869],
        [-4.9688, -3.2031, -4.6406,  ..., -5.6250, -5.8750, -0.7349],
        [-3.2207, -3.5332, -2.8613,  ..., -2.3145, -3.7051, -2.0332],
        ...,
        [-2.6230, -2.9668, -3.9980,  ..., -3.7949, -4.5273, -1.3887],
        [-4.0352, -3.1895, -3.4082,  ..., -3.3301, -3.7832, -1.0176],
        [-3.8496, -3.4902, -3.9453,  ..., -4.8516, -5.1484, -1.3506]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4,  7, 25,  7,  3, 27,  5, 27,  4, 27,  7, 10, 10, 14, 24, 17, 10, 14,
        18, 27, 27,  6, 27, 27, 15,  0, 27, 15, 27, 11, 27,  4, 27,  4, 24,  0,
         7, 23, 27, 27, 20, 17,  1, 27, 20, 22, 27, 23,  1, 27,  4, 27,  6,  1,
         3,  3, 27,  7,  0, 27, 27, 17,  7, 27], device='cuda:0')
step: 120
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2382, -0.6108,  ...,  0.4790, -0.0855,  1.5107],
        [ 1.2510, -1.8896, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2183,  0.7974,  ..., -1.9033, -1.3438, -0.5981],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6587,  0.6504,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.2512e-05, -4.8637e-03, -5.1880e-04,  ..., -1.3447e-03,
         -1.4057e-03, -1.0748e-03],
        [ 1.9703e-03,  2.6855e-03,  6.6910e-03,  ...,  1.4694e-02,
          4.8256e-03, -4.3945e-03],
        [-8.6069e-04,  2.3651e-03, -5.1451e-04,  ...,  1.0738e-03,
          3.1796e-03, -2.8634e-04],
        [-2.1791e-04,  1.2434e-04, -1.8015e-03,  ..., -1.7395e-03,
         -1.5717e-03, -2.7013e-04],
        [ 6.1798e-04, -5.7936e-04,  2.1410e-04,  ..., -4.2319e-04,
          3.7193e-05,  5.7602e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.7317, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.0625, -4.5781, -4.9844,  ..., -3.5625, -3.5625, -3.4043],
        [-2.9102, -3.8945, -4.1289,  ..., -2.8008, -4.2070, -1.2695],
        [-4.0898, -3.3398, -4.4023,  ..., -2.6680, -4.9961, -0.7144],
        ...,
        [-4.1719, -3.5312, -3.7344,  ..., -4.1719, -5.8125, -0.6567],
        [-3.7852, -2.9570, -3.1133,  ..., -5.0977, -4.5039, -1.4102],
        [-3.4395, -2.9238, -3.6738,  ..., -2.4395, -3.5332, -2.0020]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 27,  3,  6, 25, 27,  9,  2,  0, 20, 27,  7,  4,  9, 27, 20, 23,  4,
         7, 15,  4,  4, 27, 27, 15, 27, 15, 15, 15,  1, 27, 15, 27, 24, 22,  7,
         2, 22, 27, 10,  2,  7, 27, 10,  0, 25,  6, 27, 11,  1, 27, 27, 15,  0,
         9,  9,  9,  7, 20, 22, 27,  4,  7, 18], device='cuda:0')
step: 121
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2382, -0.6108,  ...,  0.4790, -0.0855,  1.5107],
        [ 1.2510, -1.8896, -0.2051,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2183,  0.7974,  ..., -1.9033, -1.3438, -0.5981],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6587,  0.6504,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.3975e-05,  8.8806e-03, -7.9155e-04,  ..., -3.5439e-03,
          1.3781e-03, -1.9188e-03],
        [ 2.4164e-04,  2.9163e-03,  2.9736e-03,  ...,  5.0850e-03,
         -2.1019e-03, -2.9755e-03],
        [-1.0002e-04,  7.3957e-04, -4.7755e-04,  ...,  6.6376e-04,
          2.2469e-03,  4.1151e-04],
        [-1.4067e-03, -5.1689e-04,  3.0918e-03,  ...,  1.7252e-03,
         -1.5450e-03,  1.5774e-03],
        [ 1.1730e-03, -4.5490e-04, -1.2312e-03,  ...,  8.5497e-04,
         -3.2091e-04, -4.6158e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5906, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9707, -3.2832, -4.7539,  ..., -3.9414, -3.5332, -1.1592],
        [-2.5742, -3.6836, -4.8555,  ..., -4.1055, -4.3711, -2.0586],
        [-5.4805, -4.4961, -4.6836,  ..., -3.8848, -5.2148, -0.2761],
        ...,
        [-1.1172, -3.9141, -3.7422,  ..., -4.3359, -3.7109, -2.6484],
        [-5.3516, -4.5859, -4.8828,  ..., -3.5391, -3.2891, -0.9761],
        [-3.1152, -2.9434, -4.2070,  ..., -3.7695, -3.6621, -1.0674]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 20, 27, 10, 10, 27, 27, 20, 10,  0,  7, 15, 20, 10, 27, 20,  4,  2,
         9,  9, 17,  4, 27, 27, 10, 27,  0,  5, 17, 15,  1, 22, 12, 27,  9, 15,
        11, 27, 27,  4, 25, 22,  0, 18, 27, 27,  6, 27, 27,  0,  1, 10, 27, 22,
        27, 27, 26, 27,  0, 27, 27, 13, 27, 27], device='cuda:0')
step: 122
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2382, -0.6108,  ...,  0.4790, -0.0855,  1.5107],
        [ 1.2510, -1.8896, -0.2052,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2183,  0.7974,  ..., -1.9033, -1.3438, -0.5981],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6587,  0.6504,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0319e-03,  1.0735e-02, -2.9812e-03,  ..., -9.2545e-03,
          2.1210e-03, -4.3182e-03],
        [-3.6907e-04,  2.7027e-03, -9.3579e-05,  ..., -6.0234e-03,
         -4.0474e-03,  1.7080e-03],
        [-2.6989e-04, -7.3099e-04,  2.2030e-03,  ...,  5.9986e-04,
          1.7090e-03,  3.2616e-03],
        [-1.3485e-03, -1.3428e-03,  2.7008e-03,  ..., -2.1763e-03,
         -1.6232e-03,  2.2068e-03],
        [ 4.2248e-04, -4.4441e-04, -1.7471e-03,  ...,  9.0408e-04,
         -2.8062e-04, -9.5367e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3033, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.5918, -3.6543, -3.3887,  ..., -3.4043, -3.7949, -1.3252],
        [-3.9121, -2.8184, -4.0977,  ..., -4.7383, -4.7852, -1.5371],
        [-3.2812, -3.1875, -3.0625,  ..., -4.4531, -4.1719, -1.1865],
        ...,
        [-4.6562, -3.8613, -4.6875,  ..., -5.1094, -3.8301, -1.0635],
        [-2.1777, -3.0547, -3.8203,  ..., -3.5234, -4.2422, -1.6006],
        [-5.0625, -3.2188, -4.5156,  ..., -3.5469, -2.5156, -1.2188]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  3, 15, 25, 13, 18,  3, 17, 27,  2, 25,  6, 11, 18, 27, 27, 27,
        27, 27, 27, 17, 27, 24, 27, 27, 18,  0, 17, 27, 22, 15, 24, 27, 15, 27,
        15,  9, 27, 27, 27, 26, 27, 15, 10,  1, 27,  1, 26, 10, 15, 27,  6, 22,
        27, 15, 15, 27, 11, 27,  9,  4, 27, 27], device='cuda:0')
step: 123
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2382, -0.6108,  ...,  0.4790, -0.0855,  1.5107],
        [ 1.2510, -1.8896, -0.2052,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2183,  0.7974,  ..., -1.9033, -1.3438, -0.5981],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6587,  0.6504,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.0905e-03,  7.9803e-03,  1.3046e-03,  ..., -3.1710e-04,
          1.3437e-03,  3.3932e-03],
        [-1.5345e-03,  5.9929e-03,  6.4125e-03,  ..., -9.6273e-04,
         -5.9090e-03, -5.1079e-03],
        [-1.1702e-03, -3.6418e-05,  7.7343e-04,  ...,  2.6722e-03,
         -1.2627e-03, -2.1324e-03],
        [-2.1515e-03, -1.4639e-03,  1.7524e-04,  ...,  9.8038e-04,
          4.4394e-04,  1.1225e-03],
        [-1.7776e-03, -9.0408e-04,  1.6031e-03,  ..., -7.4983e-05,
         -3.1328e-04,  6.8092e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6942, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.1777, -2.9121, -3.3809,  ..., -4.0703, -4.0352, -2.1621],
        [-2.8125, -2.7500, -3.6094,  ..., -4.2812, -3.2656, -1.5791],
        [-2.8262, -3.7793, -3.1543,  ..., -3.0918, -3.7480, -1.3252],
        ...,
        [-3.8223, -3.5254, -4.2109,  ..., -3.4941, -3.9141, -1.1504],
        [-4.1406, -3.2207, -4.3594,  ..., -4.1914, -5.3008, -0.5957],
        [-2.8926, -2.6582, -2.8008,  ..., -3.7988, -4.4727, -0.9092]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 8, 17, 25, 10,  1,  3, 27, 27, 27, 27,  6,  5, 27,  4, 12, 20, 20, 27,
        11, 25,  9,  5, 26, 27, 27, 27, 24, 27, 17, 22, 27, 27,  4,  1, 27,  1,
         4, 27, 10, 15, 22, 25, 25, 27, 27,  2, 27,  7, 20, 27, 27, 27,  6, 27,
         0, 27, 26, 22, 27, 27, 17, 27,  3,  2], device='cuda:0')
step: 124
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2382, -0.6108,  ...,  0.4790, -0.0855,  1.5107],
        [ 1.2510, -1.8896, -0.2052,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2184,  0.7974,  ..., -1.9033, -1.3438, -0.5981],
        [-0.4104, -0.2986, -1.7168,  ..., -0.6587,  0.6504,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.5987e-04,  8.5306e-04, -3.3360e-03,  ..., -2.1267e-03,
          8.6546e-04,  2.9373e-04],
        [ 7.5102e-06, -5.0354e-04,  3.5357e-04,  ...,  2.6302e-03,
          2.2354e-03, -1.7548e-03],
        [ 1.5345e-03, -1.9622e-04, -1.0786e-03,  ..., -1.3590e-04,
          5.4073e-04, -1.3828e-03],
        [ 3.1586e-03, -1.1549e-03,  9.9945e-04,  ...,  2.2945e-03,
          7.4434e-04, -2.7866e-03],
        [ 2.4116e-04, -2.7466e-04,  9.3269e-04,  ..., -8.1301e-04,
          9.1267e-04,  6.5851e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5134, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.1797, -3.2402, -4.9922,  ..., -3.6484, -2.8359, -1.2725],
        [-3.6543, -3.3730, -2.9512,  ..., -3.6387, -4.5117, -0.7319],
        [-4.5156, -3.7969, -4.4844,  ..., -3.8906, -5.1719, -0.6245],
        ...,
        [-2.8887, -2.9980, -3.7012,  ..., -3.7793, -4.1094, -2.2168],
        [-3.1719, -3.4531, -4.1562,  ..., -3.8750, -3.3594, -1.4531],
        [-3.2852, -3.2402, -3.4883,  ..., -2.7227, -4.5977, -1.5986]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 26, 20, 15, 27, 11, 17,  9, 27,  4, 15, 20,  2, 10, 27, 10,  0, 15,
         3, 27, 27, 27, 18, 27, 18, 27, 27, 27, 13,  2, 27,  4, 27,  6, 22,  2,
        17, 26, 27,  6, 27, 17, 27, 18, 22, 27, 19, 10,  4, 26,  0, 27, 22, 27,
        27,  4, 27,  2, 27, 27, 18, 15, 27,  3], device='cuda:0')
step: 125
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2382, -0.6108,  ...,  0.4792, -0.0855,  1.5107],
        [ 1.2510, -1.8896, -0.2053,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2184,  0.7974,  ..., -1.9033, -1.3438, -0.5981],
        [-0.4104, -0.2983, -1.7168,  ..., -0.6587,  0.6504,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.1076e-04, -9.0408e-03, -4.0321e-03,  ...,  1.2550e-03,
          3.5763e-06, -1.3552e-03],
        [-4.6062e-04, -9.7871e-05,  3.3259e-04,  ..., -3.3627e-03,
         -2.8858e-03,  6.3324e-04],
        [ 5.2500e-04, -3.1319e-03,  2.6112e-03,  ..., -2.8629e-03,
         -4.3793e-03,  2.0065e-03],
        [ 3.0637e-04, -1.2655e-03, -1.9512e-03,  ...,  1.3390e-03,
         -2.1672e-04, -6.3658e-04],
        [ 5.3310e-04, -3.7551e-04, -3.8528e-04,  ..., -1.1787e-03,
          3.7670e-05,  1.0080e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5653, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.5488, -3.3457, -3.8613,  ..., -5.4258, -3.7832, -1.5801],
        [-4.0078, -4.0859, -3.4121,  ..., -2.0527, -3.5527, -1.3662],
        [-3.8242, -3.3105, -3.4805,  ..., -4.4961, -3.7148, -0.9502],
        ...,
        [-3.4434, -3.5215, -3.6777,  ..., -4.5039, -5.1445, -0.9897],
        [-5.0781, -3.7168, -3.1250,  ..., -3.0000, -4.9531, -0.6245],
        [-2.5605, -3.6074, -3.8730,  ..., -2.8574, -3.6074, -1.3574]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([13, 24,  7, 27,  9, 26, 25, 27, 27,  7,  4,  9, 27, 25, 27, 15, 27, 14,
        18,  0, 13,  7, 18, 10, 27,  7,  6,  5, 27, 27, 27, 27, 24,  1,  6,  5,
        15,  4, 18, 27,  6, 27, 22, 18, 27, 27, 27,  0, 10, 25,  7, 20, 17,  4,
        15,  7, 27, 27, 22, 25, 11, 10, 27, 11], device='cuda:0')
step: 126
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2382, -0.6108,  ...,  0.4792, -0.0855,  1.5107],
        [ 1.2510, -1.8896, -0.2053,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2184,  0.7974,  ..., -1.9033, -1.3438, -0.5981],
        [-0.4104, -0.2983, -1.7168,  ..., -0.6587,  0.6504,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0519e-03, -4.1924e-03,  4.2892e-04,  ...,  1.4505e-03,
         -1.3151e-03, -2.0695e-03],
        [ 1.5736e-04,  2.2449e-03,  1.4400e-03,  ..., -3.1357e-03,
         -2.5215e-03, -2.3389e-04],
        [-2.2650e-06,  3.9864e-04,  9.7322e-04,  ..., -1.0386e-03,
          3.9148e-04,  2.5940e-03],
        [ 6.6185e-04,  4.1962e-04,  5.1260e-06,  ...,  7.4291e-04,
          1.0118e-03, -8.2493e-04],
        [-5.6934e-04,  4.0245e-04,  1.1339e-03,  ...,  1.0574e-04,
          6.4754e-04,  8.2970e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3093, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.7363, -3.4883, -4.2070,  ..., -3.9707, -5.0977, -0.9404],
        [-3.4531, -3.2637, -3.7480,  ..., -3.8418, -4.9844, -1.2803],
        [-1.4902, -3.5371, -3.3340,  ..., -5.3633, -4.8477, -2.1465],
        ...,
        [-4.2305, -3.5410, -3.5566,  ..., -4.7773, -4.5586, -0.6045],
        [-4.6680, -3.7793, -4.8555,  ..., -4.4961, -5.3242, -0.5298],
        [-4.7734, -2.4141, -4.0234,  ..., -4.9922, -5.7266, -0.9146]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 18, 27, 25,  3,  7,  1,  4, 17, 27, 18,  9, 27, 14, 25,  0, 27,
        27,  1,  7,  6, 27, 24, 17, 26,  3, 27, 10,  0,  8, 27,  1, 15,  3, 22,
        15, 27, 27, 15, 15, 27,  1, 27,  2, 18, 20, 27, 27, 11,  6,  7,  3,  5,
         5, 27, 27, 11,  3, 27, 27, 27, 27, 27], device='cuda:0')
step: 127
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6108,  ...,  0.4792, -0.0855,  1.5107],
        [ 1.2510, -1.8896, -0.2053,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2184,  0.7974,  ..., -1.9033, -1.3438, -0.5981],
        [-0.4104, -0.2983, -1.7168,  ..., -0.6587,  0.6504,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0011, -0.0047,  0.0031,  ...,  0.0060, -0.0041,  0.0015],
        [-0.0015,  0.0035,  0.0049,  ..., -0.0034, -0.0084, -0.0042],
        [-0.0002,  0.0004,  0.0001,  ...,  0.0007, -0.0007, -0.0010],
        [-0.0013,  0.0007,  0.0024,  ...,  0.0010, -0.0003,  0.0027],
        [-0.0009,  0.0002,  0.0015,  ...,  0.0009,  0.0004, -0.0006]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.2502, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6992, -2.8379, -2.7598,  ..., -3.8223, -4.9180, -0.9629],
        [-2.1270, -3.8145, -3.4863,  ..., -2.8301, -4.0000, -1.9238],
        [-3.9004, -3.6504, -4.3359,  ..., -1.7285, -3.3672, -1.6963],
        ...,
        [-4.6055, -3.1211, -4.9180,  ..., -4.6992, -5.0898, -0.9033],
        [-3.3867, -3.4648, -4.7617,  ..., -5.2305, -3.9961, -0.8086],
        [-2.9004, -3.4180, -3.6211,  ..., -3.9961, -4.5430, -1.0420]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 18,  1, 27,  0, 27, 27,  4, 27,  0, 27, 18, 27,  8, 24, 15, 27,  1,
         1, 20, 15,  4,  4,  5, 17, 27, 27, 27, 27, 27, 26,  2,  2,  9,  2, 27,
        27, 15, 26, 27, 27, 27, 27, 27,  0,  5, 27, 27, 10, 15,  9,  7, 22,  7,
        27,  7,  0, 27,  2, 15, 17, 27, 15,  7], device='cuda:0')
step: 128
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6108,  ...,  0.4792, -0.0855,  1.5107],
        [ 1.2510, -1.8896, -0.2054,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2184,  0.7974,  ..., -1.9033, -1.3438, -0.5981],
        [-0.4104, -0.2983, -1.7168,  ..., -0.6587,  0.6504,  1.2695],
        [-1.1729,  2.2285, -0.0962,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0005,  0.0013, -0.0041,  ...,  0.0017,  0.0021, -0.0010],
        [-0.0001,  0.0014,  0.0004,  ..., -0.0113, -0.0057,  0.0106],
        [-0.0003,  0.0006, -0.0010,  ..., -0.0002,  0.0015, -0.0015],
        [-0.0004, -0.0015, -0.0030,  ..., -0.0011, -0.0006, -0.0024],
        [ 0.0001, -0.0004, -0.0008,  ..., -0.0001, -0.0001,  0.0007]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.6027, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.2891, -3.4902, -3.4277,  ..., -3.9434, -5.3047, -1.1943],
        [-2.4121, -3.5840, -2.7715,  ..., -4.3984, -4.9297, -0.8188],
        [-3.1875, -3.7656, -4.0312,  ..., -5.3438, -5.0938, -0.6406],
        ...,
        [-1.1729, -3.5469, -3.5625,  ..., -4.5156, -3.9844, -2.0625],
        [-4.6172, -2.9316, -4.7734,  ..., -5.3047, -4.6016, -0.7754],
        [-3.9473, -3.6348, -4.7109,  ..., -5.3828, -4.5234, -0.5874]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([11, 11, 27, 17, 25,  8,  6, 27, 22,  7, 27, 15, 15, 27, 27, 24,  0,  2,
        27, 27, 20, 27, 27, 27, 13, 20, 18, 11, 18, 22, 24, 11, 27, 26,  6, 14,
         0,  1, 27,  7, 25,  0,  9,  3,  0,  0, 27,  1, 27, 27, 25, 27, 27, 15,
        27, 10,  4, 27, 27,  9, 22, 27,  3,  0], device='cuda:0')
step: 129
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6108,  ...,  0.4792, -0.0855,  1.5107],
        [ 1.2510, -1.8896, -0.2054,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2184,  0.7974,  ..., -1.9033, -1.3438, -0.5981],
        [-0.4104, -0.2983, -1.7168,  ..., -0.6587,  0.6504,  1.2695],
        [-1.1729,  2.2285, -0.0962,  ...,  0.6577,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.3409e-03,  5.5008e-03, -2.6474e-03,  ..., -2.6054e-03,
          1.1883e-03, -1.2696e-04],
        [-1.7853e-03, -7.6103e-04, -2.0676e-03,  ..., -6.9771e-03,
         -3.5229e-03,  2.1496e-03],
        [-7.4005e-04,  6.8092e-04,  1.3905e-03,  ...,  1.8597e-03,
          2.9564e-03,  2.5654e-03],
        [-2.1172e-03, -2.8014e-04,  2.7504e-03,  ..., -2.2876e-04,
         -2.9125e-03,  2.4433e-03],
        [ 1.8370e-04,  1.1402e-04, -1.6794e-03,  ...,  3.1292e-05,
         -7.3671e-04, -1.2684e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6065, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4922, -3.3965, -4.3008,  ..., -2.2402, -4.6758, -0.7402],
        [-5.4961, -5.7617, -5.9180,  ..., -5.8711, -6.6055, -0.1060],
        [-3.4395, -3.5176, -3.2363,  ..., -4.1406, -3.9863, -1.1113],
        ...,
        [-4.4258, -3.2363, -3.5488,  ..., -3.9570, -5.0352, -0.7529],
        [-2.0449, -2.9336, -2.8398,  ..., -4.7305, -3.9805, -1.7002],
        [-3.9219, -4.8281, -4.5312,  ..., -3.4219, -5.2500, -0.4058]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 10, 27,  4, 27, 27, 27,  0,  3, 18,  4,  4, 17, 20, 27,  3,  4, 27,
        27,  2, 27, 17, 25, 10, 22, 27,  5,  8, 27, 27,  4, 27, 27, 27, 10,  6,
         7, 27,  0, 10, 27, 27, 15,  7, 22, 15, 19,  9,  0, 17,  5, 18,  3,  6,
        11, 15, 27, 27,  7, 27, 27, 14, 20, 27], device='cuda:0')
step: 130
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6108,  ...,  0.4792, -0.0855,  1.5107],
        [ 1.2510, -1.8896, -0.2054,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2184,  0.7974,  ..., -1.9033, -1.3438, -0.5981],
        [-0.4104, -0.2983, -1.7168,  ..., -0.6587,  0.6504,  1.2695],
        [-1.1729,  2.2285, -0.0962,  ...,  0.6577,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.9114e-04,  4.6349e-03,  2.8973e-03,  ...,  5.0163e-04,
          2.0921e-04, -5.0125e-03],
        [-1.8463e-03, -1.5783e-04,  3.9637e-05,  ..., -3.8166e-03,
          7.0858e-04,  2.2507e-03],
        [ 1.3947e-05, -4.8208e-04,  2.1629e-03,  ...,  1.2779e-04,
          1.0118e-03,  3.2082e-03],
        [ 3.6931e-04, -4.7183e-04,  2.4300e-03,  ..., -1.4982e-03,
         -2.8157e-04,  1.5411e-03],
        [ 1.6761e-04, -9.7609e-04, -7.1430e-04,  ...,  7.4005e-04,
         -1.5392e-03, -8.9073e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4772, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4258, -3.7070, -3.4102,  ..., -2.7383, -3.9883, -0.9253],
        [-4.5703, -4.5859, -4.6172,  ..., -6.4297, -7.0234, -0.3208],
        [-5.5469, -4.0469, -3.4375,  ..., -3.1562, -4.1250, -0.9067],
        ...,
        [-2.8496, -3.5527, -4.1641,  ..., -4.4727, -3.8965, -1.5840],
        [-2.1465, -4.1133, -3.3809,  ..., -4.8320, -4.5820, -0.8335],
        [-4.2227, -4.5352, -4.9727,  ..., -5.3477, -5.5820, -0.4399]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 20,  3,  5,  3, 25,  3,  2, 25,  7,  7, 27,  6,  0,  0, 27, 27,  0,
        27, 27, 18, 26, 15, 27, 15, 14, 27, 14, 27, 18, 27, 12, 18, 27,  5,  5,
        18, 27,  2, 27, 27,  1, 27, 17, 18, 13, 27, 27, 10,  3, 27, 11, 11, 10,
        27,  9, 27,  0,  0, 27,  9,  0,  7,  1], device='cuda:0')
step: 131
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6108,  ...,  0.4792, -0.0855,  1.5107],
        [ 1.2510, -1.8896, -0.2056,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2184,  0.7974,  ..., -1.9033, -1.3438, -0.5981],
        [-0.4104, -0.2981, -1.7168,  ..., -0.6587,  0.6504,  1.2695],
        [-1.1729,  2.2285, -0.0962,  ...,  0.6577,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.1343e-03,  9.7275e-04,  6.6710e-04,  ...,  1.7166e-03,
         -2.2335e-03,  1.8959e-03],
        [-7.4005e-04,  1.1978e-03, -9.4414e-05,  ..., -2.7390e-03,
         -2.6417e-03,  2.9182e-03],
        [-3.2520e-04, -1.9789e-05,  1.5497e-06,  ...,  2.3437e-04,
         -8.1301e-04,  3.6788e-04],
        [-3.5596e-04,  2.1324e-03, -9.5940e-04,  ..., -7.6580e-04,
         -1.7548e-03,  1.1835e-03],
        [-1.2455e-03,  4.2915e-04,  2.9421e-04,  ...,  2.7108e-04,
         -8.7166e-04,  2.7132e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6845, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.4707, -2.7363, -4.7812,  ..., -5.2188, -4.3125, -2.1582],
        [-4.1836, -3.4805, -4.7930,  ..., -4.0586, -4.2305, -1.0439],
        [-2.7676, -3.6738, -3.9238,  ..., -3.7363, -4.4219, -1.1260],
        ...,
        [-4.1172, -4.2109, -4.8672,  ..., -6.4297, -6.8203, -0.2264],
        [-3.4062, -3.7812, -4.4375,  ..., -3.8125, -2.8438, -1.5791],
        [-1.2822, -3.6719, -3.0645,  ..., -4.1094, -4.5625, -1.4541]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15, 14, 27, 27, 13, 27,  3, 27,  0, 15, 17, 27,  9, 24,  0, 27, 10,  2,
        22,  5, 10, 27,  7, 10, 11,  7, 20,  2, 15, 20, 27, 15, 27, 10, 27, 15,
         1, 15,  0, 27,  3,  0, 19, 10, 25, 27,  2, 18, 27, 22, 10, 13,  0,  4,
         7, 27, 13, 20,  4, 18,  0,  4,  1, 27], device='cuda:0')
step: 132
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6108,  ...,  0.4792, -0.0855,  1.5107],
        [ 1.2520, -1.8896, -0.2056,  ..., -2.3828,  0.6030, -0.8237],
        [ 1.9111, -0.2184,  0.7974,  ..., -1.9033, -1.3438, -0.5981],
        [-0.4104, -0.2981, -1.7168,  ..., -0.6587,  0.6504,  1.2695],
        [-1.1729,  2.2285, -0.0962,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.9612e-04, -3.8948e-03,  1.2898e-04,  ...,  2.3880e-03,
          2.9349e-04,  6.7234e-05],
        [-5.0592e-04,  1.2083e-03,  5.0068e-04,  ...,  1.4896e-03,
          8.3208e-04, -1.1187e-03],
        [ 6.8235e-04, -1.0405e-03, -2.0580e-03,  ..., -4.0131e-03,
          3.2353e-04,  2.4490e-03],
        [ 6.0177e-04, -1.6713e-04,  1.6918e-03,  ..., -4.6992e-04,
         -2.8515e-04,  1.6630e-04],
        [ 4.5061e-04,  9.7752e-05, -3.2091e-04,  ..., -2.3246e-04,
         -3.8815e-04, -2.2781e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.7975, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.0938, -2.6094, -3.8281,  ..., -4.3594, -3.2031, -2.2500],
        [-2.2246, -3.5996, -3.5234,  ..., -4.0547, -4.3672, -1.6475],
        [-3.2480, -3.5605, -4.1094,  ..., -4.0273, -4.1367, -1.1855],
        ...,
        [-2.2461, -3.4180, -3.9336,  ..., -2.3086, -4.1680, -2.2773],
        [-1.9697, -4.1406, -4.6719,  ..., -4.5938, -4.3281, -3.0000],
        [-3.5645, -4.7812, -3.8145,  ..., -4.5820, -2.7207, -1.5020]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 9, 27, 10, 27, 27, 20,  2, 18, 27, 22, 27,  4, 27, 27, 27, 27, 15,  3,
        10, 27, 27,  4,  7,  8,  7, 26,  3, 27, 10,  0, 20, 15,  3,  7,  1,  3,
         1, 15, 27,  3,  1, 11, 25, 27, 24, 27,  0,  5,  2, 27, 15, 10,  7,  4,
        27,  4, 27, 26, 22,  1,  4,  9,  0,  4], device='cuda:0')
step: 133
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4792, -0.0855,  1.5107],
        [ 1.2520, -1.8896, -0.2056,  ..., -2.3828,  0.6035, -0.8237],
        [ 1.9111, -0.2184,  0.7974,  ..., -1.9033, -1.3447, -0.5981],
        [-0.4102, -0.2981, -1.7168,  ..., -0.6587,  0.6504,  1.2695],
        [-1.1729,  2.2285, -0.0962,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.6422e-03,  4.9248e-03,  2.8563e-04,  ...,  4.6444e-04,
          3.1929e-03,  4.4365e-03],
        [-3.6335e-03,  2.6627e-03,  2.1973e-03,  ..., -2.1484e-02,
         -1.4214e-02,  1.1749e-02],
        [-9.2316e-04, -2.3346e-03,  1.3847e-03,  ..., -1.1053e-03,
         -6.7482e-03, -1.3714e-03],
        [-2.5349e-03, -4.5896e-04, -1.8272e-03,  ..., -3.7003e-04,
         -5.1117e-04, -5.5122e-04],
        [-1.1139e-03,  9.8109e-05, -6.6471e-04,  ..., -7.7534e-04,
         -7.3433e-04, -3.7241e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6007, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.8184, -4.0977, -3.7871,  ..., -4.5352, -5.9414, -0.5674],
        [-3.0195, -3.6289, -4.8633,  ..., -4.3320, -4.5508, -1.0186],
        [-4.1523, -2.5273, -3.1680,  ..., -6.2461, -5.6836, -0.9180],
        ...,
        [-3.8516, -2.8203, -3.5547,  ..., -4.2578, -4.4609, -1.5391],
        [-4.1758, -3.7715, -3.6309,  ..., -2.9277, -5.0352, -0.9590],
        [-1.7236, -3.9570, -3.7539,  ..., -4.6133, -4.1758, -1.8164]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  4, 27,  9,  0, 27,  9,  4,  7, 27,  1, 10, 25, 27,  0,  3, 18, 27,
        27, 27, 10, 20, 12, 14, 22, 27, 18, 25,  1,  9, 27, 27, 10, 15, 27,  0,
        25, 14, 10, 27,  3, 27, 27, 27, 27,  4, 25, 20,  7, 14,  9, 27, 27, 16,
         5, 22, 17,  1, 10,  5, 27, 22,  2, 27], device='cuda:0')
step: 134
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4790, -0.0855,  1.5107],
        [ 1.2520, -1.8896, -0.2056,  ..., -2.3828,  0.6035, -0.8237],
        [ 1.9111, -0.2184,  0.7974,  ..., -1.9033, -1.3438, -0.5986],
        [-0.4102, -0.2981, -1.7168,  ..., -0.6587,  0.6504,  1.2695],
        [-1.1729,  2.2285, -0.0962,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0002, -0.0009, -0.0007,  ...,  0.0008,  0.0014, -0.0017],
        [ 0.0014,  0.0035,  0.0053,  ..., -0.0092, -0.0110, -0.0003],
        [-0.0003,  0.0014, -0.0008,  ...,  0.0007,  0.0007, -0.0013],
        [-0.0008, -0.0006, -0.0015,  ...,  0.0016, -0.0008, -0.0019],
        [ 0.0012, -0.0015, -0.0020,  ...,  0.0001, -0.0010, -0.0008]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.5287, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.1738, -3.9258, -3.6582,  ..., -1.1904, -3.5488, -1.5342],
        [-4.2344, -4.2031, -5.1719,  ..., -4.2188, -5.3125, -0.3113],
        [-4.9883, -4.1328, -4.9727,  ..., -4.5195, -3.6621, -1.1309],
        ...,
        [-2.8652, -3.1465, -4.1953,  ..., -5.0391, -5.2734, -1.0381],
        [-2.6758, -3.8477, -4.8945,  ..., -3.1914, -3.2227, -1.0654],
        [-2.2637, -3.9199, -4.3711,  ..., -2.6230, -3.2949, -1.3730]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25, 27, 27, 27, 27, 11, 27, 27, 27, 27,  0, 17, 26, 27, 27, 27,  4,  3,
         8,  1, 27,  0, 15, 27,  7,  8, 27, 15, 18, 13, 27, 27,  9, 22,  7,  0,
        26,  0,  2,  3,  0,  7, 27, 10, 27, 27, 27, 27,  3,  6,  6, 27, 10,  4,
        26,  1, 15,  7, 20,  4, 18, 27, 22,  7], device='cuda:0')
step: 135
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4790, -0.0855,  1.5107],
        [ 1.2520, -1.8896, -0.2057,  ..., -2.3828,  0.6035, -0.8237],
        [ 1.9111, -0.2184,  0.7974,  ..., -1.9033, -1.3438, -0.5986],
        [-0.4102, -0.2981, -1.7168,  ..., -0.6587,  0.6509,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.3757e-04,  2.7847e-03,  9.8896e-04,  ...,  1.9894e-03,
         -1.6012e-03,  3.8791e-04],
        [ 8.0109e-04,  1.8578e-03,  8.0633e-04,  ...,  3.7556e-03,
         -2.4056e-04, -3.9902e-03],
        [ 1.1120e-03, -1.3485e-03,  1.6642e-03,  ...,  1.9503e-04,
         -1.5059e-03,  7.6675e-04],
        [-7.3814e-04,  9.1314e-04, -2.5332e-05,  ..., -1.0424e-03,
          2.4462e-04,  1.1988e-03],
        [-1.3952e-03,  1.0195e-03,  6.5994e-04,  ...,  6.1083e-04,
         -6.1369e-04,  7.7963e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3220, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.0664, -3.6289, -4.3008,  ..., -5.0352, -4.6133, -0.8008],
        [-1.7422, -4.5547, -4.7734,  ..., -3.8047, -4.4922, -4.7734],
        [-4.2891, -3.9922, -5.1797,  ..., -4.9297, -2.8828, -1.9609],
        ...,
        [-3.5605, -3.5605, -3.5312,  ..., -4.8125, -4.8281, -0.9058],
        [-2.9414, -3.6445, -3.5195,  ..., -4.1445, -4.9570, -0.8936],
        [-2.7676, -3.6895, -4.3477,  ..., -1.4082, -4.5352, -1.7832]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 21, 26, 27, 17, 21, 27,  7,  0, 27,  0, 27, 27, 27,  1,  1, 25, 20,
        15, 27,  5, 27, 27, 22, 27, 27, 24,  0, 10, 20, 17, 27, 27,  1, 11, 15,
        15,  0,  0, 27, 27,  1, 26,  0, 27, 27, 27,  3,  4, 27, 20, 13, 27,  3,
         4, 27,  6, 27, 26, 10, 15, 27, 27,  1], device='cuda:0')
step: 136
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4790, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2057,  ..., -2.3828,  0.6035, -0.8242],
        [ 1.9111, -0.2184,  0.7974,  ..., -1.9033, -1.3438, -0.5986],
        [-0.4102, -0.2981, -1.7168,  ..., -0.6587,  0.6509,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.4734e-04, -5.2834e-03, -5.2881e-04,  ...,  2.8515e-03,
         -6.8855e-04, -1.3590e-04],
        [ 1.1563e-05, -4.6005e-03, -9.7122e-03,  ..., -2.3346e-03,
          2.7046e-03,  4.3983e-03],
        [-9.2363e-04,  5.4741e-04,  8.9467e-05,  ...,  2.2590e-05,
         -9.4509e-04,  1.4343e-03],
        [ 5.2977e-04,  1.9035e-03, -3.1166e-03,  ..., -3.6545e-03,
          3.4380e-04,  2.4319e-03],
        [-2.0885e-04,  7.2193e-04, -4.6825e-04,  ..., -7.9775e-04,
          4.5156e-04,  1.4400e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3565, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.6602, -4.1602, -3.2695,  ..., -3.5820, -4.3008, -0.7388],
        [-4.1172, -3.6309, -4.4609,  ..., -4.4922, -3.2715, -0.6465],
        [-3.9629, -3.8066, -4.3516,  ..., -4.8711, -5.6367, -0.8379],
        ...,
        [-2.7949, -2.5605, -3.2793,  ..., -5.1719, -4.9961, -1.6699],
        [-1.7764, -3.6816, -4.4492,  ..., -6.1211, -5.3086, -1.0107],
        [-1.9541, -3.7500, -4.0781,  ..., -4.2969, -5.3125, -1.1104]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  4, 11,  8, 25, 27, 27, 27, 15, 15, 27, 27,  5, 15,  2,  0, 27,
        27,  1, 22, 26, 27, 27,  6, 14, 27, 27,  4, 27, 15,  0, 27, 27, 27,  4,
         3, 27,  6, 10, 27, 25, 27, 27,  7, 27, 15, 27,  2, 20, 26, 20, 18,  0,
         1, 11, 18,  1, 20,  5, 15, 27,  0, 27], device='cuda:0')
step: 137
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4790, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2057,  ..., -2.3828,  0.6035, -0.8242],
        [ 1.9111, -0.2184,  0.7974,  ..., -1.9033, -1.3438, -0.5986],
        [-0.4102, -0.2981, -1.7168,  ..., -0.6587,  0.6509,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0017,  0.0007, -0.0013,  ...,  0.0004,  0.0006, -0.0009],
        [-0.0018, -0.0013, -0.0027,  ..., -0.0037,  0.0015,  0.0054],
        [ 0.0009, -0.0008,  0.0016,  ..., -0.0022, -0.0016,  0.0025],
        [-0.0007, -0.0007,  0.0019,  ...,  0.0008, -0.0022,  0.0007],
        [-0.0004, -0.0003, -0.0006,  ...,  0.0012, -0.0004, -0.0003]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.6976, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3906, -3.7363, -4.1875,  ..., -5.4375, -6.2969, -0.9707],
        [-2.3477, -3.9570, -3.4707,  ..., -2.9238, -4.5039, -1.0811],
        [-4.1641, -3.7578, -4.5547,  ..., -4.9922, -5.1953, -0.6011],
        ...,
        [-3.3086, -3.5273, -4.1367,  ..., -3.4805, -3.6992, -1.4482],
        [-3.4473, -4.2422, -4.2266,  ..., -3.2285, -3.0723, -1.3848],
        [-3.1309, -2.9902, -3.1152,  ..., -4.5352, -5.1133, -0.7397]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  1,  1,  1, 13, 27, 27, 24,  1, 20, 10,  1, 27, 27,  2, 20, 15, 27,
        27, 10,  0,  0, 26,  8, 27,  4,  1, 27,  1, 27, 27, 27,  9, 20, 27,  3,
         9,  7, 11,  1, 22,  3, 15, 10, 14, 21, 27, 24, 17,  4,  3, 27,  1,  3,
        27,  6,  4, 27,  2,  0, 27, 27,  3, 27], device='cuda:0')
step: 138
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4790, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2057,  ..., -2.3828,  0.6035, -0.8242],
        [ 1.9111, -0.2184,  0.7974,  ..., -1.9033, -1.3438, -0.5986],
        [-0.4102, -0.2981, -1.7168,  ..., -0.6587,  0.6509,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.3024e-04, -3.4580e-03, -1.4019e-03,  ...,  8.5688e-04,
         -2.5344e-04, -7.6818e-04],
        [ 3.4857e-04, -1.4782e-03, -1.1139e-03,  ...,  1.7471e-03,
          4.6539e-04, -1.5631e-03],
        [-4.6635e-04, -3.3021e-05, -7.5531e-04,  ...,  1.3647e-03,
          2.6286e-05, -1.0891e-03],
        [-2.2030e-04, -7.6675e-04, -9.1267e-04,  ...,  5.4657e-05,
          1.8620e-04, -1.9445e-03],
        [ 9.7096e-05, -8.6737e-04,  1.0338e-03,  ..., -3.6836e-05,
         -1.9884e-04,  6.5899e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4834, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.7324, -3.9512, -3.9668,  ..., -5.1211, -3.6074, -1.3105],
        [-4.4609, -3.7734, -3.7109,  ..., -5.1953, -5.8672, -0.6323],
        [-2.4551, -3.6270, -3.7988,  ..., -4.7695, -3.5957, -1.4717],
        ...,
        [-5.1914, -4.4258, -5.2227,  ..., -5.3477, -5.5508, -0.3948],
        [-4.3047, -4.8203, -4.1484,  ..., -2.3340, -2.8340, -1.3350],
        [-3.6875, -4.2500, -4.3281,  ..., -3.6406, -5.2969, -0.7026]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18,  9,  4,  2,  3, 27, 18, 15, 22, 10, 27,  3,  9,  3, 27, 27, 27,  5,
         4,  1, 15,  3,  3, 27,  3, 10,  2,  4, 27, 15,  4,  5, 27, 27, 27, 17,
         1,  4, 27,  7, 27, 27,  3, 18, 27, 14,  7, 18, 10, 27, 27, 26, 22,  2,
         7,  1, 27, 20,  3, 27,  0,  4,  9, 27], device='cuda:0')
step: 139
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4790, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2057,  ..., -2.3828,  0.6035, -0.8242],
        [ 1.9111, -0.2183,  0.7974,  ..., -1.9033, -1.3438, -0.5986],
        [-0.4102, -0.2981, -1.7168,  ..., -0.6587,  0.6509,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.4544e-05,  1.0366e-03,  6.7902e-04,  ...,  2.8019e-03,
          1.3685e-04,  1.0033e-03],
        [-4.2558e-04,  3.5534e-03,  5.0211e-04,  ...,  4.5624e-03,
          6.3744e-03, -3.8471e-03],
        [-3.4666e-04,  1.3466e-03,  3.6526e-04,  ..., -3.6502e-04,
          8.8692e-04, -1.6184e-03],
        [-5.7697e-04,  6.6710e-04,  3.7823e-03,  ...,  1.0777e-03,
          3.1161e-04,  3.9291e-04],
        [-3.4833e-04,  1.6260e-04,  3.0184e-04,  ...,  1.7941e-04,
         -2.2590e-04, -5.5075e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6423, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9043, -3.5605, -3.9961,  ..., -3.9668, -3.6074, -1.4346],
        [-4.9336, -4.3398, -4.0742,  ..., -5.0117, -6.1523, -0.3081],
        [-3.7891, -3.5703, -4.4453,  ..., -2.4609, -4.1641, -1.0244],
        ...,
        [-2.6172, -3.4766, -3.6641,  ..., -4.0078, -4.1016, -1.7900],
        [-4.2539, -3.7227, -4.4258,  ..., -4.3477, -5.1914, -0.5972],
        [-3.7402, -3.8965, -3.2402,  ..., -2.2891, -4.4297, -2.0234]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 10,  3,  7, 15, 27, 27, 10,  5, 25,  9,  3, 25, 27,  9, 22, 10, 27,
         4,  4,  0, 27, 11, 17, 20,  3, 18, 27, 27, 10, 27, 27,  1, 22,  7,  3,
         1,  5, 25, 27, 27, 25, 17, 27, 27, 15,  0,  4, 27, 27, 24,  1,  3, 10,
        20, 25, 27, 22, 15, 27, 18, 17, 10, 25], device='cuda:0')
step: 140
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4790, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2057,  ..., -2.3828,  0.6035, -0.8242],
        [ 1.9111, -0.2183,  0.7974,  ..., -1.9033, -1.3438, -0.5986],
        [-0.4099, -0.2981, -1.7168,  ..., -0.6587,  0.6509,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.0866e-03,  2.9612e-04,  1.5240e-03,  ...,  1.3294e-03,
         -3.7789e-05,  3.2024e-03],
        [-1.0357e-03,  1.4696e-03,  1.6899e-03,  ..., -1.7633e-03,
         -9.2621e-03,  2.1095e-03],
        [-2.0580e-03,  1.9150e-03, -1.4696e-03,  ...,  2.6875e-03,
          2.1362e-03, -4.8876e-04],
        [-1.1244e-03,  6.9571e-04,  1.2321e-03,  ..., -4.9686e-04,
         -1.3399e-03,  1.8620e-04],
        [ 1.8013e-04, -5.3978e-04,  6.4230e-04,  ...,  7.4577e-04,
         -3.1757e-04,  3.7241e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5300, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4219, -3.0957, -3.8301,  ..., -3.4863, -5.6406, -1.1895],
        [-1.5029, -3.4395, -3.8613,  ..., -3.9395, -4.2227, -1.9561],
        [-3.1777, -3.8340, -3.0527,  ..., -3.2402, -4.4766, -0.7720],
        ...,
        [-3.5078, -3.0098, -3.0566,  ..., -4.1328, -3.8672, -1.0400],
        [-4.6992, -3.7324, -3.8105,  ..., -7.0742, -4.6523, -0.2474],
        [-1.5254, -3.9941, -3.8848,  ..., -3.2129, -3.8223, -2.0254]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10,  6, 27,  1, 17, 20,  0, 27,  4,  7, 27, 21,  3,  4,  5, 27, 20, 27,
        17, 27, 18, 20, 12, 15, 27, 27, 27, 15,  1, 20, 26,  3, 27,  4, 20, 27,
        27, 27,  8,  9,  5, 15,  3, 27,  8, 27,  7,  3, 25, 15, 17,  2, 27, 17,
         7, 15, 15, 27, 25, 17, 26,  2,  7, 27], device='cuda:0')
step: 141
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4790, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2057,  ..., -2.3828,  0.6040, -0.8242],
        [ 1.9111, -0.2183,  0.7974,  ..., -1.9033, -1.3438, -0.5986],
        [-0.4099, -0.2981, -1.7168,  ..., -0.6587,  0.6509,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6577,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.3556e-03, -5.8365e-04, -1.4458e-03,  ...,  1.0519e-03,
         -1.1578e-03, -2.0409e-03],
        [ 8.7929e-04, -7.9575e-03, -7.6103e-03,  ..., -4.6768e-03,
          1.6041e-03,  7.6027e-03],
        [ 1.1406e-03, -2.0580e-03, -9.4700e-04,  ..., -2.2774e-03,
         -6.0320e-04,  1.5011e-03],
        [-2.8944e-04, -1.1927e-04, -4.1628e-04,  ..., -8.9550e-04,
         -9.4461e-04,  6.0129e-04],
        [ 6.5470e-04, -3.2377e-04, -4.0722e-04,  ...,  4.7731e-04,
         -6.2323e-04, -2.9743e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4249, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.3125, -2.7500, -4.2656,  ..., -4.7969, -5.4062, -0.6558],
        [-3.9785, -4.0234, -4.4297,  ..., -5.3828, -5.2734, -0.4460],
        [-6.8789, -2.2227, -3.0352,  ..., -5.2539, -6.3633, -0.6123],
        ...,
        [-2.3027, -3.3965, -3.5059,  ..., -4.1445, -3.7090, -1.3965],
        [-2.4102, -3.7383, -3.8496,  ..., -3.3789, -3.7871, -2.5684],
        [-2.0957, -3.8926, -3.6895,  ..., -4.4102, -2.9863, -1.9863]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22, 10, 10, 10, 27, 13, 10, 25, 27, 24,  0, 20, 15, 16,  0, 27, 15, 27,
        22, 20, 27, 18, 22, 27, 27, 27, 27, 27, 25, 27, 27, 27, 11,  4, 25, 27,
        10,  7, 27, 18, 27,  3, 27, 27,  1, 27,  6,  5, 18,  7, 27, 10, 15, 27,
        27, 27, 27,  4, 15, 26,  0, 27, 17, 13], device='cuda:0')
step: 142
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4790, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2056,  ..., -2.3828,  0.6040, -0.8242],
        [ 1.9111, -0.2183,  0.7974,  ..., -1.9033, -1.3438, -0.5986],
        [-0.4099, -0.2981, -1.7168,  ..., -0.6587,  0.6509,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6577,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.0051e-04,  1.3428e-03, -4.6849e-04,  ..., -1.2579e-03,
          3.9005e-04, -6.3229e-04],
        [ 6.7949e-05,  1.2045e-03,  2.7924e-03,  ...,  8.5068e-03,
          3.1509e-03, -3.0193e-03],
        [ 1.8873e-03, -1.1702e-03,  1.3428e-03,  ..., -3.5381e-03,
         -7.3004e-04,  2.0351e-03],
        [-6.9332e-04, -6.1893e-04,  1.3809e-03,  ...,  6.4707e-04,
          1.3828e-05,  2.3532e-04],
        [-5.8055e-05, -1.8632e-04,  6.0415e-04,  ...,  5.9009e-05,
          4.4703e-05, -4.2009e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.7088, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.7070, -3.4121, -3.9902,  ..., -5.0352, -4.3008, -1.2715],
        [-2.2539, -3.0195, -4.0820,  ..., -2.1133, -3.4258, -2.6914],
        [-1.2256, -3.2422, -4.1797,  ..., -4.8203, -4.2578, -1.8828],
        ...,
        [-1.9023, -3.6992, -3.7305,  ..., -5.6992, -3.4805, -1.0430],
        [-3.6270, -3.6895, -3.1270,  ..., -4.6289, -4.6875, -1.1895],
        [-2.4785, -3.5410, -2.3535,  ..., -2.7754, -3.8691, -1.7607]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6,  7, 17,  9, 17, 27, 27, 27, 14, 27, 27, 25, 27, 13,  0, 27, 18,  3,
        27,  3,  3,  3, 20, 18, 20, 15, 27,  2, 27,  1, 15, 13,  7, 18, 15, 25,
        26, 12,  9, 10, 15,  0, 27,  6, 27, 27, 27,  3, 27, 22,  7,  9, 20, 11,
         3,  2, 27, 17, 24, 23, 27, 18, 27, 25], device='cuda:0')
step: 143
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4790, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2056,  ..., -2.3828,  0.6040, -0.8242],
        [ 1.9111, -0.2183,  0.7974,  ..., -1.9033, -1.3438, -0.5986],
        [-0.4099, -0.2981, -1.7168,  ..., -0.6587,  0.6509,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6577,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.4830e-03, -9.4414e-04,  1.9608e-03,  ..., -7.3910e-04,
         -4.8780e-04,  1.6546e-03],
        [ 4.5586e-04,  1.3237e-03,  1.5602e-03,  ..., -6.2275e-04,
         -7.6818e-04, -2.1422e-04],
        [-4.2629e-04, -1.7576e-03,  1.7719e-03,  ...,  9.1696e-04,
          8.8978e-04,  3.0365e-03],
        [-5.0688e-04, -4.3011e-04,  2.6951e-03,  ...,  1.7166e-05,
          9.5892e-04,  1.2865e-03],
        [-4.0913e-04,  5.4359e-04,  5.7554e-04,  ..., -1.9014e-05,
          6.6376e-04, -2.9802e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3409, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.3594, -2.8281, -3.9062,  ..., -3.7344, -4.1406, -1.6240],
        [-2.7871, -3.7246, -3.2871,  ..., -3.3496, -4.3203, -1.0693],
        [-4.5039, -3.4570, -4.7070,  ..., -5.8633, -5.9727, -0.3789],
        ...,
        [-2.8789, -3.3789, -3.2852,  ..., -2.5195, -4.1758, -1.8320],
        [-3.1367, -3.8867, -3.6680,  ..., -4.6680, -4.4805, -0.6519],
        [-3.8730, -3.5449, -3.9355,  ..., -3.8887, -5.3398, -1.2314]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20, 27, 27, 22, 14,  7,  7, 13,  7, 27,  3,  0, 13, 27, 27, 10, 18,  3,
        17, 27, 18, 27, 27, 18,  3, 27, 27,  8, 14,  9, 18,  0,  6,  7, 27,  3,
        27, 17, 18, 15, 27, 18, 25, 27, 15,  0,  0, 15, 27,  4, 18, 10,  4, 27,
        25, 27, 17, 27, 27, 27, 27, 27,  0, 27], device='cuda:0')
step: 144
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4788, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2056,  ..., -2.3828,  0.6040, -0.8242],
        [ 1.9111, -0.2183,  0.7974,  ..., -1.9033, -1.3438, -0.5986],
        [-0.4099, -0.2981, -1.7168,  ..., -0.6587,  0.6509,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6577,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1120e-03, -2.4509e-03,  9.1839e-04,  ...,  1.6508e-03,
         -1.2121e-03,  2.6665e-03],
        [-7.7915e-04, -2.1183e-04,  2.3460e-04,  ..., -1.8940e-03,
         -1.7242e-03, -1.0080e-03],
        [-1.5318e-04,  1.0958e-03,  9.3269e-04,  ...,  1.3313e-03,
         -6.0177e-04, -1.3647e-03],
        [-8.0061e-04,  1.0881e-03, -6.2227e-04,  ...,  1.7443e-03,
         -5.8413e-04,  1.2779e-03],
        [-4.2367e-04,  4.8923e-04,  1.4925e-03,  ..., -5.6446e-05,
          4.2892e-04,  1.6298e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6274, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.4199, -3.3730, -3.6387,  ..., -4.4375, -4.2656, -0.7324],
        [-3.8711, -3.3242, -4.2305,  ..., -4.2148, -4.2148, -1.1992],
        [-5.1602, -4.2266, -3.9902,  ..., -4.5664, -4.1641, -0.6626],
        ...,
        [-3.9688, -3.6250, -3.7031,  ..., -5.1094, -5.0156, -1.3281],
        [-3.1953, -4.1797, -3.9766,  ..., -3.9609, -3.7266, -0.9614],
        [-1.0986, -4.1133, -4.9414,  ..., -5.5352, -3.4434, -2.3184]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 12,  7, 20, 18, 18, 15, 27, 27, 27,  0, 17,  8,  5, 10, 27,  4,  5,
         3, 26, 13, 17, 27, 20,  3, 10, 27, 27,  1, 27, 10, 13, 27,  0, 17,  3,
        10, 13, 14, 15, 27, 27, 10, 27,  4, 15, 13,  9,  8, 27, 22, 18, 27,  4,
        27, 25, 18, 25,  0, 27,  1, 20,  0, 18], device='cuda:0')
step: 145
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4788, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2056,  ..., -2.3828,  0.6040, -0.8242],
        [ 1.9111, -0.2183,  0.7969,  ..., -1.9023, -1.3438, -0.5986],
        [-0.4099, -0.2981, -1.7168,  ..., -0.6587,  0.6509,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6572,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0006,  0.0024, -0.0012,  ..., -0.0016,  0.0007,  0.0013],
        [-0.0006, -0.0005,  0.0013,  ..., -0.0025, -0.0022,  0.0013],
        [-0.0011, -0.0012,  0.0010,  ...,  0.0024, -0.0007, -0.0020],
        [-0.0004, -0.0002,  0.0005,  ..., -0.0008,  0.0004, -0.0002],
        [-0.0003, -0.0003, -0.0007,  ...,  0.0005,  0.0004, -0.0012]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.4978, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.7109, -4.6836, -4.1055,  ..., -3.6191, -5.3359, -0.4629],
        [-2.9492, -3.6523, -3.3711,  ..., -4.9492, -4.3711, -1.0264],
        [-5.2422, -3.5371, -3.5684,  ..., -3.5215, -4.3164, -0.6777],
        ...,
        [-3.6797, -4.1328, -3.8516,  ..., -2.4922, -2.7891, -1.2900],
        [-2.1426, -4.3633, -4.3320,  ..., -5.9258, -5.2852, -0.5498],
        [-3.2207, -3.4863, -3.4863,  ..., -3.9707, -2.9082, -1.8604]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25, 18, 22,  1, 15, 27, 27, 27,  0, 15,  5, 27, 13, 27,  6, 27, 27, 26,
        27, 27,  4,  7, 27, 15, 25,  0,  5, 27,  5,  4, 18,  7, 13,  0, 26,  3,
        27, 25, 20, 27,  0,  4, 27, 15,  4, 25, 27, 27, 18, 17, 27, 26, 26,  4,
        25, 17, 15, 18, 27, 26,  1, 25, 18, 13], device='cuda:0')
step: 146
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4788, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2056,  ..., -2.3828,  0.6040, -0.8242],
        [ 1.9111, -0.2183,  0.7969,  ..., -1.9023, -1.3438, -0.5986],
        [-0.4097, -0.2981, -1.7168,  ..., -0.6587,  0.6509,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6572,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.9373e-04,  1.4439e-03, -2.2459e-04,  ...,  1.1435e-03,
         -1.0099e-03, -6.3610e-04],
        [-2.3437e-04, -2.4967e-03, -1.7176e-03,  ...,  1.0078e-02,
          6.6566e-03, -1.5612e-03],
        [-5.2071e-04,  2.7561e-04,  2.8572e-03,  ...,  2.6016e-03,
         -1.6165e-03,  1.6527e-03],
        [ 2.0103e-03,  1.1148e-03, -1.0672e-03,  ..., -4.6682e-04,
         -2.1601e-04, -2.4736e-05],
        [-3.9959e-04,  1.8454e-04, -5.9462e-04,  ...,  7.7438e-04,
          2.3091e-04, -1.1426e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2868, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.2051, -3.2676, -3.6738,  ..., -4.6758, -4.5195, -1.0654],
        [-2.9277, -3.1309, -4.1172,  ..., -3.0547, -4.5859, -0.8511],
        [-4.0508, -3.9238, -4.7188,  ..., -4.8945, -4.8945, -0.5020],
        ...,
        [-3.1953, -4.1016, -3.4766,  ..., -2.3984, -3.3672, -1.1016],
        [-3.8730, -3.3418, -3.3887,  ..., -1.6396, -3.1074, -2.4668],
        [-3.6953, -4.1016, -4.5234,  ..., -4.7578, -3.9141, -0.5537]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  9, 13,  0, 18,  5, 11,  6,  7, 27, 27, 27, 27, 27, 26,  1, 15,
        18, 27,  0, 27,  4,  9, 27, 10, 18,  0,  1, 27, 25, 10, 27, 10, 24,  6,
         7,  5,  4, 10, 27,  7,  3,  7, 27,  4,  0,  9, 27, 20,  1,  4, 18, 27,
        27, 27, 27, 27, 15, 27, 15, 11, 26, 15], device='cuda:0')
step: 147
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4788, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2056,  ..., -2.3828,  0.6040, -0.8242],
        [ 1.9111, -0.2183,  0.7969,  ..., -1.9023, -1.3438, -0.5986],
        [-0.4097, -0.2981, -1.7168,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6572,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.8757e-03, -1.5316e-03,  5.5046e-03,  ...,  2.9812e-03,
         -1.2522e-03,  1.0841e-02],
        [-8.3237e-03,  5.5885e-03, -8.8596e-04,  ..., -4.3976e-02,
         -2.4216e-02,  2.4826e-02],
        [-8.9741e-04,  6.7949e-04,  5.0974e-04,  ...,  1.1587e-03,
          1.4143e-03,  2.9144e-03],
        [-2.2173e-04,  9.3162e-05, -4.2114e-03,  ...,  3.7909e-05,
         -1.4105e-03, -4.1389e-03],
        [-1.4076e-03,  3.4857e-04, -1.7643e-04,  ..., -1.6584e-03,
          6.5863e-05, -5.5313e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1687, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.4492, -3.9492, -3.5430,  ..., -4.5586, -4.4336, -0.6836],
        [-3.6270, -3.6738, -4.0820,  ..., -4.0664, -4.3633, -0.7837],
        [-2.4648, -3.7305, -3.8555,  ..., -2.8086, -3.9336, -1.5732],
        ...,
        [-1.9961, -3.3242, -3.9805,  ..., -3.9492, -4.0898, -2.1836],
        [-3.4707, -3.0957, -3.6113,  ..., -4.3594, -5.3125, -0.9233],
        [-3.6016, -4.2734, -4.5078,  ..., -3.4922, -3.7891, -0.8350]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 20, 10, 10, 15, 17,  5, 15, 27,  1,  5, 27,  7, 27, 15, 27, 27,
         1, 27, 27,  6, 10, 27, 27, 15,  3, 27,  0,  3, 15,  4, 27, 27, 27, 27,
        27, 27,  1, 27, 27,  0, 26,  5,  0,  2,  9, 27,  4, 27,  1,  1, 27, 17,
        27, 27,  1, 27, 27,  7, 27,  0,  5, 20], device='cuda:0')
step: 148
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4788, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2056,  ..., -2.3828,  0.6040, -0.8242],
        [ 1.9111, -0.2183,  0.7969,  ..., -1.9023, -1.3438, -0.5986],
        [-0.4097, -0.2981, -1.7168,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6572,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.9896e-04,  6.3515e-03,  9.4318e-04,  ..., -7.9060e-04,
          1.1511e-03,  6.7329e-04],
        [ 4.2796e-04, -2.7046e-03, -5.0201e-03,  ..., -9.4700e-04,
          5.2071e-04,  1.1749e-03],
        [ 1.6451e-03, -2.4395e-03,  2.2049e-03,  ..., -2.0275e-03,
         -2.4643e-03,  4.5776e-03],
        [-3.6931e-04, -1.7118e-04,  2.4033e-03,  ..., -3.9983e-04,
         -6.1512e-04, -6.1214e-05],
        [-8.7452e-04, -2.4962e-04, -1.2608e-03,  ...,  2.2449e-03,
         -3.1877e-04, -1.3342e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4817, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.5938, -2.9531, -4.1406,  ..., -4.3906, -2.6719, -1.7490],
        [-2.8262, -3.2949, -4.0430,  ..., -4.3555, -4.4492, -1.0762],
        [-2.2246, -4.0078, -3.7715,  ..., -2.5059, -3.7090, -1.6309],
        ...,
        [-6.2461, -3.6523, -3.6836,  ..., -6.3086, -5.4336, -0.2462],
        [-1.9951, -3.8867, -3.1191,  ..., -4.1055, -3.5273, -1.6357],
        [-2.5703, -2.8984, -3.8984,  ..., -4.0391, -4.3828, -1.3682]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6, 27, 12, 27,  0, 27,  4,  0, 27, 27,  3, 22, 27,  0, 27, 13, 18,  9,
         3, 13,  7, 27, 27, 18,  6, 27, 27,  0,  0, 15,  3, 27, 27, 25, 27, 11,
        18,  0, 18,  0, 22, 20,  0,  2, 15, 13, 11,  1, 15, 26, 26, 27, 18, 27,
         1, 27, 27, 27,  5, 27,  2, 27, 27, 22], device='cuda:0')
step: 149
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4788, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2056,  ..., -2.3828,  0.6040, -0.8242],
        [ 1.9111, -0.2181,  0.7969,  ..., -1.9023, -1.3438, -0.5986],
        [-0.4097, -0.2981, -1.7168,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6572,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.6041e-03, -1.8644e-03,  5.5122e-04,  ...,  8.8358e-04,
         -8.5020e-04,  8.1921e-04],
        [-9.1743e-04,  5.4626e-03,  7.2174e-03,  ...,  6.4850e-04,
         -3.3112e-03,  5.1975e-04],
        [-7.2861e-04,  2.8858e-03,  4.3893e-04,  ..., -1.6603e-03,
          1.1396e-03,  3.2864e-03],
        [-1.0052e-03, -3.0255e-04,  1.0900e-03,  ..., -1.3769e-05,
          3.6383e-04,  2.9507e-03],
        [-3.8433e-04, -3.4809e-04,  9.4700e-04,  ..., -3.2544e-04,
          2.7061e-04,  4.7755e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6018, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9766, -3.6348, -3.6797,  ..., -2.6973, -4.2109, -1.3994],
        [-2.2559, -3.9434, -4.1953,  ..., -4.8828, -3.0527, -1.1934],
        [-2.2676, -3.5645, -2.9707,  ..., -2.6113, -3.8613, -1.4072],
        ...,
        [-3.4004, -3.3223, -3.8223,  ..., -3.2285, -2.9629, -1.2129],
        [-3.3594, -3.6250, -4.6406,  ..., -3.8750, -4.9844, -0.8125],
        [-3.5293, -3.2949, -3.5137,  ..., -3.3418, -4.4531, -0.8267]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20, 15,  2, 27,  7,  7, 27, 27, 17, 27, 27, 18,  4, 27, 20, 17,  5, 19,
         0, 27, 10, 14, 27,  9,  1,  4,  2,  4, 27, 27, 10, 27, 17, 26, 27,  7,
         6, 10, 27,  7, 10, 15, 15, 18, 27, 13,  8, 14, 22, 14, 18, 10, 23, 27,
        27,  5,  0, 17, 27, 26, 25, 27, 27, 15], device='cuda:0')
step: 150
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6104,  ...,  0.4788, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2056,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2181,  0.7969,  ..., -1.9023, -1.3438, -0.5991],
        [-0.4097, -0.2981, -1.7168,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6572,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0003,  0.0001,  0.0019,  ..., -0.0016, -0.0018, -0.0004],
        [ 0.0012,  0.0025,  0.0027,  ...,  0.0019,  0.0016, -0.0024],
        [-0.0003,  0.0016,  0.0046,  ...,  0.0011,  0.0054,  0.0078],
        [ 0.0005, -0.0007,  0.0014,  ...,  0.0009,  0.0002, -0.0004],
        [ 0.0004, -0.0004,  0.0003,  ..., -0.0002,  0.0003, -0.0002]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.3817, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.2734, -3.0391, -3.2734,  ..., -3.8047, -4.3516, -1.6484],
        [-3.8750, -3.3281, -3.6250,  ..., -1.4385, -3.9219, -2.1250],
        [-5.4531, -4.2969, -4.9531,  ..., -5.6406, -5.9062, -0.2357],
        ...,
        [-6.8047, -4.0859, -5.1953,  ..., -6.1953, -7.3359, -0.3208],
        [-5.0508, -3.6289, -4.3633,  ..., -4.1602, -5.1133, -0.3948],
        [-2.0781, -3.1855, -2.8281,  ..., -3.1074, -4.2031, -1.6709]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20,  2, 10, 27,  6, 27, 23, 27, 27, 18,  7, 27, 27, 27, 27, 27,  0,  5,
        27,  1, 27, 14, 27,  0,  9, 27,  0, 13, 20, 26,  7, 27,  2, 15,  0, 22,
        20, 27, 27,  1,  1, 27, 22, 18, 27, 17, 27, 27,  5,  0,  6, 26, 27, 22,
         3, 27,  7,  7, 27, 13, 27, 10, 10,  2], device='cuda:0')
step: 151
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2380, -0.6104,  ...,  0.4788, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2056,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2181,  0.7969,  ..., -1.9023, -1.3438, -0.5991],
        [-0.4097, -0.2981, -1.7168,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0961,  ...,  0.6572,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.6362e-04,  4.7836e-03, -2.4605e-03,  ..., -4.5738e-03,
          1.7366e-03, -3.6812e-04],
        [-5.2643e-04,  3.3379e-04,  5.1880e-04,  ..., -3.3054e-03,
         -4.2114e-03,  1.2302e-03],
        [-4.4870e-04, -6.2561e-04,  7.0763e-04,  ...,  2.6054e-03,
         -6.6578e-05, -2.1534e-03],
        [-3.7050e-04, -8.8453e-04, -1.3828e-03,  ...,  4.0293e-05,
         -6.1083e-04, -1.3947e-04],
        [ 4.3941e-04, -1.5097e-03, -1.1749e-03,  ..., -1.1843e-04,
         -5.6934e-04, -8.6355e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4911, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.8633, -4.0039, -4.6914,  ..., -4.8477, -4.5977, -0.3789],
        [-4.5469, -3.2344, -4.5625,  ..., -3.2812, -3.8281, -1.4375],
        [-4.1172, -3.4453, -3.4453,  ..., -5.3984, -5.3359, -1.2725],
        ...,
        [-2.1484, -3.2891, -4.5859,  ..., -3.5234, -3.8047, -2.8359],
        [-3.6016, -3.5703, -2.7734,  ..., -1.4932, -4.6484, -3.1328],
        [-1.6504, -3.9785, -4.3398,  ..., -4.8203, -3.8379, -1.9639]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 9, 14, 20,  3, 27, 18, 27, 27,  9,  4,  7,  5,  1, 15, 27, 27,  0, 18,
        17,  2, 10,  3,  7,  9, 18, 25, 20, 18, 27, 27, 10, 15,  0,  3,  4, 27,
        18, 17,  1, 22, 27, 15,  3, 27, 16, 10, 27, 22, 27, 27, 15,  0,  3,  4,
        27, 11,  1, 27, 18, 27, 10, 27, 11, 17], device='cuda:0')
step: 152
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2380, -0.6104,  ...,  0.4788, -0.0856,  1.5107],
        [ 1.2520, -1.8896, -0.2056,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2181,  0.7969,  ..., -1.9023, -1.3438, -0.5991],
        [-0.4097, -0.2981, -1.7168,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6572,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.3924e-03,  1.6928e-04, -4.8041e-04,  ...,  4.5896e-06,
         -1.1530e-03,  9.0420e-05],
        [ 4.7469e-04, -1.5020e-04, -1.2512e-03,  ..., -4.2200e-05,
         -5.9080e-04, -2.2678e-03],
        [-1.5173e-03,  8.7976e-05,  9.2697e-04,  ...,  2.5654e-03,
         -3.3092e-04, -1.7557e-03],
        [ 5.5504e-04, -3.1090e-04,  2.6264e-03,  ...,  3.3927e-04,
         -7.7105e-04, -5.6267e-04],
        [ 3.3236e-04, -6.7663e-04, -5.7840e-04,  ...,  5.9986e-04,
          2.4700e-04, -1.5745e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2088, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.8594, -4.1875, -5.0156,  ..., -4.9531, -5.4531, -0.4233],
        [-4.8320, -2.8027, -4.0352,  ..., -4.7070, -3.9902, -1.0684],
        [-1.6504, -3.1504, -4.7891,  ..., -4.5078, -2.4629, -1.5723],
        ...,
        [-3.3379, -3.4004, -2.4941,  ..., -3.7598, -3.3223, -1.0732],
        [-5.1445, -3.7539, -4.2070,  ..., -3.0195, -4.1289, -0.6758],
        [-3.5391, -4.4609, -4.6328,  ..., -3.5703, -4.3516, -0.5693]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20,  3, 15, 18, 27, 27, 17, 27, 20,  7, 20, 22,  7, 27,  4, 17, 27,  2,
        15, 27, 27, 27, 15, 27,  0, 20, 27,  0, 27, 27, 25,  4, 27, 27,  1, 27,
         5,  1,  4, 27,  6,  3, 20, 27, 27, 27,  1, 27, 10,  4,  7, 27,  0,  2,
         0, 14, 27,  2, 27,  2, 15, 27, 27, 22], device='cuda:0')
step: 153
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2380, -0.6104,  ...,  0.4788, -0.0855,  1.5107],
        [ 1.2529, -1.8906, -0.2056,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2181,  0.7964,  ..., -1.9023, -1.3438, -0.5991],
        [-0.4094, -0.2981, -1.7168,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6572,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.1689e-04, -2.5406e-03,  9.6607e-04,  ...,  1.5199e-04,
         -3.9721e-04, -7.1764e-04],
        [-6.5327e-04,  3.5143e-04,  4.2462e-04,  ..., -3.7632e-03,
         -1.9665e-03,  8.2207e-04],
        [-2.6727e-04,  1.5421e-03, -7.4387e-04,  ...,  7.2622e-04,
          1.3828e-03,  5.2023e-04],
        [ 1.1129e-03, -9.1839e-04,  7.5340e-04,  ...,  1.2941e-03,
          6.0940e-04, -1.3838e-03],
        [ 9.0265e-04, -1.8954e-04, -1.3649e-04,  ..., -4.2081e-04,
         -3.1209e-04,  5.8532e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5802, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.9688, -4.0312, -4.3906,  ..., -1.2207, -2.9375, -1.4697],
        [-2.3613, -2.9395, -3.5645,  ..., -3.9551, -4.4727, -1.4551],
        [-3.6816, -3.2910, -3.5566,  ..., -2.5723, -4.8086, -1.2607],
        ...,
        [-5.3398, -2.7793, -3.7637,  ..., -3.9824, -4.2617, -0.9038],
        [-1.8320, -2.9727, -2.8008,  ..., -3.4551, -4.1289, -2.3320],
        [-3.7461, -3.2305, -4.0742,  ..., -4.9805, -4.0742, -1.3252]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 8, 27,  8, 13,  3, 25,  0,  1, 27,  4, 27,  0, 27,  8, 22, 25,  6,  8,
         0,  1, 27, 13, 15, 27,  1,  3,  0, 20, 27, 27, 15, 18,  7,  5, 26,  5,
         2, 27,  3,  7, 27, 15, 20, 27,  9, 10, 25, 27, 27, 27,  1, 10, 14, 18,
        27, 27,  7, 27,  1, 15, 25, 27, 21, 15], device='cuda:0')
step: 154
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2380, -0.6104,  ...,  0.4788, -0.0855,  1.5107],
        [ 1.2529, -1.8906, -0.2056,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2181,  0.7964,  ..., -1.9023, -1.3438, -0.5991],
        [-0.4094, -0.2981, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6572,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.0103e-03,  3.3150e-03,  5.4169e-04,  ..., -4.9210e-04,
          2.7227e-04,  2.9564e-03],
        [ 9.4700e-04,  2.0409e-04, -6.2180e-04,  ...,  1.4806e-04,
          2.4772e-04, -7.1764e-04],
        [-1.5764e-03,  3.3045e-04, -7.2098e-04,  ...,  8.5258e-04,
         -5.2786e-04, -1.7300e-03],
        [ 1.3151e-03,  3.8147e-05, -1.7900e-03,  ..., -2.0370e-03,
          1.0881e-03, -1.7471e-03],
        [-9.7847e-04,  1.0157e-04, -9.0933e-04,  ...,  3.3259e-04,
         -1.4830e-04, -5.9462e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5486, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.1641, -3.3965, -4.2734,  ..., -5.6016, -4.5703, -0.6318],
        [-4.8867, -3.6211, -4.4961,  ..., -3.1523, -4.8242, -0.7773],
        [-2.6699, -4.9375, -4.3906,  ..., -5.4219, -4.0156, -1.5146],
        ...,
        [-1.4062, -4.1406, -3.5000,  ..., -4.4219, -4.5938, -1.5781],
        [-1.8525, -3.6348, -2.7754,  ..., -1.7588, -5.0547, -2.8379],
        [-5.6016, -3.3047, -4.9297,  ..., -6.0078, -6.1016, -0.7573]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 27, 18, 27, 12, 27, 27, 27,  7, 11, 26, 10, 17, 18, 17,  0,  5, 27,
         0,  3, 13, 24,  0, 27,  9, 27,  2, 17, 27,  6, 27, 20, 27, 27, 16, 25,
        18, 27, 16,  1,  9,  0, 27, 12, 27,  1, 27,  7, 15, 24, 13,  5, 10, 24,
        27, 27,  9, 20, 27,  8, 18, 10, 25, 27], device='cuda:0')
step: 155
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2380, -0.6104,  ...,  0.4788, -0.0855,  1.5107],
        [ 1.2529, -1.8906, -0.2056,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2181,  0.7964,  ..., -1.9023, -1.3438, -0.5991],
        [-0.4094, -0.2981, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6572,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.4547e-04,  1.2531e-03,  5.4264e-04,  ...,  6.7329e-04,
         -3.9887e-04,  6.1560e-04],
        [ 6.6221e-05,  1.7347e-03,  5.3549e-04,  ..., -5.1804e-03,
         -8.0442e-04,  2.8229e-03],
        [-1.3065e-03,  1.2321e-03, -3.8719e-04,  ...,  5.4741e-03,
          5.4932e-04, -3.4866e-03],
        [-3.9148e-04, -4.5037e-04, -1.2341e-03,  ...,  2.1534e-03,
         -4.3941e-04, -8.3351e-04],
        [ 2.7323e-04,  4.5490e-04,  2.5201e-04,  ..., -5.4264e-04,
          4.3750e-04,  1.1225e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4073, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.5742, -2.6680, -3.9492,  ..., -4.6211, -4.4492, -0.8701],
        [-4.4102, -3.6445, -4.3477,  ..., -3.8477, -4.5664, -0.7075],
        [-3.9258, -3.5820, -3.7383,  ..., -3.4883, -3.0977, -1.0049],
        ...,
        [-2.9395, -3.5645, -4.0352,  ..., -4.3164, -4.4727, -1.2676],
        [-3.7129, -2.7754, -4.5703,  ..., -4.6016, -4.6016, -0.8687],
        [-3.3867, -3.3242, -3.1367,  ..., -2.7129, -2.8711, -1.7607]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 10,  7,  0,  4,  7, 27,  4, 15, 27, 27,  7, 27, 27, 27, 15, 27,  2,
        27,  2,  2,  3,  9, 13,  6, 15,  0,  4,  0, 27, 14,  4, 18, 13,  1,  4,
         7,  0,  1, 11,  6, 27, 27, 27, 27, 27, 25, 27, 13,  0,  7,  8, 17, 25,
         7, 13, 27, 15, 27,  1, 27,  0, 27,  3], device='cuda:0')
step: 156
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2380, -0.6104,  ...,  0.4788, -0.0855,  1.5107],
        [ 1.2529, -1.8906, -0.2056,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2181,  0.7964,  ..., -1.9033, -1.3438, -0.5991],
        [-0.4094, -0.2981, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6572,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.2178e-03,  2.9716e-03, -1.0014e-03,  ..., -1.7538e-03,
         -8.2636e-04, -7.1144e-04],
        [ 6.1083e-04,  1.9369e-03, -4.3535e-04,  ..., -6.9332e-04,
          9.2363e-04, -7.1192e-04],
        [ 9.9373e-04, -2.3651e-04,  2.0943e-03,  ...,  2.1019e-03,
          4.1866e-04,  3.3379e-03],
        [-1.9729e-05,  1.0605e-03, -6.1226e-04,  ...,  3.6550e-04,
         -3.4809e-04, -2.3413e-04],
        [ 4.4167e-05,  9.2173e-04,  2.2650e-04,  ...,  3.3045e-04,
         -2.3210e-04,  4.1890e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1939, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3750, -3.6426, -4.5781,  ..., -5.2344, -5.1250, -0.4858],
        [-3.8164, -3.0996, -4.1133,  ..., -4.3164, -5.1445, -0.8018],
        [-3.4551, -2.9258, -4.2695,  ..., -4.5039, -4.9883, -0.8784],
        ...,
        [-2.2695, -4.2383, -6.3477,  ..., -4.6289, -4.1289, -4.0508],
        [-4.4141, -3.2910, -4.2109,  ..., -5.9961, -3.8223, -0.5718],
        [-2.6582, -3.6113, -3.0801,  ..., -3.0645, -3.6895, -1.3770]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 10, 27, 14, 27,  6, 17, 27, 27, 27, 10,  1, 18, 26, 27, 18, 27, 27,
         1,  7, 12, 27,  6, 17, 26, 27, 17,  0, 27,  3,  0, 17, 27, 18, 17, 27,
        27,  2, 15,  1,  4, 15,  7, 27, 27, 25, 18,  3,  0, 27, 10, 15, 15,  4,
         3, 27, 27, 27,  8,  2,  3, 20,  1, 27], device='cuda:0')
step: 157
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6104,  ...,  0.4788, -0.0854,  1.5107],
        [ 1.2529, -1.8906, -0.2056,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2183,  0.7964,  ..., -1.9033, -1.3438, -0.5991],
        [-0.4094, -0.2981, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6572,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.7943e-04,  6.8378e-04,  4.7016e-04,  ...,  4.0054e-04,
         -3.2568e-04, -3.7122e-04],
        [ 2.8872e-04, -1.5020e-03, -2.7676e-03,  ...,  1.0405e-03,
          2.3499e-03,  3.2616e-04],
        [ 7.9250e-04, -1.3981e-03,  6.1655e-04,  ..., -7.8487e-04,
         -7.2575e-04, -3.2973e-04],
        [ 1.6391e-05,  3.3522e-04,  9.4461e-04,  ...,  3.2544e-04,
          3.6287e-04,  9.5892e-04],
        [-4.1318e-04,  4.5013e-04, -3.8195e-04,  ...,  5.5313e-04,
         -1.0300e-04, -1.3180e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4915, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4688, -3.2012, -5.0000,  ..., -4.9688, -4.8906, -0.7803],
        [-4.2930, -2.9492, -3.1367,  ..., -3.7461, -3.3887, -1.0283],
        [-2.3145, -3.6582, -3.5332,  ..., -3.9863, -3.3145, -1.3457],
        ...,
        [-5.2344, -3.7188, -4.8594,  ..., -3.8301, -4.4531, -0.4385],
        [-3.1875, -2.7656, -3.3594,  ..., -3.9219, -4.9062, -1.0625],
        [-5.1328, -2.0078, -3.8809,  ..., -3.9141, -5.3047, -1.1475]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 26,  2, 15,  3,  3, 27,  5, 27, 17, 27, 27, 27, 18,  2, 10, 13, 11,
        26, 27,  0,  0, 26, 25,  4,  3,  3,  5, 27, 27,  1,  0,  7, 10, 20,  4,
         3,  8, 17,  7, 15,  0, 15, 18,  0,  1, 27, 10, 27, 27, 27,  1, 17, 15,
        27, 13, 22,  4, 26, 26, 27, 27,  3, 27], device='cuda:0')
step: 158
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6104,  ...,  0.4788, -0.0854,  1.5107],
        [ 1.2529, -1.8906, -0.2056,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2183,  0.7964,  ..., -1.9033, -1.3438, -0.5991],
        [-0.4094, -0.2981, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6572,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.8583e-04,  3.1686e-04,  2.8629e-03,  ..., -2.0542e-03,
         -9.0885e-04, -1.1063e-03],
        [-3.6669e-04,  1.1587e-04, -7.6866e-03,  ...,  6.4564e-04,
          9.9258e-03,  2.0084e-03],
        [ 5.1928e-04,  2.2507e-03,  4.2381e-03,  ..., -3.2845e-03,
         -8.4114e-04,  3.1776e-03],
        [ 6.2752e-04, -3.3379e-04, -9.4891e-04,  ..., -1.2579e-03,
         -3.1292e-05,  1.5974e-03],
        [ 4.5848e-04,  5.7077e-04, -4.2534e-04,  ..., -1.4424e-04,
         -3.0696e-05, -4.9067e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3338, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.8184, -3.8184, -4.1641,  ..., -5.2734, -3.8340, -2.3340],
        [-4.1992, -3.4355, -3.1543,  ..., -3.9043, -2.5918, -1.6230],
        [-2.5215, -3.0371, -3.9746,  ..., -4.7852, -3.9121, -1.3652],
        ...,
        [-4.5898, -4.0430, -3.4980,  ..., -4.6523, -4.0586, -0.5288],
        [-4.3477, -3.4277, -4.4727,  ..., -3.3496, -4.5039, -1.0205],
        [-3.0371, -3.1934, -4.6133,  ..., -4.1758, -4.5195, -0.9590]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  1, 27, 26, 15,  0, 27, 15,  4, 15,  0,  4, 27, 27,  9,  4,  3, 27,
         0, 10,  1, 27, 27, 10,  3,  4, 20, 27, 15,  1, 15,  0, 27,  3,  8,  3,
        24,  4, 20, 18, 15, 16,  4, 27, 27, 26,  0, 27, 11, 27, 27,  0, 13, 10,
         0,  9,  0, 20, 15,  0, 27, 27,  9, 27], device='cuda:0')
step: 159
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6104,  ...,  0.4788, -0.0854,  1.5107],
        [ 1.2529, -1.8906, -0.2056,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2183,  0.7964,  ..., -1.9033, -1.3438, -0.5991],
        [-0.4094, -0.2981, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6572,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.1757e-04, -1.1072e-03, -7.5102e-04,  ...,  1.2541e-03,
          4.7445e-04, -3.5000e-04],
        [ 1.6632e-03, -6.1893e-04,  1.5316e-03,  ..., -2.7676e-03,
         -3.4294e-03, -7.2479e-05],
        [ 1.0605e-03, -1.3494e-03, -1.6212e-04,  ..., -2.2240e-03,
         -4.5252e-04,  9.4414e-04],
        [ 5.4312e-04, -5.3501e-04,  7.0095e-04,  ..., -1.4246e-05,
          1.1045e-04, -7.1144e-04],
        [ 5.3835e-04, -3.2735e-04, -3.6955e-05,  ...,  5.4479e-05,
         -4.0352e-05, -6.8235e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4418, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.1191, -3.1523, -3.0430,  ..., -4.5430, -5.0586, -1.2920],
        [-4.0117, -2.3223, -4.7930,  ..., -5.4961, -5.4492, -1.6357],
        [-3.3359, -2.7578, -3.9297,  ..., -3.0703, -2.9297, -1.7734],
        ...,
        [-4.5469, -2.9512, -3.9980,  ..., -5.4492, -4.7969, -0.6538],
        [-2.5723, -3.4629, -3.2598,  ..., -3.5879, -2.4473, -2.2285],
        [-4.8867, -4.4648, -4.2305,  ..., -5.4648, -5.0742, -0.2289]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15, 27, 23, 27, 27, 13, 10, 27, 27, 27, 18, 18, 27, 24,  0, 27, 27,  9,
        15,  5,  6, 15,  4, 27,  0, 24, 26, 13,  1,  4, 27, 13, 15,  4, 15, 17,
        27, 14, 10, 19, 13, 27,  9, 27,  4, 26, 18,  0, 15,  1,  7, 27, 11,  4,
         3, 27, 27, 13, 18, 27, 13, 27, 18, 27], device='cuda:0')
step: 160
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6104,  ...,  0.4788, -0.0854,  1.5098],
        [ 1.2529, -1.8906, -0.2056,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2183,  0.7964,  ..., -1.9033, -1.3438, -0.5991],
        [-0.4094, -0.2981, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6572,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.1549e-03, -1.3762e-03, -7.4911e-04,  ..., -2.8539e-04,
         -3.9876e-05, -1.0662e-03],
        [ 4.1425e-05, -9.6607e-04, -1.0509e-03,  ..., -1.0633e-03,
          1.7214e-03,  3.0994e-03],
        [-4.4370e-04, -7.6437e-04, -5.8556e-04,  ...,  1.7242e-03,
         -3.7861e-04, -1.5278e-03],
        [ 6.0797e-04, -1.5316e-03,  1.3084e-03,  ...,  2.1076e-03,
          1.0042e-03, -1.0834e-03],
        [ 5.6505e-04, -5.4884e-04,  2.1052e-04,  ...,  4.3631e-04,
          2.3603e-04, -4.3297e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2989, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.3594, -3.8262, -3.3887,  ..., -3.9219, -3.1387, -1.4834],
        [-3.0215, -3.7402, -3.8340,  ..., -4.2852, -3.6152, -0.5059],
        [-3.1875, -3.3438, -4.5469,  ..., -3.7344, -4.2969, -1.0010],
        ...,
        [-4.6055, -4.4961, -4.1367,  ..., -2.1523, -2.6211, -1.2461],
        [-2.8887, -3.6406, -3.4199,  ..., -2.5156, -3.5605, -0.9360],
        [-3.2441, -3.4316, -4.0273,  ..., -5.1328, -5.0117, -1.4629]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  5, 27, 27,  5, 11, 27, 27,  0,  4,  3, 27,  1, 15, 27, 18,  3,
        22,  4, 26, 17,  9, 27, 27,  3, 27,  4, 27, 27, 27, 27, 27, 13, 27, 22,
        10,  3, 27,  4, 27, 27,  1, 20,  9, 17, 27,  7, 27, 27, 27, 27, 22, 27,
        27, 20, 27, 22, 25, 27, 27,  3, 27, 27], device='cuda:0')
step: 161
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6104,  ...,  0.4788, -0.0854,  1.5098],
        [ 1.2529, -1.8906, -0.2056,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2183,  0.7959,  ..., -1.9033, -1.3438, -0.5991],
        [-0.4094, -0.2979, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6572,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.1226e-04, -6.6996e-04,  6.5374e-04,  ..., -4.2820e-04,
         -3.9673e-04,  2.6531e-03],
        [-1.1759e-03,  1.5602e-03, -9.1124e-04,  ..., -4.1962e-03,
         -1.3428e-03,  1.2989e-03],
        [ 1.1883e-03,  1.5564e-03,  6.4659e-04,  ...,  1.1039e-04,
         -6.2752e-04,  2.3162e-04],
        [-9.6846e-04,  1.0109e-03,  1.5402e-03,  ..., -1.1940e-03,
         -6.2799e-04,  2.5558e-03],
        [ 3.3212e-04, -3.3259e-04,  7.8487e-04,  ..., -2.7180e-04,
          1.3173e-05, -4.2629e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4115, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.0488, -3.7676, -3.2520,  ..., -2.0645, -3.5801, -1.9717],
        [-5.6680, -3.8691, -4.4805,  ..., -6.8867, -5.8242, -0.1353],
        [-3.7031, -3.1406, -3.7031,  ..., -4.5156, -5.2812, -0.8276],
        ...,
        [-4.1641, -3.2109, -3.9453,  ..., -5.5234, -5.1484, -0.4131],
        [-4.4219, -4.3438, -3.1113,  ..., -3.4863, -2.9082, -1.0010],
        [-3.4102, -3.7070, -4.9727,  ..., -5.1758, -4.1289, -0.6768]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([11, 27, 27, 27,  0, 11,  4, 15, 26, 27, 15, 27, 27, 18,  1,  1, 27,  7,
        27,  1,  7, 10, 27,  2, 17, 22, 20, 27, 27, 19, 27,  1, 27,  7,  1, 27,
        27,  0, 26,  4,  7,  8, 16, 18,  8, 27, 11, 26, 27,  7, 27, 15, 17, 21,
         7, 10,  0, 27, 27, 27,  7, 27,  1, 27], device='cuda:0')
step: 162
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6104,  ...,  0.4788, -0.0854,  1.5098],
        [ 1.2529, -1.8906, -0.2054,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2183,  0.7959,  ..., -1.9033, -1.3438, -0.5991],
        [-0.4094, -0.2979, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6572,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0006, -0.0004,  0.0010,  ..., -0.0001, -0.0020, -0.0006],
        [ 0.0003,  0.0016,  0.0002,  ...,  0.0018,  0.0021, -0.0009],
        [-0.0019,  0.0012, -0.0012,  ...,  0.0021,  0.0023, -0.0012],
        [-0.0002,  0.0007, -0.0011,  ...,  0.0005, -0.0003, -0.0006],
        [ 0.0005,  0.0002,  0.0001,  ...,  0.0002, -0.0002,  0.0008]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.3291, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.4727, -3.2383, -3.9727,  ..., -4.1758, -3.1133, -2.0977],
        [-2.0664, -3.0508, -2.9727,  ..., -3.5977, -3.0508, -1.5820],
        [-2.6660, -3.4941, -4.2461,  ..., -3.6973, -4.5547, -3.1660],
        ...,
        [-3.3887, -3.9199, -4.2500,  ..., -3.5449, -4.5781, -0.8267],
        [-1.5312, -2.6719, -2.7812,  ..., -4.0625, -3.7031, -2.6094],
        [-3.5996, -3.7090, -3.8965,  ..., -3.5684, -4.2383, -0.8496]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20,  0, 27,  1,  0, 27, 27,  0, 27,  4, 27, 25, 25, 24,  6, 27,  2,  3,
        18,  3,  0,  1, 27,  6, 27, 27,  4, 27, 27, 15, 14, 27,  5, 27, 27, 25,
        17, 15, 27, 15,  4, 27,  4, 18, 27,  2, 25, 27, 17,  4, 27, 11, 18,  8,
         0,  4, 13,  9, 27,  4,  4, 27, 12, 27], device='cuda:0')
step: 163
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6108,  ...,  0.4788, -0.0853,  1.5098],
        [ 1.2529, -1.8906, -0.2054,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2183,  0.7959,  ..., -1.9033, -1.3438, -0.5991],
        [-0.4094, -0.2979, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6572,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0252e-05,  1.2655e-03, -1.3292e-05,  ..., -2.4390e-04,
         -8.9788e-04,  6.3848e-04],
        [-1.0757e-03, -1.9989e-03, -3.7909e-04,  ..., -9.7322e-04,
          1.5485e-04,  6.8474e-04],
        [-1.6022e-03,  1.2321e-03, -9.4557e-04,  ...,  8.4829e-04,
          1.6603e-03,  2.1343e-03],
        [-4.6790e-05,  6.8843e-05, -8.7738e-05,  ..., -2.0790e-04,
          8.3089e-05,  1.6489e-03],
        [-6.5565e-07, -1.4186e-04,  2.1636e-05,  ...,  2.4343e-04,
         -3.7527e-04,  1.1673e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5411, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.6348, -3.5410, -2.8691,  ..., -2.4629, -3.7285, -1.0889],
        [-4.3867, -3.5898, -3.3086,  ..., -3.8086, -4.7305, -0.6836],
        [-3.7773, -3.6367, -3.3555,  ..., -5.0586, -2.5898, -0.9028],
        ...,
        [-2.8730, -3.2168, -3.6699,  ..., -4.0273, -3.9512, -1.5127],
        [-4.1055, -3.0098, -4.6172,  ..., -5.2930, -4.3398, -1.0410],
        [-1.8438, -3.2188, -4.5312,  ..., -3.0312, -3.3438, -2.6562]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25, 27, 13,  1, 27,  2,  4, 12,  0, 26, 27,  7,  0, 27, 20, 27, 17, 17,
        27, 27, 27, 27,  4,  4, 27, 22, 18,  1, 10,  9,  4, 18, 10, 27, 13, 25,
         4, 10, 22,  4, 27, 27, 27, 27, 25, 13, 27, 14, 27, 27, 12, 27, 27, 27,
         4, 25,  0,  3, 15,  4, 18,  4,  5, 27], device='cuda:0')
step: 164
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6108,  ...,  0.4788, -0.0853,  1.5098],
        [ 1.2529, -1.8906, -0.2054,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2183,  0.7959,  ..., -1.9033, -1.3438, -0.5991],
        [-0.4094, -0.2979, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6572,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.9217e-04, -1.3657e-03, -4.8375e-04,  ..., -1.2922e-03,
         -8.8358e-04,  8.2159e-04],
        [ 8.4460e-05,  2.8181e-04, -1.7147e-03,  ..., -2.4872e-03,
         -4.2868e-04, -1.1511e-03],
        [-1.2684e-03,  1.8473e-03,  2.3985e-04,  ...,  6.7186e-04,
          4.4489e-04,  3.4523e-04],
        [ 1.7905e-04, -5.1689e-04,  1.2693e-03,  ..., -5.8651e-05,
          4.6134e-05,  1.0452e-03],
        [ 6.7890e-05, -1.6522e-04,  2.4819e-04,  ..., -7.3624e-04,
         -1.5473e-04,  4.0007e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6297, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.9082, -3.2676, -4.0352,  ..., -3.2988, -3.8926, -0.9395],
        [-5.2812, -3.9375, -4.6094,  ..., -2.5781, -5.4062, -0.4849],
        [-4.8125, -3.1094, -3.2188,  ..., -2.0312, -3.6406, -2.0156],
        ...,
        [-2.4062, -3.4219, -3.4844,  ..., -3.4688, -4.3750, -1.5000],
        [-2.5254, -3.4941, -3.4785,  ..., -5.0742, -3.2129, -1.5723],
        [-1.5557, -3.4004, -4.3203,  ..., -5.2891, -3.4160, -1.5869]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10,  9,  3, 11, 18, 18, 10, 24, 27,  7,  7, 27, 27,  6, 27,  6, 15, 12,
        27,  1, 18, 27, 27,  3, 27, 11,  0, 27, 22, 27, 22, 27, 11,  7,  2,  9,
         3, 11,  2, 27, 18,  4, 15, 26, 18,  7, 27,  4,  3,  1, 27, 24,  7, 22,
         1, 27, 13, 15, 27, 27, 20, 18, 18,  7], device='cuda:0')
step: 165
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6108,  ...,  0.4788, -0.0853,  1.5098],
        [ 1.2529, -1.8906, -0.2054,  ..., -2.3809,  0.6045, -0.8247],
        [ 1.9111, -0.2183,  0.7959,  ..., -1.9033, -1.3438, -0.5996],
        [-0.4094, -0.2979, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6572,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0633e-03,  1.5497e-04,  3.2825e-03,  ...,  5.7340e-05,
         -1.9722e-03,  7.2002e-04],
        [ 4.4560e-04,  3.7193e-04, -3.5453e-04,  ...,  1.1826e-03,
          1.3361e-03, -1.0843e-03],
        [-1.9894e-03,  2.1114e-03, -7.1585e-05,  ...,  3.4657e-03,
          3.7594e-03,  1.3170e-03],
        [ 1.9050e-04,  4.0388e-04,  5.1613e-03,  ...,  1.3742e-03,
         -8.5592e-04,  2.9583e-03],
        [ 2.8419e-04,  9.1362e-04,  1.5621e-03,  ...,  1.3256e-04,
          2.1482e-04,  9.7561e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2149, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.7324, -2.6367, -3.2148,  ..., -5.4023, -3.5918, -0.8726],
        [-1.2314, -2.3242, -3.9805,  ..., -5.4336, -3.4336, -2.8555],
        [-4.4570, -3.3164, -3.9414,  ..., -4.1914, -5.1289, -1.1289],
        ...,
        [-1.2090, -3.3496, -4.0977,  ..., -4.8320, -4.1445, -2.2402],
        [-1.2061, -3.5352, -3.4727,  ..., -3.8789, -3.6445, -1.9717],
        [-2.9199, -1.9346, -2.4961,  ..., -4.7773, -3.9961, -1.7158]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0, 11, 27, 26, 27, 14,  0, 20, 15, 18, 27,  3,  6, 18,  4, 27, 26,
         2,  4, 26, 15, 25, 25, 27, 18, 27, 27, 15,  2,  6, 27, 27,  0, 26, 27,
        17, 27,  2,  9,  0, 27,  7, 27,  7, 18, 27,  0,  6, 10,  7, 10, 18, 27,
        27, 18, 25, 20,  1, 27,  4, 20, 27, 27], device='cuda:0')
step: 166
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6108,  ...,  0.4788, -0.0852,  1.5098],
        [ 1.2529, -1.8906, -0.2054,  ..., -2.3809,  0.6045, -0.8247],
        [ 1.9111, -0.2184,  0.7959,  ..., -1.9033, -1.3438, -0.5996],
        [-0.4094, -0.2979, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6572,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0006, -0.0035,  0.0005,  ...,  0.0024,  0.0006, -0.0014],
        [ 0.0023,  0.0014, -0.0005,  ..., -0.0044, -0.0029, -0.0001],
        [-0.0005,  0.0064, -0.0028,  ...,  0.0054,  0.0086,  0.0005],
        [-0.0003, -0.0005, -0.0004,  ..., -0.0004, -0.0004, -0.0015],
        [-0.0004, -0.0007, -0.0005,  ...,  0.0004, -0.0003, -0.0002]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.4963, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.8574, -2.7168, -3.7324,  ..., -5.1719, -4.1367, -1.3262],
        [-3.3496, -3.4297, -4.5703,  ..., -5.8516, -4.1016, -0.8511],
        [-3.5000, -3.5000, -3.4688,  ..., -3.2500, -4.4688, -0.9531],
        ...,
        [-2.4355, -3.1855, -5.3867,  ..., -4.8555, -3.5293, -2.2168],
        [-1.6953, -3.8828, -4.7109,  ..., -4.2109, -5.0703, -3.1953],
        [-2.9746, -3.3789, -3.9258,  ..., -3.8945, -3.2227, -1.6299]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 10, 27, 15, 27,  3,  4, 25, 10, 26, 27, 25, 27, 18, 27, 27,  3, 27,
        22, 15, 15, 13,  4, 22, 18,  7, 27,  4, 27,  1,  1, 14,  2, 27, 27,  0,
        24, 15,  1, 26, 27, 13,  4,  5, 27,  4, 15,  0, 18, 18, 27, 15,  9, 27,
        20,  3, 10, 20, 24, 17,  3, 18,  8, 27], device='cuda:0')
step: 167
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6108,  ...,  0.4788, -0.0852,  1.5098],
        [ 1.2529, -1.8906, -0.2054,  ..., -2.3809,  0.6045, -0.8252],
        [ 1.9111, -0.2184,  0.7959,  ..., -1.9033, -1.3438, -0.5996],
        [-0.4094, -0.2979, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6572,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.9741e-04, -2.7027e-03,  1.6489e-03,  ..., -2.1515e-03,
         -8.0013e-04, -1.8272e-03],
        [-3.5810e-04,  2.2745e-04,  1.7996e-03,  ..., -2.3575e-03,
          3.9749e-03,  4.0550e-03],
        [ 1.4257e-03, -2.4662e-03,  1.1044e-03,  ..., -1.2655e-03,
         -1.3008e-03,  3.5648e-03],
        [-2.5988e-05, -1.7405e-04,  3.0804e-03,  ..., -3.1972e-04,
          9.8169e-05,  2.3308e-03],
        [ 5.3287e-05,  4.0245e-04,  6.0892e-04,  ...,  4.3774e-04,
          3.2330e-04, -8.9788e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4483, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0156, -4.0469, -3.3887,  ..., -3.5137, -3.9512, -1.0293],
        [-4.3438, -2.2480, -3.5605,  ..., -4.1875, -5.0469, -1.1543],
        [-1.4111, -3.2871, -4.8008,  ..., -5.0820, -4.7695, -3.5996],
        ...,
        [-4.1484, -2.7871, -2.8027,  ..., -3.8184, -3.7402, -1.1777],
        [-2.5410, -3.2129, -4.0078,  ..., -4.7891, -3.2598, -0.6650],
        [-2.2695, -3.4414, -3.8789,  ..., -2.9570, -3.5508, -2.9258]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  4, 20,  0,  4, 27, 17, 27, 26, 10, 19, 22,  7, 20,  3, 27, 27, 27,
         7,  0, 18, 18, 27,  0, 13, 27, 11, 15, 27, 25,  0, 27, 26,  2,  1, 27,
        27, 23,  0, 25, 17, 27, 15, 27, 27, 27, 19, 12, 22, 27,  6,  3, 27, 27,
        24, 15, 27, 27, 15,  4, 22, 19,  3,  0], device='cuda:0')
step: 168
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6108,  ...,  0.4788, -0.0851,  1.5098],
        [ 1.2529, -1.8906, -0.2053,  ..., -2.3809,  0.6045, -0.8252],
        [ 1.9111, -0.2185,  0.7959,  ..., -1.9033, -1.3438, -0.5996],
        [-0.4094, -0.2979, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6572,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0020, -0.0022, -0.0004,  ..., -0.0027, -0.0016, -0.0043],
        [ 0.0014, -0.0044, -0.0033,  ...,  0.0279,  0.0181, -0.0142],
        [-0.0020,  0.0024, -0.0001,  ...,  0.0022,  0.0034,  0.0005],
        [ 0.0015,  0.0004,  0.0022,  ...,  0.0003,  0.0016,  0.0033],
        [ 0.0011,  0.0010,  0.0007,  ...,  0.0004,  0.0012,  0.0005]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.3357, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.9023, -4.6055, -4.5117,  ..., -3.6992, -4.8555, -0.2307],
        [-1.5791, -3.8594, -3.7031,  ..., -5.0469, -2.8906, -2.7031],
        [-3.2949, -3.0137, -4.1680,  ..., -4.7617, -3.1230, -3.6855],
        ...,
        [-3.3613, -2.6895, -3.6895,  ..., -4.7656, -4.9531, -0.6577],
        [-5.6484, -4.3516, -5.0547,  ..., -5.1016, -4.3984, -0.2271],
        [-2.1621, -3.5059, -4.1445,  ..., -5.1289, -4.1758, -3.8496]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([24, 18, 17, 10, 18, 21,  2, 27,  7, 15, 27, 27, 27, 27, 11, 27, 27, 27,
        27, 10, 15, 27, 27,  0, 20, 27,  2,  4,  4, 18, 27, 27,  0,  0, 15,  7,
         5, 27,  7, 27,  0,  0, 27, 27, 27, 27,  1, 27,  5, 15, 15, 17, 27, 27,
        10, 20,  6,  3, 25,  4,  6,  4, 10, 17], device='cuda:0')
step: 169
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6108,  ...,  0.4788, -0.0851,  1.5107],
        [ 1.2529, -1.8906, -0.2053,  ..., -2.3809,  0.6040, -0.8252],
        [ 1.9121, -0.2186,  0.7959,  ..., -1.9033, -1.3447, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0959,  ...,  0.6567,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.8515e-04, -4.3640e-03,  1.3275e-03,  ...,  6.7186e-04,
         -1.7271e-03, -8.0967e-04],
        [ 9.2316e-04, -2.5916e-04,  7.8058e-04,  ...,  2.9907e-03,
          2.2240e-03, -2.2240e-03],
        [-5.0163e-04,  4.5598e-05,  2.0485e-03,  ..., -1.4544e-03,
          2.2583e-03,  3.4008e-03],
        [-6.8283e-04,  5.6744e-04,  1.3552e-03,  ...,  2.2650e-04,
         -1.3232e-04,  3.3569e-04],
        [ 1.5652e-04,  1.0815e-03,  4.5824e-04,  ..., -3.0518e-04,
          2.8086e-04,  5.4502e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4057, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6406, -3.7500, -4.0781,  ..., -4.8750, -4.4688, -0.4377],
        [-3.8105, -2.7949, -3.3105,  ..., -4.7812, -4.1250, -1.2480],
        [-4.6602, -4.0664, -3.5977,  ..., -4.3320, -2.9883, -0.8169],
        ...,
        [-4.4453, -3.6953, -4.2734,  ..., -4.6172, -3.8672, -0.5083],
        [-3.2832, -3.3594, -3.2188,  ..., -3.2656, -4.5156, -1.1260],
        [-3.4258, -3.5508, -4.3008,  ..., -3.3945, -3.1289, -1.1904]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  6, 27,  9, 27, 11,  4, 27, 27, 10,  9, 27, 15, 20, 19, 13, 10, 27,
         7, 27, 15,  7, 15, 10,  0,  4, 27,  4,  1, 26, 27,  0, 26, 27, 27, 27,
        27,  4, 27,  6,  3, 10, 20,  3,  1, 25, 22, 11,  0, 22,  6, 10,  0, 27,
        27,  0,  3, 27, 27,  3, 27,  4, 12, 27], device='cuda:0')
step: 170
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6108,  ...,  0.4788, -0.0851,  1.5107],
        [ 1.2529, -1.8906, -0.2053,  ..., -2.3809,  0.6040, -0.8252],
        [ 1.9121, -0.2186,  0.7959,  ..., -1.9033, -1.3447, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6567,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0605e-03,  5.5170e-04, -7.3242e-04,  ...,  7.1228e-05,
          5.4312e-04,  4.0412e-04],
        [ 8.4925e-04, -4.9210e-04, -3.4857e-04,  ..., -1.2493e-03,
         -7.6723e-04, -1.2007e-03],
        [ 7.4816e-04,  5.2691e-04, -8.9586e-05,  ..., -3.1872e-03,
          4.3726e-04,  1.1263e-03],
        [-4.7028e-05,  3.1662e-04, -4.5133e-04,  ..., -6.8045e-04,
         -6.7711e-04, -1.3523e-03],
        [ 2.7490e-04, -7.7367e-05,  5.8794e-04,  ..., -9.6464e-04,
          1.1623e-05,  2.2173e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2784, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.6055, -4.0586, -4.6055,  ..., -3.1211, -5.3555, -0.7925],
        [-4.6484, -2.9297, -3.3828,  ..., -4.0391, -5.1172, -1.2578],
        [-3.7539, -3.4102, -3.2559,  ..., -2.3477, -2.6602, -1.5205],
        ...,
        [-3.6562, -3.2500, -3.9219,  ..., -4.6562, -3.9688, -1.3594],
        [-4.9766, -3.5078, -3.1953,  ..., -4.2109, -5.2266, -0.5708],
        [-4.2500, -3.7363, -4.0195,  ..., -4.9883, -4.3008, -0.6118]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 25, 11, 27, 27, 27, 27, 13, 18,  0, 15, 19, 27, 25,  3, 23,  2,  2,
        27,  0, 27,  0,  5,  2, 27, 27, 27, 14, 11, 27, 27, 27, 27, 22,  0, 25,
        27, 15, 18, 15, 27,  0,  4,  1,  1, 10,  0, 27, 27,  3, 27, 25,  6, 27,
        27, 26, 22, 13,  6, 14, 27,  7,  3, 27], device='cuda:0')
step: 171
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2382, -0.6108,  ...,  0.4788, -0.0850,  1.5107],
        [ 1.2529, -1.8906, -0.2053,  ..., -2.3809,  0.6040, -0.8247],
        [ 1.9121, -0.2188,  0.7959,  ..., -1.9033, -1.3447, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6567,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.3289e-03, -7.3528e-04, -1.2302e-03,  ..., -1.0948e-03,
         -7.4565e-05, -4.1809e-03],
        [ 1.7881e-04,  3.6125e-03, -5.8289e-03,  ..., -6.8426e-04,
          1.2970e-02,  1.1835e-03],
        [-6.3229e-04,  3.4332e-03, -2.0924e-03,  ...,  2.8629e-03,
          3.5496e-03, -8.9836e-04],
        [ 1.0214e-03, -5.6458e-04, -2.3060e-03,  ..., -3.6049e-04,
         -1.3661e-04, -4.6909e-05],
        [ 2.5535e-04,  2.7704e-04, -1.8826e-03,  ...,  1.3151e-03,
          3.1304e-04, -6.6161e-06]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2737, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.3047, -2.5684, -3.5527,  ..., -3.9590, -4.4297, -2.6465],
        [-2.8027, -3.0996, -3.3809,  ..., -5.0820, -4.2695, -2.4902],
        [-2.8613, -4.0508, -2.9707,  ..., -4.6250, -3.1113, -1.0332],
        ...,
        [-5.3164, -4.5352, -4.6758,  ..., -0.3330, -4.4258, -2.8027],
        [-3.7969, -3.6074, -2.7012,  ..., -2.9043, -5.0469, -0.9985],
        [-2.9473, -3.7754, -3.2734,  ..., -3.9473, -3.7910, -0.5557]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([13,  8, 17, 25, 27, 27, 27, 27, 25,  2, 27, 24, 27, 27,  6, 24,  5,  2,
        27,  1, 27, 27,  7, 27, 10,  5,  9, 11,  1, 27,  3, 18, 27, 27, 10, 27,
         5,  8, 26, 19, 27, 27, 15,  1,  9,  7, 27, 17,  7, 27, 27, 18, 11,  0,
        27,  1, 18,  0, 26,  3, 27, 25,  3,  6], device='cuda:0')
step: 172
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2380, -0.6108,  ...,  0.4788, -0.0850,  1.5107],
        [ 1.2529, -1.8906, -0.2053,  ..., -2.3809,  0.6040, -0.8247],
        [ 1.9121, -0.2188,  0.7959,  ..., -1.9033, -1.3447, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6514,  1.2695],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6567,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.1461e-04, -7.8058e-04, -1.7214e-04,  ...,  1.9245e-03,
         -4.1866e-04, -3.1662e-04],
        [ 1.1559e-03, -1.9550e-04,  9.8228e-04,  ...,  2.1112e-04,
         -1.3371e-03, -1.0805e-03],
        [ 3.0804e-04, -1.0214e-03, -6.9284e-04,  ..., -1.1568e-03,
         -6.1846e-04, -4.1127e-04],
        [ 3.0708e-04,  1.6797e-04,  1.4477e-03,  ...,  7.9250e-04,
          4.0174e-04, -1.1454e-03],
        [ 1.6928e-04,  6.2084e-04, -1.5438e-04,  ...,  1.0481e-03,
         -7.6175e-05, -1.0281e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2445, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.5586, -2.4648, -2.4023,  ..., -2.7930, -3.4492, -2.4492],
        [-1.7773, -2.6367, -3.7148,  ..., -6.3555, -4.9336, -0.8872],
        [-3.3535, -3.9785, -4.3711,  ..., -4.5898, -2.1191, -0.7441],
        ...,
        [-2.3594, -3.7188, -3.8438,  ..., -5.1562, -4.2500, -2.0938],
        [-2.5098, -3.2285, -3.6973,  ..., -3.0723, -3.1660, -1.5098],
        [-1.6621, -3.0996, -3.7871,  ..., -4.5508, -3.7246, -1.0361]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3,  0, 27, 27,  0, 18,  6, 27, 27, 27, 26, 27,  6,  2,  2, 17, 27,  5,
         2, 11, 27, 27, 27,  4, 27,  7, 18, 27,  1,  6, 27, 27, 27,  0, 22, 14,
         0,  7, 23,  6,  7, 27,  7,  0, 15, 27, 27,  4, 14, 18, 27, 24,  3,  0,
        27, 27, 27, 26,  0, 27, 20,  0, 27, 27], device='cuda:0')
step: 173
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2380, -0.6108,  ...,  0.4788, -0.0850,  1.5107],
        [ 1.2529, -1.8906, -0.2052,  ..., -2.3809,  0.6040, -0.8247],
        [ 1.9121, -0.2189,  0.7959,  ..., -1.9033, -1.3447, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6592,  0.6514,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6567,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.8324e-04, -2.2106e-03, -1.0443e-03,  ..., -1.6375e-03,
          1.0693e-04, -4.7636e-04],
        [ 2.2554e-04,  6.1703e-04, -5.8603e-04,  ..., -4.6754e-04,
          1.2121e-03, -8.9121e-04],
        [ 1.0767e-03,  1.5402e-03,  1.8177e-03,  ..., -6.4039e-04,
          1.5545e-03,  2.8534e-03],
        [ 4.3535e-04, -1.2279e-04,  1.6489e-03,  ...,  1.6651e-03,
          5.3596e-04, -7.2718e-04],
        [-3.0518e-04,  6.4182e-04,  3.5954e-04,  ...,  4.9293e-05,
          1.1759e-03, -2.9612e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3839, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.5039, -3.3184, -4.3203,  ..., -4.6328, -4.1758, -0.5684],
        [-3.2051, -2.9551, -3.3926,  ..., -4.5820, -4.8477, -1.2529],
        [-4.5547, -3.1035, -3.8066,  ..., -4.2578, -5.4609, -0.4160],
        ...,
        [-3.5273, -3.7148, -4.4336,  ..., -3.8555, -2.8730, -1.4033],
        [-3.7754, -3.0566, -3.1191,  ..., -3.7285, -4.0898, -1.4639],
        [-3.0312, -3.6895, -4.7344,  ..., -3.8301, -4.5156, -0.6416]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6,  1, 10, 27, 25, 25, 27, 27,  7, 27,  3, 27, 18, 27,  9, 27,  7,  0,
        15, 11, 10, 27, 14,  0,  0, 10, 27,  6, 17,  3,  0,  2,  0, 13,  0, 27,
        22, 15, 27, 20,  3,  8, 27, 27, 14, 18, 27, 10, 25, 24, 18, 15, 27, 18,
         0, 27, 27,  4,  0,  7, 24,  6, 27, 25], device='cuda:0')
step: 174
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6108,  ...,  0.4788, -0.0849,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3809,  0.6040, -0.8247],
        [ 1.9121, -0.2189,  0.7959,  ..., -1.9033, -1.3447, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6592,  0.6514,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6567,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.3895e-03, -1.2827e-03,  2.7037e-04,  ...,  1.5659e-03,
         -8.4019e-04, -5.0449e-04],
        [ 9.3412e-04, -4.0321e-03, -2.0275e-03,  ..., -5.6362e-04,
          3.3379e-03,  5.1155e-03],
        [ 5.6410e-04, -1.0395e-03, -1.0854e-04,  ..., -1.7996e-03,
          3.3665e-04,  2.1935e-03],
        [-9.0408e-04,  8.1587e-04,  1.3227e-03,  ..., -1.0891e-03,
         -1.1721e-03,  2.1915e-03],
        [ 3.2210e-04, -6.4516e-04,  1.3771e-03,  ...,  1.3483e-04,
         -8.7202e-05,  1.4324e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5095, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0234, -2.4766, -2.5234,  ..., -3.1953, -4.1797, -0.8364],
        [-4.1914, -4.0508, -4.2695,  ..., -4.4883, -4.0664, -0.6597],
        [-3.0801, -3.2363, -3.2520,  ..., -3.5020, -2.3926, -1.6123],
        ...,
        [-4.6445, -3.5508, -4.8789,  ..., -6.5039, -3.7539, -0.7217],
        [-4.2617, -3.4336, -4.2305,  ..., -4.0273, -4.3086, -0.7612],
        [-4.4258, -3.0039, -3.1289,  ..., -2.1602, -4.5195, -1.8467]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 14, 11,  0, 24, 27, 27,  1, 27, 10, 25,  4, 10, 27,  4,  1, 27, 27,
        27, 20, 27, 27, 18, 22, 14,  7, 27, 19,  2, 27, 26,  9, 18, 24,  7, 12,
        17, 18,  3, 10,  2,  4, 27, 14,  0, 27, 27, 27, 27, 14, 20, 27, 27,  4,
        17,  1, 13, 25, 20, 22, 27, 27,  3, 20], device='cuda:0')
step: 175
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6108,  ...,  0.4788, -0.0849,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3809,  0.6040, -0.8247],
        [ 1.9121, -0.2190,  0.7959,  ..., -1.9033, -1.3447, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6592,  0.6514,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6567,  0.7070, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.3828e-03, -2.1529e-04, -5.4836e-04,  ..., -7.9012e-04,
          1.2569e-03, -2.6455e-03],
        [-6.5327e-04,  1.1986e-04, -3.8576e-04,  ..., -1.2531e-03,
         -1.4553e-03, -1.3742e-03],
        [-1.2207e-04,  6.9332e-04, -8.0538e-04,  ..., -1.2598e-03,
          7.0429e-04, -2.3365e-04],
        [-9.8705e-05, -4.8101e-05,  4.2462e-04,  ...,  8.5354e-05,
         -7.6866e-04, -1.3351e-03],
        [-4.3321e-04, -3.4165e-04, -6.6137e-04,  ...,  5.6386e-05,
         -7.9632e-05, -7.0143e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5592, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0391, -3.4941, -4.2891,  ..., -4.6953, -3.4609, -0.5093],
        [-5.3867, -3.2930, -3.9023,  ..., -4.1680, -4.4336, -0.5581],
        [-1.7979, -4.3281, -4.2188,  ..., -4.4844, -5.8594, -0.4536],
        ...,
        [-3.0000, -2.9219, -3.0625,  ..., -3.3906, -4.1094, -1.6396],
        [-2.8594, -3.3125, -3.5000,  ..., -4.2188, -2.5469, -1.5156],
        [-4.5000, -3.7812, -3.6250,  ..., -1.9678, -3.3750, -2.0625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4,  5,  6,  3, 18, 27, 27, 27,  6,  3, 27,  4, 27,  1,  7,  4, 27, 15,
         4, 27, 20, 17, 18, 26,  4, 10, 27,  1, 27, 15, 22,  3, 10, 26, 22, 26,
        17, 27, 10,  3, 18,  3, 14, 27, 25, 27,  1, 17, 18, 27, 27, 27,  1, 27,
         4, 27, 25, 20, 10, 20,  1, 20, 26,  3], device='cuda:0')
step: 176
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6108,  ...,  0.4788, -0.0848,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2190,  0.7959,  ..., -1.9033, -1.3447, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6592,  0.6514,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6567,  0.7070, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.4598e-04,  9.8515e-04, -5.3549e-04,  ..., -1.1981e-04,
         -6.5327e-05,  7.8869e-04],
        [-1.1702e-03, -3.6955e-06, -3.6278e-03,  ...,  9.2506e-04,
          3.2291e-03,  1.4734e-03],
        [-1.1902e-03,  3.2959e-03,  3.5048e-04,  ...,  1.0233e-03,
          1.2226e-03, -1.7433e-03],
        [-1.1139e-03,  2.0599e-04, -9.1934e-04,  ..., -2.1601e-04,
         -1.7142e-04,  1.7653e-03],
        [ 1.8573e-04,  4.9305e-04, -5.7602e-04,  ...,  1.0376e-03,
          2.1100e-04,  1.0687e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1441, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.6484, -3.4141, -3.1484,  ..., -4.1953, -3.3359, -1.3672],
        [-4.8867, -4.4766, -4.1055,  ..., -4.6367, -5.1484, -0.3848],
        [-3.0293, -3.9043, -4.5312,  ..., -4.3125, -3.6562, -1.0303],
        ...,
        [-2.3105, -3.9219, -4.0469,  ..., -4.8906, -4.4531, -0.8110],
        [-2.7129, -3.7891, -3.1172,  ..., -3.5547, -3.4160, -2.0234],
        [-2.8984, -4.0391, -4.2109,  ..., -3.2109, -2.3359, -1.0859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 15, 17, 27, 27, 10,  5, 27, 25, 27, 27, 27,  1,  3, 27, 27, 27,
         4, 22,  1, 22, 27,  5,  1, 20,  0, 27,  3, 20, 15, 27, 27, 27,  6,  9,
        27, 27, 27, 27, 27, 13, 27,  5,  4, 27,  3,  4, 27, 27, 27, 18, 15, 22,
        27, 27, 10,  2, 27, 25,  1,  4, 27, 27], device='cuda:0')
step: 177
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6108,  ...,  0.4788, -0.0848,  1.5107],
        [ 1.2520, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2190,  0.7959,  ..., -1.9033, -1.3447, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6592,  0.6514,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6567,  0.7070, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.6049e-04, -9.6989e-04, -5.2834e-04,  ...,  5.6648e-04,
         -2.7990e-04, -8.8215e-05],
        [-7.4387e-04,  7.3433e-03,  2.8915e-03,  ...,  2.8114e-03,
         -1.7071e-03, -7.6523e-03],
        [-1.1005e-03,  2.3899e-03, -4.4084e-04,  ...,  1.0128e-03,
          2.1839e-03,  4.4405e-05],
        [ 2.3866e-04, -1.9050e-04, -5.0211e-04,  ..., -7.1859e-04,
         -6.5851e-04,  1.1665e-04],
        [-3.3450e-04, -6.3801e-04, -5.2631e-05,  ...,  3.6764e-04,
         -6.1274e-04,  2.0599e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6660, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0703, -2.7285, -3.1816,  ..., -3.2285, -4.0703, -1.3535],
        [-4.2227, -2.7227, -3.6914,  ..., -6.4883, -3.9727, -0.8169],
        [-4.4375, -3.6875, -3.9062,  ..., -6.4531, -4.6250, -0.6406],
        ...,
        [-4.3945, -3.7871, -3.8809,  ..., -4.8945, -3.6152, -0.6309],
        [-4.4570, -3.0801, -3.6113,  ..., -3.4082, -5.5352, -1.0498],
        [-4.8047, -3.8672, -3.5234,  ..., -3.3984, -5.5547, -0.6943]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([11,  7, 26,  5, 20,  3,  4,  4,  6, 18,  9, 18,  3,  0, 22, 27, 27,  2,
        27,  3, 22,  4, 27, 27,  4,  5,  1, 13,  8, 27, 27,  1, 27, 27,  1, 12,
         4, 27, 27,  5, 10, 27, 17, 15, 10,  7,  1, 27, 17, 27, 27,  0,  3,  7,
        27, 10, 15, 27, 18,  7, 27,  7,  2, 24], device='cuda:0')
step: 178
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6108,  ...,  0.4788, -0.0848,  1.5107],
        [ 1.2520, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2191,  0.7959,  ..., -1.9033, -1.3447, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6592,  0.6514,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6567,  0.7070, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.3113e-04, -4.9067e-04,  7.7581e-04,  ...,  3.7060e-03,
         -6.6042e-04,  1.0500e-03],
        [ 1.9102e-03, -6.1493e-03,  9.9087e-04,  ..., -1.5778e-02,
         -1.0315e-02,  1.2405e-02],
        [-3.9330e-03,  6.2447e-03, -3.7098e-03,  ...,  5.5122e-03,
          4.7684e-03, -2.6245e-03],
        [ 2.4438e-05,  1.1835e-03, -5.7042e-05,  ..., -2.8992e-04,
         -1.1101e-03, -7.2193e-04],
        [-6.1131e-04,  2.0373e-04, -1.7571e-04,  ..., -9.2506e-05,
         -1.0881e-03,  8.1491e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6487, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.3066, -2.6660, -3.5410,  ..., -5.1328, -3.9473, -1.9473],
        [-2.4941, -3.0879, -3.2441,  ..., -3.7598, -2.7598, -1.6035],
        [-5.1094, -3.5918, -4.5938,  ..., -5.8281, -5.0781, -0.3889],
        ...,
        [-2.5859, -3.1309, -4.3672,  ..., -4.6797, -3.9297, -1.6787],
        [-2.8438, -2.7168, -2.1719,  ..., -2.9062, -3.1387, -1.8896],
        [-3.5527, -3.2559, -3.5996,  ..., -3.6934, -4.7109, -0.6626]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2, 26,  7, 27,  1,  0, 27, 15, 27, 27,  4,  7,  4, 27,  1, 27, 15, 27,
        14,  7,  7, 15, 27,  4, 22,  6, 13,  2, 12,  1,  6, 26, 26, 20, 11, 18,
        26, 25,  4, 18, 11,  1, 22,  0, 27,  4, 27, 11,  0, 27,  3, 10, 22,  3,
        27,  0, 26, 10, 18, 24, 19,  4,  3,  4], device='cuda:0')
step: 179
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2380, -0.6108,  ...,  0.4788, -0.0848,  1.5107],
        [ 1.2520, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2192,  0.7959,  ..., -1.9033, -1.3447, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6592,  0.6514,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6567,  0.7070, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.7105e-04,  8.7070e-04,  8.8120e-04,  ...,  1.4143e-03,
          8.6725e-05,  1.9360e-03],
        [ 7.0095e-04, -4.1795e-04,  1.4963e-03,  ..., -2.3041e-03,
         -3.6163e-03, -2.3861e-03],
        [-1.1768e-03,  8.4829e-04, -1.8816e-03,  ...,  3.6478e-04,
         -2.2697e-04,  5.4359e-04],
        [-4.5586e-04,  1.3936e-04,  4.2486e-04,  ..., -5.9700e-04,
         -7.6103e-04,  1.2007e-03],
        [ 1.4949e-04,  3.9339e-05,  2.7823e-04,  ..., -3.7766e-04,
         -5.0879e-04,  8.8453e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0479, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.2656, -3.3125, -2.9375,  ..., -2.6875, -4.1562, -1.2803],
        [-2.4727, -4.5195, -4.1758,  ..., -5.0352, -3.7070, -3.9570],
        [-2.0488, -3.2051, -4.0195,  ..., -4.3633, -3.4551, -2.0176],
        ...,
        [-4.2656, -4.3750, -4.6094,  ..., -4.2031, -5.3906, -0.2961],
        [-2.2227, -3.8477, -3.1289,  ..., -2.1133, -3.5664, -1.9570],
        [-1.9990, -3.2324, -3.2480,  ..., -6.6094, -5.1250, -1.3115]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 17, 21, 18,  0, 27,  9, 27, 15, 27, 27,  6, 27, 27, 11, 27,  0,  7,
         4, 27, 17,  2, 10,  4,  0, 27, 26,  3, 20, 27, 27, 27, 26, 27, 15, 10,
        27,  6,  7,  4, 27, 27, 13, 27,  7, 25, 27, 18, 27, 27,  7, 27, 27, 27,
        10, 27,  0,  9, 24, 14, 10, 27, 27,  4], device='cuda:0')
step: 180
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2379, -0.6108,  ...,  0.4788, -0.0848,  1.5107],
        [ 1.2520, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2192,  0.7959,  ..., -1.9033, -1.3447, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6592,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6567,  0.7070, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.5463e-04, -1.8864e-03, -2.4581e-04,  ..., -1.1234e-03,
          3.9148e-04,  2.2078e-04],
        [ 8.3447e-04, -1.2894e-03,  3.6263e-04,  ..., -2.1687e-03,
         -9.3365e-04,  2.0752e-03],
        [-1.3561e-03,  1.1654e-03,  1.1425e-03,  ...,  3.8128e-03,
          4.5280e-03,  4.8981e-03],
        [ 3.0446e-04, -3.1734e-04, -1.6193e-03,  ..., -3.1042e-04,
         -3.7718e-04,  2.1982e-04],
        [ 8.7452e-04,  2.3007e-04, -8.7559e-05,  ...,  2.0051e-04,
          4.6802e-04,  9.4843e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4766, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.8418, -3.8262, -3.9355,  ..., -4.7500, -4.6992, -2.9980],
        [-4.1172, -3.2441, -3.2441,  ..., -3.8223, -4.1328, -1.1963],
        [-4.3555, -2.8086, -2.6367,  ..., -3.6680, -3.9023, -1.2627],
        ...,
        [-2.5684, -3.5527, -3.2559,  ..., -4.3672, -2.6309, -1.3193],
        [-4.9609, -3.4590, -3.7871,  ..., -5.9609, -5.0547, -0.8667],
        [-4.1719, -3.3301, -3.2832,  ..., -4.6406, -3.8770, -0.6416]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 5, 27, 22, 27,  8, 12,  4,  9,  2, 27,  7, 13, 26, 27,  3,  0, 25,  4,
         9, 25, 27, 18,  7, 27, 27, 27,  1,  4, 18, 27, 27, 26, 27, 12,  4, 27,
         6, 27,  6, 19, 27, 26, 27, 18, 20, 18, 10,  9, 27,  3, 18,  2, 18, 27,
        20, 27,  3, 27, 23, 24, 27, 27, 27, 27], device='cuda:0')
step: 181
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2379, -0.6108,  ...,  0.4788, -0.0848,  1.5107],
        [ 1.2520, -1.8906, -0.2051,  ..., -2.3809,  0.6040, -0.8247],
        [ 1.9121, -0.2194,  0.7959,  ..., -1.9033, -1.3447, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6592,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6567,  0.7070, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.7569e-03, -3.5744e-03,  1.8253e-03,  ...,  2.8782e-03,
         -1.4935e-03, -4.7183e-04],
        [ 6.9809e-03,  4.3831e-03,  1.3084e-02,  ..., -4.8208e-04,
         -1.3199e-02,  4.6387e-03],
        [ 2.6464e-04,  1.6928e-03, -3.0708e-03,  ...,  4.7417e-03,
         -1.9169e-04, -3.9558e-03],
        [ 5.2357e-04,  3.2520e-04,  9.7752e-05,  ..., -4.3774e-04,
         -1.3638e-04,  3.3140e-04],
        [-3.8838e-04, -1.9455e-04,  9.6273e-04,  ...,  3.3355e-04,
          2.3305e-05,  7.0810e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2736, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.6719, -3.1719, -3.2500,  ..., -4.6094, -2.8750, -1.5312],
        [-4.1719, -2.3906, -3.2656,  ..., -4.4844, -4.1094, -1.2646],
        [-1.5117, -3.5430, -4.1055,  ..., -4.8711, -3.2148, -2.3086],
        ...,
        [-4.8398, -3.5742, -3.1367,  ..., -3.0430, -3.9492, -1.2461],
        [-3.0000, -3.7031, -3.1562,  ..., -3.2344, -4.7188, -1.0312],
        [-4.1875, -3.4199, -4.1875,  ..., -4.2930, -4.4961, -0.5762]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 25, 15, 27, 10,  9, 27, 10,  7,  0, 27,  6, 18, 27, 11, 27, 27, 27,
        27, 27,  6, 10, 27, 27, 27, 18, 17, 20,  7, 27, 27, 27, 27,  1,  6, 18,
        24, 27, 18, 12, 27,  7, 27, 10, 15, 27,  7,  0, 22, 27,  1, 12, 27, 22,
        18, 23, 20, 27, 18, 27, 27, 27, 11,  1], device='cuda:0')
step: 182
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2379, -0.6108,  ...,  0.4788, -0.0848,  1.5107],
        [ 1.2520, -1.8906, -0.2051,  ..., -2.3809,  0.6040, -0.8247],
        [ 1.9121, -0.2194,  0.7964,  ..., -1.9033, -1.3457, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6567,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.4162e-04, -5.9700e-04,  8.8811e-06,  ..., -3.9768e-04,
         -2.9564e-05, -6.2418e-04],
        [-2.5129e-04, -3.0613e-03, -1.5507e-03,  ..., -1.8597e-03,
          2.8706e-04,  4.1504e-03],
        [-2.3117e-03,  1.7977e-03, -1.0433e-03,  ...,  4.5395e-03,
          4.6654e-03,  3.3016e-03],
        [ 2.9755e-04, -6.2764e-05, -1.5116e-03,  ..., -2.1577e-05,
          5.7518e-05, -1.3847e-03],
        [-1.1009e-04,  8.5711e-05, -5.0449e-04,  ..., -2.0194e-04,
         -7.8022e-05,  7.9441e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2573, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.8398, -4.3711, -3.5898,  ..., -4.5898, -4.1992, -0.4182],
        [-1.7373, -3.6738, -4.2383,  ..., -5.1914, -4.3008, -3.1738],
        [-3.6270, -3.8926, -3.7363,  ..., -5.1562, -3.8770, -0.8452],
        ...,
        [-3.8125, -4.1875, -3.7500,  ..., -4.2188, -3.7500, -0.8281],
        [-3.3223, -2.8848, -3.8848,  ..., -4.4805, -3.8066, -1.8535],
        [-4.4219, -3.4531, -3.3418,  ..., -3.9355, -3.7637, -0.7959]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 27, 27, 20,  4, 27, 17,  2, 10, 27, 27, 27, 10, 10, 27,  3, 27, 27,
        27,  2, 27, 18, 15, 27, 18, 13, 20, 15, 20, 27, 27, 18, 27, 27,  9, 11,
        10,  9, 10, 20,  5, 27,  3, 27, 15,  0,  1, 15, 27, 27, 27, 27,  1,  3,
        27, 24, 20,  3, 13, 27,  7, 27,  1, 26], device='cuda:0')
step: 183
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2379, -0.6108,  ...,  0.4788, -0.0847,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3809,  0.6040, -0.8247],
        [ 1.9121, -0.2195,  0.7964,  ..., -1.9043, -1.3457, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6562,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.5191e-04, -4.5586e-04, -7.2813e-04,  ...,  1.1215e-03,
         -1.1902e-03, -7.1859e-04],
        [ 3.0220e-05,  3.6621e-03, -2.5558e-03,  ...,  8.6441e-03,
          2.2984e-03, -4.0474e-03],
        [-8.7082e-05,  7.1526e-07, -2.7680e-04,  ..., -5.4073e-04,
          9.9599e-05,  3.4547e-04],
        [-1.7719e-03, -3.2139e-04,  5.9414e-04,  ...,  5.5492e-05,
          1.0824e-03,  2.2678e-03],
        [ 1.6332e-04, -7.3051e-04, -2.3603e-05,  ..., -3.8552e-04,
         -6.1274e-04,  7.0190e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.6498, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.0488, -3.2832, -4.3008,  ..., -3.6270, -4.6875, -2.0020],
        [-4.1719, -3.9707, -3.0957,  ..., -2.7520, -4.0156, -1.2051],
        [-1.7041, -3.5020, -4.4062,  ..., -4.5000, -3.9238, -3.9238],
        ...,
        [-3.6602, -3.0508, -3.2852,  ..., -3.4258, -3.4883, -1.1445],
        [-4.2773, -3.2617, -3.5742,  ..., -4.2148, -3.7773, -0.7300],
        [-4.1328, -3.8184, -3.4902,  ..., -2.4434, -4.0508, -0.9116]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18,  4, 20, 27, 27, 27, 27, 27,  6, 22, 17,  9, 17,  3, 15, 27,  2, 15,
        27, 15, 22,  8,  3, 18,  5,  1, 27,  7,  3, 27,  0, 27, 27,  7, 24,  6,
        26,  3,  0, 26, 27,  6, 10, 15, 26, 15, 27, 22, 26, 27, 11, 27,  9, 18,
        22, 27, 20, 25,  7, 27, 27,  2,  3,  3], device='cuda:0')
step: 184
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2379, -0.6108,  ...,  0.4788, -0.0847,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3809,  0.6040, -0.8247],
        [ 1.9121, -0.2196,  0.7964,  ..., -1.9043, -1.3457, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6562,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.3699e-04,  1.0002e-04, -6.4850e-04,  ..., -5.7507e-04,
         -1.1140e-04,  3.0231e-04],
        [-1.8988e-03, -2.0142e-03, -6.1607e-03,  ...,  1.0881e-03,
          3.9330e-03,  6.0034e-04],
        [ 1.2341e-03,  4.9257e-04,  2.2087e-03,  ..., -2.0599e-03,
         -1.2083e-03,  1.5039e-03],
        [ 1.2779e-03, -1.9813e-04, -2.4471e-03,  ..., -2.3842e-03,
         -2.9802e-06, -1.0929e-03],
        [ 5.8293e-05, -4.4847e-04,  4.9293e-05,  ..., -5.5456e-04,
         -1.9979e-04,  2.3842e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3544, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6016, -3.9160, -3.6191,  ..., -4.2891, -4.6797, -0.4153],
        [-5.6484, -4.1641, -4.0078,  ..., -4.7891, -5.3203, -0.3364],
        [-2.9453, -3.4922, -2.8203,  ..., -3.5391, -4.0078, -1.5391],
        ...,
        [-4.9141, -2.8809, -3.6152,  ..., -4.7109, -1.9277, -1.5215],
        [-2.3594, -4.2188, -3.7012,  ..., -4.2969, -2.6875, -1.7490],
        [-4.6445, -3.0527, -4.6602,  ..., -3.4121, -4.5508, -1.9590]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 14,  2,  0, 13, 20, 27,  4,  3, 27, 10,  4, 27, 15, 24, 27,  4,  2,
         8,  8,  0, 27,  1, 27, 18, 27, 27, 22, 25,  7, 11, 26, 15,  0, 15,  0,
        27,  5, 27, 27, 26,  4, 18,  6, 27, 27, 27, 27, 20, 27, 17, 27, 15, 27,
         7, 10, 27, 27, 27, 25, 18, 20,  4, 11], device='cuda:0')
step: 185
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2379, -0.6108,  ...,  0.4788, -0.0847,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3809,  0.6040, -0.8247],
        [ 1.9121, -0.2196,  0.7964,  ..., -1.9043, -1.3457, -0.5996],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6562,  0.7075, -1.7969]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.6403e-03, -8.3160e-04, -8.3923e-04,  ..., -1.1091e-03,
          7.4863e-04, -2.7924e-03],
        [ 4.7982e-05, -8.1635e-04, -5.1308e-04,  ..., -2.8000e-03,
          2.8458e-03,  6.9847e-03],
        [ 9.7847e-04, -6.0368e-04,  4.4107e-04,  ..., -3.1967e-03,
          1.1864e-03,  3.2024e-03],
        [ 7.8917e-05, -5.5218e-04, -7.4100e-04,  ...,  5.9319e-04,
         -3.3259e-04, -5.7220e-04],
        [-4.0770e-04, -4.6492e-04, -8.6021e-04,  ...,  4.4274e-04,
         -1.5783e-04, -9.2363e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4335, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.6172, -3.3047, -3.2559,  ..., -3.8965, -4.5547, -1.5068],
        [-0.3984, -3.6797, -4.8672,  ..., -6.0078, -4.6172, -3.5234],
        [-5.0977, -4.1445, -3.2520,  ..., -5.7070, -4.2695, -0.2683],
        ...,
        [-3.9004, -2.9473, -4.4453,  ..., -5.3203, -4.7891, -1.0713],
        [-4.3789, -3.7070, -3.0039,  ..., -2.6445, -5.1445, -1.1748],
        [-1.6123, -3.9863, -4.1445,  ..., -6.5195, -4.0664, -3.7676]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  7, 15, 27,  0, 27, 27, 26, 10, 25, 15, 22, 27,  1, 27,  2, 18,
         0, 26, 27, 20, 27,  7, 20, 27,  0, 27, 27, 10,  1, 25, 18, 18, 20, 20,
        25, 13, 26, 27,  7,  3, 12,  9, 15, 27,  5,  6, 27, 27, 14,  2,  3, 27,
         2, 27,  2,  6,  0, 15,  7, 20,  2, 18], device='cuda:0')
step: 186
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2378, -0.6108,  ...,  0.4788, -0.0847,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3809,  0.6040, -0.8252],
        [ 1.9121, -0.2197,  0.7964,  ..., -1.9043, -1.3457, -0.6001],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.3787e-04,  3.5572e-03,  7.3957e-04,  ..., -7.4089e-05,
          7.8630e-04,  6.0463e-04],
        [ 1.5879e-03, -1.4343e-03,  1.4944e-03,  ..., -1.5202e-03,
         -1.7586e-03,  3.7241e-04],
        [-3.8218e-04, -1.4172e-03,  1.3304e-03,  ..., -2.3136e-03,
         -3.5267e-03, -2.5249e-04],
        [ 2.3127e-04, -5.6863e-05,  1.8940e-03,  ...,  1.0786e-03,
          2.4462e-04,  1.1492e-03],
        [-3.0804e-04, -1.3864e-04,  8.1396e-04,  ...,  1.6630e-05,
          5.8174e-05, -2.8133e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1760, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3320, -2.6758, -2.4395,  ..., -4.0664, -4.8320, -0.6436],
        [-1.3301, -3.5645, -2.8926,  ..., -3.4395, -3.0645, -3.8301],
        [-4.4961, -4.1211, -4.1211,  ..., -4.3711, -3.5098, -0.7607],
        ...,
        [-5.4258, -4.7539, -5.3633,  ..., -5.5664, -4.8945, -0.1431],
        [-4.7070, -3.2207, -2.4395,  ..., -2.2051, -4.0352, -2.4082],
        [-3.7695, -3.5352, -4.5977,  ..., -5.5195, -4.5977, -0.7388]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 27, 27, 27,  0, 26, 20, 27, 26,  0, 27,  1, 27, 20, 20, 10, 22, 25,
        20, 20, 27, 17,  1, 27, 27, 22, 27, 22, 17, 27, 27, 27, 27, 27, 18, 21,
        27, 18, 26, 18, 27, 27, 20,  1, 10, 27, 25, 18, 27, 27, 27,  0, 27,  0,
        24,  8, 27, 24,  0, 15, 27, 27,  3, 27], device='cuda:0')
step: 187
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2378, -0.6108,  ...,  0.4788, -0.0847,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3809,  0.6040, -0.8252],
        [ 1.9121, -0.2197,  0.7964,  ..., -1.9043, -1.3457, -0.6001],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.2026e-03, -5.0278e-03, -3.5572e-03,  ..., -1.4353e-03,
          6.9571e-04, -1.6479e-03],
        [-2.0523e-03,  2.5330e-03, -3.4924e-03,  ...,  1.1988e-03,
          4.1428e-03,  8.4257e-04],
        [-7.0667e-04,  2.0485e-03, -2.1133e-03,  ...,  1.7872e-03,
          2.4948e-03, -1.5998e-04],
        [-6.6710e-04, -7.1049e-04, -2.1191e-03,  ...,  2.5320e-04,
         -5.2571e-05,  2.7657e-04],
        [ 3.1352e-04, -1.1549e-03, -3.2330e-04,  ..., -3.0565e-04,
         -4.0483e-04,  5.0020e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3159, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.6211, -2.3086, -1.9961,  ..., -2.4336, -4.9961, -2.3711],
        [-0.5830, -4.3945, -4.7070,  ..., -5.4258, -3.9590, -1.8027],
        [-3.9141, -4.4922, -4.7734,  ..., -6.3047, -4.1172, -0.3508],
        ...,
        [-3.8711, -3.6836, -3.0742,  ..., -1.9180, -4.6836, -1.0898],
        [-2.8164, -4.0039, -4.5977,  ..., -5.2695, -2.9258, -1.6914],
        [-2.0840, -3.1777, -3.5215,  ..., -4.7266, -3.0996, -2.0684]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2,  0,  7, 27,  2, 27, 27, 27,  0,  1, 15,  3, 12,  0, 11,  3,  5, 27,
         5, 27, 27,  5, 22, 15, 17, 18, 27,  7, 18,  3,  0, 15, 15, 27,  1, 20,
         4,  6, 27, 27, 17,  2, 27, 10, 18, 27,  5,  0, 24, 27,  4, 13, 15, 19,
         4, 10,  0, 13, 27, 27, 27, 25,  7,  0], device='cuda:0')
step: 188
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2378, -0.6108,  ...,  0.4788, -0.0846,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3809,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3457, -0.6001],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0005,  0.0020, -0.0001,  ...,  0.0015,  0.0018, -0.0015],
        [ 0.0086,  0.0013,  0.0095,  ..., -0.0030, -0.0108, -0.0005],
        [ 0.0009, -0.0035,  0.0004,  ...,  0.0005, -0.0017,  0.0005],
        [ 0.0002, -0.0019,  0.0020,  ...,  0.0025,  0.0023, -0.0029],
        [ 0.0004, -0.0004,  0.0008,  ..., -0.0002,  0.0005, -0.0009]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.2838, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.9531, -3.6562, -4.1094,  ..., -3.7344, -3.6719, -0.4219],
        [-5.2461, -3.6523, -2.1055,  ..., -1.6689, -4.7305, -1.6992],
        [-4.1797, -3.8535, -4.6641,  ..., -4.6953, -4.1016, -0.5718],
        ...,
        [-3.9219, -4.7031, -4.6250,  ..., -3.5156, -2.3906, -0.4385],
        [-3.3418, -3.2168, -3.6230,  ..., -2.7480, -5.9648, -1.3105],
        [-5.0703, -2.6016, -3.4297,  ..., -4.3203, -3.6953, -1.9297]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 12, 27, 25, 11,  1, 15, 27,  6, 15, 27, 18,  1, 27, 27, 27, 10,  7,
         4,  7, 17,  2, 27,  5,  3, 12, 27, 25, 10,  0, 27,  7, 27, 27, 13, 17,
         6,  4,  1, 27, 13, 27, 27, 25,  7, 13, 27, 27,  0, 27,  0,  6,  3, 15,
        27, 27, 25, 27,  0, 18,  1, 26, 10,  6], device='cuda:0')
step: 189
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2378, -0.6108,  ...,  0.4788, -0.0847,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3809,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3457, -0.6001],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0009,  0.0016,  0.0031,  ...,  0.0005, -0.0018,  0.0017],
        [-0.0010,  0.0012,  0.0013,  ...,  0.0033, -0.0002,  0.0019],
        [-0.0001,  0.0011,  0.0019,  ...,  0.0003,  0.0014,  0.0029],
        [-0.0007,  0.0016,  0.0006,  ..., -0.0004, -0.0005,  0.0017],
        [-0.0007,  0.0010,  0.0005,  ...,  0.0011,  0.0003,  0.0007]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.0380, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0195, -3.8320, -4.5977,  ..., -4.9258, -4.0039, -0.4731],
        [-4.4023, -5.3086, -3.9160,  ..., -5.8398, -4.8086, -1.7451],
        [-4.7070, -3.0684, -2.5215,  ..., -3.1934, -3.1621, -1.2559],
        ...,
        [-4.5625, -4.9844, -3.9395,  ..., -4.5312, -6.2500, -0.2512],
        [-2.8535, -3.4004, -3.8535,  ..., -3.4629, -2.4785, -2.1973],
        [-1.9863, -3.0020, -3.6426,  ..., -3.3770, -3.7676, -1.9863]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 18, 27,  4, 13, 27,  2, 27, 18,  0, 17,  7, 25, 27, 27, 10, 27, 20,
        10, 15, 27, 13,  0,  7, 27, 14,  4,  1,  3, 27, 17, 15, 27, 27, 27, 27,
        27, 27, 27,  5, 27, 25, 27,  6,  3, 27, 27, 15,  7, 15, 11, 10, 24, 10,
        17, 15, 10,  0, 18, 14, 17, 18, 22, 27], device='cuda:0')
step: 190
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2378, -0.6108,  ...,  0.4788, -0.0846,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3809,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3457, -0.6001],
        [-0.4097, -0.2976, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.1699e-04, -6.9523e-04,  2.3210e-04,  ...,  4.5776e-04,
         -2.2495e-04, -4.9591e-04],
        [-1.5354e-04, -5.2309e-04, -2.8458e-03,  ..., -5.2404e-04,
          3.4618e-04, -2.7084e-04],
        [-1.4758e-04, -7.2718e-05,  7.0715e-04,  ...,  3.0375e-04,
          7.0000e-04,  1.3266e-03],
        [ 3.1495e-04,  5.9724e-05,  6.3181e-04,  ...,  1.8334e-04,
         -6.7186e-04, -1.0262e-03],
        [-7.4506e-05, -5.2333e-05, -1.3208e-04,  ...,  1.5950e-04,
         -4.0960e-04,  1.5771e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3373, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.3203, -4.3828, -3.6953,  ..., -3.3516, -4.1328, -2.7578],
        [-1.7080, -4.1758, -5.0977,  ..., -6.2539, -3.5996, -4.0977],
        [-2.8926, -3.6738, -3.5801,  ..., -5.1094, -4.0156, -2.0020],
        ...,
        [-5.8281, -4.1719, -4.8438,  ..., -7.0781, -6.3281, -0.2644],
        [-2.8926, -3.6582, -3.7031,  ..., -4.8906, -4.9375, -0.5635],
        [-3.1270, -2.9238, -3.5957,  ..., -4.7227, -3.2676, -2.0645]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 15, 27,  4, 17, 27, 27, 20, 27,  7,  3, 27,  4,  7,  0, 10, 13,  2,
        27, 15, 10, 14, 10, 27, 27, 27,  0, 20, 27,  0, 27,  0, 27, 26, 27,  3,
        27, 27, 27, 15,  7, 27,  1, 25,  0,  7,  3, 15,  3,  7, 10, 27, 27, 27,
        10, 10,  9, 25,  5,  4, 10, 27, 27, 17], device='cuda:0')
step: 191
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2378, -0.6108,  ...,  0.4788, -0.0846,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3809,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3457, -0.6001],
        [-0.4097, -0.2976, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1072e-03,  1.3981e-03,  3.2806e-03,  ...,  2.2259e-03,
         -1.9283e-03,  2.1815e-04],
        [-4.1924e-03,  3.3340e-03, -2.4967e-03,  ...,  5.3520e-03,
          2.2430e-03,  2.6073e-03],
        [-6.1369e-04, -9.1016e-05,  8.4305e-04,  ..., -1.5235e-04,
         -1.5712e-04,  6.3944e-04],
        [-2.0046e-03,  1.2674e-03,  1.0490e-03,  ..., -5.4455e-04,
         -5.4359e-04,  2.9392e-03],
        [-4.7922e-04, -2.4247e-04, -1.5450e-04,  ...,  5.8746e-04,
         -4.8709e-04, -4.9353e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4602, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.5303, -4.2031, -3.4844,  ..., -5.6094, -3.4668, -3.9355],
        [-1.4316, -3.6348, -3.4160,  ..., -4.5391, -3.7910, -2.5410],
        [-4.4336, -4.7617, -4.2930,  ..., -4.1367, -4.3555, -0.3235],
        ...,
        [-2.8691, -3.2910, -3.8379,  ..., -6.3086, -5.2461, -0.8848],
        [-2.6074, -3.8887, -2.9824,  ..., -4.7500, -3.9824, -3.3418],
        [-3.7910, -3.1191, -3.4316,  ..., -2.9941, -4.0547, -1.5713]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 17,  4,  7, 27,  7, 27, 27,  2, 27,  8, 23, 23,  3, 15,  3, 10,  1,
        27, 27,  2,  1, 27, 25, 20,  4, 27, 27, 27,  4, 20, 20,  4,  9,  1, 27,
        22, 15, 27,  7,  1,  7, 25, 27, 18, 10,  4, 25,  0, 27, 27, 15,  3,  9,
        18,  6, 27, 18,  3, 27,  6,  4, 27, 27], device='cuda:0')
step: 192
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2378, -0.6108,  ...,  0.4788, -0.0846,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3809,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3457, -0.6001],
        [-0.4097, -0.2976, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.8709e-04, -2.0256e-03, -2.7704e-04,  ...,  1.4019e-03,
          5.3763e-05, -1.1444e-03],
        [-9.2316e-04,  2.3818e-04, -5.6648e-04,  ...,  1.0376e-03,
          5.2786e-04,  4.2200e-04],
        [ 5.6207e-05, -2.7966e-04,  1.5364e-03,  ...,  2.0523e-03,
          3.0804e-03,  3.4752e-03],
        [ 2.5868e-04,  3.3951e-04,  2.9659e-04,  ...,  2.2292e-04,
         -3.5429e-04,  8.0872e-04],
        [ 2.4915e-05, -1.0405e-03, -2.6059e-04,  ..., -1.0443e-03,
         -9.4891e-04, -7.9095e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2957, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.5078, -3.5703, -4.3047,  ..., -3.2422, -2.3828, -1.9443],
        [-4.4062, -4.2188, -5.1250,  ..., -4.3438, -3.5312, -0.5938],
        [-5.4414, -4.2852, -4.0039,  ..., -6.6289, -4.8320, -0.2537],
        ...,
        [-4.9297, -4.1953, -4.6641,  ..., -5.9297, -4.0703, -2.3203],
        [-5.5977, -3.4707, -3.3613,  ..., -3.6426, -4.4883, -0.7524],
        [-5.9531, -3.3438, -2.0312,  ..., -1.9688, -4.6875, -1.4365]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 27,  6, 18, 18,  3, 18,  1, 20,  4, 19,  9, 27, 10, 27,  0, 27,  2,
         0,  2, 20,  0, 27, 27, 10, 20, 27, 13, 27,  2,  5, 27, 27, 27,  0, 27,
        25, 26,  7,  2, 27, 14,  1,  3, 10,  4, 27,  2,  2, 20, 27, 27, 27, 15,
         4, 27, 18, 27, 15, 11, 10,  7, 27, 27], device='cuda:0')
step: 193
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2378, -0.6108,  ...,  0.4788, -0.0846,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3809,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7964,  ..., -1.9043, -1.3457, -0.6001],
        [-0.4097, -0.2976, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.7190e-04, -4.5776e-04, -1.4143e-03,  ..., -1.8942e-04,
         -3.0684e-04, -7.1573e-04],
        [-9.8586e-05,  1.2932e-03, -1.7366e-03,  ...,  1.0643e-03,
          1.6479e-03, -1.2054e-03],
        [-5.0592e-04, -8.0919e-04,  1.1101e-03,  ...,  1.6165e-03,
          2.6875e-03,  3.6869e-03],
        [ 6.5613e-04, -1.3947e-04,  7.1430e-04,  ..., -3.9220e-05,
          1.6701e-04, -1.4555e-04],
        [ 1.0926e-04, -2.0969e-04, -2.8563e-04,  ...,  2.0444e-05,
          3.7169e-04,  2.4080e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2696, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.9492, -3.6992, -3.9629,  ..., -3.8848, -3.8066, -0.7759],
        [-5.0977, -3.8926, -2.8301,  ..., -5.0195, -4.9727, -0.4707],
        [-5.1797, -3.0234, -4.3516,  ..., -5.1484, -4.3359, -1.0078],
        ...,
        [-1.4209, -3.9355, -4.2656,  ..., -4.9688, -3.5918, -2.9199],
        [-1.2441, -3.3691, -4.5742,  ..., -4.8711, -3.4316, -3.0566],
        [-4.0469, -3.7188, -3.3438,  ..., -4.4062, -4.5938, -1.4062]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27, 11, 27,  6,  4,  5, 24, 27, 20, 10,  5, 26,  7, 20, 25,  2,
         0,  1, 20, 27, 13, 18, 27, 27, 27, 15, 27,  4, 26, 20, 15, 10,  7,  4,
        27, 27, 27, 13, 27, 27,  4,  3, 27,  1, 27, 15, 27, 27, 27, 18,  7,  3,
         4,  4,  3,  3,  7, 15, 27,  0, 20,  8], device='cuda:0')
step: 194
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2378, -0.6108,  ...,  0.4788, -0.0845,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3809,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7964,  ..., -1.9043, -1.3457, -0.6001],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.2469e-03, -1.8463e-03,  8.7214e-04,  ..., -4.1544e-05,
         -9.5701e-04, -5.8317e-04],
        [ 5.1117e-03,  1.4200e-03, -1.9684e-03,  ...,  4.6844e-03,
         -4.4632e-03, -8.3399e-04],
        [ 1.8644e-03, -1.1501e-03, -8.5306e-04,  ...,  2.0447e-03,
          1.0900e-03,  1.4191e-03],
        [-7.1001e-04, -1.4257e-04, -9.7752e-04,  ..., -2.9254e-04,
          3.8528e-04,  1.8520e-03],
        [-1.8573e-04, -8.8358e-04,  6.6340e-05,  ..., -2.5749e-04,
         -1.1139e-03,  5.7936e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2661, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.1680, -3.6055, -3.3555,  ..., -4.5742, -4.4023, -0.6206],
        [-1.7451, -4.0898, -4.1836,  ..., -5.2305, -3.8848, -2.8691],
        [-5.2109, -4.4609, -3.4297,  ..., -5.0547, -5.3672, -0.2727],
        ...,
        [-6.2812, -3.3301, -3.4238,  ..., -3.7363, -3.7988, -0.4546],
        [-2.5840, -4.3633, -2.8809,  ..., -2.7871, -3.2402, -1.8496],
        [-5.8281, -3.3281, -4.0625,  ..., -0.5156, -5.5938, -2.9688]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 18, 27,  5, 20,  5,  0, 10,  0,  7, 27,  2, 25, 27, 27, 27, 27, 27,
         0, 20, 10, 18, 27,  3, 14, 15, 11,  7,  4, 27, 27, 27, 15,  2, 27, 15,
        25,  4, 27, 19, 18, 27, 17, 15,  6, 13, 17, 27,  7,  6,  1, 27, 14, 20,
         4, 27,  7,  0,  9, 27, 22, 10, 18,  9], device='cuda:0')
step: 195
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2377, -0.6108,  ...,  0.4788, -0.0845,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7964,  ..., -1.9043, -1.3457, -0.6001],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0961,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.9889e-05,  2.2709e-04,  5.5313e-04,  ..., -5.6839e-04,
         -1.0023e-03, -5.1975e-04],
        [ 8.4734e-04,  1.6737e-04, -5.1558e-05,  ...,  2.8820e-03,
          1.9197e-03, -1.0643e-03],
        [ 3.3474e-04, -1.1673e-03, -4.6921e-04,  ...,  7.8249e-04,
          3.9411e-04, -3.5262e-04],
        [-2.8348e-04,  1.1623e-04,  1.1921e-03,  ..., -1.1390e-04,
          3.6478e-04,  1.5087e-03],
        [ 1.3137e-04,  7.8726e-04, -1.1361e-04,  ...,  5.5265e-04,
          1.4126e-04, -6.3276e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3953, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.0820, -3.1895, -3.9707,  ..., -5.0195, -6.4102, -0.4707],
        [-2.6133, -3.4082, -3.0176,  ..., -4.9570, -4.4727, -2.1270],
        [-1.3613, -2.8457, -3.4238,  ..., -6.1406, -3.7676, -2.9238],
        ...,
        [-3.1699, -3.2793, -2.8418,  ..., -3.9512, -3.9199, -1.3730],
        [-2.3789, -4.1914, -3.6133,  ..., -2.0508, -3.7383, -2.4883],
        [-4.4922, -3.9316, -4.7734,  ..., -3.3066, -2.5879, -3.0410]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  0, 27, 11,  4, 27, 27, 27, 15, 27, 10,  0, 27,  4,  9, 19, 23,
        18, 27, 10,  2, 10,  0,  9, 26,  1, 24,  3,  3, 17,  0, 18,  1, 20, 27,
        25,  1, 27, 20, 17, 27, 27,  0, 27, 18, 20, 23,  6,  3, 15, 27,  4, 27,
         3, 27, 26, 27, 15, 15, 10, 27,  5, 27], device='cuda:0')
step: 196
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2377, -0.6108,  ...,  0.4785, -0.0845,  1.5107],
        [ 1.2520, -1.8906, -0.2052,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7964,  ..., -1.9043, -1.3457, -0.6001],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1729,  2.2285, -0.0960,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.8038e-04,  4.5586e-04,  1.8797e-03,  ...,  9.1934e-04,
         -1.6088e-03,  1.8597e-03],
        [-6.8188e-04, -5.0879e-04, -2.3055e-04,  ..., -2.3155e-03,
         -2.3670e-03,  4.5776e-04],
        [-3.1590e-04, -2.1911e-04, -3.5906e-04,  ...,  7.1287e-05,
          5.7459e-04,  1.2046e-04],
        [-6.5756e-04, -2.4271e-04,  1.7881e-03,  ...,  2.2304e-04,
         -4.2033e-04,  8.9049e-05],
        [ 2.3484e-05, -2.4319e-04,  1.7667e-04,  ...,  2.4080e-04,
         -3.4690e-04,  5.5730e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3230, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.0508, -3.2227, -3.2227,  ..., -5.4414, -4.1602, -0.6924],
        [-4.9336, -3.4961, -3.9180,  ..., -4.0898, -5.4336, -0.6670],
        [-4.3359, -4.0703, -4.3516,  ..., -5.8828, -5.2422, -0.4124],
        ...,
        [-2.7832, -4.2344, -3.5801,  ..., -5.1719, -3.6270, -2.0020],
        [-2.4258, -2.2852, -3.3164,  ..., -4.0508, -3.8008, -1.5352],
        [-2.1250, -3.9219, -3.0000,  ..., -5.0000, -5.0000, -2.0625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  4, 15,  0, 27, 27,  3, 27, 10, 27, 10,  9, 27, 10, 27, 15, 18,
        27, 22, 27, 26,  6, 27, 27, 27,  2,  2, 10, 27, 10,  4, 27, 24, 18, 27,
        15, 18, 27,  4,  0, 15, 25,  4, 27,  6, 27,  9, 18,  5, 23, 18, 27, 27,
        27, 27, 27, 18, 18,  5, 27, 15,  1,  5], device='cuda:0')
step: 197
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2377, -0.6108,  ...,  0.4785, -0.0844,  1.5107],
        [ 1.2520, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7964,  ..., -1.9043, -1.3457, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1719,  2.2285, -0.0960,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.6427e-04, -4.0829e-05,  3.7789e-05,  ..., -4.5776e-05,
         -4.9829e-04, -3.1042e-04],
        [ 1.8907e-04,  1.2379e-03,  2.3384e-03,  ..., -1.6899e-03,
         -7.2813e-04, -7.9441e-04],
        [ 3.0470e-04,  1.3351e-03,  7.5245e-04,  ..., -1.8101e-03,
         -4.2343e-04,  6.6471e-04],
        [ 8.0681e-04,  6.5708e-04,  2.5201e-04,  ..., -6.3539e-05,
          5.8365e-04, -1.0796e-03],
        [-1.5926e-04, -1.7130e-04,  4.0770e-04,  ...,  3.5381e-04,
          1.2839e-04,  3.8338e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2616, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.6172, -3.7734, -3.9141,  ..., -4.2891, -3.6797, -3.3828],
        [-2.8652, -3.6152, -2.4746,  ..., -2.5840, -3.8027, -1.2871],
        [-5.2266, -3.9941, -2.6660,  ..., -4.8047, -5.3203, -0.5098],
        ...,
        [-1.4727, -3.6758, -2.5039,  ..., -4.5977, -4.2070, -3.0820],
        [-5.8906, -3.4863, -3.8145,  ..., -4.6875, -3.9863, -0.3606],
        [-1.1611, -2.6914, -3.1777,  ..., -4.3945, -3.0215, -2.8027]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 27, 27, 25, 23,  7,  1, 22, 13,  0, 27, 18, 27, 27, 27, 26, 27, 17,
        27,  2, 27, 25, 18, 27, 27, 22,  3, 26, 15, 27, 20, 17,  0, 15, 13, 27,
         3, 17, 11,  7,  3,  3, 27, 10, 27, 15, 15, 18,  4, 27,  3,  8, 27, 15,
        22, 15,  4,  9, 27,  0,  6, 27,  7,  0], device='cuda:0')
step: 198
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2377, -0.6113,  ...,  0.4785, -0.0844,  1.5107],
        [ 1.2520, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7964,  ..., -1.9043, -1.3457, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1719,  2.2305, -0.0961,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.0122e-04, -2.9063e-04, -1.4057e-03,  ...,  4.2152e-04,
          1.2493e-03,  8.6963e-05],
        [ 1.0085e-04, -1.2598e-03, -1.7729e-03,  ...,  1.0926e-04,
          1.4448e-03,  1.1629e-04],
        [ 7.4768e-04, -1.3399e-03, -9.4509e-04,  ...,  3.0851e-04,
          1.9360e-04, -1.0405e-03],
        [ 3.1638e-04,  1.3828e-04, -1.2178e-03,  ..., -7.2861e-04,
         -2.7800e-04, -1.1263e-03],
        [ 3.0756e-04, -3.6573e-04, -1.1104e-04,  ..., -4.3917e-04,
          2.1470e-04,  6.5708e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4271, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.6543, -3.2324, -3.2793,  ..., -2.7012, -3.6387, -1.8262],
        [-2.6719, -3.1875, -4.1562,  ..., -3.2344, -3.8438, -1.9062],
        [-0.6250, -3.3594, -4.4219,  ..., -4.5156, -4.1719, -2.7188],
        ...,
        [-5.6602, -3.1504, -2.8535,  ..., -3.0254, -3.9004, -0.8384],
        [-4.0781, -4.0156, -3.3730,  ..., -3.2480, -4.4375, -0.8262],
        [-5.4180, -3.3105, -3.6055,  ..., -4.4961, -4.8242, -0.6064]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  0,  3, 13, 27, 23, 27, 20,  3, 26, 10,  5, 16, 27, 27,  1, 27,
         0, 17,  1, 18,  3,  3, 27, 27,  1,  0,  4, 27,  4, 27, 27, 26,  8,  3,
         4, 15, 27, 14, 27,  3, 27,  0, 10, 18, 26, 25,  8, 18, 19, 14, 27,  5,
        14, 27, 27,  3, 27,  4,  0, 10, 18, 27], device='cuda:0')
step: 199
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2377, -0.6113,  ...,  0.4785, -0.0844,  1.5107],
        [ 1.2520, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3457, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1719,  2.2305, -0.0961,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.0174e-04,  1.5855e-04, -1.8799e-04,  ..., -9.2411e-04,
         -1.9479e-04, -5.1165e-04],
        [-5.1594e-04, -2.2793e-04, -1.3697e-04,  ...,  4.5013e-04,
          6.7890e-05,  2.7943e-04],
        [-4.7708e-04, -7.8726e-04, -1.3218e-03,  ...,  4.6229e-04,
         -2.3413e-04, -1.3781e-03],
        [ 4.5717e-05,  2.3794e-04, -6.2180e-04,  ..., -6.4707e-04,
         -1.0014e-05,  1.2100e-05],
        [-2.4867e-04,  4.2367e-04,  1.4853e-04,  ...,  2.6870e-04,
          1.4150e-04, -6.8367e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3158, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.2051, -3.9551, -3.6582,  ..., -5.0156, -4.5469, -0.5020],
        [-4.1602, -3.0977, -3.0176,  ..., -3.6270, -3.1738, -1.6123],
        [-4.1406, -3.2363, -4.0625,  ..., -5.0469, -4.8281, -1.8770],
        ...,
        [-4.2773, -3.0879, -2.3848,  ..., -4.0273, -4.5898, -0.8071],
        [-1.7344, -4.2344, -4.2500,  ..., -4.0625, -4.4219, -1.9531],
        [-4.8203, -3.0078, -2.8047,  ..., -3.0859, -4.3672, -1.4609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 26, 22, 27,  3,  1, 14,  4,  1, 27,  3, 22, 25,  1, 15, 17, 18, 27,
        15,  7,  9,  3,  2, 27,  9, 22, 12, 18, 27,  3,  1, 27, 10, 24, 27, 15,
        27, 27, 18, 27, 27,  6, 27, 27, 27,  3, 27, 23, 27,  9, 27,  0,  0,  4,
         9, 11,  1, 11, 27,  4, 27, 27, 27,  3], device='cuda:0')
step: 200
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2377, -0.6113,  ...,  0.4785, -0.0843,  1.5107],
        [ 1.2520, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3467, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1719,  2.2305, -0.0961,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.3508e-04,  5.1880e-04,  3.8910e-04,  ..., -1.8358e-04,
         -6.6376e-04, -1.1921e-03],
        [ 3.3355e-04, -7.2598e-05,  1.2989e-03,  ...,  1.3781e-03,
         -1.0800e-04, -1.3721e-04],
        [ 5.7042e-05,  5.4026e-04, -1.2010e-04,  ..., -1.2803e-04,
          5.0640e-04, -2.2483e-04],
        [ 1.0357e-03, -2.7895e-04,  7.6103e-04,  ...,  1.5211e-03,
         -2.3818e-04,  2.4366e-04],
        [ 3.0684e-04, -4.0460e-04,  7.0143e-04,  ...,  2.8324e-04,
         -1.9646e-04,  4.6372e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3352, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.9219, -4.6719, -3.3125,  ..., -3.7969, -4.3594, -0.5156],
        [-6.5273, -3.9824, -3.6543,  ..., -6.5273, -4.1836, -0.3882],
        [-3.7500, -2.7969, -2.8438,  ..., -3.4531, -4.5625, -1.6406],
        ...,
        [-5.3320, -4.3320, -3.3809,  ..., -6.1602, -5.9102, -0.2710],
        [-2.6855, -3.4824, -3.8105,  ..., -4.1367, -4.3555, -2.6855],
        [-3.9707, -3.7812, -3.5625,  ..., -5.2031, -4.4844, -0.5322]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 27, 27, 12, 25, 15, 27, 27, 13, 20, 27, 27,  5,  4, 25, 27, 20,  4,
        14, 13,  7, 12,  8, 22,  7, 27, 27, 13,  3, 27, 27,  1,  4, 27,  4,  6,
         0, 18, 27, 27, 24,  3,  7, 27, 27, 27, 15, 11, 26, 18, 11,  7, 27, 27,
         4,  3, 27, 27,  3,  2, 27, 27, 27,  4], device='cuda:0')
step: 201
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2377, -0.6113,  ...,  0.4785, -0.0843,  1.5107],
        [ 1.2520, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3467, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2695],
        [-1.1719,  2.2305, -0.0961,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0449e-04, -8.6880e-04, -1.1845e-03,  ..., -6.4516e-04,
         -1.4889e-04, -1.2255e-04],
        [-1.0328e-03, -1.1730e-03, -1.4849e-03,  ..., -1.2283e-03,
          1.6289e-03,  1.8559e-03],
        [ 3.6669e-04, -2.3007e-04, -7.2479e-04,  ...,  2.8849e-05,
          9.5654e-04, -5.0879e-04],
        [-5.0211e-04, -2.3901e-05, -5.1355e-04,  ...,  9.2793e-04,
          5.7364e-04,  1.0796e-03],
        [-6.1035e-05,  7.0333e-04, -3.4630e-05,  ..., -2.7514e-04,
         -2.8312e-05,  5.2929e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3404, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.8438, -3.6719, -3.5781,  ..., -5.8125, -5.4219, -0.5303],
        [-5.4258, -3.5488, -4.1719,  ..., -4.9219, -4.6406, -1.2051],
        [-3.7070, -4.0820, -3.4414,  ..., -4.4102, -4.8008, -0.7065],
        ...,
        [-4.3945, -3.1289, -3.1133,  ..., -3.2695, -4.6914, -1.0361],
        [-6.8984, -3.5254, -3.9473,  ..., -7.3359, -4.4453, -0.6499],
        [-3.9688, -2.9844, -4.3438,  ..., -3.5781, -5.0938, -1.1260]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6, 10, 18,  4, 27, 27, 10, 27, 27, 27, 27,  6, 27, 10,  0,  7, 27, 20,
        27, 14, 15, 20, 27, 27,  0, 17,  5, 27,  3, 27, 27, 27, 11, 26, 27,  3,
         0, 27, 20, 26, 27, 18, 15, 27,  1,  8,  5, 27, 27,  8,  6, 20, 20, 15,
        15, 27, 27, 18, 27,  7, 15, 27,  7, 27], device='cuda:0')
step: 202
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2377, -0.6113,  ...,  0.4785, -0.0842,  1.5107],
        [ 1.2520, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3467, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1719,  2.2305, -0.0961,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.7684e-04, -1.3885e-03, -2.0504e-04,  ..., -3.5906e-04,
         -1.1339e-03, -1.4505e-03],
        [-4.7731e-04,  1.1921e-03, -2.6345e-04,  ..., -2.6226e-03,
         -8.0168e-05,  8.5592e-04],
        [-7.4196e-04, -3.1137e-04, -4.4322e-04,  ...,  1.4849e-03,
         -6.8569e-04, -1.4515e-03],
        [-8.4352e-04, -6.2644e-05, -9.7513e-04,  ...,  3.9959e-04,
         -4.7708e-04,  1.2951e-03],
        [-5.2929e-04, -8.6308e-04,  1.4515e-03,  ..., -2.0862e-04,
         -4.6444e-04,  1.0643e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1535, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.9199, -3.2793, -3.8887,  ..., -3.8730, -2.8418, -1.4355],
        [-4.4414, -3.5020, -2.2520,  ..., -2.5801, -3.9395, -1.3936],
        [-2.9883, -4.0508, -3.9727,  ..., -4.2539, -3.1133, -1.6914],
        ...,
        [-2.5508, -3.6133, -3.5977,  ..., -5.2539, -3.6133, -1.9414],
        [-2.8828, -2.1172, -3.1016,  ..., -4.1797, -2.8984, -2.7266],
        [-3.7520, -2.8301, -1.9395,  ..., -2.5801, -4.6094, -1.3613]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15, 27, 26, 27, 10, 27, 10, 12, 25,  3, 27, 23, 25, 25, 27,  5, 27, 25,
        27, 27, 27, 17, 27,  7, 15,  1,  6, 27, 18, 27, 27, 13,  1,  0, 27,  7,
         2, 15, 27, 27, 27, 27,  0, 27, 27, 27, 10,  0, 14, 17, 27,  4,  1, 27,
        14,  2,  7, 27, 25, 25,  1, 18, 15,  9], device='cuda:0')
step: 203
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2377, -0.6113,  ...,  0.4785, -0.0842,  1.5107],
        [ 1.2510, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3467, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1719,  2.2305, -0.0961,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.5678e-04,  2.2163e-03, -4.2129e-04,  ...,  8.3256e-04,
         -1.1024e-03, -5.2869e-05],
        [-3.3307e-04, -7.1716e-04, -1.2660e-04,  ...,  5.1832e-04,
         -1.1253e-03,  3.2806e-04],
        [ 5.7936e-04,  3.0975e-03,  9.3994e-03,  ..., -5.0507e-03,
         -3.8872e-03,  5.3062e-03],
        [ 8.8024e-04,  1.3423e-04,  2.0123e-03,  ..., -7.3910e-04,
          3.4451e-04,  1.1104e-04],
        [ 3.3951e-04, -6.9857e-04, -4.0889e-04,  ...,  3.1042e-04,
         -4.1306e-05, -1.8921e-03]], device='cuda:0', dtype=torch.float16)
Progress 30.04%, loss: 3.5052881836891174, time 145.47s
loss: tensor(2.5701, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.7090, -4.1328, -3.6777,  ..., -4.2891, -3.9902, -0.8027],
        [-2.3262, -3.2949, -3.4531,  ..., -3.6230, -2.8730, -2.4043],
        [-3.6543, -3.3730, -3.8574,  ..., -5.2305, -3.9199, -2.9980],
        ...,
        [-2.2754, -4.0273, -4.1055,  ..., -4.4023, -2.9160, -1.9014],
        [-4.7773, -3.5742, -2.6055,  ..., -3.4805, -4.0898, -1.8398],
        [-3.2520, -3.5488, -4.0156,  ..., -3.6426, -3.1582, -2.3770]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([13, 18, 15,  5, 26, 15, 27, 25, 27, 20, 27,  0,  3, 27, 10, 10,  3, 27,
         5, 24, 22, 27, 27,  3, 11,  0, 17, 27, 27,  3, 27, 14, 27,  4,  4, 13,
         3, 15, 27,  7,  6, 11,  4,  7,  7,  7, 17, 23, 18, 10,  0, 18, 27,  6,
        20, 10, 27,  5,  2, 27, 27,  0, 14,  1], device='cuda:0')
step: 204
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2377, -0.6113,  ...,  0.4785, -0.0842,  1.5107],
        [ 1.2510, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3467, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1719,  2.2305, -0.0961,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.9250e-04, -1.3523e-03, -3.8266e-04,  ..., -7.8142e-05,
          2.3007e-05, -1.0099e-03],
        [-2.0802e-04,  9.0504e-04,  1.8625e-03,  ...,  1.1806e-03,
         -1.8368e-03, -1.4839e-03],
        [-1.7047e-05, -1.1473e-03, -2.9993e-04,  ...,  2.2161e-04,
          9.0218e-04,  1.2064e-04],
        [-9.5069e-05,  1.9312e-04,  6.8521e-04,  ...,  1.5080e-05,
          1.6737e-04,  1.2608e-03],
        [-7.2718e-05, -5.4741e-04,  3.4523e-04,  ..., -2.9802e-06,
          4.8637e-05, -1.8573e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2054, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.8984, -3.3203, -2.9141,  ..., -1.9131, -3.0547, -2.3984],
        [-3.6855, -3.5762, -2.9043,  ..., -2.2637, -4.3398, -1.4033],
        [-4.7031, -3.5625, -2.8906,  ..., -1.7500, -4.6719, -1.2969],
        ...,
        [-4.9219, -3.1582, -3.8145,  ..., -5.0938, -3.5801, -0.7358],
        [-4.0039, -3.3789, -3.3008,  ..., -3.1133, -4.0039, -1.0508],
        [-3.7617, -4.1055, -4.5742,  ..., -5.8086, -3.9043, -1.0127]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26, 27, 24,  1, 20, 27, 12, 27, 27, 27, 27,  6, 27,  0, 10, 27, 27, 27,
        27,  2,  3, 20, 25, 27, 27, 10, 27,  9, 24,  0, 25,  0, 15, 15,  6, 27,
         5, 27,  6, 27, 26, 17, 24,  9,  7, 11,  0,  0,  1,  7, 15, 18,  9, 27,
        27, 22, 27, 26, 27, 11, 22, 27,  6,  4], device='cuda:0')
step: 205
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7676, -0.2377, -0.6113,  ...,  0.4785, -0.0842,  1.5107],
        [ 1.2510, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3467, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1719,  2.2305, -0.0961,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.9986e-04, -7.7772e-04,  3.8242e-04,  ...,  2.0523e-03,
          1.1480e-04, -4.5419e-05],
        [-1.8358e-04,  1.7662e-03,  3.0422e-04,  ..., -3.7861e-04,
         -2.4068e-04, -1.3781e-04],
        [ 1.0910e-03,  1.0471e-03,  1.0948e-03,  ..., -2.4185e-03,
         -8.4043e-05,  1.2360e-03],
        [ 4.1389e-04,  9.4175e-04, -3.1185e-04,  ..., -7.4482e-04,
         -5.2023e-04, -5.3024e-04],
        [-1.5974e-05,  6.7425e-04, -4.6158e-04,  ...,  2.4271e-04,
          3.2735e-04, -2.9063e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0952, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2266, -3.4121, -3.4434,  ..., -2.1465, -4.7578, -1.3350],
        [-4.9023, -4.1484, -3.8848,  ..., -5.3555, -3.1660, -1.6348],
        [-3.8906, -3.8906, -4.4531,  ..., -4.1562, -3.3418, -1.6240],
        ...,
        [-4.5273, -4.3242, -3.9805,  ..., -3.4023, -4.3086, -0.5439],
        [-1.2363, -2.8770, -3.7988,  ..., -6.5625, -4.3281, -2.8301],
        [-5.0547, -4.6328, -4.6172,  ..., -5.6016, -4.6953, -0.2432]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2, 13, 27,  0,  9,  1, 18, 27, 26, 27,  5, 27,  3, 27, 10,  0,  0, 27,
        27, 13,  6, 11, 27,  0, 27, 27, 11, 27, 27,  4,  7, 15, 27,  3, 15, 22,
         5, 27, 27, 26, 27, 27, 27, 27, 15, 27,  7, 27, 27, 22, 26,  6, 11, 15,
         7,  7, 27, 27, 27,  0,  4, 10,  0, 27], device='cuda:0')
step: 206
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6113,  ...,  0.4785, -0.0841,  1.5117],
        [ 1.2510, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3467, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1719,  2.2305, -0.0961,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.2720e-04, -2.4974e-05,  1.7130e-04,  ..., -7.2098e-04,
         -4.9114e-04, -6.2847e-04],
        [ 3.6669e-04, -1.2279e-04, -1.0080e-03,  ...,  5.2452e-04,
          1.4629e-03,  2.2864e-04],
        [-2.4939e-04,  8.3971e-04, -8.5235e-06,  ..., -4.3893e-04,
          2.4676e-04, -1.6689e-06],
        [-2.1267e-04, -3.0899e-04,  1.2217e-03,  ...,  4.8733e-04,
          1.6475e-04,  2.3289e-03],
        [ 5.9080e-04, -6.5422e-04,  1.0138e-03,  ..., -2.8396e-04,
          7.8499e-05,  5.0449e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2697, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.2949, -3.9355, -4.3594,  ..., -5.9844, -3.7168, -0.7954],
        [-3.4629, -3.1660, -2.0410,  ..., -1.5107, -4.0430, -3.0410],
        [-3.1914, -3.7695, -3.9570,  ..., -3.8477, -3.7852, -2.1758],
        ...,
        [-3.9629, -4.2422, -2.7910,  ..., -2.4004, -4.0078, -0.8066],
        [-3.7129, -3.8691, -3.6191,  ..., -3.7754, -3.0098, -1.6035],
        [-2.4766, -3.5859, -2.6328,  ..., -3.1016, -2.6797, -1.5547]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  3, 18, 27, 27,  4, 27, 27,  7, 27, 22, 22, 10, 27,  1,  4, 11,  1,
        27, 18,  2, 10, 27, 27, 27,  3,  9,  3,  0, 27, 20, 27,  7,  1,  0,  8,
        27,  0,  4, 27, 27, 25,  3, 18,  2, 20, 27, 10,  1, 14, 27, 27,  3, 15,
         0, 18, 27,  9, 27, 27, 27, 24,  8,  3], device='cuda:0')
step: 207
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6113,  ...,  0.4785, -0.0841,  1.5117],
        [ 1.2510, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3467, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1719,  2.2305, -0.0961,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0016,  0.0021, -0.0002,  ..., -0.0075,  0.0008, -0.0058],
        [ 0.0005,  0.0006,  0.0031,  ...,  0.0075,  0.0020, -0.0005],
        [-0.0014, -0.0011, -0.0284,  ...,  0.0183,  0.0115, -0.0114],
        [-0.0035, -0.0004, -0.0087,  ..., -0.0012,  0.0003, -0.0017],
        [ 0.0013,  0.0037,  0.0044,  ..., -0.0059,  0.0003,  0.0057]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.2515, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.8359, -3.5078, -3.3984,  ..., -2.6016, -4.1797, -1.8213],
        [-5.5273, -3.6836, -3.6348,  ..., -4.3242, -3.9805, -0.5894],
        [-5.6211, -4.1211, -3.1680,  ..., -5.4336, -3.6055, -0.5894],
        ...,
        [-2.5625, -3.9062, -3.3906,  ..., -4.6406, -4.2188, -0.7031],
        [-3.4316, -3.9004, -3.8848,  ..., -1.7129, -3.4766, -2.5723],
        [-4.9023, -5.0625, -4.1250,  ..., -5.2344, -4.1094, -0.3418]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 14, 26, 27, 15, 27,  0, 27,  6, 27, 27, 10, 27,  0, 20,  2, 27, 18,
        12, 27,  0, 27, 27, 26,  1, 27,  0, 27, 27,  0, 27, 27, 22,  0,  4,  2,
         9,  3,  0, 27,  5,  8, 24, 27,  7, 27,  3,  0,  0, 15,  2,  1, 27, 20,
        15, 14, 20, 26, 13, 15, 13, 27,  3, 22], device='cuda:0')
step: 208
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6113,  ...,  0.4785, -0.0840,  1.5117],
        [ 1.2510, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3467, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1719,  2.2305, -0.0962,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.4820e-03, -1.0681e-03,  1.5297e-03,  ..., -3.2854e-04,
         -1.2989e-03, -2.8110e-04],
        [ 2.1839e-03, -1.5936e-03, -1.3189e-03,  ...,  5.6953e-03,
          3.5686e-03,  1.1569e-04],
        [ 1.5986e-04,  8.6927e-04,  3.8471e-03,  ..., -3.2902e-03,
         -1.0242e-03,  3.4790e-03],
        [-2.6727e-04, -1.9956e-04,  3.1071e-03,  ...,  4.0913e-04,
         -1.5223e-04,  8.2302e-04],
        [-3.2854e-04,  9.5904e-05,  6.1798e-04,  ...,  2.1112e-04,
         -2.4724e-04, -5.7697e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2806, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.6270, -3.5488, -3.4863,  ..., -5.4883, -2.9238, -2.8301],
        [-5.9883, -4.6602, -5.2070,  ..., -4.8633, -4.5664, -0.4575],
        [-2.5254, -4.1836, -3.5586,  ..., -3.0586, -3.9805, -1.3857],
        ...,
        [-2.9512, -3.9668, -3.8887,  ..., -4.5742, -3.7480, -0.9038],
        [-4.2539, -4.0039, -2.4570,  ..., -4.6758, -4.6914, -0.6748],
        [-3.0723, -4.7578, -2.9473,  ..., -3.5410, -3.3359, -1.4463]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 27,  0, 27, 20,  7, 27, 13, 15,  2,  4,  9,  5, 27,  7, 17, 10, 20,
         3, 27, 27, 27, 27, 27, 27, 11, 27,  2, 27,  7, 27, 27, 18, 18, 15, 21,
        26, 10, 26, 27, 20, 27, 27, 27,  0, 20, 22,  7, 17,  3,  7, 25, 27, 20,
         0, 17,  7,  1, 27, 17,  4,  8,  0, 27], device='cuda:0')
step: 209
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6113,  ...,  0.4785, -0.0840,  1.5117],
        [ 1.2510, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3467, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1719,  2.2305, -0.0962,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0109e-03, -2.0504e-03, -1.8988e-03,  ...,  3.4380e-04,
          9.0218e-04, -1.2817e-03],
        [ 6.9916e-05, -5.8651e-04, -8.2016e-04,  ..., -7.2479e-03,
         -2.8343e-03,  3.0231e-03],
        [-2.7061e-05,  4.3511e-04,  1.8382e-04,  ..., -1.5440e-03,
         -1.8902e-03,  7.3624e-04],
        [ 9.0981e-04, -2.0313e-04, -1.3962e-03,  ..., -1.7595e-04,
         -8.0919e-04, -5.2118e-04],
        [ 5.6362e-04, -1.3351e-04, -1.4806e-04,  ..., -7.0274e-05,
          5.5790e-04,  8.1778e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2492, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.7578, -3.4453, -3.3203,  ..., -3.8672, -5.0703, -1.0234],
        [-0.8525, -3.9316, -4.8047,  ..., -5.8047, -4.2109, -1.9473],
        [-5.5938, -3.4863, -2.8145,  ..., -2.8770, -3.3457, -2.3301],
        ...,
        [-2.0938, -3.4219, -3.0938,  ..., -4.8438, -3.5312, -2.3125],
        [-3.2715, -4.7578, -4.5234,  ..., -5.0859, -2.9590, -3.4277],
        [-4.4023, -4.0742, -3.4492,  ..., -3.6367, -3.4961, -0.9336]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2,  0, 27, 17,  5,  7,  3, 18, 20, 27, 27,  0, 18, 18,  6, 27, 27,  2,
         1, 27, 27,  4,  1, 27, 10, 18,  6, 10, 27, 27, 18,  6,  4,  3,  1, 27,
        27,  3, 15, 27, 15, 25,  8, 18,  4,  0,  0, 27, 22, 25, 17,  9, 10, 15,
        27,  0,  7,  1, 27, 22, 27, 25,  4, 27], device='cuda:0')
step: 210
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6113,  ...,  0.4785, -0.0840,  1.5117],
        [ 1.2510, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3467, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1719,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.3745e-04, -1.2445e-04, -3.2365e-05,  ..., -1.8120e-04,
          1.5259e-04,  7.2098e-04],
        [ 4.6730e-04, -2.4986e-04, -3.6311e-04,  ...,  1.3304e-03,
          7.3671e-04,  4.0472e-05],
        [ 1.1787e-03, -1.4400e-03, -9.7275e-04,  ...,  5.7220e-04,
          9.1648e-04, -2.6703e-04],
        [-3.5930e-04,  2.3627e-04,  1.3123e-03,  ...,  6.7043e-04,
          4.7994e-04,  1.5097e-03],
        [ 1.5688e-04,  8.5688e-04, -4.8065e-04,  ...,  1.0042e-03,
          1.4055e-04, -2.5272e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4415, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.5234, -4.2578, -3.9453,  ..., -4.2266, -5.1953, -0.4756],
        [-2.8242, -4.0742, -3.7617,  ..., -6.1680, -4.1211, -1.6680],
        [-6.9414, -5.4258, -4.4570,  ..., -5.5820, -6.4102, -0.1290],
        ...,
        [-2.4570, -3.9727, -4.1289,  ..., -6.7695, -3.9258, -0.5361],
        [-5.4023, -4.0586, -3.1055,  ..., -4.6367, -5.3086, -1.1367],
        [-6.8789, -5.0039, -5.5977,  ..., -5.6289, -4.8633, -0.1757]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 17, 10,  3, 10, 27, 18, 27, 27, 25, 19,  1, 13,  7, 27, 20, 18, 10,
        27, 27, 20, 25,  4, 22,  9,  4,  3,  7, 27, 27, 27,  0,  3, 25,  5,  7,
        13, 27, 27, 20, 18,  0,  3,  2, 13,  5,  3,  0,  0, 18, 27, 27, 27, 27,
        12,  2,  6, 27,  0, 10, 17, 18, 27, 27], device='cuda:0')
step: 211
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0840,  1.5117],
        [ 1.2510, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3467, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1719,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.7520e-04, -2.3441e-03, -1.3885e-03,  ...,  3.0079e-03,
         -4.7684e-06,  2.4261e-03],
        [ 7.7820e-04,  3.2349e-03,  4.0936e-04,  ..., -1.8883e-03,
         -3.2306e-04, -3.9597e-03],
        [ 1.0691e-03, -3.1614e-04,  1.5282e-02,  ..., -1.1818e-02,
         -9.5444e-03,  5.5962e-03],
        [ 3.1948e-04, -4.7946e-04,  5.0011e-03,  ...,  1.0042e-03,
          1.8139e-03,  2.6360e-03],
        [ 1.0967e-03, -7.7009e-04,  2.3282e-04,  ...,  1.4162e-03,
          1.0138e-03, -3.3607e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1421, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2773, -4.4961, -2.6074,  ..., -1.8418, -4.6523, -0.9507],
        [-4.2461, -4.1055, -3.0273,  ..., -3.2305, -4.7930, -1.2627],
        [-2.5293, -3.4668, -2.2480,  ..., -1.6387, -4.5781, -2.9199],
        ...,
        [-1.2148, -3.5430, -4.7305,  ..., -6.6523, -3.2773, -2.3867],
        [-4.0234, -4.1953, -3.9766,  ..., -4.7422, -4.5859, -0.6182],
        [-4.1328, -3.6328, -3.5703,  ..., -5.5234, -5.1328, -1.3203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25, 11,  2, 25,  7, 27, 15, 26, 27, 27,  4,  1, 15,  2,  4, 27, 27, 27,
        27,  9, 27, 17, 27, 10,  6, 27,  4,  0, 10, 27,  6,  3, 27, 17, 20,  4,
        27,  3, 15, 27, 27, 17,  4,  4, 27, 18, 18, 27,  0, 27, 22, 27, 13, 27,
        10,  7,  2, 10, 27, 10, 27, 15, 27, 11], device='cuda:0')
step: 212
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0840,  1.5117],
        [ 1.2510, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3467, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.2697e-04, -9.2089e-05,  8.4066e-04,  ...,  4.6253e-04,
         -1.5631e-03,  1.1997e-03],
        [ 4.3511e-05,  1.4858e-03, -1.2350e-04,  ...,  7.0286e-04,
         -4.9877e-04, -6.4898e-04],
        [-4.0054e-04,  1.7776e-03,  6.2065e-03,  ..., -1.2951e-03,
          1.3304e-03,  4.3678e-03],
        [ 4.9829e-04, -4.0865e-04,  1.4935e-03,  ...,  7.8773e-04,
          2.1029e-04,  2.1725e-03],
        [-1.8656e-05, -1.3721e-04, -7.6818e-04,  ...,  1.9002e-04,
         -5.5361e-04, -3.3283e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3552, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0742, -3.0586, -3.6055,  ..., -3.8691, -3.6816, -1.1670],
        [-2.0840, -4.3164, -3.4277,  ..., -5.6133, -4.6445, -1.5996],
        [-1.3936, -2.9863, -4.0820,  ..., -6.4414, -5.1133, -2.9863],
        ...,
        [-1.3662, -4.0078, -5.4922,  ..., -4.7734, -3.1152, -3.8027],
        [-5.4414, -3.7695, -3.1914,  ..., -2.0352, -4.3477, -1.3643],
        [-7.6094, -4.7188, -4.2656,  ..., -7.2188, -6.2500, -0.2029]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 18,  0, 20, 17,  6, 18, 27, 22, 20, 26, 27, 27,  1,  9, 27,  5, 27,
        27, 27, 15,  7, 27,  2, 27, 27, 27, 10, 27, 15,  6,  2,  7,  6,  1,  8,
        11, 27,  0,  4, 26, 27, 12, 26, 17,  5,  9, 27, 27, 22,  2, 27,  9, 27,
        27, 27,  0, 27, 20,  2, 27,  1, 10, 27], device='cuda:0')
step: 213
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0840,  1.5117],
        [ 1.2510, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7964,  ..., -1.9043, -1.3467, -0.6006],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.9993e-04, -2.7704e-04,  1.2503e-03,  ..., -1.6870e-03,
         -4.2677e-04,  9.2316e-04],
        [-1.1234e-03, -3.2282e-04, -7.6065e-03,  ...,  4.4365e-03,
          4.4975e-03, -2.7103e-03],
        [-3.4976e-04,  2.1210e-03,  1.3351e-03,  ..., -1.2302e-03,
          9.0933e-04,  1.3094e-03],
        [-2.3210e-04, -4.6396e-04, -8.4066e-04,  ..., -4.2319e-04,
         -8.1444e-04, -9.8991e-04],
        [-7.1192e-04, -2.8324e-04,  1.1415e-03,  ..., -7.9393e-04,
         -1.6332e-05,  1.3733e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3589, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.9023, -4.2617, -3.5742,  ..., -5.9492, -3.5430, -1.3408],
        [-1.0039, -3.6289, -3.8164,  ..., -4.4258, -2.8633, -2.7695],
        [-4.7852, -3.0977, -3.1133,  ..., -3.2227, -4.0820, -1.3467],
        ...,
        [-5.2695, -3.8164, -3.5977,  ..., -3.1445, -3.5195, -1.1445],
        [-2.7285, -3.8379, -3.5566,  ..., -5.1680, -3.4629, -1.6973],
        [-5.4492, -3.4648, -3.2305,  ..., -4.7930, -4.1367, -0.3718]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 26, 23, 27, 27, 27, 10, 27, 27,  0, 27, 26,  6, 25,  6, 18,  1,  4,
        27, 27, 27,  1, 27, 22, 15, 27,  0,  0, 27,  9,  0, 27,  7, 21, 26, 27,
         3,  0, 14, 27, 22, 27,  0, 27, 15, 10,  2, 20, 26, 27, 20,  9, 26, 27,
        18,  8, 27,  3,  1, 10, 11, 27, 27, 27], device='cuda:0')
step: 214
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0839,  1.5117],
        [ 1.2510, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7959,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4097, -0.2979, -1.7178,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.9431e-04,  2.4033e-03,  2.0275e-03,  ..., -9.8348e-06,
         -9.4557e-04,  9.5749e-04],
        [-3.2234e-04,  1.7285e-06, -2.3937e-04,  ...,  2.8954e-03,
          2.2373e-03,  2.7037e-04],
        [-2.2769e-05,  1.4277e-03,  3.0880e-03,  ..., -2.0447e-03,
         -1.4935e-03,  2.6493e-03],
        [-2.4748e-04,  1.9717e-04,  4.9877e-04,  ...,  4.0770e-04,
         -1.2708e-04,  1.1282e-03],
        [-3.5405e-05,  2.5678e-04, -3.0398e-04,  ...,  3.2783e-04,
          3.7289e-04, -4.9829e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2395, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6641, -4.3672, -4.4766,  ..., -5.0078, -4.3047, -0.3501],
        [-3.1660, -3.9004, -4.1328,  ..., -5.4609, -3.9297, -0.7119],
        [-6.1094, -5.2188, -5.1562,  ..., -6.8125, -4.5312, -0.1392],
        ...,
        [-2.8594, -5.2188, -5.2969,  ..., -5.4375, -1.9844, -3.1719],
        [-2.6426, -3.8613, -4.8750,  ..., -3.5645, -3.2363, -3.2832],
        [-3.1211, -3.1367, -4.0586,  ..., -5.8398, -4.6992, -1.5732]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 9, 27,  4, 27,  8, 27,  7, 27,  4, 27, 27,  8,  0, 17,  9,  4, 27, 26,
        13, 18, 27, 25, 27,  1,  3, 27, 25,  1, 27,  7, 12,  0, 27, 27,  0, 27,
        15, 27,  0, 26,  9, 17, 27, 27, 26,  0,  0, 27, 18, 15, 27, 14,  0, 27,
         6, 26,  1, 26, 27,  3,  7,  7, 17,  1], device='cuda:0')
step: 215
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0839,  1.5117],
        [ 1.2510, -1.8906, -0.2050,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7959,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4097, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0005, -0.0024, -0.0009,  ...,  0.0035, -0.0006, -0.0002],
        [ 0.0005,  0.0014,  0.0002,  ...,  0.0002, -0.0010, -0.0021],
        [ 0.0024, -0.0013,  0.0128,  ..., -0.0071, -0.0053,  0.0039],
        [ 0.0005, -0.0010,  0.0017,  ..., -0.0008, -0.0006, -0.0014],
        [ 0.0007, -0.0017, -0.0012,  ...,  0.0012, -0.0003, -0.0023]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.1502, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.8496, -3.0840, -4.2383,  ..., -4.3789, -5.6758, -0.6309],
        [-3.0273, -3.1836, -3.7461,  ..., -5.4961, -3.6992, -1.1992],
        [-4.7461, -4.0742, -3.6367,  ..., -6.3555, -5.7617, -0.3718],
        ...,
        [-3.5625, -3.3750, -3.1875,  ..., -5.6250, -4.2031, -1.1250],
        [-3.9863, -3.5020, -2.4707,  ..., -2.8594, -3.1094, -1.4229],
        [-1.5762, -3.3887, -4.1367,  ..., -5.2656, -3.7168, -1.4980]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 15,  4,  4, 27,  1, 27, 27,  7, 27, 27, 27, 18, 27, 11, 20,  0, 27,
        11, 27, 17,  4, 27,  0,  0, 18, 27,  4, 11, 27,  1, 27,  4, 27, 18,  3,
         5, 15, 15,  1,  9, 15, 11,  3,  7, 27,  2, 14, 27,  0,  4, 11, 19, 27,
         4,  4, 22, 27,  3, 18, 27, 27, 27, 15], device='cuda:0')
step: 216
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0839,  1.5117],
        [ 1.2510, -1.8906, -0.2050,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7959,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4097, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.4986e-03,  9.2411e-04,  2.1877e-03,  ...,  7.1297e-03,
         -3.7708e-03,  6.8016e-03],
        [-3.5934e-03,  1.8906e-02,  3.4210e-02,  ..., -3.1700e-03,
         -2.4475e-02, -2.1133e-02],
        [ 1.6336e-03, -1.8477e-04,  1.3123e-02,  ..., -6.5765e-03,
         -3.4637e-03,  5.6343e-03],
        [-1.4906e-03, -5.6088e-05,  7.7209e-03,  ..., -4.0674e-04,
          1.4448e-03,  3.6659e-03],
        [ 5.4979e-04, -2.7466e-03, -4.0092e-03,  ...,  2.6493e-03,
         -1.3900e-04, -5.4703e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3425, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.9961, -3.4824, -4.3086,  ..., -6.3398, -5.6680, -0.8413],
        [-5.9766, -3.5840, -4.2891,  ..., -5.2578, -4.1484, -0.7725],
        [-3.2129, -2.7598, -2.5254,  ..., -2.5879, -4.9297, -1.7598],
        ...,
        [-1.8984, -3.6953, -3.6172,  ..., -4.0703, -3.7734, -1.2734],
        [-5.0977, -5.0195, -4.5820,  ..., -5.1133, -3.9902, -0.3020],
        [-4.3828, -3.5996, -4.2266,  ..., -4.8828, -5.1953, -0.8506]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  7,  2,  5, 27, 27, 27, 18,  0, 26,  1, 27,  0,  8, 22,  0,  4, 27,
        27, 27, 27,  4, 27, 25,  7, 18, 27, 18, 18, 27,  9,  9,  0,  0,  7, 10,
        22, 25,  6, 27,  1, 13, 27, 22, 27,  1, 27, 27, 15, 27, 27,  3,  3,  3,
         7, 10, 17, 13, 26, 10,  4,  1, 27,  4], device='cuda:0')
step: 217
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0839,  1.5117],
        [ 1.2510, -1.8906, -0.2050,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7959,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.8835e-04, -6.0797e-06, -6.5327e-05,  ...,  4.1652e-04,
          3.0136e-04,  1.4353e-03],
        [-5.9175e-04,  4.8089e-04,  2.0924e-03,  ..., -1.8978e-03,
         -4.0665e-03, -1.9093e-03],
        [ 5.6219e-04, -1.1950e-03, -5.6419e-03,  ..., -1.0473e-04,
          7.1716e-04, -2.4815e-03],
        [ 6.6090e-04,  3.4904e-04, -2.0428e-03,  ..., -6.9904e-04,
         -2.7132e-04, -1.3800e-03],
        [-1.1373e-04, -2.5702e-04,  3.4428e-04,  ..., -7.6437e-04,
         -2.0814e-04,  1.0233e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2523, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.3496, -3.6777, -3.7559,  ..., -5.0664, -3.6465, -1.1152],
        [-1.8203, -4.5234, -5.0547,  ..., -4.6328, -4.1016, -3.0078],
        [-1.8535, -3.4785, -4.3828,  ..., -5.7266, -3.3848, -2.0723],
        ...,
        [-3.1523, -3.1367, -2.6055,  ..., -1.6211, -5.8086, -2.4961],
        [-5.1641, -3.7559, -4.0859,  ..., -3.9434, -4.1484, -0.4287],
        [-3.2266, -4.1016, -3.8203,  ..., -2.5234, -3.7891, -1.7881]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15, 18, 15, 27, 17,  7, 25, 15, 22, 27, 22,  4,  4, 27, 27,  8, 27,  7,
        20, 27, 27, 27, 25,  1,  4, 27, 27, 25, 27,  6, 27, 22, 20, 27,  4, 11,
        23, 27,  0, 14,  0,  3, 27,  5, 27, 10,  1,  4,  5, 27,  5, 27, 27, 27,
        27, 22, 27, 27, 27, 27, 20,  6,  1, 22], device='cuda:0')
step: 218
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0838,  1.5117],
        [ 1.2510, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7959,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4097, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.3900e-04, -2.5690e-05,  8.9645e-04,  ..., -5.6982e-04,
         -8.2016e-04,  1.7910e-03],
        [ 4.4608e-04, -1.1454e-03, -2.8076e-03,  ..., -7.0572e-04,
          4.3559e-04,  7.7820e-04],
        [ 8.9598e-04, -1.4982e-03, -5.0316e-03,  ...,  2.2449e-03,
          1.6224e-04, -4.9629e-03],
        [ 2.2221e-04, -3.5572e-04, -1.3666e-03,  ...,  2.1398e-05,
         -1.9646e-04, -1.4124e-03],
        [-2.5487e-04,  1.7786e-03, -5.2929e-04,  ...,  8.8096e-05,
         -6.8188e-05,  4.6206e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1196, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.0020, -3.7051, -4.2383,  ..., -4.2812, -4.0195, -1.5020],
        [-2.4434, -3.3652, -4.9102,  ..., -5.4883, -3.0059, -1.6152],
        [-5.2812, -4.1562, -5.1719,  ..., -6.9531, -5.6250, -0.2185],
        ...,
        [-2.6250, -3.5469, -3.8438,  ..., -6.4062, -3.5156, -1.3760],
        [-3.0176, -4.0000, -4.3906,  ..., -6.0781, -4.6602, -2.3145],
        [-3.6953, -3.9141, -4.1328,  ..., -4.1172, -3.8359, -0.9771]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1,  5, 27,  0, 18, 27, 26, 11, 14, 15, 13,  7, 27, 15, 17, 27,  4,  7,
        27, 27, 27,  4, 27,  7, 27, 27,  0, 25, 10, 27, 15,  9,  7, 20, 25, 10,
         6,  7,  6, 25, 27,  4, 18, 27, 22,  0, 13, 26, 24,  0, 27,  2, 27, 27,
        27, 27,  8, 18, 27, 27, 18, 26, 20, 27], device='cuda:0')
step: 219
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0837,  1.5117],
        [ 1.2510, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7959,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4097, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0052,  0.0021,  0.0009,  ..., -0.0055, -0.0004, -0.0084],
        [-0.0126,  0.0054, -0.0290,  ...,  0.0002,  0.0108, -0.0093],
        [-0.0014,  0.0003, -0.0026,  ...,  0.0018,  0.0015, -0.0020],
        [-0.0045, -0.0021, -0.0059,  ...,  0.0002, -0.0021, -0.0064],
        [-0.0035,  0.0016,  0.0046,  ..., -0.0030,  0.0002,  0.0044]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.7047, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.7852, -3.9707, -4.4258,  ..., -4.8320, -5.8633, -0.5347],
        [-2.1641, -3.5391, -4.5391,  ..., -4.2578, -3.8047, -2.0859],
        [-3.0625, -4.7344, -5.8438,  ..., -5.7656, -4.6094, -0.3896],
        ...,
        [-3.8672, -3.6641, -3.9766,  ..., -4.5234, -4.6797, -0.5703],
        [-3.1387, -3.8105, -4.0586,  ..., -5.2930, -3.9512, -0.8569],
        [-4.0859, -3.4922, -4.0547,  ..., -6.1172, -4.6328, -0.7261]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4,  0, 18, 11, 26, 27,  0, 11, 25,  4, 17, 27,  9,  9,  2, 27, 27, 23,
        27, 18, 14, 27, 20,  3, 27, 27, 17, 27,  6,  2,  9, 18, 18,  4,  7, 22,
        27,  0,  7,  2,  5, 22, 27,  4, 20,  5,  3, 10, 18, 17, 20, 26, 20, 27,
        15, 18, 25, 15, 27,  1,  3, 22, 20,  4], device='cuda:0')
step: 220
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0837,  1.5117],
        [ 1.2510, -1.8906, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7959,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.6798e-03,  8.7357e-04,  2.0676e-03,  ...,  1.8225e-03,
         -4.5609e-04,  9.5749e-04],
        [ 3.5381e-03,  3.8075e-04,  1.0391e-02,  ...,  2.2316e-03,
         -4.8561e-03,  7.9060e-04],
        [ 1.5650e-03, -1.9779e-03,  1.2598e-03,  ...,  1.2326e-04,
         -7.6532e-04,  2.1000e-03],
        [-9.9778e-05,  6.4850e-04,  2.1648e-03,  ..., -1.2034e-04,
         -1.7715e-04,  1.2083e-03],
        [ 7.3528e-04,  1.3065e-04, -3.7360e-04,  ...,  1.7738e-04,
          2.4414e-04, -2.6762e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4249, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.1104, -3.7207, -4.8438,  ..., -4.7812, -3.4062, -3.1738],
        [-2.8340, -3.8027, -4.4922,  ..., -4.4453, -3.2402, -1.2256],
        [-2.2344, -3.5312, -4.4062,  ..., -5.7812, -3.8906, -1.7969],
        ...,
        [-4.0195, -3.2539, -3.2227,  ..., -2.1133, -3.5977, -1.2539],
        [-2.9531, -3.7500, -4.4844,  ..., -4.4688, -5.1250, -1.1250],
        [-3.9844, -4.1562, -2.6406,  ..., -2.3750, -3.8594, -0.9058]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  0, 27, 20,  7,  0, 27,  3,  3,  1,  3, 15, 25,  4,  2, 26,  7,
        12,  1, 27, 27,  0, 20,  4, 10, 22, 27,  7,  4, 27,  9,  5, 27,  3, 27,
         8, 18, 27, 14, 27, 10,  9, 27, 27, 27, 27,  4,  2, 27, 27, 14,  3, 27,
         8,  6, 15,  9, 27, 13, 14,  4, 20, 10], device='cuda:0')
step: 221
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0837,  1.5117],
        [ 1.2510, -1.8916, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7954,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.8801e-04,  7.8726e-04, -7.6294e-04,  ..., -1.0633e-03,
         -4.8590e-04, -1.4896e-03],
        [ 1.1402e-04,  1.0891e-03,  2.1019e-03,  ...,  3.1128e-03,
          2.3537e-03, -4.7231e-04],
        [-8.5592e-04,  6.3181e-04, -1.0574e-02,  ...,  6.2180e-03,
          4.9934e-03, -7.5417e-03],
        [-1.4267e-03,  1.4544e-03, -1.3173e-04,  ...,  6.5041e-04,
          5.3215e-04,  9.8038e-04],
        [ 1.8954e-04,  1.6308e-03, -5.2929e-04,  ...,  3.6240e-05,
          1.3757e-04,  6.3705e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2695, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.4141, -4.2266, -4.6641,  ..., -4.2891, -2.1484, -0.6797],
        [-6.2852, -4.2070, -3.4277,  ..., -6.5820, -4.4883, -0.4431],
        [-1.5752, -4.2148, -3.6543,  ..., -6.6367, -3.9668, -0.9819],
        ...,
        [-3.2891, -3.9297, -3.9297,  ..., -5.0078, -4.6328, -0.5859],
        [-5.2383, -3.9746, -2.2090,  ..., -0.6309, -5.4258, -4.3164],
        [-2.1172, -3.3203, -4.4297,  ..., -6.9453, -4.3359, -1.4141]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  7, 12,  1, 10,  1, 27,  0, 27, 27, 25, 27,  1, 22,  9, 27,  0,  1,
        27,  9, 27,  8,  7, 27, 20,  6, 27, 26, 15, 27, 27,  4, 11, 13,  4, 27,
         1,  0, 27, 27, 23,  7,  0, 27, 27, 20, 25, 20,  7, 27, 27,  0,  7, 27,
        15,  7, 12, 13,  4,  7, 27, 27, 25, 18], device='cuda:0')
step: 222
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0837,  1.5117],
        [ 1.2510, -1.8916, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7959,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.6880e-04,  1.1396e-03, -2.9325e-04,  ..., -3.4571e-06,
          4.4870e-04,  2.3711e-04],
        [-1.1253e-04,  2.4796e-04, -1.4124e-03,  ..., -3.0303e-04,
         -5.6171e-04,  2.3804e-03],
        [ 8.1873e-04, -1.6737e-04,  1.4534e-03,  ...,  1.5507e-03,
          5.5695e-04,  1.6642e-03],
        [ 7.3242e-04,  1.7488e-04,  1.7345e-05,  ...,  3.1829e-05,
         -1.7500e-03,  3.3474e-04],
        [ 1.1940e-03, -5.7101e-05, -6.8331e-04,  ..., -1.7047e-04,
          9.4366e-04, -6.6221e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8937, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.5859, -3.3496, -3.2246,  ..., -5.8516, -5.3516, -0.5537],
        [-2.2500, -3.7812, -4.6094,  ..., -5.0000, -4.3750, -0.5312],
        [-4.0391, -3.1797, -3.3516,  ..., -4.0234, -4.9297, -0.6484],
        ...,
        [-1.5615, -3.6719, -4.0781,  ..., -5.1562, -3.5938, -3.2031],
        [-2.3535, -4.0430, -4.3047,  ..., -4.8047, -3.4160, -1.4160],
        [-5.5000, -4.1875, -4.6875,  ..., -3.0000, -3.9375, -0.7188]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  4,  1, 27, 27, 15, 15, 20,  3,  7, 26, 22, 27, 27,  0,  9, 19,  3,
         6,  0,  4, 27, 14,  4, 27, 15,  0, 27, 27, 27, 24, 11,  0, 27, 27, 27,
        27, 27, 27, 25, 27,  3,  4, 27, 27, 20,  8, 15,  0, 15, 25, 22, 27,  0,
         0,  8, 10, 26, 26, 10,  0, 13, 17, 27], device='cuda:0')
step: 223
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7959,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.5354e-04, -1.7633e-03,  1.2178e-03,  ..., -6.4278e-04,
          1.3447e-04, -8.6880e-04],
        [ 1.1611e-04,  1.1168e-03, -5.1193e-03,  ..., -2.0542e-03,
          2.9526e-03, -6.4671e-05],
        [-1.4801e-03,  2.4357e-03,  3.0375e-04,  ..., -1.5602e-03,
          3.0041e-03,  1.8768e-03],
        [-2.2483e-04, -3.2604e-05, -7.8678e-04,  ..., -1.4353e-04,
          8.0013e-04, -1.6508e-03],
        [-7.4267e-05,  8.2207e-04,  6.0892e-04,  ..., -5.1212e-04,
          5.2023e-04, -1.0318e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9876, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.5391, -3.6191, -3.8379,  ..., -3.8691, -2.9004, -0.7749],
        [-5.1094, -4.2031, -4.8281,  ..., -6.2969, -5.1875, -0.4514],
        [-3.2129, -3.2754, -3.5410,  ..., -5.8828, -3.6816, -0.5874],
        ...,
        [-3.1016, -3.9922, -3.4609,  ..., -4.3672, -2.4453, -0.9292],
        [-1.7891, -3.4453, -4.4766,  ..., -5.8203, -4.2266, -4.1016],
        [-3.6934, -3.5527, -4.1484,  ..., -6.8203, -4.4922, -0.6938]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 9, 27, 27, 27, 22,  3, 27, 15,  0,  0, 27, 27, 22, 27, 27, 27, 27, 27,
        15,  4, 27,  4, 27, 20, 27,  8, 20, 25, 18, 27, 15, 18, 26, 15, 14,  6,
         0, 27, 27, 27, 17, 27,  3,  3, 27, 27, 27,  4, 18, 27, 22, 27,  6,  1,
         2,  7, 27, 25,  3, 27, 15,  4, 15, 27], device='cuda:0')
step: 224
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7959,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0026, -0.0006,  0.0013,  ...,  0.0033, -0.0012,  0.0023],
        [ 0.0006,  0.0048,  0.0022,  ..., -0.0025, -0.0023, -0.0046],
        [ 0.0003, -0.0002,  0.0101,  ..., -0.0071, -0.0037,  0.0050],
        [-0.0003, -0.0004,  0.0026,  ...,  0.0009,  0.0015,  0.0026],
        [-0.0011, -0.0014, -0.0007,  ...,  0.0018, -0.0006, -0.0016]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.4887, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.4473, -2.7910, -3.4160,  ..., -5.2461, -3.7910, -2.5254],
        [-3.4609, -2.6328, -2.8984,  ..., -3.9141, -4.5078, -1.4287],
        [-4.5156, -3.7344, -4.4688,  ..., -6.0000, -2.6875, -0.7959],
        ...,
        [-5.0234, -3.3672, -4.2578,  ..., -3.4609, -3.1172, -1.1650],
        [-6.2656, -3.3125, -4.1719,  ..., -4.4531, -5.0938, -0.9067],
        [-2.4316, -3.4023, -4.3242,  ..., -6.0898, -3.9629, -1.2607]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  4, 27,  4, 25,  4, 25, 27, 22, 17, 27,  1, 27, 11, 27,  3, 27,  3,
        27,  7, 10,  0, 15,  2, 26, 27, 27,  0,  7, 20,  5, 27,  9, 13, 27, 15,
         0, 18, 10, 10,  0, 18, 27, 26, 27, 27, 27,  5,  7,  0, 27,  3, 18, 10,
         3, 27, 25, 11, 10, 11, 15, 19,  4,  4], device='cuda:0')
step: 225
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7959,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.0150e-04, -9.7752e-04, -4.9448e-04,  ..., -7.0238e-04,
         -2.5392e-04, -6.6662e-04],
        [-1.7047e-04,  1.1253e-03,  2.7466e-04,  ..., -2.9335e-03,
         -2.0027e-03,  8.2493e-05],
        [-7.5245e-04, -8.8024e-04, -1.6222e-03,  ...,  2.2621e-03,
          4.6921e-03,  3.6640e-03],
        [-4.9400e-04,  3.7289e-04,  1.5411e-03,  ...,  4.4107e-04,
         -1.1086e-04,  8.7118e-04],
        [ 1.0738e-03,  6.7997e-04,  7.5245e-04,  ..., -6.2561e-04,
         -2.8348e-04,  5.1022e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3049, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.0625, -3.2480, -3.6074,  ..., -3.0137, -3.0449, -1.4053],
        [-1.9365, -3.6406, -4.8750,  ..., -5.7969, -4.3438, -2.7500],
        [-3.9043, -3.0605, -2.7480,  ..., -2.3730, -4.9336, -0.9033],
        ...,
        [-2.5234, -3.0547, -5.0234,  ..., -4.9766, -2.9297, -1.9766],
        [-2.6602, -4.0508, -3.9277,  ..., -5.5352, -4.3320, -1.0361],
        [-5.4531, -3.7637, -4.0781,  ..., -5.3438, -5.0625, -0.6392]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  5, 24, 27, 27, 27, 25, 27, 13, 10,  0, 27, 10,  7, 27,  0,  0, 27,
         3, 27, 27, 25,  0, 27, 17, 11,  9, 27,  6, 22, 10, 10, 27, 13, 10,  9,
        10, 27, 17, 10,  1, 10, 27,  1,  7, 20,  1, 27,  6, 17, 27,  1, 27,  0,
        27,  0, 17,  1, 27, 26, 27,  7, 27, 27], device='cuda:0')
step: 226
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7959,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.1437e-04,  6.5613e-04,  1.1826e-03,  ...,  1.8225e-03,
         -6.6948e-04, -2.0161e-03],
        [-3.1066e-04,  5.4455e-04, -1.6308e-03,  ...,  2.9869e-03,
          2.3651e-03, -4.9591e-05],
        [ 5.8174e-04,  1.6918e-03,  8.2321e-03,  ..., -4.8866e-03,
         -1.8730e-03,  5.9586e-03],
        [ 1.5414e-04, -1.4997e-04,  1.7176e-03,  ...,  2.4152e-04,
         -1.3065e-03,  5.4359e-04],
        [ 1.2004e-04, -3.8362e-04,  4.4990e-04,  ...,  7.4196e-04,
         -3.1257e-04,  8.2731e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1191, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.4727, -3.8320, -4.2695,  ..., -5.1602, -2.8633, -1.7529],
        [-2.3535, -3.5879, -4.3203,  ..., -3.7441, -2.4004, -3.2910],
        [-0.8193, -3.7109, -3.8672,  ..., -6.6953, -3.7422, -1.9600],
        ...,
        [-3.4316, -3.3867, -3.5742,  ..., -3.4023, -4.8398, -1.5576],
        [-2.1777, -2.9277, -4.2539,  ..., -5.4258, -4.3633, -2.0371],
        [-4.7539, -3.2227, -3.1758,  ..., -3.1445, -4.5352, -0.8169]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15,  1,  0,  7, 14, 25, 27, 27, 27, 17, 27, 18, 27,  4, 27, 22, 27, 17,
        27, 27, 27,  0,  5, 10,  4,  5, 13, 27,  6, 20, 27, 20, 18,  1, 11,  0,
         3, 27, 18,  7,  5, 27, 27, 10, 27,  2,  6,  0, 18,  3,  4, 27, 27, 18,
        27,  6,  2, 20, 27, 27, 27,  2,  7,  3], device='cuda:0')
step: 227
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7959,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.0926e-04, -1.1902e-03, -3.4308e-04,  ...,  1.6868e-05,
          9.6607e-04, -5.9175e-04],
        [ 1.6832e-03, -8.4877e-04, -3.5691e-04,  ..., -1.0271e-03,
         -1.8334e-04,  3.9315e-04],
        [-5.5552e-04, -3.2864e-03,  6.8245e-03,  ...,  1.8158e-02,
          1.5926e-04, -1.0803e-02],
        [ 1.6994e-03, -1.0767e-03,  5.1689e-04,  ...,  1.0252e-03,
          1.4925e-04,  4.3488e-04],
        [ 7.3373e-05, -1.9097e-04,  2.6894e-03,  ..., -3.2253e-03,
          1.1549e-03,  4.8370e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1441, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0273, -2.9629, -3.0254,  ..., -3.9629, -4.1680, -1.2598],
        [-3.2949, -3.8574, -4.7148,  ..., -6.2461, -4.2930, -0.6226],
        [-2.4688, -2.9688, -5.0156,  ..., -5.4375, -4.2188, -1.6396],
        ...,
        [-2.0039, -3.0664, -4.1133,  ..., -3.6289, -3.0352, -1.5830],
        [-3.9570, -3.0195, -4.1758,  ..., -5.1914, -3.4102, -1.2383],
        [-2.6328, -3.2109, -4.1016,  ..., -5.1484, -2.8984, -1.8047]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0, 15, 24, 27,  0, 18, 12, 13, 25, 27, 22,  0, 27,  5, 27, 27,  0,
        27, 27, 18,  1, 27, 27, 13, 27,  0,  8, 15,  9,  0, 27, 15,  7, 27,  3,
        20,  3, 27, 20, 15, 17,  1,  1, 27, 27, 27,  4, 13, 22, 15, 27,  0, 15,
        27, 20,  0,  4, 27, 27, 26,  0, 26, 26], device='cuda:0')
step: 228
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0835,  1.5117],
        [ 1.2510, -1.8916, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7954,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0963,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0007,  0.0002, -0.0025,  ...,  0.0004, -0.0002, -0.0002],
        [ 0.0007, -0.0011, -0.0028,  ...,  0.0004,  0.0017,  0.0009],
        [ 0.0009,  0.0046,  0.0158,  ..., -0.0099, -0.0058,  0.0116],
        [ 0.0007, -0.0003,  0.0004,  ..., -0.0013,  0.0007,  0.0009],
        [ 0.0015, -0.0026, -0.0003,  ...,  0.0005,  0.0007, -0.0025]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.3268, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.0391, -3.5391, -3.5078,  ..., -5.5547, -3.8516, -1.0381],
        [-2.2480, -3.1543, -4.8398,  ..., -7.8555, -4.6992, -1.2012],
        [-4.3438, -3.2324, -3.3730,  ..., -2.8730, -4.0625, -2.0918],
        ...,
        [-1.7139, -2.9785, -3.9336,  ..., -6.3555, -3.0410, -1.4482],
        [-4.4219, -4.1719, -4.9062,  ..., -6.2188, -4.6406, -0.3289],
        [-3.0215, -3.1914, -4.1914,  ..., -5.5664, -3.0215, -1.3330]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([23,  4, 10, 25,  7, 27, 18,  0,  9,  0,  3,  2, 27,  3, 20,  6, 27, 20,
         0, 15, 25,  0,  0,  3, 27,  0, 13, 26,  0, 26, 15, 27, 18, 15, 22,  7,
         0, 20,  9,  5, 18, 12, 13, 15, 27, 15, 11, 27,  0, 27, 13, 10, 27,  7,
         4,  3, 27,  6,  8,  0, 27, 18, 27, 20], device='cuda:0')
step: 229
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0835,  1.5117],
        [ 1.2510, -1.8916, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7954,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0964,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0003,  0.0028, -0.0011,  ..., -0.0015,  0.0016, -0.0012],
        [ 0.0008, -0.0028,  0.0020,  ..., -0.0019, -0.0024,  0.0028],
        [ 0.0015, -0.0015, -0.0051,  ..., -0.0010, -0.0009, -0.0015],
        [-0.0006,  0.0006, -0.0011,  ..., -0.0007, -0.0014, -0.0013],
        [ 0.0005, -0.0005, -0.0002,  ...,  0.0007,  0.0007, -0.0003]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.1556, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.9346, -4.3086, -5.8242,  ..., -6.3555, -4.6055, -2.7637],
        [-3.0625, -2.8594, -2.9375,  ..., -4.5469, -2.1094, -2.6074],
        [-3.1602, -3.0820, -2.5371,  ..., -3.5820, -4.0039, -2.0996],
        ...,
        [-2.7891, -3.1328, -4.4609,  ..., -4.6328, -5.2578, -0.9458],
        [-4.3320, -3.8477, -4.8477,  ..., -5.1602, -4.8320, -0.9106],
        [-4.8438, -3.5176, -2.5645,  ..., -4.1250, -4.1875, -1.1426]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 22,  2,  7, 18, 27,  0, 27, 25,  9, 17, 27,  2, 27, 27, 15, 25, 27,
        27, 11, 27,  3,  7, 13,  5,  6, 20, 27, 22, 17, 27, 27,  1,  1, 18,  2,
        27, 26,  1, 27, 27,  1, 10, 14,  3, 27, 26, 27,  7, 27, 20, 27, 27, 15,
        20,  4, 17, 27, 11, 18, 15, 27, 27, 25], device='cuda:0')
step: 230
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0835,  1.5117],
        [ 1.2510, -1.8916, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7954,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0964,  ...,  0.6562,  0.7080, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0087, -0.0088, -0.0119,  ...,  0.0157,  0.0043, -0.0145],
        [ 0.0013,  0.0056,  0.0032,  ..., -0.0007,  0.0011, -0.0098],
        [ 0.0194, -0.0079,  0.1128,  ..., -0.0404, -0.0243,  0.0594],
        [ 0.0083, -0.0026,  0.0088,  ..., -0.0092, -0.0027,  0.0017],
        [ 0.0026, -0.0057,  0.0014,  ..., -0.0043,  0.0032, -0.0078]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.6658, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.9229, -3.1582, -4.6875,  ..., -6.5156, -4.7344, -1.4697],
        [-2.0645, -3.8926, -4.4375,  ..., -5.2188, -2.6738, -2.4707],
        [-1.5928, -3.0625, -4.3281,  ..., -7.5312, -3.8262, -2.1406],
        ...,
        [-4.3555, -2.6230, -2.9043,  ..., -3.0605, -4.5742, -1.9980],
        [-2.7324, -3.5762, -3.7480,  ..., -2.8887, -3.5293, -1.4512],
        [-0.6836, -3.6211, -5.0117,  ..., -7.4336, -4.8867, -3.6055]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 26, 17, 11, 15, 27, 27, 27,  1,  7, 18,  2,  4, 22, 27, 27,  1, 20,
         7, 11,  1, 10,  3, 15,  3, 20, 26, 27, 27,  3, 10, 13,  7, 11, 27, 17,
         9,  6,  4, 26,  4, 27,  0, 18, 19, 17, 18, 27,  0, 24, 24, 27, 27,  1,
        27, 27,  5, 27, 19, 27,  5,  9, 10,  0], device='cuda:0')
step: 231
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0835,  1.5117],
        [ 1.2510, -1.8916, -0.2051,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7954,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0964,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.1858e-03,  4.3273e-05, -2.4629e-04,  ..., -2.3365e-03,
          9.2602e-04, -1.8873e-03],
        [-1.1921e-03, -3.3593e-04, -1.0666e-02,  ..., -3.3894e-03,
          2.7370e-03, -4.8375e-04],
        [ 3.5048e-04, -1.1396e-03,  1.6117e-03,  ...,  2.3117e-03,
          3.3927e-04, -3.1872e-03],
        [-1.6677e-04,  1.7977e-04, -5.0468e-03,  ..., -1.9407e-03,
         -3.3760e-04, -2.7885e-03],
        [-4.2558e-05,  2.5487e-04,  1.2398e-03,  ..., -1.5173e-03,
          5.5885e-04,  8.7547e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4080, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4180, -3.2324, -3.4199,  ..., -3.6699, -3.4824, -1.2793],
        [-3.6367, -3.8398, -3.6680,  ..., -2.9648, -4.5898, -0.6836],
        [-2.8145, -3.7207, -3.9238,  ..., -5.1562, -3.6113, -1.1426],
        ...,
        [-2.6602, -3.8164, -4.6914,  ..., -4.8008, -2.9883, -1.0664],
        [-1.4229, -3.5000, -4.3594,  ..., -5.0469, -2.8750, -1.7969],
        [-2.8008, -3.9414, -4.0508,  ..., -2.8477, -2.9727, -1.9893]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2,  1,  4,  4, 25,  3, 27, 27, 27, 27, 27, 15, 18,  0, 11, 27, 27, 18,
        20,  0,  3,  9, 26, 20,  1, 27,  7,  6,  3, 20, 27, 27,  7,  2,  7, 27,
         2, 27, 17, 15, 15,  1,  9, 12, 22, 27, 25, 12, 27, 25,  4,  3,  0, 11,
         2, 25, 22,  4, 10, 27,  1, 27,  0, 11], device='cuda:0')
step: 232
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0835,  1.5117],
        [ 1.2510, -1.8916, -0.2050,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7954,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0964,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.3708e-03, -4.8041e-04,  4.5657e-04,  ..., -6.3133e-04,
          5.1737e-05, -5.3711e-03],
        [-2.1517e-04, -2.3899e-03, -1.9424e-02,  ...,  5.8479e-03,
          1.3084e-02, -6.8626e-03],
        [-1.7605e-03,  1.2312e-03,  3.4943e-02,  ..., -1.2550e-02,
         -9.0637e-03,  1.8127e-02],
        [ 2.4605e-04,  1.9150e-03,  5.5580e-03,  ..., -1.2312e-03,
         -2.2030e-03, -1.2760e-03],
        [ 1.1265e-05, -1.3647e-03, -2.7132e-04,  ...,  1.7524e-04,
          3.3283e-03, -4.9171e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3542, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3711, -4.0742, -4.6523,  ..., -6.0898, -4.1055, -0.4338],
        [-3.9512, -3.9355, -4.0938,  ..., -5.3125, -4.8750, -0.4521],
        [-3.7207, -3.7520, -4.2852,  ..., -4.4102, -4.2695, -0.5957],
        ...,
        [-4.7422, -3.5254, -3.9160,  ..., -6.1641, -3.8223, -0.5410],
        [-3.5273, -2.7617, -3.8555,  ..., -4.2305, -3.5430, -1.0898],
        [-2.0332, -3.5332, -3.7051,  ..., -4.1758, -3.5957, -0.9082]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 20,  2,  1, 15,  2,  1, 27, 27,  6, 27, 13,  0, 27,  0, 27, 27, 27,
        27, 27, 17, 27, 27,  0,  9,  9,  2,  4, 10, 27,  3,  4, 15, 10,  2, 20,
         3, 15, 27, 25, 22, 26,  1,  4, 27, 27, 27, 23,  4, 15, 10,  7, 27, 15,
        15,  7, 27, 27,  3,  1, 25,  3,  1, 26], device='cuda:0')
step: 233
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0835,  1.5117],
        [ 1.2510, -1.8916, -0.2050,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7949,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0965,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0036,  0.0024, -0.0028,  ..., -0.0153, -0.0006, -0.0032],
        [ 0.0001, -0.0065, -0.0047,  ...,  0.0071,  0.0043,  0.0034],
        [ 0.0047, -0.0139, -0.0962,  ..., -0.0308,  0.0252, -0.0152],
        [-0.0094, -0.0003, -0.0139,  ...,  0.0002,  0.0046, -0.0088],
        [-0.0024,  0.0068,  0.0049,  ...,  0.0015, -0.0039,  0.0014]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.2998, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.1064, -3.4023, -4.0586,  ..., -3.7148, -2.7637, -2.3711],
        [-2.6133, -3.8008, -3.8320,  ..., -5.1602, -2.4258, -1.2998],
        [-3.8848, -3.8848, -4.5586,  ..., -6.8086, -3.9941, -0.4324],
        ...,
        [-1.9707, -3.5801, -4.6719,  ..., -4.8750, -4.5625, -2.1270],
        [-3.7031, -3.5000, -4.5312,  ..., -4.0938, -3.3594, -1.0000],
        [-4.0859, -3.6934, -2.5840,  ..., -1.7402, -2.9746, -1.7256]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([14, 17,  4,  3,  1, 27, 27,  4, 27,  9, 13, 27, 27,  0,  1, 27, 27, 18,
        27,  8,  7, 18, 10, 10,  2,  9, 27, 27,  4, 27,  0, 15, 16,  4, 24,  4,
        18,  8, 27, 15,  9, 27, 27,  3, 10, 10, 26,  0,  0, 17,  7,  0, 15, 22,
        27, 18, 27, 18, 13, 21, 20, 18,  0,  9], device='cuda:0')
step: 234
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0835,  1.5117],
        [ 1.2510, -1.8916, -0.2050,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7949,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0966,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0002,  0.0024,  0.0011,  ..., -0.0041,  0.0003,  0.0022],
        [ 0.0006,  0.0003,  0.0034,  ...,  0.0030, -0.0006,  0.0018],
        [-0.0010, -0.0008, -0.0273,  ...,  0.0140,  0.0082, -0.0122],
        [-0.0024,  0.0008, -0.0029,  ..., -0.0008,  0.0005, -0.0019],
        [ 0.0006,  0.0009,  0.0038,  ..., -0.0004,  0.0010,  0.0043]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.3670, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.6680, -3.0273, -3.6992,  ..., -3.6680, -4.2148, -1.5273],
        [-3.9902, -3.9590, -4.6289,  ..., -5.7695, -4.6289, -0.3962],
        [-2.4609, -4.3047, -4.9141,  ..., -5.7891, -4.3203, -3.3828],
        ...,
        [-5.6641, -3.1328, -4.1328,  ..., -4.6172, -4.2109, -0.8511],
        [-1.5801, -2.9082, -4.7969,  ..., -6.3789, -4.6602, -2.2363],
        [-2.9043, -2.7168, -4.5938,  ..., -6.4688, -4.4688, -1.4053]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([11, 27, 20, 27, 27, 22, 27, 27,  3,  5,  0, 27,  1, 27,  6,  1,  5, 18,
        27, 27, 15, 17,  6, 10, 23,  1, 27, 14, 27,  6,  1, 27, 27, 18, 11,  4,
        27, 22, 25,  4, 27,  5,  0, 15,  6, 19,  8,  3, 25,  4,  2,  7,  1,  0,
        27, 27, 27, 10,  7, 27, 27,  1, 15,  0], device='cuda:0')
step: 235
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0835,  1.5117],
        [ 1.2510, -1.8916, -0.2050,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2200,  0.7949,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0966,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0018, -0.0015, -0.0011,  ..., -0.0050,  0.0053,  0.0012],
        [ 0.0010,  0.0032,  0.0007,  ...,  0.0009, -0.0009, -0.0056],
        [ 0.0006,  0.0003, -0.0197,  ...,  0.0135,  0.0077, -0.0194],
        [-0.0005, -0.0006, -0.0020,  ...,  0.0007, -0.0005, -0.0061],
        [ 0.0007,  0.0020,  0.0026,  ..., -0.0023,  0.0043,  0.0022]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.1482, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.5283, -3.4824, -3.6211,  ..., -4.5273, -3.3242, -1.6377],
        [-3.0723, -3.1504, -4.0391,  ..., -5.0859, -3.5703, -1.0557],
        [-3.2988, -3.6582, -2.7344,  ..., -2.6582, -4.2812, -1.2666],
        ...,
        [-1.6943, -3.2559, -4.4922,  ..., -4.7891, -4.5391, -2.1777],
        [-1.6865, -2.7012, -3.1387,  ..., -3.3105, -3.9355, -2.4980],
        [-1.3936, -3.1133, -4.1133,  ..., -5.7070, -4.0977, -2.2832]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  3,  3, 22,  3, 27, 17, 15, 27, 27, 27, 27, 16, 15, 27, 27, 18,
        27, 27,  1, 17,  4,  0, 15, 18,  0,  1, 15, 20,  1,  7, 27,  0, 27,  8,
        27,  0, 17,  6, 20, 27, 25, 10,  1,  0, 27, 27,  0,  6, 11,  2, 27, 27,
         2, 27, 27,  3, 27,  4, 11, 27, 25,  4], device='cuda:0')
step: 236
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0835,  1.5117],
        [ 1.2510, -1.8916, -0.2048,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7949,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0966,  ...,  0.6562,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0128e-03, -7.1907e-03,  6.0425e-03,  ...,  1.0345e-02,
          7.2241e-04, -4.0474e-03],
        [ 1.7958e-03,  2.1076e-03, -1.3571e-03,  ...,  5.1308e-03,
          7.0143e-04, -1.6804e-03],
        [-9.4748e-04,  4.2191e-03,  1.7044e-02,  ...,  1.7517e-02,
          1.2064e-03,  4.5991e-04],
        [ 2.0909e-04, -3.1910e-03, -3.7766e-03,  ...,  2.4071e-03,
          2.2144e-03,  2.8324e-03],
        [ 1.2665e-03,  3.3915e-05,  6.1703e-04,  ..., -3.4637e-03,
          3.7327e-03, -5.0621e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2561, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.1660, -2.5254, -3.9785,  ..., -4.7578, -2.7754, -2.4473],
        [-3.0918, -3.3418, -4.3398,  ..., -2.2461, -2.4512, -1.9971],
        [-3.5469, -3.2656, -2.9688,  ..., -3.1426, -4.2656, -1.5635],
        ...,
        [-1.6084, -3.7012, -4.5938,  ..., -4.1719, -2.9062, -3.2656],
        [-2.8359, -3.3203, -3.8359,  ..., -4.5703, -3.4297, -0.9292],
        [-0.9160, -3.0566, -3.7129,  ..., -4.8516, -3.5723, -3.6816]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  3, 27, 13,  4, 15, 17, 25, 10,  4, 10, 27, 27,  4, 27, 27, 27, 27,
         7,  6,  0,  3,  4, 27, 20, 26, 15, 27, 25,  7, 24, 17,  3,  4, 15, 27,
        27, 27, 17, 11, 27,  7,  0, 27, 26, 22, 14, 10, 27,  0, 26, 27, 10, 19,
         1, 27,  7,  0,  9, 27,  3, 17,  3, 15], device='cuda:0')
step: 237
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2048,  ..., -2.3828,  0.6040, -0.8252],
        [ 1.9121, -0.2198,  0.7949,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0967,  ...,  0.6567,  0.7075, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.3460e-04,  6.5536e-03, -6.7770e-05,  ..., -4.4518e-03,
         -1.8654e-03, -1.0834e-02],
        [-1.2608e-03,  4.9477e-03, -5.2719e-03,  ...,  1.2817e-03,
          3.1147e-03, -8.0414e-03],
        [-2.5082e-04,  5.3253e-03,  1.6068e-02,  ..., -1.4771e-02,
          7.2136e-03,  1.8158e-02],
        [-9.5367e-04, -1.6184e-03,  3.6926e-03,  ...,  2.9240e-03,
         -2.8095e-03, -1.2712e-03],
        [-2.8753e-04, -2.7027e-03, -4.8590e-04,  ...,  1.5011e-03,
          4.9305e-04, -1.2751e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1249, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4062, -3.6875, -4.2031,  ..., -5.8281, -3.6719, -0.9219],
        [-1.5000, -3.3906, -5.0938,  ..., -5.7812, -3.4844, -2.9531],
        [-3.6777, -3.4121, -4.4414,  ..., -5.2422, -4.6133, -1.0527],
        ...,
        [-1.9941, -3.4160, -4.1523,  ..., -5.6680, -2.9629, -2.2441],
        [-2.7617, -2.5273, -3.4805,  ..., -5.7930, -4.2461, -1.0117],
        [-1.2393, -3.3496, -4.8320,  ..., -6.3320, -3.5840, -2.9746]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([19,  8, 27, 27,  2, 27, 27,  3, 10,  1, 20, 27,  0, 27, 27, 18, 27, 10,
         6, 27, 27, 27, 27, 27, 22,  6, 27, 27, 27, 20,  1,  1,  7,  9,  0, 27,
         3, 25, 15, 27, 27,  4, 27, 18, 27,  4,  9,  3, 27, 11, 27, 27,  0, 27,
        27, 11, 20, 11, 27, 27,  6,  8,  0, 13], device='cuda:0')
step: 238
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2048,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2200,  0.7949,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0967,  ...,  0.6567,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.0766e-03, -1.9989e-03,  3.4866e-03,  ..., -4.9934e-03,
         -7.0620e-04,  1.8024e-03],
        [-2.7580e-03,  6.1607e-04, -1.3206e-02,  ...,  5.7831e-03,
          9.8724e-03,  2.7790e-03],
        [ 2.7251e-04,  1.5764e-03, -1.1124e-02,  ...,  5.3444e-03,
          2.5005e-03, -7.3204e-03],
        [-4.6310e-03, -8.5831e-05, -4.5509e-03,  ..., -1.3602e-04,
         -1.0338e-03, -1.5812e-03],
        [ 5.0902e-05,  3.3360e-03,  5.0011e-03,  ..., -4.6234e-03,
          1.0166e-03,  5.1575e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2811, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.7949, -4.2148, -4.5781,  ..., -4.3750, -5.3242, -3.2949],
        [-3.8418, -3.8242, -3.3105,  ..., -3.4961, -4.0898, -0.9971],
        [-3.1387, -3.4844, -3.7637,  ..., -3.3281, -4.3750, -1.6865],
        ...,
        [-4.3711, -2.3574, -2.6992,  ..., -4.5430, -4.0117, -1.8096],
        [-5.3477, -3.9414, -3.0195,  ..., -2.7539, -3.8320, -3.2227],
        [-2.8477, -3.3164, -4.1133,  ..., -5.0977, -4.2695, -1.1133]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 8,  3, 25,  0, 18,  1, 22,  4, 25, 14, 15,  5, 24,  6, 26,  0,  1,  7,
        13,  7,  1, 25,  0, 13,  3, 27, 10, 22, 27, 27,  0, 18, 22, 27,  1,  7,
        11, 20,  6, 27, 26, 27,  4, 27, 27, 27, 27,  8, 27, 15, 27, 25, 25,  8,
        27, 25, 27, 27, 27, 27, 15, 27, 14, 27], device='cuda:0')
step: 239
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2047,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2200,  0.7949,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6587,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0968,  ...,  0.6567,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.6241e-03,  2.6665e-03,  1.5545e-04,  ..., -1.8492e-03,
          2.6369e-04, -7.1430e-04],
        [-1.4019e-03,  3.5172e-03, -8.8348e-03,  ..., -5.4588e-03,
         -7.9117e-03,  5.2376e-03],
        [ 6.0844e-04, -7.6115e-05, -1.6647e-02,  ..., -3.4847e-03,
          5.1460e-03, -1.2026e-03],
        [-2.9716e-03, -9.9277e-04, -5.9090e-03,  ..., -2.8491e-04,
         -5.8031e-04, -2.0161e-03],
        [-1.4219e-03,  2.3136e-03,  1.0138e-03,  ...,  6.3062e-05,
         -2.1935e-03,  2.7828e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2147, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.7627, -3.1074, -4.2305,  ..., -4.8398, -3.9043, -1.7012],
        [-3.1680, -3.1680, -4.7148,  ..., -5.3086, -5.0273, -0.9814],
        [-0.9204, -3.7012, -4.4375,  ..., -5.6406, -2.7012, -2.0918],
        ...,
        [-2.4590, -3.6934, -3.9590,  ..., -5.1445, -3.7715, -1.0674],
        [-2.8145, -2.5332, -3.5332,  ..., -3.8145, -3.3145, -1.2832],
        [-4.6016, -3.7871, -3.6152,  ..., -4.3672, -5.1328, -0.6001]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 27, 20, 22, 17,  7,  9,  9, 26, 13,  7, 27, 27, 11, 18,  4, 27, 11,
        27, 27,  9, 27, 13,  0,  7, 20, 27,  5,  0, 27,  0, 13, 27, 20,  1, 27,
        13,  3,  7, 27, 24, 27, 24, 27, 27,  2,  0,  0, 24, 11,  4, 27, 27,  4,
        27, 27,  4, 27, 15, 27,  0, 27,  7, 27], device='cuda:0')
step: 240
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6113,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2047,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2200,  0.7949,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4094, -0.2979, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0969,  ...,  0.6567,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.7704e-04, -1.4715e-03, -7.9155e-04,  ..., -7.6056e-04,
          3.5238e-04, -2.8858e-03],
        [-2.5129e-04,  4.2582e-04, -1.3475e-03,  ...,  9.0790e-04,
          1.4448e-03, -2.3270e-03],
        [-3.1769e-05,  2.9697e-03,  1.0292e-02,  ..., -4.1809e-03,
          1.1997e-03,  1.0742e-02],
        [ 5.0783e-04, -3.3164e-04,  3.9139e-03,  ...,  2.3022e-03,
          9.4318e-04,  1.2951e-03],
        [-1.4544e-05, -1.7500e-04, -1.0109e-03,  ...,  1.9705e-04,
          1.6451e-04, -2.7084e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1022, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.6816, -3.2285, -2.0879,  ..., -1.5723, -3.5723, -2.3691],
        [-6.1797, -3.9297, -4.0703,  ..., -6.5547, -4.8203, -0.3042],
        [-3.0723, -3.3066, -3.6816,  ..., -2.7441, -2.7285, -1.4932],
        ...,
        [-3.6426, -2.9238, -2.9082,  ..., -3.4238, -4.7070, -1.2988],
        [-1.9121, -3.6777, -3.8027,  ..., -5.8359, -3.3965, -2.0215],
        [-2.7891, -2.9141, -4.1641,  ..., -4.4922, -2.9297, -1.4922]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 9, 22, 26, 27,  0,  1, 27,  3, 27, 20, 27, 15,  3,  5, 27, 25, 11,  3,
        10, 27, 15,  7, 17, 18, 27, 27, 11,  3, 18, 27, 22, 17, 25, 15, 15, 18,
         2, 27, 27,  3, 20, 27,  0, 27, 14, 25,  1, 27, 27, 14, 19, 27, 27, 27,
        27, 20,  0,  0, 20,  1, 23,  3, 18,  4], device='cuda:0')
step: 241
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6118,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2047,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2201,  0.7949,  ..., -1.9043, -1.3467, -0.6011],
        [-0.4092, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0969,  ...,  0.6567,  0.7070, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.2902e-03, -5.9280e-03,  1.9569e-03,  ...,  1.4969e-02,
         -2.8431e-05,  5.1117e-03],
        [-2.0885e-03, -1.1539e-03,  1.0117e-02,  ..., -5.6076e-03,
          6.2981e-03, -1.8635e-03],
        [-2.1667e-03,  7.4272e-03,  1.5793e-02,  ..., -2.8900e-02,
         -9.1648e-04,  2.0798e-02],
        [-6.6853e-04,  1.6937e-03,  5.5656e-03,  ..., -2.3022e-03,
          1.1129e-03,  4.8218e-03],
        [ 1.4925e-04, -3.9215e-03,  1.8654e-03,  ...,  2.4891e-03,
          1.5898e-03, -5.3482e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2163, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.5781, -2.9199, -3.9199,  ..., -2.9512, -4.2344, -0.7017],
        [-2.0586, -2.8398, -4.0586,  ..., -4.6211, -3.5742, -1.1055],
        [-2.7324, -2.2637, -3.1387,  ..., -3.1074, -2.6699, -2.9199],
        ...,
        [-2.3867, -2.8867, -3.2305,  ..., -6.1211, -3.8711, -1.1523],
        [-2.6523, -3.7617, -3.5742,  ..., -4.1523, -3.9648, -0.6519],
        [-4.3828, -3.1309, -3.6465,  ..., -4.1797, -4.5859, -0.7568]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 9, 15,  1, 18, 18, 27,  2,  6, 12,  2, 10, 27,  0, 20,  7, 27, 27,  7,
         0,  0, 27, 20,  4,  1,  6, 10,  1,  4, 27, 22, 13, 20,  4,  3, 20, 23,
        27,  0, 27, 27, 10, 27,  0, 27, 26,  7, 18,  8, 10,  8, 27, 20, 27,  1,
         8, 10, 27,  2, 20, 26, 11, 27, 27, 27], device='cuda:0')
step: 242
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6118,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2201,  0.7949,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4092, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0970,  ...,  0.6567,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0016,  0.0014,  0.0004,  ..., -0.0003, -0.0001,  0.0020],
        [-0.0005, -0.0013,  0.0072,  ..., -0.0002, -0.0030, -0.0007],
        [-0.0012, -0.0014, -0.0022,  ...,  0.0056, -0.0002, -0.0029],
        [-0.0013, -0.0005,  0.0030,  ...,  0.0021,  0.0002,  0.0029],
        [ 0.0002,  0.0001, -0.0010,  ...,  0.0002,  0.0007, -0.0017]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.3305, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.7266, -3.2266, -4.2422,  ..., -3.8672, -5.5078, -0.9302],
        [-3.4902, -3.6465, -2.9746,  ..., -4.5195, -4.6602, -0.8022],
        [-1.8789, -3.1445, -2.7539,  ..., -4.5039, -3.1914, -2.2539],
        ...,
        [-3.3867, -3.9180, -3.7930,  ..., -0.6680, -5.1836, -1.9180],
        [-4.1289, -2.9102, -3.6445,  ..., -3.5039, -3.6133, -1.2236],
        [-3.2090, -3.6797, -3.5059,  ..., -4.4609, -2.5527, -1.0381]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 27, 11, 15,  0,  6,  4, 15, 27,  6, 15,  3,  2, 11, 17,  0,  7, 27,
        27,  8, 17, 11, 18,  3, 27, 18, 13, 27,  3, 27, 15,  6, 27,  4,  6,  9,
        26, 27,  4,  1, 27, 27,  1, 17, 27, 22, 25,  1, 27, 27, 27, 27,  4, 27,
        20, 27, 14, 22, 17, 18, 22, 25, 27,  3], device='cuda:0')
step: 243
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6118,  ...,  0.4783, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2201,  0.7949,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4092, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0970,  ...,  0.6567,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.9448e-04,  1.8110e-03,  1.7052e-03,  ..., -1.2722e-03,
         -2.5201e-04,  8.3148e-05],
        [ 5.3120e-04,  5.7697e-04, -1.9550e-03,  ..., -1.4448e-03,
         -1.3351e-05,  7.6294e-04],
        [ 1.1902e-03, -1.6508e-03, -3.7708e-03,  ...,  2.5234e-03,
         -1.7643e-04, -4.0169e-03],
        [-1.1826e-03,  4.2248e-04,  6.4945e-04,  ..., -3.7003e-04,
         -3.4237e-04,  3.7861e-04],
        [-4.9639e-04,  1.0138e-03,  1.6565e-03,  ..., -7.8726e-04,
          8.1003e-05,  1.0977e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8925, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.2500, -3.2988, -2.9551,  ..., -4.7812, -4.1875, -0.7822],
        [-1.8467, -3.3926, -4.4258,  ..., -6.0664, -4.5820, -3.1602],
        [-4.7305, -2.6055, -3.0254,  ..., -3.9492, -4.8867, -1.0576],
        ...,
        [-3.6094, -2.3438, -2.7188,  ..., -3.4531, -5.5000, -1.3750],
        [-5.6562, -2.9199, -3.7168,  ..., -4.0430, -4.4492, -1.8887],
        [-4.2227, -3.0684, -4.4102,  ..., -6.2383, -5.4102, -0.8955]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 18, 27, 27, 27,  9, 15, 12,  6, 18, 27, 10,  6, 20,  2, 27, 18,  4,
        11, 27, 27,  0, 27,  9,  1, 27,  0, 18,  0, 27,  0, 27, 27, 27,  3, 18,
        27, 27,  3, 27,  7,  6, 27, 27, 27,  1,  4, 27, 27, 18, 27, 18,  2, 27,
         5,  1, 23, 11,  2,  0, 27, 10, 27, 27], device='cuda:0')
step: 244
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6118,  ...,  0.4783, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2201,  0.7949,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4092, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0970,  ...,  0.6567,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0017,  0.0005,  0.0012,  ..., -0.0027,  0.0003, -0.0011],
        [-0.0022, -0.0011, -0.0085,  ...,  0.0048,  0.0058, -0.0025],
        [ 0.0002, -0.0013, -0.0067,  ..., -0.0002, -0.0019, -0.0069],
        [-0.0018,  0.0004, -0.0021,  ..., -0.0007,  0.0003, -0.0009],
        [-0.0005,  0.0002,  0.0014,  ..., -0.0002,  0.0001,  0.0008]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.3283, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0781, -3.1719, -3.5938,  ..., -4.9219, -3.9355, -0.7339],
        [-0.6143, -3.5059, -3.4902,  ..., -4.6602, -4.1602, -3.4570],
        [-3.0371, -3.4570, -4.1914,  ..., -2.5840, -3.3496, -1.3643],
        ...,
        [-2.6602, -2.9570, -4.5352,  ..., -5.3320, -3.9883, -3.0352],
        [-3.8965, -2.7402, -3.5684,  ..., -4.8828, -5.6797, -0.8970],
        [-4.8594, -3.4531, -3.1426,  ..., -4.7344, -3.4082, -0.7197]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 17, 22, 22,  4, 24, 10, 20,  4, 18,  1, 27, 27, 22, 27, 18, 26,  7,
        15, 20,  0, 27,  3, 27,  8, 27, 15,  4, 20, 27, 18,  7,  4, 15, 27, 27,
        27, 27, 20, 27, 10, 15, 17, 24, 27,  7, 25, 25,  0,  1,  1, 27, 18, 27,
        27, 18, 27, 27, 17, 18, 15, 20, 27,  6], device='cuda:0')
step: 245
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6118,  ...,  0.4783, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2201,  0.7949,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4092, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0971,  ...,  0.6567,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0010,  0.0004, -0.0011,  ..., -0.0017,  0.0008,  0.0003],
        [ 0.0005, -0.0011,  0.0002,  ..., -0.0035, -0.0021,  0.0018],
        [-0.0002, -0.0008, -0.0059,  ...,  0.0051,  0.0021, -0.0035],
        [ 0.0006,  0.0005, -0.0019,  ...,  0.0001, -0.0002, -0.0003],
        [ 0.0004,  0.0007,  0.0004,  ..., -0.0008,  0.0001,  0.0010]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.0027, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.5312, -3.2344, -3.3125,  ..., -6.1094, -5.2031, -0.8906],
        [-2.9375, -2.7832, -3.8906,  ..., -4.9219, -4.1094, -1.4385],
        [-2.9668, -3.0312, -3.6250,  ..., -5.1094, -4.4375, -1.9834],
        ...,
        [-5.0312, -4.2031, -3.2188,  ..., -2.3594, -5.7969, -0.6558],
        [-4.0195, -3.5977, -3.6289,  ..., -6.9883, -3.9258, -1.0664],
        [-3.5254, -3.3379, -2.5098,  ..., -1.5879, -3.7598, -1.8223]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22,  4, 15, 27, 27, 25, 27,  4, 27, 27,  1, 27,  0, 18, 25, 27, 17, 27,
        10, 17, 26, 18, 27, 27, 27,  0,  1,  4, 27, 27,  4,  4, 10, 13,  6, 24,
        27, 25,  3, 11, 17, 15, 27, 20, 27, 22, 14,  6, 27, 27, 25, 11,  0, 27,
         3, 17, 13, 27, 27,  9, 18,  2,  4, 25], device='cuda:0')
step: 246
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6118,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2201,  0.7949,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4089, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0972,  ...,  0.6572,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.3440e-04, -7.1526e-07, -1.7667e-04,  ...,  9.0933e-04,
         -5.1403e-04,  4.3440e-04],
        [ 2.7657e-04, -1.2598e-03, -7.0810e-04,  ...,  6.2943e-04,
          3.1888e-05,  2.0428e-03],
        [-3.2544e-04,  1.4038e-03,  8.6975e-03,  ..., -7.4615e-03,
         -3.0594e-03,  3.4866e-03],
        [ 9.2077e-04,  1.2589e-03,  2.6722e-03,  ...,  7.1335e-04,
          1.0719e-03,  9.5940e-04],
        [ 2.5320e-04, -5.7507e-04, -9.8610e-04,  ...,  2.5892e-04,
         -3.5167e-04, -1.8864e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3964, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.6523, -3.5410, -2.5879,  ..., -3.8555, -3.9004, -1.4482],
        [-5.6992, -3.5605, -4.4023,  ..., -6.5898, -5.2773, -0.2786],
        [-3.8555, -3.5742, -3.4355,  ..., -3.2617, -4.9023, -0.9502],
        ...,
        [-4.9961, -2.8848, -3.5566,  ..., -5.6836, -5.6836, -0.5728],
        [-5.4531, -4.0312, -3.3594,  ..., -4.7500, -4.6094, -0.3599],
        [-2.7012, -2.9199, -2.0137,  ..., -3.4355, -4.3555, -1.0293]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6, 27, 10,  9,  7, 27, 27, 22, 14, 17, 18,  0, 27, 17,  0, 18, 17,  0,
         0, 18,  7, 27, 10,  9, 27, 15, 27,  2, 14, 27, 20, 20, 11, 15,  3,  3,
        27, 13,  7,  7, 27, 15, 18,  6, 27, 10, 25,  4, 18,  2, 27,  5,  8, 10,
         7, 27, 27,  9, 12, 20,  4, 20, 27, 26], device='cuda:0')
step: 247
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6118,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2201,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4089, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0972,  ...,  0.6572,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.8764e-04,  4.0436e-03,  1.5764e-03,  ..., -7.1983e-03,
         -6.6614e-04, -4.3030e-03],
        [-1.0071e-03, -3.3855e-04, -1.1452e-02,  ...,  1.3390e-03,
          4.1428e-03, -2.0218e-03],
        [-5.3978e-04, -1.1444e-05, -5.9776e-03,  ...,  2.9778e-04,
          3.1986e-03, -1.2674e-03],
        [-1.2665e-03,  5.2691e-04, -2.5349e-03,  ..., -3.4666e-04,
          8.4972e-04, -2.2297e-03],
        [-7.5912e-04,  1.6365e-03,  9.7942e-04,  ..., -4.9353e-04,
          1.7710e-03, -2.4271e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0420, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.9648, -3.7910, -4.4961,  ..., -5.3711, -5.7461, -0.2910],
        [-3.2441, -3.0723, -3.7910,  ..., -4.8359, -3.6973, -1.5566],
        [-4.4141, -2.7266, -2.7422,  ..., -3.9141, -3.7109, -1.4453],
        ...,
        [-1.0195, -2.7383, -3.8164,  ..., -5.6289, -3.9883, -2.1133],
        [-5.1406, -3.9707, -4.3125,  ..., -4.9102, -6.1562, -0.3303],
        [-3.9316, -2.5879, -3.2285,  ..., -5.0273, -4.3359, -1.7129]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 5, 27,  3, 27, 27, 27, 27, 18,  3, 15, 11, 27,  3, 24, 27, 25, 13, 25,
        17,  0, 15, 27, 27, 15, 24,  6, 18, 27,  4, 18, 18, 27, 26, 27,  1, 22,
        20, 20,  0,  4, 27, 27, 17, 14,  4, 27, 15,  9, 27,  6,  1, 25, 22,  4,
         4, 15, 27, 17,  0, 27, 27,  0, 27, 17], device='cuda:0')
step: 248
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6118,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2202,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4089, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0972,  ...,  0.6572,  0.7065, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.9591e-04,  2.9278e-04,  1.0633e-03,  ..., -2.0142e-03,
         -1.0662e-03,  9.8288e-05],
        [ 6.2752e-04, -1.0452e-03,  3.7193e-04,  ...,  5.7640e-03,
          2.8706e-03,  2.2278e-03],
        [ 1.2231e-04, -9.7942e-04, -2.1100e-04,  ...,  8.9312e-04,
          3.7122e-04, -4.4966e-04],
        [-9.7156e-05, -6.1798e-04, -7.4339e-04,  ...,  7.5817e-04,
          9.3746e-04, -2.7418e-04],
        [ 4.7421e-04,  6.0701e-04,  5.3215e-04,  ...,  2.5868e-04,
          3.1519e-04, -1.1957e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2470, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.9375, -1.8594, -4.5156,  ..., -6.5781, -5.4688, -1.7188],
        [-5.2109, -3.7246, -3.4434,  ..., -3.2559, -4.0391, -0.5850],
        [-1.9590, -4.1797, -5.2734,  ..., -5.9922, -3.3184, -3.4902],
        ...,
        [-1.5791, -3.3281, -4.2656,  ..., -5.4375, -3.9863, -2.9219],
        [-1.9199, -3.7637, -4.3750,  ..., -6.2344, -4.3398, -2.9668],
        [-2.5996, -2.7402, -4.1484,  ..., -3.8340, -3.4590, -1.6943]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 10,  1,  9, 27, 10, 24, 27, 20,  7,  2, 18, 15, 15, 15, 27, 15,  9,
        24, 14, 11,  1, 15, 22,  1, 20,  9, 18,  3, 20, 27,  5,  7, 27,  2,  7,
        22,  6, 15, 27, 27, 10,  7,  1,  4,  3,  9,  0, 27,  0, 27, 27, 25,  0,
        18,  2,  3,  9, 15, 27, 27,  0, 18, 15], device='cuda:0')
step: 249
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6118,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2202,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4089, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0973,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.0783e-04,  2.3518e-03,  2.4796e-03,  ..., -2.6226e-03,
          1.1820e-04,  1.1301e-03],
        [ 6.4182e-04, -3.6526e-03, -1.2932e-03,  ...,  1.4572e-03,
         -1.0853e-03,  2.6493e-03],
        [ 7.9584e-04, -3.4428e-03, -8.6212e-03,  ...,  5.1346e-03,
          3.4022e-04, -6.1150e-03],
        [-8.7881e-04,  5.8556e-04,  1.1206e-04,  ...,  3.5429e-04,
         -2.6703e-04,  1.4770e-04],
        [-1.3828e-04,  6.7806e-04,  1.2856e-03,  ..., -4.3106e-04,
          2.8610e-05,  2.1954e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1882, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.2266, -3.9141, -5.1953,  ..., -5.3359, -5.0078, -3.6953],
        [-4.8281, -3.5488, -4.3906,  ..., -5.6406, -5.1875, -0.5327],
        [-6.8008, -2.5488, -3.7520,  ..., -6.3008, -4.3477, -0.6738],
        ...,
        [-4.8789, -2.7090, -3.7246,  ..., -2.0508, -4.8008, -2.4277],
        [-3.0352, -3.1602, -4.8164,  ..., -3.6758, -4.3789, -2.1758],
        [-6.6680, -3.1191, -4.2305,  ..., -5.2617, -4.3242, -0.4785]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17,  8, 27, 15, 25,  2, 14, 27, 18, 27,  3, 23, 27, 27,  7, 27,  0, 15,
        27, 25, 27, 22, 27, 10, 15, 26,  0, 27, 15, 27, 10, 13,  7,  7,  2,  6,
        27, 19, 13, 27,  4,  5,  4, 27, 25, 20,  2, 27, 10, 27, 18, 27, 18, 27,
         1, 15, 27,  0,  8, 27,  6, 27, 27, 27], device='cuda:0')
step: 250
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6118,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2201,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4089, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2285, -0.0974,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.9564e-04, -1.2760e-03, -1.6069e-04,  ...,  2.7008e-03,
          9.8038e-04, -1.5755e-03],
        [ 7.4387e-04, -1.1482e-03,  1.0567e-03,  ...,  1.5850e-03,
          1.1711e-03,  8.3733e-04],
        [ 3.9368e-03,  1.1673e-03,  7.1259e-03,  ..., -4.6997e-03,
         -1.8435e-03,  4.8065e-03],
        [-1.6737e-03, -6.3753e-04,  1.0815e-03,  ...,  2.7704e-04,
         -8.9741e-04,  1.1921e-06],
        [ 3.2663e-04, -1.6060e-03,  2.2068e-03,  ...,  7.7868e-04,
          4.7970e-04,  1.0204e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3019, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.6602, -3.4727, -3.9727,  ..., -4.5977, -4.0039, -1.1904],
        [-5.1172, -3.0254, -3.5879,  ..., -3.1035, -3.3691, -2.1973],
        [-2.5527, -3.4746, -3.3965,  ..., -4.0391, -5.0859, -1.1943],
        ...,
        [-2.4004, -3.0566, -5.0430,  ..., -5.8867, -3.9785, -2.5566],
        [-0.9121, -3.3027, -4.3672,  ..., -3.8027, -2.7871, -3.5996],
        [-2.7637, -2.6699, -3.3730,  ..., -2.6387, -3.7324, -1.4668]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 27, 25,  7, 27, 22, 15, 27, 18, 27, 26,  3, 13,  4, 27, 15, 25, 27,
         1, 15, 27,  0, 27, 27, 27, 18, 17,  2, 10,  9, 15, 27, 17,  1, 27, 27,
         2, 27,  0, 15, 27, 18, 12,  9,  6,  6, 27, 24,  0,  4, 27, 14, 27, 27,
        15,  7, 11, 27, 17, 17, 27, 15,  1, 22], device='cuda:0')
step: 251
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6118,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9121, -0.2201,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4089, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2285, -0.0974,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.5667e-04,  1.9836e-04,  4.7278e-04,  ...,  7.1383e-04,
         -6.0844e-04,  1.1168e-03],
        [-1.3947e-04,  3.3522e-04,  1.0414e-03,  ...,  9.0408e-04,
         -4.0770e-04,  1.2279e-05],
        [-5.8937e-04,  7.2336e-04,  1.1902e-03,  ...,  1.7262e-03,
         -2.5797e-04, -2.7251e-04],
        [-1.4782e-04, -7.9536e-04,  1.5459e-03,  ...,  3.9911e-04,
         -7.5245e-04,  1.4472e-04],
        [ 1.5569e-04, -3.9518e-05, -1.7047e-05,  ..., -1.2517e-06,
          1.1045e-04, -2.2674e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1265, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0547, -3.7578, -5.0391,  ..., -6.6016, -3.5547, -1.9619],
        [-4.3164, -2.8770, -3.5332,  ..., -6.1523, -4.0508, -0.8306],
        [-3.6484, -3.2891, -3.0859,  ..., -3.4922, -3.3516, -1.4463],
        ...,
        [-5.3398, -5.5586, -5.1992,  ..., -0.2793, -4.6367, -2.6699],
        [-3.9355, -3.6855, -3.9668,  ..., -4.7305, -3.8730, -0.5288],
        [-4.0156, -3.3770, -4.1562,  ..., -4.7656, -3.9863, -0.9395]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 17, 26,  6, 27,  2, 22, 27, 15,  1, 17,  7, 13, 27, 15, 17, 15,  5,
         1,  0,  4,  4, 27, 12, 15,  1, 11, 27, 27, 27, 18,  0, 27, 27, 20, 17,
        27,  4, 27, 27, 27,  4, 27, 27, 27,  3, 20, 15, 17, 18, 27,  6, 27,  4,
         3, 18, 27,  0,  6, 20, 27, 27, 27, 27], device='cuda:0')
step: 252
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6118,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6035, -0.8247],
        [ 1.9121, -0.2202,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4089, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2285, -0.0975,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.1096e-04, -1.5783e-03,  1.3046e-03,  ..., -1.5044e-04,
         -7.4244e-04,  2.0146e-04],
        [ 5.4181e-05, -3.0231e-04,  3.5882e-05,  ..., -8.8692e-04,
         -4.3929e-05,  2.2507e-04],
        [ 1.3247e-03, -8.3351e-04, -1.3793e-04,  ..., -8.8501e-04,
          9.7942e-04,  1.2226e-03],
        [-1.2026e-03,  2.8610e-04,  2.6512e-03,  ..., -5.6171e-04,
         -4.1962e-04,  1.2674e-03],
        [ 7.4387e-05, -4.3893e-04,  8.9502e-04,  ..., -2.1219e-05,
         -7.1001e-04,  4.2033e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1139, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-7.3672, -3.4434, -3.7246,  ..., -5.1758, -5.2539, -0.5840],
        [-6.1758, -3.5527, -4.1289,  ..., -4.5039, -5.5195, -0.4116],
        [-4.0352, -2.1426, -1.5186,  ..., -1.7373, -5.3633, -2.1426],
        ...,
        [-1.7998, -4.1445, -4.8008,  ..., -5.5195, -4.7070, -3.4707],
        [-1.2705, -2.8164, -3.8945,  ..., -4.2539, -2.2383, -2.5820],
        [-4.0391, -3.0527, -3.7871,  ..., -2.8652, -2.7402, -1.1777]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 12, 27,  9,  1, 20,  9,  2, 15, 27,  0,  9, 27,  4, 27, 15, 15,
        27, 27, 15,  0, 11, 27, 27, 27,  8,  7,  3,  4, 15, 15, 27,  3,  3,  7,
        15,  7, 13, 20,  4, 15, 10,  0, 27,  8, 27,  9, 15, 27, 27, 27,  6,  5,
        18,  6, 27,  4, 18,  7, 27, 20,  0, 27], device='cuda:0')
step: 253
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6118,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6035, -0.8247],
        [ 1.9121, -0.2202,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4087, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0975,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.0352e-05,  8.6963e-05,  3.4714e-04,  ...,  2.3198e-04,
         -1.4079e-04, -1.4153e-03],
        [ 3.5143e-04, -1.0462e-03,  6.0081e-04,  ...,  2.8191e-03,
         -1.8797e-03,  8.0764e-05],
        [-7.8011e-04,  2.2678e-03, -7.7133e-03,  ...,  9.1553e-03,
          5.7526e-03, -4.6539e-03],
        [ 2.2221e-04, -8.9979e-04, -1.2560e-03,  ...,  1.0967e-03,
          3.0684e-04, -1.2197e-03],
        [ 1.9073e-04,  8.0347e-04,  4.1866e-04,  ..., -3.6955e-04,
         -3.9673e-04,  1.9741e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8638, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0586, -3.0117, -4.2773,  ..., -5.1367, -4.9180, -0.8257],
        [-6.0039, -3.1133, -3.7383,  ..., -3.2383, -3.4414, -1.5820],
        [-4.7695, -3.1445, -3.7695,  ..., -5.4570, -5.0352, -0.7080],
        ...,
        [-5.6367, -3.1504, -3.4785,  ..., -3.3066, -4.4648, -0.7290],
        [-4.4805, -3.4648, -3.7305,  ..., -4.2773, -3.6523, -0.5894],
        [-6.2031, -3.3613, -5.0312,  ..., -5.1250, -4.7656, -0.8296]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 15,  5,  7,  0, 27, 27, 27,  7, 25, 18,  4, 18, 25, 18,  1, 27,  0,
        25, 27,  3, 26, 27, 20, 27, 27, 27, 27,  1,  0,  3, 27, 27, 27, 15,  2,
        27, 27, 15, 27, 25, 27, 26, 10, 27, 27, 20,  7, 27, 27, 25,  9, 25, 27,
        15, 27, 27, 27, 27, 27, 27, 27, 27, 27], device='cuda:0')
step: 254
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6118,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6035, -0.8247],
        [ 1.9121, -0.2202,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4087, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2285, -0.0976,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.0388e-04, -1.3723e-03, -6.1989e-06,  ..., -1.2696e-04,
         -1.2484e-03,  1.8740e-04],
        [-8.0943e-05, -1.3266e-03, -4.9829e-05,  ..., -1.3189e-03,
         -1.0242e-03,  1.0939e-03],
        [-4.1318e-04, -7.4291e-04, -2.8515e-03,  ...,  9.7370e-04,
          8.3733e-04, -1.0891e-03],
        [-1.3237e-03, -6.1178e-04, -1.6403e-04,  ..., -1.7715e-04,
          8.5354e-04,  8.5831e-04],
        [-3.1662e-04, -3.3021e-04,  1.2817e-03,  ..., -5.3787e-04,
         -1.4091e-04,  4.6515e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1091, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.8828, -3.7754, -5.0703,  ..., -6.3516, -2.3691, -2.0566],
        [-5.0977, -3.2363, -3.2832,  ..., -3.9102, -4.1133, -0.9717],
        [-2.5391, -3.9766, -4.9453,  ..., -3.8984, -4.5703, -3.8828],
        ...,
        [-2.9043, -3.5137, -3.3418,  ..., -2.7148, -2.4648, -2.0898],
        [-3.2930, -3.5137, -4.4180,  ..., -3.1523, -4.1992, -1.4971],
        [-2.4473, -3.4316, -4.1016,  ..., -5.5234, -5.3203, -1.3691]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([13, 22, 18, 27,  3, 27, 15,  7, 22, 15, 27, 27, 18, 18,  5,  0,  1, 14,
        14, 27,  7, 20, 27,  1, 27, 15, 27, 18,  3,  1,  4, 27, 27,  4, 24, 27,
         2, 27, 17,  2, 27,  3, 10, 27,  4, 23,  4, 17,  3,  0,  3, 20, 15, 27,
        18, 27, 27, 27, 27, 27,  3, 26, 20,  4], device='cuda:0')
step: 255
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6118,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6035, -0.8247],
        [ 1.9121, -0.2202,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4087, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2285, -0.0977,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.4298e-04,  9.7942e-04,  1.8835e-05,  ...,  4.9353e-04,
         -4.7493e-04,  1.3895e-03],
        [ 3.4285e-04, -1.7393e-04,  1.1358e-03,  ..., -1.2531e-03,
         -1.7710e-03, -8.7261e-04],
        [ 3.3331e-04,  1.8120e-04, -5.6887e-04,  ...,  7.0763e-04,
         -6.4802e-04, -1.3533e-03],
        [ 6.9284e-04,  1.9073e-04,  6.7139e-04,  ..., -7.5006e-04,
         -5.0354e-04, -8.0204e-04],
        [ 2.8181e-04, -1.4496e-04, -5.9986e-04,  ..., -1.5140e-04,
         -3.6025e-04, -2.3484e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3416, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.7461, -4.5430, -4.1680,  ..., -4.1680, -5.2617, -0.2781],
        [-4.0000, -3.7031, -0.8584,  ..., -2.0156, -4.5000, -1.7178],
        [-5.4219, -4.2188, -4.5156,  ..., -4.5625, -0.1263, -4.3906],
        ...,
        [-6.4375, -3.6699, -4.2188,  ..., -5.9102, -6.5859, -0.2173],
        [-4.0664, -3.6133, -3.5195,  ..., -4.2383, -4.1602, -0.6133],
        [-1.5117, -3.3398, -4.2773,  ..., -4.5273, -4.4492, -2.9961]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2,  2, 26, 27, 27, 15,  6, 22,  1, 27, 18, 27,  1,  1, 27, 27, 15,  2,
        27, 27, 27, 27, 17, 26, 15, 27,  2, 26, 11,  9,  3, 27,  9, 14, 17, 27,
        27,  7, 13, 27, 27, 15, 22, 27, 27,  9,  3, 15,  4, 16, 10, 13, 27,  5,
         3, 26,  4,  7,  7,  9, 20, 27, 27, 15], device='cuda:0')
step: 256
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6118,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6035, -0.8247],
        [ 1.9121, -0.2202,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4087, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0977,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.2609e-04, -4.9400e-04,  1.4830e-04,  ...,  1.0128e-03,
         -4.2152e-04,  9.3699e-04],
        [-2.2864e-04,  1.1415e-03,  2.1422e-04,  ..., -5.2986e-03,
         -3.9406e-03, -4.7088e-04],
        [ 6.1417e-04, -2.4533e-04,  8.3828e-04,  ..., -2.2006e-04,
         -1.5373e-03,  1.2922e-04],
        [ 6.0415e-04, -6.1798e-04,  6.9332e-04,  ..., -2.0742e-04,
          5.8770e-05, -8.9455e-04],
        [ 4.6635e-04, -8.3327e-05, -5.6887e-04,  ..., -1.8346e-04,
         -2.0516e-04, -9.0551e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1430, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.0977, -3.1602, -2.7852,  ..., -2.9727, -1.9414, -2.9414],
        [-5.6289, -2.6602, -4.2383,  ..., -5.2070, -4.3164, -0.9888],
        [-3.6758, -3.2539, -3.6602,  ..., -4.4102, -4.7070, -0.5972],
        ...,
        [-4.7500, -5.0195, -3.5332,  ..., -2.7207, -4.6289, -4.9414],
        [-2.1543, -2.3418, -4.1523,  ..., -6.3398, -4.9023, -1.7168],
        [-5.2461, -2.9160, -2.8066,  ..., -3.8379, -4.9805, -0.6509]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 27, 17, 10, 27, 27, 13, 27, 25, 27, 27, 10,  0, 20, 27,  7, 25, 27,
        27,  0,  9, 27, 10,  5,  1,  2, 27, 18, 27,  1, 17, 27, 18, 27, 27, 15,
        27, 27,  3,  0, 18, 27, 27, 26, 27, 27,  0,  7, 22, 10, 25, 18, 15,  2,
        24, 10, 27, 27,  0, 27, 27,  3,  4,  3], device='cuda:0')
step: 257
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6118,  ...,  0.4785, -0.0836,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6035, -0.8247],
        [ 1.9121, -0.2202,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4087, -0.2976, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1729,  2.2305, -0.0978,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.9697e-04,  8.5735e-04,  2.4166e-03,  ...,  5.4598e-04,
         -1.0681e-03,  1.5812e-03],
        [-1.4687e-04,  2.9373e-03,  1.8330e-03,  ..., -9.5654e-04,
         -1.1406e-03, -7.3767e-04],
        [-2.9206e-04,  4.3344e-04, -2.6560e-04,  ...,  1.7567e-03,
         -7.2122e-06, -4.5609e-04],
        [ 2.0528e-04, -1.3924e-03,  2.8896e-03,  ...,  3.8624e-04,
          3.5119e-04,  4.1389e-03],
        [ 3.1233e-05,  6.4993e-04,  9.0301e-05,  ...,  1.4019e-04,
          3.3283e-04,  4.4560e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1847, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3750, -3.3281, -4.4844,  ..., -3.8281, -4.0625, -0.7188],
        [-1.4414, -2.7383, -4.2539,  ..., -5.8945, -6.0664, -2.0820],
        [-4.6758, -3.7383, -3.4414,  ..., -4.8477, -2.8477, -2.4102],
        ...,
        [-5.3555, -3.0254, -4.1055,  ..., -5.6797, -4.6172, -0.7129],
        [-1.8613, -3.5020, -4.1719,  ..., -6.2969, -3.5020, -3.3770],
        [-4.3008, -3.0059, -2.0371,  ..., -1.4434, -5.1758, -3.4434]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0,  7, 27, 13, 26, 20,  6, 12, 18,  0, 27, 27, 27, 27, 27, 27, 27,
        18,  7, 11, 10,  5, 13, 15, 27,  1, 24, 27, 17, 26,  0, 27, 15,  6, 17,
        26, 25, 20,  4, 25, 27,  8, 25, 10, 26, 27, 15, 27, 27, 27, 12, 22,  4,
         1, 27, 18, 27, 27, 27,  4, 27,  0,  8], device='cuda:0')
step: 258
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0835,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6035, -0.8247],
        [ 1.9121, -0.2202,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4087, -0.2974, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1738,  2.2285, -0.0978,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.7456e-04, -1.3971e-03, -1.2100e-04,  ...,  1.2803e-04,
         -6.7711e-04,  4.6325e-04],
        [-1.6785e-04, -1.2219e-04,  2.9683e-04,  ...,  6.1798e-04,
         -4.7028e-05, -4.9877e-04],
        [-3.5954e-04,  3.2258e-04,  3.0589e-04,  ..., -4.1676e-04,
         -2.9850e-04,  8.3733e-04],
        [-6.0797e-04, -3.6573e-04,  8.0967e-04,  ...,  1.3435e-04,
         -1.2076e-04,  1.4222e-04],
        [-1.5116e-04, -2.6250e-04,  4.8256e-04,  ..., -4.8995e-05,
         -4.8208e-04,  5.8270e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9346, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.7656, -4.0000, -4.8125,  ..., -5.3906, -6.1094, -0.2981],
        [-5.8359, -2.8027, -2.9434,  ..., -1.6475, -6.6953, -1.8965],
        [-4.4570, -3.5664, -3.0039,  ..., -3.2227, -3.7227, -0.9878],
        ...,
        [-4.3359, -2.7734, -2.2109,  ..., -2.9453, -2.9141, -1.5078],
        [-3.2656, -2.9219, -2.6094,  ..., -2.1250, -4.4844, -1.2031],
        [-5.1914, -3.2090, -3.5820,  ..., -7.0820, -6.4414, -0.4575]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 10, 14, 17, 27, 27,  7, 25, 27,  3, 27, 27,  0, 27, 27,  8,  3,  4,
        26,  4,  1, 27,  0, 27,  0, 27, 24,  8,  8,  6, 27, 18, 18, 15, 22, 27,
        27, 27, 27,  0, 10, 11, 13, 27, 27, 26, 11, 27, 27,  5, 27, 27, 20, 27,
        27,  4,  2, 27, 27, 27, 27,  2, 25,  4], device='cuda:0')
step: 259
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0835,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6035, -0.8247],
        [ 1.9121, -0.2202,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4087, -0.2974, -1.7188,  ..., -0.6592,  0.6519,  1.2686],
        [-1.1738,  2.2285, -0.0978,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.1291e-04, -2.8782e-03, -6.9284e-04,  ...,  2.2912e-04,
         -9.6917e-05, -1.4448e-03],
        [-8.1158e-04,  8.4496e-04, -8.5354e-04,  ..., -2.2335e-03,
          1.7464e-05, -6.7949e-05],
        [ 7.8773e-04,  8.2970e-04, -1.4372e-03,  ..., -2.3956e-03,
          2.7161e-03,  1.2417e-03],
        [-6.1369e-04,  7.4148e-04, -1.4277e-03,  ...,  2.9087e-04,
          2.7466e-04, -2.2745e-04],
        [-2.9874e-04,  6.1226e-04,  3.2091e-04,  ...,  2.0766e-04,
         -1.2553e-04,  5.1260e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0630, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.2109, -2.8066, -3.3516,  ..., -3.8691, -4.2266, -0.9463],
        [-3.3047, -3.3223, -2.9160,  ..., -4.4609, -1.9150, -1.3369],
        [-5.1016, -3.2129, -3.3535,  ..., -3.4941, -4.1484, -0.8843],
        ...,
        [-2.3965, -2.7871, -4.3828,  ..., -4.8945, -3.9277, -1.6465],
        [-1.9268, -3.5195, -3.5996,  ..., -4.4258, -3.9277, -2.9434],
        [-2.6953, -3.8516, -5.2266,  ..., -7.3359, -3.6172, -3.7266]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1,  7, 27, 27, 27,  7, 14, 21,  9, 13, 27, 15,  8, 27,  4, 17, 25, 24,
        27, 27, 27,  1, 10, 27,  4, 27, 27,  4, 17, 18, 15, 24, 14, 27, 12, 13,
         0, 14, 11, 27, 20,  0, 18, 27, 15, 18, 27, 27, 13, 25,  0,  3,  0, 27,
        14,  0, 14,  4, 27,  0, 15, 27,  0, 18], device='cuda:0')
step: 260
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0834,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4084, -0.2974, -1.7188,  ..., -0.6597,  0.6519,  1.2686],
        [-1.1738,  2.2285, -0.0979,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.8491e-05,  7.7057e-04,  7.4482e-04,  ..., -5.0545e-04,
         -4.1342e-04, -3.6597e-04],
        [-7.8082e-06, -1.1320e-03, -6.9046e-04,  ...,  2.1958e-04,
          8.6021e-04,  2.1191e-03],
        [-1.7214e-04, -2.6464e-04, -4.5848e-04,  ...,  1.5783e-03,
          5.6028e-04, -6.0654e-04],
        [ 2.9612e-04,  5.3787e-04,  4.3917e-04,  ...,  5.9509e-04,
         -1.7023e-04, -3.3438e-05],
        [-9.5248e-05,  4.9782e-04,  3.5405e-04,  ...,  2.9659e-04,
          8.9407e-05,  7.3969e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2301, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.9414, -3.3008, -3.7695,  ..., -4.7383, -2.5508, -1.5664],
        [-2.1484, -4.3203, -4.0078,  ..., -3.2109, -5.1016, -1.8828],
        [-3.6797, -3.3672, -3.6328,  ..., -4.7109, -2.5547, -1.0078],
        ...,
        [-4.8945, -3.5508, -3.3633,  ..., -0.9565, -4.5352, -1.8467],
        [-4.9453, -2.3223, -2.3223,  ..., -1.6191, -6.1797, -2.2129],
        [-7.0039, -4.5664, -4.9727,  ..., -6.9570, -6.2383, -0.0979]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 18, 15,  7, 27, 14, 27, 27,  7, 27,  1, 27, 27,  0, 27, 17,  2,  4,
        27, 27,  3, 13,  1,  1,  1, 15, 27, 10, 22,  3,  3, 27, 25,  1, 27,  0,
        27,  7, 27,  3,  1, 11, 27,  0,  0,  1, 18, 22, 18,  4,  4, 20, 15, 27,
        25,  4,  1, 15, 27, 18,  6, 25,  9, 27], device='cuda:0')
step: 261
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0834,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4084, -0.2974, -1.7188,  ..., -0.6597,  0.6519,  1.2686],
        [-1.1738,  2.2285, -0.0979,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.9951e-03,  1.1568e-03,  1.9112e-03,  ...,  6.1655e-04,
          5.5969e-05,  4.2725e-04],
        [-2.7394e-04, -3.2473e-04,  2.4223e-04,  ...,  1.3285e-03,
          7.7105e-04,  1.5628e-04],
        [-8.2779e-04,  2.1434e-04, -1.8239e-04,  ...,  1.2465e-03,
          1.5535e-03,  1.0805e-03],
        [-8.4400e-04,  9.8801e-04,  1.7424e-03,  ...,  8.5497e-04,
          1.2054e-03,  1.6146e-03],
        [-3.0375e-04,  1.8520e-03,  8.4591e-04,  ...,  8.2159e-04,
          5.5122e-04,  5.5885e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1735, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.5234, -4.2734, -5.2266,  ..., -6.3047, -6.6641, -0.1969],
        [-5.2031, -3.2363, -4.5156,  ..., -6.9062, -5.8906, -0.4233],
        [-2.6797, -2.3672, -3.4766,  ..., -3.5078, -3.2422, -1.3984],
        ...,
        [-4.6250, -3.6562, -4.5000,  ..., -6.3281, -5.7188, -0.5474],
        [-5.1562, -3.6074, -2.8574,  ..., -4.5781, -4.8438, -0.7646],
        [-1.4785, -4.6641, -5.5234,  ..., -5.0586, -3.4316, -3.3691]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  1,  8, 27, 18, 27, 16,  1, 25, 25, 27,  4, 27, 22, 27, 27,  2,
        27, 18,  7,  2, 22, 27,  4, 27, 27, 27, 18,  5, 22, 27, 27, 27,  1, 20,
        27, 27, 25, 18, 27, 27, 20,  3, 15,  0, 27, 26, 27, 18, 27,  1, 10,  7,
         1, 25, 22, 10,  4, 15,  2,  4,  8, 18], device='cuda:0')
step: 262
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0834,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4084, -0.2974, -1.7188,  ..., -0.6597,  0.6519,  1.2686],
        [-1.1738,  2.2285, -0.0980,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.0449e-04, -7.8297e-04, -1.1902e-03,  ...,  7.7105e-04,
          9.0122e-04, -9.1553e-04],
        [ 3.7885e-04, -1.2093e-03, -3.2330e-04,  ..., -9.0003e-06,
         -1.2074e-03, -1.5163e-04],
        [ 4.9734e-04, -1.5867e-04, -1.0490e-03,  ..., -1.5001e-03,
          1.0071e-03,  3.3283e-04],
        [ 6.1655e-04,  1.5116e-04, -6.0749e-04,  ...,  6.3777e-05,
         -3.8290e-04, -1.1616e-03],
        [ 1.5914e-04, -4.4250e-04,  1.1051e-04,  ...,  2.5558e-04,
         -3.3498e-04,  5.0259e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2296, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.8574, -2.8105, -4.5312,  ..., -5.5781, -4.1562, -2.2324],
        [-5.0664, -3.2090, -3.8477,  ..., -4.1758, -5.6758, -0.6924],
        [-1.6064, -2.7324, -4.1836,  ..., -5.1055, -4.1211, -3.1836],
        ...,
        [-5.0078, -3.8516, -4.4609,  ..., -4.8516, -6.3672, -0.3516],
        [-2.8008, -2.4258, -3.2695,  ..., -1.5352, -3.3633, -2.1602],
        [-6.1328, -5.0078, -5.4297,  ..., -5.8359, -6.4141, -1.7432]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 27,  0, 25, 27,  7,  7,  5, 17, 27, 22, 17, 27, 27, 27,  7,  9, 20,
         9, 24,  2, 18, 15, 15, 27,  7,  2, 26, 27,  4,  5,  4,  0, 26,  0,  9,
        20,  5, 11, 27, 10, 27,  0, 27, 27,  0,  0, 18, 22,  1, 27,  1, 27, 18,
        25, 27, 10, 27, 18, 10, 27, 27, 15, 27], device='cuda:0')
step: 263
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0834,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9043, -1.3467, -0.6016],
        [-0.4084, -0.2974, -1.7188,  ..., -0.6597,  0.6519,  1.2686],
        [-1.1738,  2.2285, -0.0980,  ...,  0.6572,  0.7056, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.4468e-04, -6.2037e-04,  5.6028e-04,  ...,  9.6607e-04,
         -4.9496e-04,  3.2592e-04],
        [ 1.3609e-03, -6.5899e-04,  1.7872e-03,  ..., -1.5917e-03,
         -2.7390e-03, -1.2469e-04],
        [ 3.6168e-04, -7.8630e-04,  3.1090e-04,  ..., -2.7618e-03,
          2.1744e-04,  2.4395e-03],
        [ 2.0409e-04,  3.8433e-04,  1.8072e-03,  ..., -1.2040e-05,
         -7.1573e-04,  5.8603e-04],
        [-2.4700e-04,  7.4208e-05,  6.6614e-04,  ...,  1.2732e-04,
         -1.9038e-04,  5.1355e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2553, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.1074, -3.0293, -4.7617,  ..., -5.9336, -5.8242, -3.5918],
        [-3.6113, -4.3008, -4.3633,  ..., -5.8945, -3.6426, -0.4553],
        [-6.3164, -3.4258, -3.4883,  ..., -3.7852, -4.2695, -0.5503],
        ...,
        [-1.9814, -3.6699, -4.5586,  ..., -5.5430, -4.5273, -4.1680],
        [-1.6406, -3.2656, -3.0781,  ..., -5.4531, -3.9531, -1.7656],
        [-3.1055, -3.3555, -2.9492,  ..., -2.1992, -2.5117, -1.6357]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 27, 27, 27,  3, 27,  1, 27, 15, 26, 27,  2,  7, 17, 24,  8,  2,  3,
         1, 25,  4, 24,  1, 17,  4,  1, 27, 10, 27, 27,  4,  9, 14, 15,  9,  7,
        27,  0,  1, 15, 10, 24,  4, 27, 27,  4,  1, 10, 27, 15, 27, 27,  2, 25,
        27, 27, 27, 27, 15,  8, 27, 18,  7,  3], device='cuda:0')
step: 264
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0834,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7188,  ..., -0.6597,  0.6519,  1.2686],
        [-1.1738,  2.2285, -0.0980,  ...,  0.6572,  0.7056, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.8651e-04,  9.2793e-04,  1.3590e-03,  ...,  2.8825e-04,
         -7.2002e-04,  1.2484e-03],
        [ 4.3464e-04,  4.4918e-04,  6.6102e-05,  ..., -2.3222e-04,
         -8.2207e-04, -2.1732e-04],
        [ 4.4584e-04, -9.3460e-05, -6.6996e-04,  ...,  1.5726e-03,
          7.2527e-04,  4.1604e-05],
        [-6.6280e-04,  1.9765e-04,  1.6422e-03,  ...,  1.7762e-04,
         -1.4114e-04,  8.2445e-04],
        [-2.1422e-04,  1.1606e-03, -3.8624e-04,  ...,  5.6171e-04,
         -1.0687e-04, -1.5438e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7524, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.3379, -2.7285, -4.6172,  ..., -4.5234, -5.1992, -2.4473],
        [-6.8945, -4.5820, -5.3789,  ..., -7.0508, -7.7227, -0.0990],
        [-6.4297, -2.8672, -3.2578,  ..., -3.8516, -4.3359, -1.1328],
        ...,
        [-5.7578, -4.2148, -3.8691,  ..., -4.8516, -5.9453, -0.3691],
        [-4.3203, -3.6973, -3.2910,  ..., -3.0566, -4.1484, -0.8687],
        [-2.5840, -4.1133, -3.8652,  ..., -4.9883, -5.0977, -2.5840]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  4, 24, 17, 27, 15,  4,  3,  9, 15, 27, 26, 27, 27, 27, 27, 18, 27,
        27, 27, 27,  9, 20, 27, 27, 27, 27, 27, 27, 20,  0,  3, 27, 27, 15, 25,
         1, 27, 15, 11, 27,  3, 27, 18, 15, 27,  1, 13,  3, 27, 15, 26, 24,  4,
        17,  0, 27, 27,  7, 27, 27, 25,  6, 18], device='cuda:0')
step: 265
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0834,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7188,  ..., -0.6597,  0.6519,  1.2686],
        [-1.1738,  2.2285, -0.0981,  ...,  0.6572,  0.7056, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.5943e-04, -2.5883e-03,  4.9353e-05,  ..., -4.7326e-05,
         -1.0176e-03, -1.0639e-04],
        [-7.3338e-04,  6.1035e-04, -1.5068e-04,  ...,  9.5785e-05,
         -1.8227e-04, -5.5552e-04],
        [-1.3518e-04, -2.4021e-04, -1.8537e-05,  ..., -4.6372e-04,
          2.0337e-04, -1.8692e-04],
        [ 3.9816e-05, -7.7724e-05,  1.5469e-03,  ..., -6.4182e-04,
         -3.0994e-06,  3.1328e-04],
        [-9.6798e-05, -4.2248e-04,  3.2449e-04,  ..., -2.6166e-05,
         -2.9540e-04, -2.2340e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4264, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3477, -3.1309, -3.3184,  ..., -4.0039, -4.2227, -1.1152],
        [-5.7930, -5.3398, -6.6680,  ..., -8.6875, -4.9023, -3.3574],
        [-5.1992, -3.0586, -4.1055,  ..., -4.8867, -4.4805, -0.7300],
        ...,
        [-3.9863, -3.7988, -4.4844,  ..., -5.5781, -4.9219, -0.3767],
        [-4.4922, -3.0840, -3.6152,  ..., -3.2891, -3.9453, -0.9131],
        [-3.3887, -3.5781, -3.7480,  ..., -3.8750, -3.6719, -0.9990]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 27,  3, 18, 18, 10, 15,  3,  1,  3,  0,  2,  4,  3,  7, 15,  7, 13,
        10,  1, 10, 17, 15, 11,  6, 13, 12, 27, 17, 27,  2, 26, 27, 27, 13,  0,
        17, 18, 27, 27,  4, 25, 27,  1, 10,  1, 27, 15,  0, 27, 11, 27, 27, 18,
        27, 27,  3,  9, 25, 18, 27, 27,  7,  2], device='cuda:0')
step: 266
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6123,  ...,  0.4785, -0.0833,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7188,  ..., -0.6597,  0.6519,  1.2686],
        [-1.1738,  2.2285, -0.0981,  ...,  0.6572,  0.7056, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.1137e-04,  1.4534e-03, -1.9956e-04,  ...,  3.2425e-04,
          9.6035e-04, -4.5013e-04],
        [-6.1393e-06, -1.1415e-03, -1.3888e-05,  ..., -2.3537e-03,
         -2.7885e-03,  2.3723e-04],
        [ 2.4891e-04, -9.3174e-04, -7.9393e-04,  ..., -2.1458e-05,
         -5.2500e-04, -1.4377e-04],
        [-3.6144e-04,  1.8156e-04,  4.3535e-04,  ...,  6.0701e-04,
          2.6321e-04, -8.3148e-05],
        [-6.1333e-05, -2.3246e-04,  9.2387e-05,  ..., -3.5429e-04,
          2.7108e-04,  2.3413e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8148, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.5537, -2.9277, -3.8965,  ..., -4.8828, -2.5840, -1.4746],
        [-6.4766, -6.1016, -5.9141,  ..., -0.0409, -8.1484, -5.1016],
        [-1.7686, -3.2988, -4.1758,  ..., -2.5332, -2.6445, -1.8936],
        ...,
        [-6.2852, -3.1523, -3.8242,  ..., -5.8086, -7.2539, -0.2607],
        [-7.2227, -4.7227, -6.4414,  ..., -7.1445, -7.3320, -0.1589],
        [-1.3643, -3.1445, -5.0039,  ..., -5.2070, -5.0039, -3.3945]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 25, 26, 27, 27, 27, 17,  2,  3,  7, 27, 18, 22, 15, 27,  7, 27, 27,
        18,  7,  5, 27,  1, 18, 27,  3, 15, 14,  6, 18, 27,  3, 27, 18, 27, 10,
        21, 10, 20, 27, 27, 10, 27, 27, 15, 26, 15, 27, 27, 27, 15, 15,  0, 27,
        20, 10, 27, 27, 27,  5,  6, 27, 27,  0], device='cuda:0')
step: 267
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6123,  ...,  0.4785, -0.0833,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7188,  ..., -0.6597,  0.6519,  1.2686],
        [-1.1738,  2.2285, -0.0981,  ...,  0.6572,  0.7056, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.1480e-05, -2.0385e-05, -4.0603e-04,  ..., -3.2902e-04,
         -8.8167e-04,  3.6621e-04],
        [-3.4952e-04,  6.6042e-04, -4.1246e-04,  ..., -5.4741e-04,
          3.8409e-04,  4.8280e-04],
        [-6.0701e-04,  4.0650e-04,  7.1812e-04,  ...,  4.8733e-04,
          7.9584e-04,  6.0940e-04],
        [ 1.4615e-04,  5.6505e-04,  8.0395e-04,  ...,  4.5991e-04,
         -3.8004e-04,  1.0252e-03],
        [ 1.4830e-04,  1.1454e-03, -5.5218e-04,  ...,  4.2915e-04,
          1.4317e-04, -5.4216e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3531, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.9805, -5.1523, -4.7305,  ..., -4.5273, -6.6211, -0.1381],
        [-5.0195, -3.2715, -3.4434,  ..., -4.1016, -3.8809, -1.0371],
        [-1.7793, -2.9199, -3.5449,  ..., -5.4961, -3.8418, -1.1230],
        ...,
        [-2.6348, -3.4629, -3.6348,  ..., -3.4160, -4.0078, -2.1191],
        [-2.6953, -3.6016, -4.5703,  ..., -5.7578, -5.3203, -0.6963],
        [-5.4961, -3.4785, -2.7129,  ..., -1.9170, -3.7129, -1.1504]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([16,  6,  3, 27,  0, 18,  3, 10, 17,  1,  6,  8, 25, 27, 10,  7, 25, 15,
         0, 25,  2, 18,  4, 27,  4,  0,  0, 27, 27, 22, 27, 11, 22, 27, 27, 15,
         0, 24,  7, 10, 27,  4, 27, 27, 22,  0,  1, 27, 20,  0, 27,  0, 15, 14,
         3, 27, 10, 27,  3, 20, 15, 25, 27, 19], device='cuda:0')
step: 268
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6123,  ...,  0.4785, -0.0833,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7188,  ..., -0.6597,  0.6519,  1.2686],
        [-1.1738,  2.2285, -0.0981,  ...,  0.6572,  0.7056, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.2874e-04,  1.1606e-03,  1.0033e-03,  ..., -2.4605e-03,
          4.2462e-04, -8.1658e-06],
        [-5.3740e-04, -1.7967e-03,  1.3065e-03,  ...,  1.3781e-03,
         -1.3962e-03, -9.7179e-04],
        [-1.6475e-04, -5.8317e-04, -1.6117e-04,  ..., -2.2101e-04,
          4.3058e-04, -5.7173e-04],
        [ 2.1541e-04,  1.8573e-04,  3.9387e-04,  ..., -2.6679e-04,
          1.0872e-03, -7.0810e-05],
        [-3.0804e-04,  8.2064e-04, -2.0266e-06,  ..., -8.3864e-05,
         -1.5378e-04, -8.1158e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3385, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.3066, -2.6191, -4.7305,  ..., -5.8242, -4.9648, -3.7910],
        [-2.6836, -3.3867, -4.3086,  ..., -5.8086, -4.4961, -2.6523],
        [-7.7656, -4.2500, -4.9375,  ..., -6.3906, -5.5000, -0.2332],
        ...,
        [-4.8203, -2.7891, -2.9277,  ..., -3.7109, -5.0703, -0.8037],
        [-2.0566, -3.9004, -5.5898,  ..., -6.7305, -6.0898, -4.1367],
        [-3.2930, -3.4961, -3.8711,  ..., -5.9023, -4.4492, -0.9648]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15, 17,  5, 26, 25, 27, 27, 25, 27, 27,  3,  1,  0,  7, 11, 15,  2, 27,
         4, 27,  7, 27, 10, 27, 26,  8,  1,  1, 15, 27, 12, 20,  0, 27, 20, 27,
         0,  2, 18, 27, 27, 15, 26, 15,  2,  9, 27, 27, 27, 27, 26,  0, 27, 27,
        25,  7,  0,  4,  4, 25, 10,  4, 18, 27], device='cuda:0')
step: 269
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6123,  ...,  0.4785, -0.0833,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7188,  ..., -0.6597,  0.6519,  1.2686],
        [-1.1738,  2.2285, -0.0982,  ...,  0.6572,  0.7056, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.6785e-04,  1.8692e-03,  8.3351e-04,  ..., -1.6260e-04,
         -1.9646e-04, -5.8174e-04],
        [ 3.1924e-04, -1.5068e-03, -9.3818e-05,  ...,  1.4992e-03,
          9.5654e-04,  1.2445e-03],
        [-1.0788e-05, -5.7793e-04, -8.7357e-04,  ..., -1.3804e-04,
         -1.1091e-03, -7.0095e-04],
        [-1.9884e-04,  3.5000e-04,  5.7316e-04,  ..., -3.1543e-04,
         -6.3038e-04,  2.3425e-04],
        [-3.6383e-04,  1.7822e-05,  3.9363e-04,  ...,  4.6206e-04,
         -4.5633e-04, -2.9802e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2987, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.1680, -3.9180, -4.5586,  ..., -3.5742, -3.6699, -0.8408],
        [-5.3047, -4.1484, -4.0859,  ..., -1.8027, -5.0078, -0.5068],
        [-4.9258, -3.0664, -3.4258,  ..., -3.6289, -4.4414, -1.6758],
        ...,
        [-3.3027, -3.3809, -2.6465,  ..., -3.3965, -3.5527, -1.0840],
        [-4.4414, -4.0508, -5.4727,  ..., -4.5977, -5.5820, -0.3315],
        [-1.7432, -2.8535, -4.3359,  ..., -5.7578, -4.5547, -2.0410]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 25, 10, 22,  0,  3, 27,  1,  0, 11, 17,  7, 27,  5, 15,  7, 27, 27,
        18,  3, 27, 17, 18,  3, 27,  0, 10, 25, 27, 15, 18,  6, 25, 25,  1,  3,
        27,  3,  1,  8,  3,  9,  9, 20, 27,  0, 27,  8,  8,  7, 27, 20,  0,  1,
         4,  1,  0, 24, 27, 27, 15,  4, 27, 15], device='cuda:0')
step: 270
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2375, -0.6123,  ...,  0.4785, -0.0833,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7188,  ..., -0.6597,  0.6519,  1.2686],
        [-1.1738,  2.2285, -0.0982,  ...,  0.6572,  0.7056, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.3294e-03,  1.1044e-03, -1.9064e-03,  ...,  4.7278e-04,
          7.5436e-04, -2.3880e-03],
        [ 1.0777e-03, -1.7357e-03, -4.0579e-04,  ...,  1.8377e-03,
         -6.7663e-04, -7.6723e-04],
        [-3.0518e-04, -1.5593e-03,  3.0518e-05,  ...,  1.7490e-03,
         -1.0452e-03, -8.8573e-05],
        [-1.8716e-04,  9.6977e-05,  2.3670e-03,  ...,  2.0623e-04,
         -2.7800e-04, -1.0033e-03],
        [-2.2626e-04, -1.9288e-04, -3.3331e-04,  ...,  9.0313e-04,
          1.0085e-04, -5.3644e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2524, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.1211, -2.1523, -4.5430,  ..., -6.6523, -4.4961, -1.7148],
        [-2.0156, -3.5625, -4.8906,  ..., -5.6250, -4.1562, -2.2969],
        [-2.6680, -3.1055, -3.0742,  ..., -4.0430, -3.7129, -2.1367],
        ...,
        [-4.9727, -3.9102, -4.9102,  ..., -6.7695, -5.5039, -0.2076],
        [-4.0977, -3.5352, -3.0820,  ..., -4.3633, -4.0352, -0.6914],
        [-1.6494, -3.9629, -4.7734,  ..., -5.7109, -4.6016, -4.8672]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15, 18, 20, 13, 14, 20, 17,  4, 27, 27, 11, 24, 20,  1, 27, 13, 26,  7,
         5,  0, 13, 19,  7, 27, 27, 27, 27,  8, 27,  8, 27, 27, 26, 27, 20,  5,
        25, 27, 27,  7, 27, 27, 27, 22, 27, 27,  6, 27,  1,  2,  7, 27,  2, 17,
        27,  6, 27,  1, 27, 14, 24, 27, 10, 18], device='cuda:0')
step: 271
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0832,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7188,  ..., -0.6597,  0.6519,  1.2686],
        [-1.1738,  2.2285, -0.0982,  ...,  0.6572,  0.7056, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.9288e-04, -1.0414e-03,  7.1478e-04,  ...,  3.9077e-04,
          5.6314e-04, -3.8075e-04],
        [-5.8842e-04,  3.2330e-04, -5.9938e-04,  ..., -1.8864e-03,
         -5.6314e-04,  4.6062e-04],
        [-1.1933e-04,  2.5005e-03,  1.6375e-03,  ..., -5.5456e-04,
         -1.6260e-04,  1.0290e-03],
        [ 6.3038e-04,  3.0208e-04, -6.2180e-04,  ...,  3.3450e-04,
          3.2425e-04,  3.2330e-04],
        [-2.9659e-04, -5.9700e-04,  2.4331e-04,  ..., -6.4015e-05,
         -1.9503e-04, -1.2517e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1013, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.5312, -3.1875, -4.0469,  ..., -6.0469, -4.9219, -1.7812],
        [-2.3555, -3.8711, -4.3086,  ..., -5.5117, -4.0898, -2.7305],
        [-4.2305, -3.4961, -4.1836,  ..., -5.9336, -4.2461, -0.9502],
        ...,
        [-4.5273, -4.9961, -5.0586,  ..., -0.1527, -5.9336, -4.8555],
        [-5.1250, -3.5625, -3.8281,  ..., -4.9531, -4.7656, -0.5781],
        [-4.5312, -3.9844, -4.8594,  ..., -4.4531, -4.5938, -0.4058]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20, 18, 27,  5, 18, 27, 27, 27, 27, 25, 27, 11, 26, 27, 15, 15, 27,  7,
        27, 17,  7, 25,  1,  1, 18, 26,  9,  6, 22, 27, 27, 17, 25, 27, 13, 27,
         2,  0, 26, 27, 15, 17,  7, 25, 10,  3, 20, 27,  5, 10, 13, 27, 27, 22,
        25, 15, 18, 11, 27,  0,  8, 25,  3, 27], device='cuda:0')
step: 272
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0832,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7188,  ..., -0.6597,  0.6519,  1.2686],
        [-1.1738,  2.2285, -0.0983,  ...,  0.6572,  0.7056, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.4595e-04,  1.4820e-03, -1.9622e-04,  ..., -9.5487e-05,
         -6.1560e-04,  1.6813e-03],
        [ 1.0544e-04, -5.0783e-04,  5.0068e-04,  ...,  1.0586e-03,
         -1.8511e-03,  6.5136e-04],
        [ 7.3433e-04, -1.0929e-03,  4.4322e-04,  ..., -1.1253e-03,
         -7.3767e-04,  1.2016e-03],
        [-2.8181e-04,  3.9434e-04,  1.0900e-03,  ..., -5.8746e-04,
         -4.1580e-04,  2.2297e-03],
        [-2.5606e-04, -1.6546e-04, -7.8487e-04,  ..., -1.7047e-04,
         -2.0766e-04, -4.5919e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3064, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.6689, -3.6055, -4.5742,  ..., -6.5430, -4.7617, -4.3242],
        [-5.4844, -3.0312, -4.5625,  ..., -5.7188, -5.9531, -0.3762],
        [-3.0859, -2.9453, -3.0703,  ..., -1.8193, -4.0703, -2.4453],
        ...,
        [-4.2266, -2.8340, -3.1777,  ..., -1.8818, -6.4766, -1.9902],
        [-4.2344, -3.1270, -3.4238,  ..., -0.9082, -6.3906, -2.4395],
        [-3.1035, -2.8379, -3.8848,  ..., -4.0703, -3.8691, -1.1348]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  4, 14, 15,  4,  3, 18, 27, 27, 18,  9, 20,  2,  0, 22, 15, 10, 27,
        15, 13, 27,  1, 27, 20, 26, 20, 25, 18,  0, 27,  7, 26,  3, 14,  3, 24,
         4, 27, 18,  4,  4,  1, 22, 27, 27,  7,  1,  6, 20,  4, 25, 27,  7,  7,
        27, 27,  0, 25,  2, 15,  3, 27, 17,  2], device='cuda:0')
step: 273
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0832,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7188,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1738,  2.2285, -0.0983,  ...,  0.6572,  0.7056, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.7586e-03,  4.1199e-04, -3.4542e-03,  ..., -9.7847e-04,
          9.3699e-04, -5.4073e-04],
        [-1.9350e-03,  5.4646e-04,  1.4172e-03,  ..., -7.6914e-04,
         -2.0695e-04, -1.7567e-03],
        [-3.0565e-04, -4.3058e-04, -2.3766e-03,  ...,  3.3307e-04,
          1.7345e-04, -2.4261e-03],
        [-3.3593e-04, -3.4988e-05, -1.6537e-03,  ...,  9.2888e-04,
         -1.9157e-04, -8.6594e-04],
        [ 5.0116e-04,  3.1877e-04,  5.4979e-04,  ..., -3.0422e-04,
         -7.9274e-06,  7.2813e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1404, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.3398, -4.0156, -3.4199,  ..., -2.7949, -4.2148, -0.6226],
        [-0.6987, -3.5898, -4.9805,  ..., -5.9336, -3.0742, -5.5117],
        [-3.6035, -3.0566, -3.8535,  ..., -3.8848, -3.8047, -0.7588],
        ...,
        [-4.1641, -2.4590, -2.6621,  ..., -2.8184, -5.6016, -0.8345],
        [-5.0625, -3.1543, -4.0938,  ..., -3.8125, -5.4219, -0.8115],
        [-4.1602, -3.2852, -4.0352,  ..., -2.6914, -3.2539, -1.2217]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([14,  0,  1,  0, 15, 18,  1, 10,  2, 17,  0,  9,  1,  4, 12, 27, 22, 27,
         4, 18, 27, 26, 27,  0,  3, 27,  6,  3, 15, 14, 10, 27, 25,  2,  4, 27,
         9, 27, 27, 27,  4, 18,  1, 27, 10,  4, 25,  1, 18, 24,  7, 25,  4, 18,
        27,  2, 25,  0, 27, 27, 20,  2, 27, 25], device='cuda:0')
step: 274
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0832,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7188,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1738,  2.2285, -0.0983,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.6914e-04, -2.2340e-04,  1.6823e-03,  ...,  1.4896e-03,
         -2.5272e-05,  1.0834e-03],
        [-1.8978e-04,  1.3285e-03,  2.3103e-04,  ..., -5.1975e-04,
         -1.1539e-03, -5.9223e-04],
        [-4.3726e-04,  1.6575e-03,  1.2760e-03,  ..., -1.6737e-04,
          1.4000e-03,  1.2493e-03],
        [ 1.2565e-04,  2.7871e-04,  1.3227e-03,  ..., -4.9067e-04,
         -2.7943e-04, -8.4496e-04],
        [ 7.8082e-06,  3.7766e-04, -5.5885e-04,  ...,  1.0214e-03,
          4.5609e-04, -7.6294e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3110, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.5703, -3.4004, -3.9629,  ..., -4.7109, -4.6172, -0.9780],
        [-2.0137, -3.4355, -3.8887,  ..., -4.2188, -3.2793, -1.5146],
        [-4.0703, -3.1309, -4.3672,  ..., -3.0840, -2.5234, -1.3975],
        ...,
        [-3.3887, -2.6543, -3.4023,  ..., -3.4355, -4.9492, -1.2939],
        [-6.0195, -3.9082, -4.9102,  ..., -5.7227, -6.4883, -0.3459],
        [-5.8711, -3.4316, -3.3379,  ..., -2.7129, -4.1797, -1.8213]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27, 15, 27, 27, 20,  0, 10, 15, 27, 24, 25, 24,  1,  4, 27, 25,
         0, 27, 27, 27, 27,  9, 15, 22,  0,  0, 27, 14,  9, 15, 20,  4, 27, 11,
        10,  7, 17, 27,  9, 27,  0, 26,  7, 20,  5, 27, 24, 27, 27,  0, 26, 18,
         5, 27, 27, 10, 27, 17,  5, 19, 27, 22], device='cuda:0')
step: 275
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0832,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7188,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1738,  2.2285, -0.0983,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.4748e-04, -4.1723e-04, -5.1546e-04,  ...,  5.4646e-04,
         -4.3893e-04, -7.4005e-04],
        [-2.4307e-04,  2.1439e-03,  1.2121e-03,  ..., -6.7825e-03,
         -3.2597e-03,  6.7711e-04],
        [-4.3774e-04,  1.1511e-03,  7.5531e-04,  ...,  2.3403e-03,
         -6.6805e-04, -9.3603e-04],
        [ 1.2803e-04, -1.6093e-04, -1.6403e-03,  ..., -1.2708e-04,
          2.5773e-04, -1.2010e-04],
        [-1.1259e-04,  3.3998e-04,  4.2856e-05,  ..., -3.3677e-05,
          2.6405e-05,  9.8467e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1377, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.7031, -2.9219, -2.6094,  ..., -2.4531, -5.0156, -1.0625],
        [-4.5352, -3.4102, -3.4551,  ..., -1.5186, -3.9102, -1.0186],
        [-4.3984, -3.4941, -3.5098,  ..., -4.2891, -4.6172, -0.6343],
        ...,
        [-5.3281, -2.3613, -2.3457,  ..., -2.8457, -4.0625, -1.8604],
        [-1.1240, -3.8574, -6.1875,  ..., -5.7188, -5.3438, -4.9531],
        [-2.4922, -3.5234, -3.9141,  ..., -6.0703, -4.6016, -0.5869]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10,  3,  1,  1, 27, 27,  3, 27, 20, 27, 27,  4, 18, 27,  4, 27,  4, 27,
        27,  4,  0, 27, 27,  7, 10,  7, 22, 27, 27,  0,  6, 26,  7, 27, 27, 27,
        22, 27, 10, 27, 25,  8, 10, 20, 27, 22,  3, 14, 18, 11, 27, 17, 15, 27,
        27,  7,  0, 17, 27,  0, 27, 13, 20,  0], device='cuda:0')
step: 276
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0832,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7197,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1729,  2.2285, -0.0983,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.5955e-03, -1.9121e-03,  1.2093e-03,  ...,  6.0749e-04,
         -9.1410e-04, -9.4843e-04],
        [-8.1968e-04,  2.8591e-03,  3.4881e-04,  ..., -2.7733e-03,
          2.7442e-04, -1.0271e-03],
        [-3.6240e-05,  7.3099e-04, -3.5119e-04,  ..., -2.6207e-03,
          2.3627e-04,  3.1424e-04],
        [-9.3269e-04,  7.4196e-04,  2.6131e-03,  ..., -1.9729e-05,
         -2.4915e-05,  2.2144e-03],
        [-2.1672e-04,  2.6202e-04,  1.2236e-03,  ..., -2.0945e-04,
         -3.8528e-04,  4.8447e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0648, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.1719, -2.9531, -3.8438,  ..., -5.4219, -4.6562, -1.2500],
        [-1.1846, -2.9199, -3.8555,  ..., -2.2773, -2.5449, -3.3711],
        [-1.5547, -3.2266, -4.4453,  ..., -4.6797, -4.5547, -4.1641],
        ...,
        [-1.4717, -2.4863, -5.0195,  ..., -5.9258, -4.0508, -3.5020],
        [-2.0254, -2.9785, -4.0430,  ..., -4.2461, -2.0098, -2.3848],
        [-4.3984, -3.8223, -4.3516,  ..., -5.2266, -3.7441, -0.6499]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  0, 27,  7,  0,  0,  8, 27, 24, 24,  4, 13, 20,  2, 18, 27, 27,
        24, 27, 15, 22,  3, 27,  0,  0,  0,  4,  5, 27, 26, 10, 27,  3,  4,  6,
         3, 18, 23, 27, 18, 27,  4, 27,  4,  4, 27, 27, 27,  4, 27,  1, 17, 18,
        18, 27, 15, 27, 27,  7,  3, 15,  4, 20], device='cuda:0')
step: 277
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0832,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7197,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1729,  2.2285, -0.0983,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.9397e-04, -1.2321e-03,  3.5310e-04,  ..., -4.4203e-04,
         -8.7500e-04, -5.2691e-04],
        [ 1.3185e-04,  2.0142e-03,  1.2312e-03,  ...,  2.7847e-03,
          1.2140e-03, -2.4910e-03],
        [ 1.6952e-04, -3.5644e-04, -1.9336e-04,  ...,  4.1604e-05,
          1.2741e-03, -1.8167e-04],
        [-3.3617e-04,  7.3290e-04,  1.2903e-03,  ..., -1.0891e-03,
         -8.2374e-05,  2.0046e-03],
        [-6.0141e-05,  7.4291e-04,  4.7565e-04,  ...,  6.0368e-04,
         -1.4544e-05, -2.5940e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2438, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.2734, -2.9609, -3.9453,  ..., -3.2246, -3.2109, -1.1006],
        [-3.0176, -2.9863, -3.1113,  ..., -3.1738, -3.3613, -1.8457],
        [-4.6680, -2.5273, -3.1211,  ..., -2.3867, -5.1211, -1.6992],
        ...,
        [-3.9961, -3.2148, -4.0430,  ..., -3.7461, -3.9336, -1.2451],
        [-5.7852, -4.1445, -4.0977,  ..., -5.0195, -5.4414, -0.2710],
        [-3.4141, -2.8203, -3.8359,  ..., -2.8828, -3.2891, -1.1650]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2, 27, 27, 10,  7,  3, 18, 27, 27, 27, 27, 27, 10, 27, 27,  9, 26,  7,
        21, 15, 27,  4,  9,  2,  2,  0,  0, 15, 27,  6, 27, 13, 11, 27,  2, 27,
        12,  1, 27, 20, 27,  3, 27,  4, 12,  4, 27, 15, 27,  2, 27, 27, 27, 27,
         0,  0, 27, 11, 25,  3, 19, 22, 10, 27], device='cuda:0')
step: 278
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0831,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7197,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1729,  2.2285, -0.0983,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.6212e-05, -2.2488e-03,  9.8515e-04,  ...,  1.5030e-03,
         -1.8959e-03,  1.7834e-03],
        [-1.3695e-03,  1.4305e-03,  1.4105e-03,  ...,  2.0714e-03,
         -8.7070e-04, -5.5742e-04],
        [-1.9419e-04,  1.5955e-03,  1.4067e-04,  ...,  5.0640e-04,
          8.3256e-04, -3.4428e-04],
        [-1.7643e-04, -2.7943e-04,  1.3485e-03,  ...,  7.5102e-06,
          3.3307e-04,  1.2293e-03],
        [ 1.9169e-04,  9.6202e-05, -1.5450e-04,  ..., -3.0231e-04,
         -2.3222e-04, -5.8699e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9595, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9141, -2.8359, -4.3828,  ..., -5.4766, -4.6328, -1.1016],
        [-3.5645, -3.1738, -4.0195,  ..., -4.6602, -4.2695, -0.5181],
        [-3.3047, -3.1953, -3.5234,  ..., -4.0859, -4.3672, -1.4443],
        ...,
        [-5.3281, -3.5000, -3.7969,  ..., -5.4531, -4.0469, -1.8281],
        [-1.8066, -2.8223, -6.0078,  ..., -6.3555, -5.9648, -4.7461],
        [-3.2930, -4.6680, -4.6055,  ..., -6.0117, -5.6523, -4.3086]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 5, 27, 24,  9, 27, 24,  0, 27, 27, 26, 18, 27,  7,  4, 27, 27, 15,  0,
        15, 27,  1, 15,  4,  8,  3,  2, 15,  1,  0, 27,  2, 18,  3, 27, 24, 27,
        10, 27, 27,  0,  5,  0,  4, 22, 20,  6, 20, 27,  7, 27, 27,  4,  0, 27,
        27,  0, 10, 27, 27,  4, 10,  6, 15, 18], device='cuda:0')
step: 279
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0831,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7197,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1729,  2.2285, -0.0983,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.5115e-05, -5.2738e-04, -1.6737e-04,  ...,  1.1158e-03,
         -8.0681e-04,  4.3273e-04],
        [-2.8944e-04,  1.6146e-03,  3.7146e-04,  ..., -2.1267e-03,
         -1.1730e-03, -4.9734e-04],
        [-2.7478e-05, -5.8937e-04, -2.2697e-04,  ..., -4.5824e-04,
         -2.8801e-04, -3.3808e-04],
        [-3.4857e-04,  5.0974e-04, -1.5841e-03,  ..., -4.1246e-04,
         -1.5366e-04,  1.8415e-03],
        [-5.9891e-04,  8.1015e-04, -2.0754e-04,  ...,  1.8382e-04,
         -2.2697e-04, -1.6880e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9718, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.5195, -3.4727, -4.7227,  ..., -6.6758, -3.2852, -4.1914],
        [-4.8984, -2.6953, -3.3047,  ..., -2.0703, -3.0547, -1.7100],
        [-3.3613, -1.9072, -3.9395,  ..., -5.7500, -4.2969, -1.8291],
        ...,
        [-0.4302, -3.1328, -5.3828,  ..., -6.2109, -5.2734, -2.7891],
        [-4.5625, -3.3125, -3.4219,  ..., -5.7656, -3.5781, -0.4370],
        [-2.4297, -3.9297, -4.0703,  ..., -5.0547, -4.2109, -0.6484]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18,  9, 15, 18, 27,  3, 12, 13,  2, 27, 27, 27,  4,  0, 25,  1,  3,  5,
        16, 27, 15, 27, 27, 27, 18, 15, 17, 10, 27, 27, 27, 16, 25, 25, 27, 18,
         1,  1, 27, 18, 27,  1, 27, 20, 20,  3, 27,  7, 15,  1, 27,  0, 17,  3,
         9, 22,  1, 27, 11, 27, 27,  0, 27, 27], device='cuda:0')
step: 280
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0831,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2974, -1.7197,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1729,  2.2285, -0.0983,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.2006e-04,  4.2343e-04,  6.3944e-04,  ..., -3.8409e-04,
         -7.9489e-04,  2.1183e-04],
        [-1.7226e-04,  1.0252e-03, -6.3896e-04,  ...,  1.7285e-06,
          8.7738e-04, -1.1468e-04],
        [ 9.5367e-05, -4.1223e-04, -1.0996e-03,  ...,  9.5844e-04,
          1.0815e-03, -6.1464e-04],
        [-8.4639e-04, -1.2827e-04,  7.6962e-04,  ...,  2.0361e-04,
         -1.3006e-04,  1.3838e-03],
        [-5.2834e-04,  6.4468e-04,  3.7265e-04,  ...,  1.6236e-04,
         -2.9516e-04,  3.6287e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5604, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.1113, -4.1602, -4.7852,  ..., -5.1758, -3.8301, -1.6123],
        [-5.0195, -2.7559, -2.4746,  ..., -2.9746, -5.2852, -1.1621],
        [-3.8008, -3.0195, -2.7852,  ..., -4.0039, -4.9102, -1.2852],
        ...,
        [-4.1641, -3.3516, -3.4297,  ..., -3.1484, -4.2266, -1.2734],
        [-4.6680, -3.2949, -3.3730,  ..., -2.8730, -4.4023, -1.5449],
        [-6.4492, -4.2617, -3.9336,  ..., -6.1055, -5.5273, -0.2620]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 10, 10, 27,  0, 13,  4, 22,  1, 15, 11, 15, 26, 27,  1,  4,  1,  3,
         0, 15, 27,  2, 27, 11, 10,  5,  9, 27, 25, 18,  3, 11,  2,  3, 20, 26,
         7,  0, 27,  4, 27, 14,  1, 27,  5, 27, 27, 20,  1, 27, 27, 27,  0,  1,
         4, 17, 27,  3,  6,  1,  0, 27, 10, 27], device='cuda:0')
step: 281
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6123,  ...,  0.4785, -0.0831,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4082, -0.2974, -1.7197,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1729,  2.2285, -0.0983,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.0518e-05,  1.8263e-03,  1.1740e-03,  ...,  6.9237e-04,
         -5.5170e-04,  5.8365e-04],
        [-5.7650e-04, -2.0542e-03, -3.6740e-04,  ..., -1.5414e-04,
         -2.2869e-03,  2.2392e-03],
        [-2.4056e-04,  1.0910e-03, -6.5506e-05,  ...,  5.7983e-04,
          4.9114e-04,  3.0971e-04],
        [-2.2638e-04,  6.2275e-04, -8.0585e-04,  ..., -7.2193e-04,
         -1.0557e-03,  1.1997e-03],
        [-1.9002e-04,  5.5075e-04, -5.1451e-04,  ...,  4.8637e-04,
          8.6665e-05, -4.1127e-06]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1078, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.0820, -3.1133, -3.7852,  ..., -6.1133, -5.2852, -4.5195],
        [-5.7500, -3.4219, -4.9844,  ..., -5.4688, -6.2656, -0.3281],
        [-4.6211, -3.3262, -3.1855,  ..., -2.7793, -2.5605, -1.1074],
        ...,
        [-4.4141, -3.2109, -4.5234,  ..., -5.3516, -5.5234, -0.6172],
        [-2.2656, -3.0469, -2.5000,  ..., -2.3281, -4.2344, -2.5781],
        [-6.0195, -3.7402, -3.9434,  ..., -4.5977, -4.9414, -0.3804]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20,  4,  3,  4, 27, 22, 17,  0, 18, 27, 11, 20, 27, 17, 10, 15, 13,  4,
        27, 10, 14, 15, 27, 25,  7,  2, 15, 27, 27, 17, 15, 27, 27, 26, 15,  4,
        18,  0, 17, 15, 27, 22,  1,  0,  6,  3, 27, 18, 27, 20, 12, 27, 12, 20,
        27, 27, 27,  7,  0,  7, 22, 27, 13, 27], device='cuda:0')
step: 282
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0831,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4082, -0.2976, -1.7197,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1729,  2.2285, -0.0983,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.9312e-04,  1.0834e-03,  2.2626e-04,  ..., -5.9271e-04,
          3.5691e-04, -5.0592e-04],
        [-4.2379e-05,  1.3704e-03, -2.3043e-04,  ...,  2.7046e-03,
          1.7185e-03, -8.5831e-04],
        [ 1.9586e-04,  5.9557e-04,  2.2268e-04,  ...,  1.8082e-03,
          2.2182e-03,  1.4353e-03],
        [-1.4591e-04,  5.5361e-04,  1.5330e-04,  ...,  1.0908e-04,
          3.7003e-04,  1.6422e-03],
        [-1.3471e-04, -6.2370e-04,  3.2902e-04,  ..., -4.5657e-04,
         -2.5928e-05,  5.4741e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1455, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.1035, -3.1504, -3.9316,  ..., -6.0117, -4.7773, -2.0879],
        [-4.4883, -3.5195, -4.0195,  ..., -3.2070, -5.7227, -0.6597],
        [-4.3672, -3.1953, -4.2891,  ..., -6.2891, -5.5547, -0.6484],
        ...,
        [-3.6992, -3.3555, -3.5742,  ..., -3.7305, -3.0273, -1.1689],
        [-5.3164, -4.0352, -5.0508,  ..., -6.4727, -7.0195, -0.2864],
        [-3.6172, -2.7578, -3.3828,  ..., -3.2422, -4.7734, -0.9302]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 10, 20,  1, 27, 15, 17,  3, 15, 27, 27,  3, 24, 27,  2, 27,  2, 27,
         0, 27, 18, 27,  0, 13,  5, 27, 27,  1, 27, 18, 18,  5, 27, 18, 26,  4,
        27,  1, 27, 22, 27, 15, 21, 19,  1, 27, 27,  4, 27,  3,  4,  0,  9, 27,
        27, 27, 10,  8, 17, 27,  7, 20, 27,  5], device='cuda:0')
step: 283
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0831,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4082, -0.2976, -1.7197,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1729,  2.2285, -0.0983,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.6657e-05,  4.7064e-04,  1.7824e-03,  ...,  1.1474e-04,
         -2.3389e-04, -3.5357e-04],
        [ 6.5994e-04, -8.5926e-04,  2.7800e-04,  ...,  6.9523e-04,
          1.1235e-04,  1.3599e-03],
        [ 3.3951e-04, -7.0810e-05, -8.9169e-04,  ...,  1.1473e-03,
          8.2684e-04,  5.4407e-04],
        [ 1.5628e-04, -6.8009e-05,  1.3185e-04,  ...,  4.6849e-04,
         -6.8808e-04, -1.7490e-03],
        [ 2.6727e-04, -5.4693e-04,  4.7541e-04,  ...,  3.2783e-05,
         -2.4617e-05, -2.2769e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9319, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.9102, -3.4102, -3.8320,  ..., -5.0977, -5.9102, -0.2537],
        [-4.2695, -3.5957, -3.4551,  ..., -4.1445, -5.5508, -0.5498],
        [-1.9180, -3.0742, -4.9336,  ..., -5.4492, -4.1367, -3.9648],
        ...,
        [-4.7500, -3.2188, -3.0801,  ..., -5.5000, -4.4219, -0.7354],
        [-5.3086, -3.9023, -3.8086,  ..., -6.8086, -5.8711, -0.3088],
        [-4.4023, -2.7305, -3.2617,  ..., -5.1055, -5.3086, -1.0898]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 15,  7, 15, 13, 17, 20, 18, 27,  5, 27, 27, 10, 25, 27,  9, 27,
        17,  1, 24, 15, 14, 13, 27, 27, 17, 27, 27, 15, 27, 27, 27, 11,  2, 22,
        27, 27,  7,  0, 27, 10,  9, 11, 15,  3, 27,  0, 27, 27, 24, 18, 18,  4,
         6, 27, 15,  2,  3, 22, 27, 27, 27, 27], device='cuda:0')
step: 284
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0830,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4082, -0.2976, -1.7197,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1729,  2.2285, -0.0984,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.1160e-05, -2.3174e-03, -2.0862e-06,  ...,  9.7084e-04,
          6.0987e-04, -4.5300e-04],
        [-4.4179e-04, -5.9009e-06, -5.2071e-04,  ..., -3.4523e-03,
         -6.6042e-04,  5.3883e-04],
        [ 1.9288e-04,  6.5327e-04, -1.8835e-04,  ..., -1.4782e-05,
          1.4627e-04, -3.9005e-04],
        [-1.0210e-04, -7.3528e-04,  8.1158e-04,  ...,  7.1812e-04,
          6.3467e-04, -2.4509e-04],
        [ 4.2140e-05, -9.3579e-06,  7.2432e-04,  ...,  2.3365e-04,
          3.0041e-04,  1.4329e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7843, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.1172, -3.6328, -3.8828,  ..., -4.3047, -4.3047, -0.6958],
        [-3.8887, -3.0293, -2.9043,  ..., -3.8887, -3.3262, -1.1230],
        [-0.7661, -3.1250, -4.8906,  ..., -7.4219, -5.1406, -2.9688],
        ...,
        [-4.3945, -3.7402, -1.9277,  ..., -3.3027, -3.7090, -1.9121],
        [-4.3750, -2.9375, -3.1875,  ..., -4.7344, -3.3438, -1.1250],
        [-5.7695, -2.9238, -4.0195,  ..., -4.3477, -4.1758, -0.7529]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 18,  3, 10, 27, 27, 27, 27,  4, 27,  0,  9,  4, 22,  7,  6,  4,
        27,  6, 27,  3,  7,  2, 20, 27,  1, 27, 27, 25, 27, 27,  7, 14, 27, 20,
        14, 27, 18,  4, 27, 27, 27, 27, 18, 15, 27, 27, 27, 25, 15,  0, 18, 27,
        19, 27,  9, 27,  1, 15, 27, 14,  2, 27], device='cuda:0')
step: 285
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0830,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4082, -0.2976, -1.7197,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1729,  2.2285, -0.0984,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.8385e-04, -2.4414e-03,  4.0269e-04,  ...,  4.5300e-06,
         -6.7711e-04, -6.6853e-04],
        [-1.4234e-04,  6.1798e-04,  1.3485e-03,  ...,  3.5515e-03,
          1.7557e-03, -7.6532e-04],
        [-3.9577e-05, -2.6298e-04,  9.1887e-04,  ..., -5.0926e-04,
          1.2655e-03,  1.7047e-05],
        [-8.2374e-05,  4.5538e-04,  9.3997e-05,  ...,  5.0020e-04,
          8.0824e-04,  9.4366e-04],
        [-7.6580e-04,  6.0320e-05,  2.2912e-04,  ...,  1.0133e-06,
         -4.3154e-04,  2.9707e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2812, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.4824, -3.4199, -2.5762,  ..., -3.9668, -3.2793, -0.8423],
        [-2.6055, -2.4648, -3.0117,  ..., -3.0273, -2.3711, -1.6514],
        [-4.9531, -3.5938, -4.4375,  ..., -4.8125, -5.5000, -0.3755],
        ...,
        [-3.4844, -3.3906, -3.8906,  ..., -4.3906, -4.4531, -1.0312],
        [-5.4219, -3.9551, -3.9707,  ..., -3.7988, -4.9688, -0.4546],
        [-5.9336, -3.2930, -3.6523,  ..., -5.7148, -3.1992, -0.7456]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 15, 10, 27, 27, 10, 27, 24, 15,  7,  4, 13, 12, 15, 20, 19, 26, 15,
         5, 27, 18, 18, 25, 22, 11, 20,  8, 27,  7,  0, 18, 26,  4, 26,  0,  6,
         0, 27, 27, 18, 22, 27, 27, 27, 20, 27, 27,  9, 27,  0,  4, 27,  5,  7,
         2, 27, 27, 11, 10, 10, 10, 10,  1,  6], device='cuda:0')
step: 286
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0830,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4082, -0.2976, -1.7197,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1729,  2.2285, -0.0984,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.4318e-04,  2.6684e-03,  3.3989e-03,  ..., -1.3018e-04,
         -9.8896e-04,  2.5291e-03],
        [ 5.4419e-05, -1.8129e-03,  1.1997e-03,  ...,  2.7390e-03,
         -4.4441e-04,  1.2646e-03],
        [-3.9506e-04, -5.6362e-04,  1.0026e-04,  ...,  1.8787e-04,
         -3.9554e-04,  3.7432e-04],
        [ 4.1509e-04,  3.2687e-04,  3.1757e-03,  ...,  7.8869e-04,
          7.3314e-06,  6.1274e-04],
        [-2.1815e-04,  9.5701e-04,  1.1492e-04,  ...,  1.2541e-04,
          3.5477e-04, -4.4584e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0643, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.4438, -2.8965, -4.9453,  ..., -7.2695, -4.2695, -4.0195],
        [-4.6797, -3.1660, -3.4160,  ..., -4.3828, -4.6953, -1.1963],
        [-6.0352, -4.4883, -4.8789,  ..., -6.9648, -6.8555, -0.1302],
        ...,
        [-6.3203, -3.9277, -3.3184,  ..., -3.9590, -5.8672, -0.4124],
        [-5.7812, -3.3926, -4.5312,  ..., -5.1875, -5.3281, -0.2671],
        [-5.2930, -3.8535, -4.4023,  ..., -5.9023, -6.0430, -0.4629]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  8, 27, 20,  0,  0, 13, 15, 27,  3, 16, 27, 20,  4,  5,  0,  4, 25,
        18,  7, 20,  7, 27, 27, 25, 27, 27, 15, 27, 25, 27,  5, 27, 15, 18, 15,
         0,  9,  0, 22, 21,  4, 27, 27, 27, 26, 27, 15, 10, 18, 27, 27, 10,  0,
         2,  9, 11, 27,  4,  4, 10, 27, 27,  8], device='cuda:0')
step: 287
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0829,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4082, -0.2976, -1.7197,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1729,  2.2285, -0.0984,  ...,  0.6567,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.1124e-04,  2.1305e-03,  1.0872e-03,  ..., -1.5030e-03,
          1.2422e-04,  6.9189e-04],
        [ 1.1492e-03, -5.5838e-04,  3.7551e-05,  ...,  3.1910e-03,
          1.0805e-03,  9.9945e-04],
        [ 1.0175e-04, -7.7546e-05,  1.4887e-03,  ...,  4.8280e-04,
          1.5914e-05,  2.3460e-03],
        [-1.8322e-04,  2.6822e-05,  1.8826e-03,  ...,  5.4896e-05,
         -1.3237e-03,  8.4114e-04],
        [ 5.3823e-05, -5.8126e-04,  3.9101e-04,  ...,  7.6234e-05,
          2.4939e-04,  6.4182e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7369, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.1211, -3.6191, -3.6973,  ..., -6.1523, -3.9492, -1.2764],
        [-4.5430, -3.2441, -3.8535,  ..., -3.7598, -4.1523, -0.6353],
        [-2.0723, -3.4629, -5.0586,  ..., -6.1953, -5.2930, -3.4473],
        ...,
        [-4.9961, -2.5898, -3.4023,  ..., -3.5273, -6.0898, -1.2930],
        [-2.5469, -2.3906, -4.2656,  ..., -6.6016, -4.4219, -1.3594],
        [-3.1836, -2.6367, -3.2148,  ..., -1.5107, -3.4180, -3.1367]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 8,  1, 13, 11, 13,  3,  2, 27,  1, 27, 27, 27,  3,  0, 17, 15, 27,  0,
         9, 27, 25,  1, 27, 27, 27,  4, 11, 25, 18, 15, 25, 15,  1,  1,  0, 18,
        27, 24,  3, 15,  1,  0,  0, 27, 15, 27, 13, 27, 15, 15, 17,  0, 14, 27,
        18, 15, 27,  4, 15,  0, 27,  3,  0,  7], device='cuda:0')
step: 288
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0829,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4082, -0.2976, -1.7197,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1729,  2.2285, -0.0984,  ...,  0.6567,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.6907e-04, -2.9540e-04, -4.2057e-04,  ..., -9.6262e-05,
          1.2755e-05,  3.0231e-04],
        [ 1.2655e-03, -1.5736e-03,  2.8086e-04,  ...,  1.5724e-04,
         -1.6909e-03,  5.4693e-04],
        [-2.1744e-04, -5.8937e-04,  2.3067e-04,  ..., -1.9610e-04,
         -1.2512e-03,  9.9659e-04],
        [ 1.4508e-04,  6.3241e-05,  5.7745e-04,  ...,  1.9121e-04,
          5.1212e-04,  2.8777e-04],
        [-2.1398e-05,  2.2817e-04,  3.6860e-04,  ..., -7.7307e-05,
          5.6601e-04,  1.2505e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8091, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.8398, -3.5273, -4.1055,  ..., -6.2461, -6.5586, -0.3704],
        [-5.4844, -3.8906, -4.1562,  ..., -5.3906, -5.7812, -0.3911],
        [-1.8311, -2.1426, -4.8477,  ..., -5.5195, -5.0352, -3.1602],
        ...,
        [-3.2559, -3.3027, -3.6934,  ..., -4.6016, -4.2891, -0.9907],
        [-6.2969, -4.0781, -4.4531,  ..., -5.4844, -4.7031, -0.1710],
        [-2.5234, -3.6016, -3.4141,  ..., -3.3672, -3.7891, -1.3506]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 15, 27,  4, 27, 27, 15, 27, 27, 26,  1, 27, 27, 27, 27, 27,  8,
         1, 22, 27, 20, 27, 27, 10, 27, 10, 27, 27, 27, 27, 27,  0,  3,  4,  8,
         0, 15, 26, 10, 15, 15, 24, 15, 27, 27,  0, 13, 18,  0,  0,  7, 27,  4,
        22, 27, 24,  1, 15, 27, 25, 22,  3,  9], device='cuda:0')
step: 289
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0829,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4082, -0.2976, -1.7197,  ..., -0.6597,  0.6519,  1.2676],
        [-1.1729,  2.2285, -0.0985,  ...,  0.6567,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.7343e-04, -1.4811e-03, -1.0233e-03,  ..., -4.2915e-05,
          3.8147e-04, -8.1205e-04],
        [ 3.4499e-04,  1.4210e-03,  5.6839e-04,  ..., -6.8665e-04,
          1.7285e-04, -2.3022e-03],
        [ 3.1638e-04, -6.1226e-04,  7.8344e-04,  ..., -8.2874e-04,
         -4.7851e-04,  1.2665e-03],
        [ 7.5340e-04, -6.4802e-04, -1.3161e-04,  ..., -2.0051e-04,
         -6.6280e-04, -1.0386e-03],
        [ 6.0177e-04, -1.8167e-03,  2.7466e-04,  ..., -2.5511e-04,
         -1.1373e-04,  2.1815e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3495, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.0000, -2.7324, -4.0312,  ..., -5.7344, -4.2500, -1.2959],
        [-0.9565, -2.5820, -4.3477,  ..., -6.9258, -5.7695, -3.8945],
        [-0.7593, -2.9941, -4.5234,  ..., -6.5078, -4.5547, -2.7441],
        ...,
        [-3.2207, -3.2051, -4.2695,  ..., -3.6113, -4.4727, -1.0957],
        [-4.7656, -3.2637, -4.2656,  ..., -5.4375, -5.5000, -0.4985],
        [-4.5820, -3.6758, -3.9883,  ..., -4.4414, -6.1289, -0.5659]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6,  4,  0, 18,  7, 24, 22, 10, 27, 27, 15,  1, 20, 27,  0, 15, 27, 15,
        18, 27,  0, 27, 27, 27,  7, 27, 22,  0, 24,  1, 18, 15, 10, 27, 10, 25,
        10,  4, 24, 27,  3, 18,  8, 24, 24, 27, 10, 14, 27, 10,  3,  3,  7,  4,
        27, 11, 25, 24,  1, 17, 27,  5,  4,  9], device='cuda:0')
step: 290
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0829,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4082, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0985,  ...,  0.6567,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0019,  0.0004, -0.0013,  ..., -0.0004,  0.0005, -0.0006],
        [ 0.0004, -0.0023, -0.0007,  ...,  0.0052,  0.0030, -0.0004],
        [ 0.0010, -0.0011, -0.0012,  ...,  0.0004,  0.0006, -0.0003],
        [ 0.0012, -0.0002, -0.0009,  ..., -0.0004, -0.0014, -0.0003],
        [ 0.0011,  0.0004, -0.0015,  ...,  0.0004,  0.0005, -0.0006]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(1.9268, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.4980, -2.9512, -4.7344,  ..., -5.1875, -3.5137, -2.5137],
        [-5.5664, -3.1777, -3.9902,  ..., -5.2070, -6.1133, -0.7705],
        [-4.7031, -4.3438, -3.9824,  ..., -4.3438, -5.8281, -0.2798],
        ...,
        [-3.9531, -3.6582, -3.6719,  ..., -6.1719, -3.7520, -0.5322],
        [-4.3242, -4.2461, -4.9805,  ..., -6.3867, -5.2930, -0.3074],
        [-3.0859, -3.3984, -2.7734,  ..., -1.3662, -4.8203, -3.0078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 5, 10, 27,  1,  7, 27,  0, 25, 27, 13,  3,  0, 27,  0, 27, 27, 12, 27,
        18, 27, 27, 10, 27,  7,  6, 27, 21, 27, 27, 27,  3,  4, 27, 15, 27, 27,
        25, 19, 11, 11, 11, 15, 27, 17, 27, 27, 18,  3,  1,  1, 27, 17, 17, 27,
        27,  0, 27, 27, 24, 25,  0, 27, 21, 26], device='cuda:0')
step: 291
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0829,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4082, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0985,  ...,  0.6567,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.5838e-04,  1.4677e-03, -3.0947e-04,  ..., -1.9121e-03,
         -3.6788e-04,  7.3814e-04],
        [-8.2111e-04,  6.5422e-04,  5.8270e-04,  ...,  9.3555e-04,
          1.4105e-03,  2.5892e-04],
        [-8.2445e-04,  8.8871e-05,  8.9788e-04,  ...,  4.9734e-04,
         -3.9124e-04,  3.5226e-05],
        [ 2.9469e-04, -1.2338e-04, -2.2697e-04,  ..., -2.9349e-04,
          3.8767e-04,  1.0481e-03],
        [ 1.7345e-05,  5.5075e-05,  8.7357e-04,  ..., -2.7239e-05,
          1.3113e-04,  8.1682e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1813, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.6406, -4.7969, -5.2812,  ..., -7.3438, -6.8281, -5.0312],
        [-1.2344, -3.4375, -4.6094,  ..., -6.5938, -4.8125, -2.8438],
        [-2.2559, -3.8184, -4.7422,  ..., -6.0703, -3.3652, -1.0059],
        ...,
        [-5.5820, -3.9570, -4.9570,  ..., -6.3320, -6.1758, -0.5352],
        [-1.0264, -3.4492, -4.0586,  ..., -3.3066, -4.9961, -3.5410],
        [-4.5820, -3.2695, -3.7070,  ..., -3.7520, -4.3633, -0.7217]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 18,  4, 18, 22, 26, 27, 13,  4,  7, 27,  7, 10,  7, 25, 25, 27, 27,
         9, 25, 15,  9, 26, 27,  7, 27, 12, 27, 18, 27, 24, 15, 20,  0, 22, 26,
        12,  0, 22, 13,  7,  2, 20,  7, 27,  3, 18, 27,  4, 27,  4, 22, 10, 27,
        17, 20,  2, 15,  1, 27, 20, 27, 13,  9], device='cuda:0')
step: 292
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0829,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4082, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0986,  ...,  0.6567,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.6459e-05,  2.0802e-04,  8.0252e-04,  ..., -1.6046e-04,
         -9.5904e-05,  1.6756e-03],
        [ 2.9469e-04, -1.5078e-03,  1.2684e-03,  ...,  2.1782e-03,
         -3.3712e-04,  1.1702e-03],
        [-3.5346e-05, -1.2655e-03, -2.8763e-03,  ...,  1.5850e-03,
         -1.0881e-03, -2.9106e-03],
        [ 2.0802e-05,  1.8775e-05, -6.2609e-04,  ..., -3.2139e-04,
         -8.8787e-04,  6.2513e-04],
        [-2.5558e-04, -9.1314e-04,  1.4153e-03,  ..., -5.0640e-04,
         -2.5225e-04,  1.7338e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2774, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.0586, -2.9336, -4.6523,  ..., -7.1367, -4.4023, -1.6523],
        [-4.3281, -3.6270, -4.6719,  ..., -6.4531, -5.3281, -0.5796],
        [-2.6914, -2.7383, -4.3789,  ..., -6.7383, -4.4883, -1.9561],
        ...,
        [-4.6445, -2.8301, -4.2383,  ..., -3.7051, -4.1445, -1.0654],
        [-2.3730, -4.0625, -5.2344,  ..., -6.6719, -5.4375, -4.5625],
        [-1.0811, -4.3477, -6.0664,  ..., -7.5820, -5.8164, -5.7070]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 15, 27,  2,  1, 18, 27, 26, 27, 10, 27, 27,  3, 27,  7, 27, 27,
         7, 20,  9, 27,  3, 27, 25, 20, 27,  5, 25, 27, 27, 27, 14, 27, 15,  6,
        27,  3, 27,  5, 18,  0,  0, 27,  7, 26, 18, 27, 27,  7, 17, 27,  0,  7,
        18, 25,  4, 10, 27,  3, 17, 27,  0, 18], device='cuda:0')
step: 293
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0829,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4082, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0986,  ...,  0.6567,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.1819e-04,  3.7098e-04, -4.6802e-04,  ..., -2.8729e-04,
         -9.5546e-05,  3.9959e-04],
        [-1.0071e-03, -2.0599e-03, -1.3943e-03,  ..., -6.8998e-04,
          5.8699e-04,  2.8458e-03],
        [-3.6716e-05,  4.6551e-05,  2.9659e-04,  ..., -7.2908e-04,
         -7.9727e-04,  1.0223e-03],
        [ 1.1930e-03,  8.4817e-05, -5.4693e-04,  ..., -5.0604e-05,
         -1.0481e-03,  2.3842e-07],
        [-6.8188e-05, -4.6539e-04, -1.6379e-04,  ..., -3.5429e-04,
         -5.0688e-04, -2.1517e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1372, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.5811, -2.5195, -3.9414,  ..., -4.2070, -4.0039, -2.7539],
        [-3.7207, -3.7207, -4.4102,  ..., -5.7852, -5.1758, -0.7061],
        [-5.1094, -3.6113, -3.3125,  ..., -4.8125, -5.1406, -0.3599],
        ...,
        [-1.4082, -4.0000, -4.8164,  ..., -6.3320, -5.7227, -4.9414],
        [-5.6602, -5.9844, -6.4375,  ..., -6.7070, -0.0958, -3.2832],
        [-4.3516, -3.5840, -3.6934,  ..., -4.5703, -3.0547, -0.7568]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 26,  7, 23, 20,  7, 17, 27,  9,  2,  7, 10,  4, 26, 27, 10, 27,
         7, 27, 24, 25, 27, 27, 27, 27,  4, 20, 10, 26, 12,  4,  8, 17, 11, 27,
         1,  4,  9,  9, 27,  3, 27,  0, 27, 27, 17,  0,  1,  5, 25, 27, 25, 27,
         9, 11, 27,  4, 27, 18, 20, 18, 22, 27], device='cuda:0')
step: 294
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0829,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4082, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0986,  ...,  0.6567,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.6376e-04, -1.0195e-03, -2.4872e-03,  ...,  3.5167e-04,
          2.6584e-05, -3.1567e-04],
        [-2.9016e-04, -1.4877e-04, -2.2125e-04,  ..., -1.1263e-03,
         -1.9455e-03,  5.5742e-04],
        [ 7.3385e-04, -5.6410e-04,  2.6760e-03,  ..., -5.4407e-04,
         -7.3338e-04, -8.3208e-05],
        [ 1.3504e-03,  1.6510e-04, -6.4421e-04,  ..., -1.0090e-03,
          6.3181e-05, -1.2522e-03],
        [-1.1873e-04, -4.4155e-04, -1.5087e-03,  ..., -8.9645e-04,
          4.7743e-05, -1.5192e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9356, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.0156, -3.7344, -4.2656,  ..., -6.0156, -4.3906, -0.6406],
        [-4.4570, -4.1133, -3.9570,  ..., -3.1133, -3.0977, -0.6606],
        [-2.7656, -3.4844, -4.0000,  ..., -5.6406, -5.8750, -2.2188],
        ...,
        [-2.8379, -2.9336, -3.3867,  ..., -4.7305, -6.1367, -1.2139],
        [-5.2773, -4.7930, -5.2461,  ..., -6.7773, -5.5586, -0.1838],
        [-5.2266, -2.7422, -2.6504,  ..., -4.1797, -3.9297, -0.8833]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6, 27, 27,  6, 10, 26, 25, 27, 27,  7, 27, 27,  8,  4,  1, 27, 27, 18,
        27, 24, 27,  4, 18, 27, 18, 27,  1, 27, 17, 15, 15,  0, 27,  5,  0, 26,
         7, 25, 17, 18, 27, 11, 27, 27, 15, 27, 24, 15, 26,  5, 11,  5, 27,  7,
        27, 15, 27, 27, 27, 17,  1, 27,  7, 19], device='cuda:0')
step: 295
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0829,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0986,  ...,  0.6567,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.8760e-04, -5.9891e-04,  3.9148e-04,  ..., -3.6478e-04,
         -3.2949e-04, -8.2910e-05],
        [ 9.7871e-05,  9.3639e-05, -2.4498e-05,  ..., -3.4332e-05,
          4.5753e-04,  2.7204e-04],
        [-8.0943e-05,  4.4441e-04,  6.7663e-04,  ..., -4.3559e-04,
         -2.1017e-04,  6.7472e-04],
        [ 5.9605e-05,  1.0836e-04,  8.8406e-04,  ..., -2.2173e-05,
         -2.5964e-04, -6.2418e-04],
        [-3.0220e-05, -4.8923e-04,  4.8923e-04,  ...,  4.8614e-04,
          3.5143e-04,  3.4511e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7170, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.7969, -3.7188, -5.0781,  ..., -5.6250, -5.5156, -0.2029],
        [-3.8223, -3.3379, -3.7598,  ..., -3.9941, -3.8848, -1.4160],
        [-0.9448, -3.5859, -5.0703,  ..., -7.0234, -5.6016, -4.8672],
        ...,
        [-3.3418, -3.4824, -3.1699,  ..., -3.5449, -3.8262, -0.6548],
        [-3.0117, -3.5430, -2.5898,  ..., -3.0273, -2.7461, -1.6680],
        [-3.2324, -3.5762, -3.1230,  ..., -6.0000, -6.6211, -1.8418]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  1,  0, 27, 27, 27, 27, 22, 20, 20, 27, 18, 27, 27, 27, 27, 27, 20,
        27, 10, 18, 27, 27, 25,  0, 27,  7, 26, 27, 10, 27, 27, 27, 27, 14, 18,
        27, 27,  7, 27, 18, 15, 27,  3, 15,  5, 27,  7, 27, 22, 18, 22,  0, 20,
         9,  2, 15,  4, 27,  4, 27, 27, 27, 27], device='cuda:0')
step: 296
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0829,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0986,  ...,  0.6567,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.4216e-04, -2.0580e-03,  1.6201e-04,  ..., -3.8719e-04,
         -5.9414e-04, -1.0052e-03],
        [-9.1028e-04,  1.4429e-03, -6.7377e-04,  ...,  1.7481e-03,
          4.8180e-03, -3.7122e-04],
        [-4.1699e-04,  1.1425e-03,  3.2806e-04,  ...,  5.4932e-04,
          9.1648e-04, -9.9659e-04],
        [-2.2507e-04,  2.0385e-04, -9.3508e-04,  ...,  7.9823e-04,
          3.8087e-05,  2.7442e-04],
        [ 8.3804e-05, -3.9053e-04,  8.7643e-04,  ..., -8.8096e-05,
         -2.7084e-04,  5.2834e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.5357, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.2344, -3.5449, -4.1406,  ..., -2.8730, -4.0312, -0.9985],
        [-3.4023, -2.6055, -2.6992,  ..., -3.1211, -4.7617, -1.3086],
        [-3.8672, -3.8965, -4.1328,  ..., -4.6797, -5.1641, -0.5537],
        ...,
        [-3.1836, -2.6992, -2.9336,  ..., -6.7617, -6.4805, -0.8550],
        [-0.9214, -3.5625, -4.4062,  ..., -4.2969, -4.2500, -1.7031],
        [-4.5938, -3.0762, -3.5449,  ..., -5.6055, -4.1875, -0.7480]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 11, 22, 27, 22,  0, 17,  0, 10,  0,  8, 27, 20, 27,  4, 14, 22, 27,
         5,  4, 27,  2, 10,  8, 24, 27,  3, 27, 18,  3, 22,  2,  9,  2, 27,  0,
        11, 27, 27, 27, 15, 18,  4, 20,  7, 25,  2,  2, 10,  3, 27,  1, 10, 27,
         2,  2,  1, 15, 20, 17,  3, 27,  2,  3], device='cuda:0')
step: 297
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0828,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7949,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0986,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.4918e-04, -1.0614e-03,  9.8991e-04,  ..., -1.2374e-04,
         -5.7745e-04,  5.6314e-04],
        [ 7.5102e-04,  1.3971e-04,  7.9298e-04,  ...,  8.4066e-04,
          5.6696e-04, -1.1778e-03],
        [ 1.2960e-03, -1.3514e-03, -2.3162e-04,  ..., -1.9474e-03,
          7.7200e-04,  2.2316e-03],
        [-4.1604e-04,  4.7588e-04,  1.9121e-03,  ..., -3.1650e-05,
         -2.3854e-04, -4.6539e-04],
        [ 2.7871e-04,  9.0313e-04,  1.3471e-04,  ...,  1.9932e-04,
          5.6171e-04, -1.9526e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1600, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.1875, -3.2500, -3.6250,  ..., -3.7969, -2.7812, -0.9067],
        [-4.2422, -3.5684, -3.8184,  ..., -3.6777, -4.1328, -0.9282],
        [-1.3701, -3.3066, -4.6211,  ..., -4.7930, -2.3223, -3.5566],
        ...,
        [-1.8799, -3.6621, -2.7871,  ..., -2.3965, -3.2090, -2.1777],
        [-5.8281, -4.3906, -5.1406,  ..., -5.7812, -4.8906, -0.2639],
        [-1.5068, -3.5059, -4.3359,  ..., -5.8828, -5.5078, -1.6162]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 10, 26, 18, 18,  0, 27, 27, 27, 20, 27, 22,  6, 22,  0,  0, 15,  1,
        27, 27,  3, 25,  2,  2, 17,  4, 27, 20, 15, 20, 27,  0, 25, 27, 20, 27,
        17,  7,  5, 27, 15, 27, 27,  1,  6, 27, 16,  4,  9,  4, 26, 20,  6, 17,
         4, 27, 27, 15,  0,  5,  8,  1, 27, 27], device='cuda:0')
step: 298
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0828,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7944,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0986,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.5749e-04, -1.1148e-03, -2.1601e-04,  ..., -4.8733e-04,
          1.5199e-05,  1.5135e-03],
        [-1.7357e-04,  2.7537e-05,  5.3453e-04,  ...,  2.1267e-03,
          3.0446e-04, -8.3065e-04],
        [ 7.9346e-04,  3.5286e-04,  4.3154e-04,  ..., -4.0512e-03,
         -9.1910e-05,  1.8482e-03],
        [-8.6832e-04, -6.9141e-04, -4.0078e-04,  ..., -1.3142e-03,
          4.7731e-04,  7.9060e-04],
        [-2.6894e-04, -1.6904e-04,  2.1005e-04,  ..., -3.8147e-06,
         -2.0432e-04, -1.4746e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1015, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.0938, -3.5020, -3.8906,  ..., -3.9219, -2.8281, -1.6260],
        [-4.2383, -3.6914, -3.7852,  ..., -2.7383, -5.3008, -1.0820],
        [-1.0645, -3.6426, -4.3945,  ..., -3.7207, -3.2363, -2.7363],
        ...,
        [-4.0547, -3.0391, -4.0859,  ..., -6.0703, -6.9609, -0.7427],
        [-3.0020, -3.0176, -3.6895,  ..., -5.5195, -4.9102, -1.3926],
        [-2.0879, -3.9316, -4.4805,  ..., -5.6836, -5.2305, -4.2773]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  9, 27,  3, 15,  0, 15, 27,  2,  1, 27, 27,  0,  7,  2, 15,  3, 20,
        27, 27, 27,  1,  0,  0, 18, 20, 26,  7, 27,  1,  4, 27, 26, 27, 17,  1,
         7, 27, 27,  5, 15, 27, 10,  2, 17,  5,  7,  4, 27, 25, 17,  0, 20,  0,
        20, 17, 18,  1,  0, 27, 17,  4, 22, 18], device='cuda:0')
step: 299
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0828,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7944,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0986,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.9441e-04, -4.7982e-05, -8.8835e-04,  ..., -5.1856e-06,
          3.6955e-04, -1.2779e-04],
        [-5.0879e-04,  1.0452e-03, -1.3256e-03,  ..., -4.9286e-03,
         -2.1629e-03, -1.2934e-05],
        [ 6.8855e-04,  7.6234e-05, -7.4291e-04,  ...,  6.1750e-04,
          1.4048e-03,  2.6608e-04],
        [ 2.7061e-04, -9.2685e-05, -1.6594e-03,  ...,  2.1291e-04,
         -3.5334e-04, -2.1400e-03],
        [-1.2803e-04, -3.3796e-05, -1.6010e-04,  ..., -2.0361e-04,
         -2.6846e-04,  3.7360e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1487, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9395, -3.4551, -3.7363,  ..., -2.9551, -3.8926, -0.6587],
        [-1.2227, -2.7539, -4.8008,  ..., -6.5664, -3.1445, -3.0195],
        [-2.1230, -2.6699, -3.7949,  ..., -2.8574, -2.6230, -2.1230],
        ...,
        [-3.5332, -2.5801, -2.7988,  ..., -2.6738, -2.7832, -1.5010],
        [-4.8633, -3.5957, -4.4727,  ..., -4.6289, -5.3164, -0.8774],
        [-3.6680, -3.7461, -4.2930,  ..., -1.3408, -5.2305, -3.0273]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10,  0,  1,  0, 15, 27, 27,  3, 18, 27, 27, 25,  3, 20,  0, 27,  0,  1,
        27,  9, 18, 27, 27, 20, 27, 21, 27, 20,  7, 17, 20, 25, 25, 27, 22,  2,
         3, 27, 27, 17,  0, 20,  9, 17,  7,  2,  0, 27,  2,  1,  1, 27, 27, 27,
        23,  1, 26, 17,  2,  1,  3,  1, 10, 10], device='cuda:0')
step: 300
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0828,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7944,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0986,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.8161e-04,  1.0166e-03,  1.4820e-03,  ..., -2.5272e-04,
         -9.3985e-04,  8.3160e-04],
        [-1.2815e-05, -4.2152e-04,  6.1321e-04,  ...,  1.8988e-03,
          5.8293e-05, -8.4043e-05],
        [-6.4909e-05, -4.9496e-04,  5.7983e-04,  ..., -4.0627e-04,
         -4.6825e-04, -2.2101e-04],
        [-3.6144e-04,  8.7678e-05,  2.5177e-03,  ...,  5.6744e-04,
          1.7822e-05,  9.8896e-04],
        [-2.9397e-04,  4.8637e-04,  1.5020e-04,  ...,  6.5994e-04,
         -4.9591e-04, -8.5640e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2271, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.8164, -2.5527, -3.7383,  ..., -5.6133, -3.7383, -0.8643],
        [-2.6250, -3.1406, -4.1719,  ..., -4.1094, -1.2656, -2.2031],
        [-1.9160, -2.5566, -4.5430,  ..., -4.9805, -4.7461, -2.6348],
        ...,
        [-3.5059, -3.4277, -3.9121,  ..., -3.7871, -3.9902, -0.8184],
        [-6.0977, -3.1914, -4.4727,  ..., -4.9727, -5.0508, -0.6924],
        [-4.8086, -4.9023, -4.1680,  ..., -4.0898, -5.3398, -0.2766]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([13, 26, 27, 24,  9, 10,  4,  3, 17, 27, 13, 20, 22,  0, 19, 15,  3, 27,
        27, 10,  9,  9, 11, 27,  5,  0,  5, 25, 10,  9, 15, 27, 15,  4, 20, 27,
         3, 27,  0, 27,  4, 26,  8, 26,  0,  0, 27, 27, 20, 27, 27, 27, 27, 10,
        10,  5, 10, 25, 27,  7, 27, 27, 10, 27], device='cuda:0')
step: 301
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0828,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7944,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0986,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.7891e-03,  9.1171e-04,  2.2960e-04,  ..., -6.8521e-04,
          2.2149e-04, -2.9588e-04],
        [ 1.0405e-03,  5.1403e-04,  1.7643e-03,  ...,  1.3523e-03,
         -8.3876e-04, -1.7910e-03],
        [-4.6313e-05, -8.6880e-04, -1.1053e-03,  ...,  5.6648e-04,
         -9.3222e-04, -1.0939e-03],
        [ 1.3924e-04,  9.2125e-04,  1.0214e-03,  ..., -6.0511e-04,
         -2.9612e-04,  1.2341e-03],
        [-1.4377e-04,  3.8290e-04,  1.4534e-03,  ..., -4.8876e-04,
          7.7426e-05,  1.0481e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1441, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.7344, -3.0156, -3.8750,  ..., -4.1250, -5.8438, -0.6885],
        [-3.2930, -3.6367, -4.3711,  ..., -4.7148, -4.0273, -1.1680],
        [-3.3887, -2.7012, -3.3594,  ..., -1.9678, -4.7344, -1.6396],
        ...,
        [-0.4834, -3.2656, -4.5469,  ..., -5.3750, -3.3281, -3.5781],
        [-6.1719, -3.0938, -3.4844,  ..., -5.0625, -5.2656, -0.8589],
        [-1.9619, -4.3984, -4.5078,  ..., -6.2422, -4.2891, -3.4297]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 27,  2, 27,  3, 10, 10, 20,  3,  5,  4, 15, 27, 27,  9, 26, 10, 13,
         3,  0, 27,  1, 27, 27,  6, 25, 15, 27, 18,  3,  2, 27,  5,  7, 19, 25,
         0, 27, 27, 18, 27,  3, 27, 20, 25, 22, 12, 27,  0, 27,  6, 27, 27,  9,
        20,  3, 20, 27, 10, 15, 10,  0, 27, 18], device='cuda:0')
step: 302
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0828,  1.5117],
        [ 1.2510, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0987,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.4380e-04,  7.0047e-04,  3.2091e-04,  ..., -2.9755e-04,
         -4.3201e-04,  6.9141e-05],
        [-8.9169e-04,  1.6108e-03,  3.3832e-04,  ...,  1.1597e-03,
          1.4172e-03, -9.0265e-04],
        [-4.4298e-04,  3.5310e-04, -8.7380e-05,  ...,  1.5802e-03,
         -2.0385e-04, -1.0633e-03],
        [-2.7227e-04, -6.8069e-05,  5.3596e-04,  ...,  7.2670e-04,
          6.0844e-04,  1.2264e-03],
        [-8.4639e-06,  7.9441e-04,  2.1994e-05,  ..., -1.0329e-04,
         -1.3590e-04, -9.0539e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3425, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.2344, -3.3281, -4.1562,  ..., -4.0000, -3.0938, -2.7656],
        [-5.8047, -4.0391, -3.1797,  ..., -4.1484, -6.3047, -0.3826],
        [-4.9141, -2.5566, -3.5547,  ..., -3.7754, -3.6035, -1.1338],
        ...,
        [-4.4023, -3.6992, -2.9492,  ..., -3.6680, -4.8555, -0.9487],
        [-4.3320, -3.0371, -2.6934,  ..., -2.2402, -4.8320, -1.5996],
        [-2.9258, -3.7695, -4.4570,  ..., -4.2383, -4.5039, -2.1602]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 27,  1,  7, 27,  3, 27,  4,  0, 26, 10, 15, 18, 15, 18, 10, 20, 27,
        21,  7, 19,  2, 18, 27, 27,  9, 27, 27,  3, 27,  6,  1, 24, 27, 27,  4,
         4,  6,  5, 11, 22, 27,  7, 10,  8, 20, 17,  5, 10, 26,  8,  7, 18,  4,
        27, 27,  1, 10, 27,  0, 27,  1,  3, 20], device='cuda:0')
step: 303
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0828,  1.5117],
        [ 1.2520, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0987,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.4511e-05,  2.2793e-03,  1.4896e-03,  ..., -2.6441e-04,
         -1.0900e-03,  2.4164e-04],
        [-8.9824e-05, -7.1144e-04, -1.3800e-03,  ..., -1.1325e-06,
          7.4863e-04,  7.5150e-04],
        [ 3.3593e-04, -2.4843e-04, -2.1315e-04,  ...,  1.4889e-04,
          3.1352e-05, -3.5822e-05],
        [-2.5773e-04,  7.7629e-04,  4.8637e-04,  ..., -1.3959e-04,
         -9.4986e-04,  2.1420e-03],
        [-2.5606e-04,  6.5517e-04, -5.0354e-04,  ...,  7.3862e-04,
         -1.0246e-04, -1.4770e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0837, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.9219, -2.4375, -5.1406,  ..., -6.4219, -5.2344, -2.4844],
        [-0.5049, -3.8633, -4.6602,  ..., -6.4883, -4.5820, -2.7715],
        [-4.0938, -3.2480, -3.2637,  ..., -5.5000, -4.9062, -0.7642],
        ...,
        [-4.1719, -3.1875, -4.5781,  ..., -6.6250, -6.6875, -1.0938],
        [-5.4844, -2.8594, -2.8125,  ..., -2.9375, -3.7207, -1.8291],
        [-2.9043, -3.5293, -3.4199,  ..., -3.2480, -4.2656, -1.0303]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  0, 10, 27, 15, 26, 27,  7, 22, 27, 26, 27, 27,  8, 15, 27,  0,  4,
         5,  1, 10, 18, 27, 18,  5, 10,  4,  2, 27,  7, 27, 27, 27, 27,  4, 27,
         0, 18,  9, 25,  8, 27,  0, 23, 20,  0, 27, 13, 20, 20, 15,  1,  9,  9,
        27, 27, 14,  3, 23, 20, 27, 27, 14, 25], device='cuda:0')
step: 304
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0828,  1.5117],
        [ 1.2520, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0987,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.7146e-04,  1.1759e-03,  8.5771e-05,  ..., -4.6396e-04,
         -4.2844e-04,  4.0746e-04],
        [-2.7132e-04, -1.7939e-03, -6.7806e-04,  ...,  1.7619e-04,
          1.3709e-04,  1.2693e-03],
        [ 4.4036e-04, -6.5851e-04, -9.3222e-05,  ...,  1.0366e-03,
         -2.0289e-04, -9.5987e-04],
        [ 2.4140e-05,  9.3746e-04,  5.6982e-04,  ...,  2.3758e-04,
         -4.1842e-05, -1.1456e-04],
        [-1.1104e-04,  4.5824e-04,  1.6117e-04,  ..., -9.6381e-05,
         -2.6703e-04,  3.2187e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0916, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.4980, -2.7480, -3.8574,  ..., -4.7148, -3.2949, -1.7480],
        [-5.8320, -2.8926, -2.5801,  ..., -4.1406, -5.6289, -0.8770],
        [-2.7363, -3.6426, -4.6094,  ..., -5.4062, -4.0938, -3.2520],
        ...,
        [-5.6211, -1.6992, -2.7930,  ..., -4.7773, -5.4648, -1.4814],
        [-4.9141, -3.3047, -4.0391,  ..., -6.6484, -6.2891, -0.5547],
        [-2.3223, -3.6504, -4.0430,  ..., -4.7461, -4.3398, -1.6670]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 27, 18, 27, 20, 27,  2,  3,  4, 14,  4, 27, 20, 18, 15, 27, 27,  6,
        27, 18, 25, 23,  0,  0, 27,  3,  6,  0, 27, 27,  7, 27, 19, 27, 20,  8,
        27, 10, 10,  9, 17,  9,  4, 11,  4, 10, 15,  1, 27,  8, 27, 27,  2, 15,
        27,  4,  0, 27, 15, 27,  6, 27, 27,  8], device='cuda:0')
step: 305
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0827,  1.5117],
        [ 1.2520, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0987,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.5477e-04, -8.1587e-04,  8.4877e-04,  ...,  6.3181e-05,
          1.2517e-04,  1.2531e-03],
        [-4.8828e-04,  2.2769e-05, -1.9836e-03,  ..., -4.0102e-04,
         -1.3208e-04,  1.0405e-03],
        [ 3.7289e-04, -3.2806e-04,  5.9366e-04,  ..., -1.9789e-04,
          5.4896e-05,  8.9264e-04],
        [ 6.0368e-04,  1.3232e-04, -1.0815e-03,  ..., -2.1553e-04,
         -5.8317e-04, -1.1349e-03],
        [-1.7977e-04,  3.7026e-04,  7.3528e-04,  ..., -3.0994e-05,
          3.9339e-05,  2.2292e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2154, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.6445, -3.2227, -4.4414,  ..., -8.2344, -5.3477, -0.3789],
        [-1.1846, -3.0117, -3.0273,  ..., -3.8867, -3.8555, -1.2002],
        [-4.0273, -3.9336, -4.0742,  ..., -1.4795, -4.0273, -1.2139],
        ...,
        [-7.5625, -4.9688, -5.5469,  ..., -6.6094, -7.2812, -0.0640],
        [-5.6016, -3.7578, -3.8516,  ..., -7.1016, -5.9297, -0.1647],
        [-1.6172, -3.4141, -4.1484,  ..., -4.3203, -3.0078, -2.4297]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7,  0,  3, 27, 27, 11, 27, 26, 27,  5, 27, 27, 27,  2, 17,  3, 27, 27,
        10, 10, 13,  9, 18,  3,  4,  6,  2, 27,  9, 15,  0, 27, 17, 27, 22,  1,
        27, 13, 20, 22,  2, 27,  3, 15,  0, 27, 20, 27,  8,  0,  3, 27,  6, 27,
        17, 27, 17, 27, 26,  7, 10, 27, 27, 13], device='cuda:0')
step: 306
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0827,  1.5117],
        [ 1.2520, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0988,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.5802e-03,  1.6890e-03,  2.1534e-03,  ..., -2.1706e-03,
         -1.0977e-03,  3.6073e-04],
        [ 4.2915e-04,  2.6340e-03, -1.0056e-02,  ...,  1.1894e-02,
          7.1602e-03, -6.8703e-03],
        [ 2.8706e-04, -1.4102e-04,  1.4391e-03,  ..., -1.6708e-03,
         -1.8394e-04,  1.8234e-03],
        [-1.6518e-03, -7.8261e-05, -2.1629e-03,  ..., -9.8228e-04,
          4.9734e-04,  5.5170e-04],
        [-4.5061e-04,  1.0900e-03,  7.8249e-04,  ..., -5.8603e-04,
         -5.5027e-04, -2.1291e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0432, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.0703, -2.8652, -3.0371,  ..., -4.9609, -4.1953, -1.3037],
        [-4.2109, -3.2402, -3.3027,  ..., -5.1953, -5.6797, -1.4277],
        [-4.0898, -3.9961, -5.0273,  ..., -6.8086, -4.5586, -0.6992],
        ...,
        [-2.2988, -2.8301, -3.7520,  ..., -2.6582, -2.5645, -2.3145],
        [-3.9883, -2.0195, -2.8320,  ..., -4.2852, -4.6602, -0.9258],
        [-3.5391, -3.7109, -3.6797,  ..., -3.1641, -3.1797, -2.3516]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6, 10, 27, 22, 17, 27, 27, 27,  0, 27,  2,  6,  2, 27, 10, 27, 10, 27,
         3,  3,  1, 27,  0, 27, 27, 27, 26, 17,  7,  7, 27, 27, 25, 27,  3, 15,
        18, 20, 27, 27, 17, 27, 20, 27,  0,  4,  2,  3, 24, 17, 14, 15, 27, 17,
        27, 20,  3, 27, 15, 27, 27, 18,  9, 17], device='cuda:0')
step: 307
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0826,  1.5117],
        [ 1.2520, -1.8916, -0.2046,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2976, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0988,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.2735e-04, -9.0933e-04,  6.2370e-04,  ...,  2.9469e-04,
         -7.2670e-04,  1.3828e-04],
        [-1.3781e-04,  1.7023e-03, -2.6441e-04,  ...,  4.3654e-04,
          3.3689e-04, -4.8578e-05],
        [-2.6989e-04,  4.7421e-04,  6.6948e-04,  ..., -4.7565e-05,
         -1.6606e-04,  5.3120e-04],
        [ 4.0197e-04, -1.3709e-04, -2.5797e-04,  ...,  3.4165e-04,
         -5.4419e-05, -2.0409e-04],
        [-2.3949e-04, -5.5885e-04,  5.6314e-04,  ..., -2.2602e-04,
         -3.2806e-04,  8.4341e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3192, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.3730, -3.7148, -4.4492,  ..., -5.3711, -4.7461, -4.7305],
        [-3.4160, -3.7598, -4.5430,  ..., -3.4941, -3.9629, -0.6509],
        [-2.0566, -3.0254, -4.3203,  ..., -5.2891, -5.5430, -1.3691],
        ...,
        [-3.3516, -3.0254, -4.0391,  ..., -3.4473, -5.4297, -1.0869],
        [-5.9648, -3.0137, -3.4668,  ..., -5.3711, -3.3105, -0.5132],
        [-5.9102, -3.2051, -2.4707,  ..., -3.2363, -4.8164, -1.0654]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 27,  0, 17, 27,  4,  1, 20,  1, 27, 27,  8,  6, 11, 27, 22,  3, 22,
        15, 27, 27,  0, 20, 17, 27,  0, 17,  0, 25,  0,  4,  8,  2,  0, 20, 27,
        27, 22, 27,  9, 27, 18,  1, 27,  0, 27,  6, 19,  4, 26, 27,  4, 27,  3,
        27, 22,  0, 27, 14,  4,  0,  0,  2, 25], device='cuda:0')
step: 308
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0826,  1.5117],
        [ 1.2520, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0988,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.2578e-04, -2.3067e-04,  9.6130e-04,  ..., -2.4307e-04,
         -1.7095e-04,  1.0878e-04],
        [-3.5572e-04,  6.1321e-04,  8.4734e-04,  ...,  4.3831e-03,
          1.0004e-03, -1.7395e-03],
        [ 3.4761e-04, -2.6846e-04,  1.3866e-03,  ...,  5.8270e-04,
         -2.6536e-04,  9.6226e-04],
        [-3.9005e-04,  1.8859e-04,  1.9894e-03,  ..., -1.6057e-04,
          7.6890e-06,  1.6937e-03],
        [ 3.6788e-04, -5.9652e-04,  6.3229e-04,  ...,  1.6546e-04,
          4.3917e-04,  1.9002e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2499, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.2051, -3.0020, -4.3594,  ..., -5.6094, -3.9707, -3.3770],
        [-5.4961, -3.8691, -4.9961,  ..., -7.8555, -5.9023, -0.5107],
        [-4.3008, -4.0664, -3.9414,  ..., -6.0977, -5.0508, -0.3621],
        ...,
        [-3.4531, -3.8438, -4.1094,  ..., -5.1094, -4.4688, -0.7188],
        [-2.1543, -2.9668, -4.3906,  ..., -6.6875, -4.0938, -3.4824],
        [-3.2871, -3.2559, -4.0703,  ..., -5.1016, -4.0547, -0.8027]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 27, 27, 27, 10,  9,  5,  4,  1, 27, 18, 17, 18, 27,  7, 23, 14,  4,
         0, 27,  7, 18,  5,  0,  4, 27, 19,  0,  2, 27, 17, 18, 27,  3, 15, 27,
        18, 20, 12, 27, 25,  6, 27, 18, 27, 27,  9, 20,  0,  4, 17, 27, 27,  0,
        27, 27, 17,  9,  3, 26,  9, 27, 27, 27], device='cuda:0')
step: 309
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0826,  1.5117],
        [ 1.2520, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0988,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.4714e-04,  1.4830e-04, -7.1144e-04,  ...,  1.0281e-03,
          4.8089e-04,  6.1131e-04],
        [ 5.7650e-04,  1.3561e-03, -6.0081e-04,  ..., -6.9160e-03,
         -2.4490e-03, -1.1677e-04],
        [-7.0035e-05,  7.0620e-04,  7.3385e-04,  ..., -8.8835e-04,
         -1.1826e-03,  2.7490e-04],
        [-2.6965e-04, -2.6107e-04, -1.4019e-03,  ..., -5.9509e-04,
         -5.6934e-04, -4.4441e-04],
        [-3.4809e-04, -1.1654e-03,  8.3745e-05,  ..., -5.2881e-04,
         -4.9496e-04,  8.0681e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1688, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.8691, -3.2910, -3.2598,  ..., -5.6484, -3.4473, -1.8213],
        [-3.0059, -3.7402, -4.0547,  ..., -6.4453, -4.1016, -0.4438],
        [-5.2812, -4.0938, -4.2500,  ..., -0.3433, -5.7656, -3.4980],
        ...,
        [-3.7559, -2.6016, -4.4453,  ..., -5.2109, -5.5703, -1.3506],
        [-2.1875, -2.9844, -2.5938,  ..., -2.3125, -4.7344, -3.6406],
        [-5.8750, -3.8770, -3.2832,  ..., -3.7988, -4.9219, -0.8140]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22,  0, 25, 13, 18, 15,  1, 27, 11, 15, 24,  7,  0, 27, 27,  4,  6,  0,
         4,  2, 14,  5, 27, 27,  0, 11, 15, 20, 12,  7,  3, 15,  7, 27, 27, 27,
         3, 18,  4, 14, 12, 25, 27,  3,  0,  7,  0, 27,  3, 27,  1, 27, 27,  2,
        27, 27,  2,  1, 27, 27, 27, 27, 18, 27], device='cuda:0')
step: 310
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6128,  ...,  0.4785, -0.0826,  1.5117],
        [ 1.2520, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0988,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.2293e-03, -7.7391e-04,  3.6097e-04,  ...,  5.5909e-05,
         -5.9223e-04,  1.4722e-04],
        [ 7.9012e-04, -1.3418e-03, -8.6689e-04,  ..., -3.1872e-03,
         -1.8501e-03,  1.5411e-03],
        [-4.1664e-05,  9.9123e-05,  1.1740e-03,  ...,  9.9087e-04,
          1.1768e-03,  2.2507e-03],
        [-1.7560e-04, -7.7200e-04,  1.0548e-03,  ...,  5.1165e-04,
          1.0414e-03, -6.1893e-04],
        [-9.9599e-05, -3.1424e-04,  8.0347e-05,  ...,  3.7372e-05,
         -5.4836e-06, -1.1784e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7821, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.8828, -3.1660, -4.1953,  ..., -6.5234, -3.9766, -1.2275],
        [-4.1250, -3.3281, -3.7969,  ..., -5.8750, -4.9844, -0.6245],
        [-2.9746, -2.9121, -3.8027,  ..., -4.3828, -3.0840, -2.9121],
        ...,
        [-1.2480, -4.1367, -4.2305,  ..., -6.7656, -4.8867, -3.2480],
        [-2.2754, -3.0723, -4.5547,  ..., -5.6641, -5.0273, -4.1211],
        [-5.5430, -3.8242, -3.5586,  ..., -0.9961, -4.7148, -2.2148]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 15, 27,  3,  1,  3,  9,  2, 27, 27, 15, 27,  0, 27, 10, 27, 27,
         1,  9, 10, 27, 25, 17, 27, 11, 18, 27, 10, 15,  4,  3, 25, 27, 27, 13,
        27,  4, 27, 27, 27,  0, 20, 27, 27, 27, 27,  1,  2, 27,  4,  4, 27, 27,
        27,  0, 27, 27, 27,  0,  0,  8, 15, 25], device='cuda:0')
step: 311
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4785, -0.0826,  1.5117],
        [ 1.2520, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9053, -1.3477, -0.6016],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0989,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.6757e-04,  5.9509e-04,  2.3117e-03,  ...,  3.3998e-04,
         -3.5620e-04,  1.2951e-03],
        [ 4.7743e-05,  4.1389e-04,  1.7843e-03,  ..., -4.3631e-05,
         -1.0071e-03, -7.9107e-04],
        [-1.9407e-04,  9.1648e-04,  1.3189e-03,  ..., -1.0614e-03,
         -5.1737e-04,  1.2112e-03],
        [-9.1648e-04,  2.7359e-05,  1.9760e-03,  ...,  6.3658e-04,
          4.4918e-04,  1.1406e-03],
        [-4.4942e-04,  7.4100e-04,  8.1205e-04,  ...,  3.3903e-04,
         -5.0402e-04,  2.1160e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0936, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.8794, -3.1133, -3.8789,  ..., -5.5352, -4.5508, -3.0195],
        [-4.7383, -2.4727, -2.3477,  ..., -2.8477, -5.5039, -1.5352],
        [-5.9570, -3.1621, -3.8184,  ..., -2.8965, -5.7070, -1.0527],
        ...,
        [-1.7627, -4.7148, -5.3555,  ..., -6.7617, -5.6211, -4.0430],
        [-1.8848, -3.2285, -3.6191,  ..., -3.1191, -4.4023, -3.3691],
        [-4.1016, -3.3809, -3.1152,  ..., -3.9121, -3.0527, -1.5996]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  2, 10, 27, 20, 17, 27,  6, 27, 13, 15, 19, 27,  2, 10, 11, 15, 18,
        18,  5, 18, 24, 14, 27, 14,  7, 15,  7, 27,  0,  5, 20, 27,  1,  7,  2,
        27, 27, 27, 27,  4, 10, 27,  0, 27, 25, 27, 27, 10, 10,  7, 27,  7, 27,
        27, 27, 24,  7, 24, 27,  7, 18, 18, 26], device='cuda:0')
step: 312
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4788, -0.0826,  1.5117],
        [ 1.2520, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0989,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.4796e-04,  2.3687e-04,  2.7895e-05,  ...,  4.9400e-04,
         -2.8276e-04, -7.6151e-04],
        [-3.1376e-04,  1.0166e-03,  3.1996e-04,  ...,  1.7767e-03,
          1.7338e-03, -6.4135e-05],
        [-4.0412e-04,  1.2302e-03,  3.9816e-04,  ...,  7.0667e-04,
         -6.6471e-04,  8.5533e-05],
        [ 7.1669e-04,  2.8324e-04, -1.0967e-03,  ...,  3.9244e-04,
          2.8348e-04,  7.1430e-04],
        [ 6.2943e-05,  4.7970e-04,  2.4891e-04,  ..., -2.0266e-05,
          3.9935e-04,  2.8157e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8307, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.5156, -4.6719, -5.3438,  ..., -6.5625, -5.0625, -4.8906],
        [-3.8555, -3.1836, -4.3555,  ..., -6.4648, -0.9648, -2.4961],
        [-3.1777, -3.0820, -3.3789,  ..., -3.8008, -3.5195, -1.3799],
        ...,
        [-1.4775, -2.8672, -3.9922,  ..., -3.9609, -3.2891, -2.2285],
        [-4.6289, -3.2363, -2.9727,  ..., -3.6289, -4.4570, -0.9873],
        [-5.8750, -3.5332, -4.4062,  ..., -6.7656, -4.5938, -0.5166]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 26, 22,  9, 25, 20,  7, 27, 27, 11,  3, 27, 18,  7, 25, 27, 18, 27,
        27,  3, 15, 27, 18,  0, 27, 27,  9, 15, 22, 11, 17, 27, 27, 27, 27, 11,
        27, 26, 15, 27, 20, 27, 27, 27,  7,  7,  3, 27, 15, 27, 27, 27,  0, 24,
        26, 20, 15, 27,  1, 10, 27,  0, 27,  7], device='cuda:0')
step: 313
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4788, -0.0825,  1.5117],
        [ 1.2520, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0989,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.0909e-04, -3.9744e-04, -7.0095e-04,  ..., -1.4563e-03,
         -4.6945e-04, -1.1522e-04],
        [-1.4591e-03,  1.0681e-03,  2.5024e-03,  ...,  8.4381e-03,
          2.8667e-03, -1.2159e-03],
        [ 5.8174e-04,  1.4639e-04,  4.6349e-04,  ..., -3.9816e-04,
         -2.0516e-04,  9.4652e-04],
        [ 8.4782e-04,  8.0645e-05,  1.6251e-03,  ...,  3.4380e-04,
          1.6081e-04,  5.3835e-04],
        [ 5.4598e-04, -2.0981e-04,  5.0020e-04,  ...,  8.7857e-05,
          2.6512e-04,  3.5191e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0355, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.3477, -3.9434, -3.8965,  ..., -6.5508, -4.5508, -0.2710],
        [-1.1768, -2.8477, -4.9727,  ..., -7.3164, -4.4570, -2.3320],
        [-4.4375, -3.2363, -3.6270,  ..., -4.7383, -4.9102, -0.7520],
        ...,
        [-2.0449, -3.4980, -4.8906,  ..., -5.9375, -4.2031, -1.9043],
        [-3.0742, -2.5742, -3.3086,  ..., -4.5586, -2.2617, -1.6836],
        [-4.9453, -3.9766, -4.6641,  ..., -5.1953, -2.9004, -0.8057]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0, 27,  9, 15,  9, 17, 16, 15, 27, 27,  8, 27, 15, 27, 27, 18,  5,
        27, 13, 26,  7,  1,  0,  4,  0, 27,  6, 27, 27,  2, 27, 27,  4, 27,  0,
         9,  2, 27,  7, 20, 15, 27, 27,  3,  4, 10,  7, 27,  8, 25, 27, 17, 27,
        27,  0, 27, 15,  5, 25, 11, 18,  1, 27], device='cuda:0')
step: 314
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4788, -0.0825,  1.5117],
        [ 1.2520, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0989,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.2687e-04, -3.5620e-04, -9.7036e-04,  ...,  9.7752e-04,
         -4.6432e-05,  6.9666e-04],
        [-3.2008e-05, -4.3631e-04,  3.8338e-04,  ..., -1.7433e-03,
         -2.3594e-03, -2.8849e-04],
        [ 4.5729e-04, -3.5906e-04,  6.4707e-04,  ..., -1.5526e-03,
         -6.8569e-04,  1.4067e-03],
        [ 5.4240e-05, -2.3746e-04, -3.0851e-04,  ..., -5.3024e-04,
          1.3626e-04, -1.6093e-05],
        [-1.1146e-04, -1.2922e-04, -5.4646e-04,  ..., -1.6034e-05,
         -1.1116e-04, -2.2078e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4213, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.0244, -3.7441, -4.8672,  ..., -6.9453, -4.2109, -2.7441],
        [-6.1367, -3.2930, -1.8232,  ..., -3.5742, -4.6055, -1.1670],
        [-1.0146, -3.0918, -5.1406,  ..., -6.2656, -4.5156, -3.7012],
        ...,
        [-3.4980, -2.7793, -3.5605,  ..., -4.5469, -4.6367, -0.7012],
        [-2.9023, -3.1523, -3.0273,  ..., -5.7461, -4.2305, -0.8877],
        [-3.7832, -3.2520, -3.2363,  ..., -5.1758, -5.3164, -0.7681]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 26,  0, 27, 20, 10, 27, 22, 27, 24, 27, 22,  0,  0, 13, 27, 17, 24,
        27, 27, 27, 27, 27, 27, 27, 27, 27,  2, 27,  2, 25, 25,  6, 21,  1, 20,
         3,  0, 14, 23, 27, 27,  2, 23,  9, 18,  1,  0, 27,  5,  0,  1, 27,  3,
        27,  4,  0,  7, 10,  4, 18,  1, 27,  1], device='cuda:0')
step: 315
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4788, -0.0825,  1.5117],
        [ 1.2520, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0990,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.4227e-05,  5.7793e-04, -1.4305e-04,  ...,  1.2121e-03,
          1.8597e-04,  5.5933e-04],
        [ 5.9032e-04, -7.3552e-05,  6.8784e-05,  ...,  8.9467e-05,
         -1.1845e-03,  3.2663e-04],
        [-6.2609e-04,  1.7881e-03,  2.0561e-03,  ..., -1.4963e-03,
         -1.1253e-03,  2.5215e-03],
        [ 9.3985e-04, -4.9925e-04, -1.2350e-04,  ..., -1.4591e-04,
         -1.8907e-04, -5.2404e-04],
        [-4.7147e-05,  1.7095e-04, -8.1158e-04,  ..., -1.5640e-04,
          4.1389e-04,  8.8573e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1185, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.5293, -3.8730, -2.6230,  ..., -4.0156, -2.7793, -1.1543],
        [-2.8281, -3.1719, -2.2969,  ..., -2.2344, -3.4844, -1.7969],
        [-4.4922, -3.7109, -3.6172,  ..., -6.4766, -5.1641, -0.5859],
        ...,
        [-4.9336, -4.2773, -4.9961,  ..., -5.5117, -4.0117, -0.3242],
        [-3.6621, -3.3340, -3.4746,  ..., -3.0215, -2.0684, -1.7705],
        [-4.4180, -3.0273, -3.2930,  ..., -3.9180, -3.3711, -1.3867]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 26, 27,  3, 25,  7, 27,  3, 27,  0, 22, 27, 27,  1, 18,  0, 27,  3,
        10, 27,  1, 27,  0, 26,  0, 27, 20, 17,  2,  0,  4, 15, 10, 27, 15,  9,
        27,  1, 27, 15,  3, 10,  8, 27,  7, 20,  0, 26, 20, 25, 25, 27, 14, 27,
         1,  5, 20,  0, 15, 20, 25, 10, 27,  2], device='cuda:0')
step: 316
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4785, -0.0825,  1.5117],
        [ 1.2520, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9043, -1.3477, -0.6016],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0990,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.0879e-04,  6.8951e-04, -1.7762e-04,  ...,  4.8780e-04,
         -1.4234e-04,  9.7692e-05],
        [ 2.6202e-04,  5.2786e-04, -5.7936e-05,  ...,  1.4234e-04,
         -2.2006e-04, -5.8222e-04],
        [-3.1948e-04, -7.2098e-04,  2.7120e-05,  ...,  5.8222e-04,
          2.8682e-04,  1.9703e-03],
        [-4.0674e-04,  3.5262e-04, -7.1049e-04,  ...,  8.1062e-05,
         -2.3890e-04, -2.9612e-04],
        [-2.2078e-04,  3.1161e-04, -4.3273e-04,  ...,  5.8031e-04,
          2.2900e-04,  7.5459e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4384, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0938, -3.8301, -2.5332,  ..., -1.9385, -2.0469, -1.3760],
        [-0.9238, -3.4395, -4.5312,  ..., -6.6562, -4.0352, -2.4082],
        [-5.2188, -3.3887, -3.7188,  ..., -4.6875, -5.4375, -0.4211],
        ...,
        [-3.2461, -4.0117, -3.4336,  ..., -3.6211, -4.6367, -0.8081],
        [-5.5625, -3.2480, -2.7949,  ..., -3.4531, -5.9844, -0.7959],
        [-2.1465, -3.7559, -4.4922,  ..., -4.8008, -3.2715, -3.0059]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26, 18,  7, 17,  1,  8, 10, 27,  0, 10, 27, 27, 27, 15,  2, 10, 25,  3,
         0, 21, 27, 17,  6, 27, 20, 27, 23,  1, 11,  4, 27, 20, 27,  4,  7,  1,
         1, 10,  4,  5, 10, 11,  0,  0, 10,  0, 20, 11, 27,  7,  1, 18, 10, 27,
        27, 20, 27, 20, 17, 27, 11, 11, 10, 23], device='cuda:0')
step: 317
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4785, -0.0825,  1.5117],
        [ 1.2520, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2202,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0990,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.8314e-04,  2.6932e-03,  7.4005e-04,  ..., -1.3161e-04,
         -2.8348e-04,  2.6417e-04],
        [ 4.2105e-04, -3.0065e-04,  1.2684e-03,  ..., -1.5707e-03,
         -3.1281e-03,  1.1694e-04],
        [-1.0509e-03,  3.8624e-04,  7.9966e-04,  ...,  1.2741e-03,
          2.1648e-04,  6.4969e-05],
        [-1.8299e-05,  6.3038e-04,  3.5048e-04,  ...,  3.7074e-04,
          9.7656e-04,  3.5000e-04],
        [ 1.7202e-04,  4.4441e-04,  1.7571e-04,  ..., -1.9765e-04,
          6.7806e-04,  2.2078e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0507, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3711, -2.3711, -2.3711,  ..., -4.5898, -5.6367, -1.0117],
        [-1.1934, -3.2246, -4.5195,  ..., -6.1914, -4.2383, -2.0215],
        [-2.8789, -3.7539, -4.3008,  ..., -4.8320, -4.2383, -0.7529],
        ...,
        [-2.1016, -4.6328, -4.4766,  ..., -5.5703, -5.3672, -5.1328],
        [-4.9883, -2.9121, -3.7402,  ..., -7.0195, -4.8164, -0.5991],
        [-5.9336, -3.5254, -3.8691,  ..., -4.7773, -6.0234, -0.5728]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  4, 27, 27,  4,  1,  4,  4, 27, 27,  8, 27,  1, 25, 25,  1, 27,  0,
         2,  0, 22, 10,  1, 12,  2, 20, 27,  1, 18, 27,  9, 27, 27, 13, 27, 27,
        24, 27, 27,  0, 27, 20, 15, 27,  0, 10,  1, 15, 27, 27, 27,  3, 27, 27,
         1,  0, 10, 27, 17, 27, 13, 18,  6, 10], device='cuda:0')
step: 318
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4785, -0.0825,  1.5117],
        [ 1.2520, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0990,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.0460e-04, -3.2926e-04,  7.2479e-04,  ...,  7.5150e-04,
         -5.7840e-04,  1.4293e-04],
        [ 3.8719e-04,  1.7633e-03,  7.9727e-04,  ...,  1.8263e-03,
          9.6703e-04, -1.6661e-03],
        [ 4.8661e-04,  1.4620e-03,  8.5449e-04,  ..., -6.6996e-04,
          1.9474e-03,  1.4811e-03],
        [-3.1757e-04,  3.0351e-04,  9.0218e-04,  ..., -1.4353e-04,
          2.7704e-04,  5.6601e-04],
        [-1.4067e-04,  5.6076e-04, -9.3281e-05,  ..., -1.8537e-05,
         -1.3053e-05, -1.1170e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9867, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.5703, -3.5078, -5.1172,  ..., -3.9609, -2.7422, -2.8672],
        [-4.6797, -3.5547, -2.9297,  ..., -3.9453, -3.1641, -1.2422],
        [-4.9336, -3.9648, -2.6367,  ..., -2.7617, -3.9336, -3.4492],
        ...,
        [-4.5469, -3.8574, -3.2480,  ..., -6.4844, -3.9668, -2.7480],
        [-4.2500, -3.7324, -3.7324,  ..., -4.6406, -3.4824, -0.7168],
        [-3.4785, -3.6660, -3.1191,  ..., -4.1641, -3.9473, -0.7124]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 27, 11, 15, 13, 27, 25, 27, 27, 24, 27, 27,  3, 27, 27, 27, 18, 27,
        25, 13,  1,  0,  3, 27, 27, 27,  2, 10, 10,  7,  4, 14, 27,  6,  5, 27,
        27, 27, 14,  0, 17, 22, 20, 22,  0,  2, 10,  1,  2, 27,  4,  4,  7, 25,
        26, 27,  2, 20,  9, 27,  0,  7, 27, 27], device='cuda:0')
step: 319
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4785, -0.0824,  1.5117],
        [ 1.2520, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0990,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.2244e-04, -1.0891e-03, -3.3784e-04,  ...,  5.2786e-04,
         -2.5225e-04, -3.6812e-04],
        [-8.0824e-04,  1.2751e-03, -6.9261e-05,  ..., -2.8038e-03,
         -1.6384e-03, -3.6716e-05],
        [ 2.3198e-04,  2.0576e-04, -5.9319e-04,  ..., -3.2997e-04,
          1.0118e-03, -1.0395e-04],
        [-2.2590e-04,  4.6635e-04,  6.0129e-04,  ...,  1.6809e-04,
         -1.0455e-04,  4.0126e-04],
        [-3.6097e-04,  8.5402e-04,  6.5744e-05,  ..., -2.3317e-04,
          1.4305e-06, -3.8481e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1807, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.4766, -4.4766, -4.4922,  ..., -4.5859, -5.8203, -0.1792],
        [-0.5718, -3.3223, -3.5879,  ..., -5.2422, -3.9473, -3.7129],
        [-5.3633, -5.0508, -5.3477,  ..., -0.3337, -5.8789, -2.0996],
        ...,
        [-4.3242, -4.2617, -4.0742,  ..., -4.9180, -5.7461, -0.4639],
        [-4.3672, -3.5391, -3.7109,  ..., -7.3047, -5.3516, -0.6162],
        [-4.3320, -3.2852, -2.8008,  ..., -1.9414, -4.0195, -1.2227]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 17, 27,  0, 17, 27, 22, 27, 18,  3, 27, 27, 17,  3,  4,  2, 16, 27,
        27, 27,  1, 22, 27, 15,  8,  4, 26, 12, 18, 22,  0, 27, 27, 18,  2, 15,
        24, 27, 18,  9, 27,  1,  3, 27, 27, 27, 27,  4, 27, 15,  5,  7, 18,  1,
        18, 13, 25,  0, 27, 27,  7, 27,  7, 25], device='cuda:0')
step: 320
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4785, -0.0824,  1.5117],
        [ 1.2520, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0990,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0538e-03,  7.5626e-04,  8.7500e-04,  ...,  3.0041e-04,
         -3.9792e-04,  1.7710e-03],
        [-3.5381e-04,  2.1744e-03, -5.5552e-05,  ..., -3.6316e-03,
          4.7779e-04,  6.2799e-04],
        [ 4.4918e-04,  1.0757e-03, -4.3178e-04,  ..., -1.2493e-03,
         -6.1893e-04,  1.9503e-04],
        [-7.5340e-05,  6.6757e-04, -1.5745e-03,  ..., -5.1737e-04,
         -9.4473e-05,  4.3988e-04],
        [-3.3593e-04,  2.7323e-04, -1.8597e-04,  ..., -3.9625e-04,
         -5.2738e-04,  7.1144e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9268, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.2617, -3.6680, -3.7930,  ..., -4.1211, -3.0898, -1.0117],
        [-1.6279, -3.9082, -3.5645,  ..., -4.3633, -3.9707, -0.9243],
        [-1.3115, -3.5625, -5.3438,  ..., -7.4219, -5.6094, -2.9688],
        ...,
        [-1.9639, -3.3223, -3.6348,  ..., -6.5742, -4.8711, -1.1191],
        [-4.3203, -3.7285, -4.5859,  ..., -4.6172, -4.7422, -0.4622],
        [-4.7188, -4.3125, -4.1406,  ..., -6.4375, -6.1562, -0.3113]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0, 18, 20,  7,  4, 15, 27, 27,  9, 27,  0, 27,  9, 27, 25, 20, 27,
         7, 27, 18, 15,  7, 27, 15,  1,  4,  1, 22, 20,  0, 15,  2, 18,  4, 27,
        22, 10,  1, 27, 27,  8, 27, 27, 27, 27, 27, 20, 27, 20, 18,  0, 27, 27,
         1, 27, 18,  1, 27, 27, 27, 27, 27, 27], device='cuda:0')
step: 321
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4785, -0.0824,  1.5117],
        [ 1.2520, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0990,  ...,  0.6572,  0.7061, -1.7979]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.9073e-04, -1.6527e-03,  1.2093e-03,  ...,  5.4181e-05,
         -1.0687e-04, -7.6199e-04],
        [-5.9032e-04,  3.8528e-04,  2.7227e-04,  ..., -1.1034e-03,
         -5.3358e-04,  7.6342e-04],
        [-7.1883e-05,  9.8324e-04,  2.1152e-03,  ...,  8.6308e-04,
         -2.6560e-04,  8.6021e-04],
        [-2.8777e-04,  5.3930e-04,  2.0146e-05,  ...,  2.5821e-04,
         -1.6534e-04,  1.4925e-04],
        [-2.5296e-04, -3.4022e-04,  1.0939e-03,  ...,  1.2851e-04,
         -5.4121e-04,  1.8239e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0141, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.8203, -3.0059, -3.2891,  ..., -4.3203, -4.5859, -1.2568],
        [-5.1953, -3.8672, -3.0859,  ..., -2.4922, -2.8047, -1.5547],
        [-2.9551, -3.4844, -4.0156,  ..., -5.3281, -4.9531, -0.8447],
        ...,
        [-6.0273, -4.1367, -3.7129,  ..., -6.5273, -6.2617, -0.2607],
        [-2.1094, -3.4531, -5.1406,  ..., -4.6250, -3.3594, -4.5781],
        [-1.9922, -3.1641, -3.3203,  ..., -4.4453, -3.8203, -1.7432]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 14, 27, 27,  4,  7, 18, 17,  0,  2,  0, 24, 20, 26,  7, 22,  3, 10,
        27, 20, 27,  4, 17,  1, 27, 27, 27, 26,  9,  2,  0, 27, 27, 27,  7, 10,
        11, 27, 26, 20, 27, 27, 13, 27, 27, 27, 27, 20, 27,  1,  8, 16, 10, 17,
        15, 27, 27, 26, 20, 18, 27, 27, 13,  1], device='cuda:0')
step: 322
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4785, -0.0823,  1.5117],
        [ 1.2520, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2203,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0990,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.3523e-03, -1.9188e-03, -4.5323e-04,  ..., -6.5708e-04,
          5.8472e-05, -5.3787e-04],
        [ 3.0756e-04,  1.1015e-03, -3.8767e-04,  ..., -1.5926e-03,
         -1.5059e-03, -5.7173e-04],
        [-1.5688e-04, -1.9860e-04,  2.9445e-05,  ...,  2.6207e-03,
          9.2459e-04, -1.6747e-03],
        [ 2.1362e-04, -7.1478e-04, -5.2929e-04,  ..., -5.0163e-04,
          3.5942e-05, -1.1196e-03],
        [ 5.1641e-04, -4.9496e-04,  7.5758e-05,  ..., -9.0885e-04,
         -6.8009e-05, -2.4438e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2679, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.1240, -3.3750, -4.1562,  ..., -3.9668, -2.4375, -3.0000],
        [-1.3867, -3.1055, -3.9805,  ..., -5.6836, -3.0742, -4.0273],
        [-3.3711, -3.2305, -2.9648,  ..., -4.2305, -3.4805, -0.7925],
        ...,
        [-4.4102, -2.9727, -2.9102,  ..., -3.1445, -3.6133, -1.2549],
        [-4.7930, -2.7129, -2.7598,  ..., -3.4492, -5.8555, -1.0107],
        [-4.7109, -3.4473, -2.7285,  ..., -3.3066, -4.4922, -0.9160]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26, 20, 10,  1,  0, 15, 10, 13, 27, 27, 15, 20, 27,  3, 19, 17,  8,  9,
        27, 13, 27, 27, 17, 27, 22,  1, 18, 25, 27,  7,  7,  4, 27,  3, 25, 19,
        27, 15, 27,  5, 15, 27, 26,  1,  4,  4,  0, 27,  7,  4, 21, 13, 27, 27,
        27, 15, 27, 27, 22, 22,  2, 10,  5, 27], device='cuda:0')
step: 323
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4785, -0.0823,  1.5117],
        [ 1.2520, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2205,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0990,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.3019e-04, -8.9884e-04, -5.8126e-04,  ..., -5.3692e-04,
         -3.2735e-04,  5.2166e-04],
        [ 1.3399e-04,  3.2735e-04,  1.4582e-03,  ..., -9.3651e-04,
         -2.6588e-03, -4.7326e-04],
        [-3.0613e-04, -3.4142e-04,  1.5163e-03,  ..., -7.7105e-04,
         -3.3736e-04,  9.9850e-04],
        [ 5.7697e-04, -7.7629e-04,  2.4056e-04,  ...,  5.3823e-05,
          4.1056e-04, -4.2379e-05],
        [-2.7955e-05, -1.1325e-04, -7.5340e-05,  ..., -1.3685e-04,
          4.0650e-05, -4.5705e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9789, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.5996, -4.2695, -3.3965,  ..., -5.7383, -4.5508, -0.4424],
        [-2.7012, -3.8105, -4.0156,  ..., -4.2344, -3.7168, -3.2480],
        [-4.8242, -3.4668, -3.6074,  ..., -4.3867, -4.6211, -0.4507],
        ...,
        [-1.7178, -3.4531, -4.4062,  ..., -5.0156, -3.6855, -2.9512],
        [-3.1680, -3.6211, -4.3867,  ..., -4.8086, -4.9180, -4.9023],
        [-0.8369, -3.5254, -4.1797,  ..., -5.9453, -3.2109, -3.7285]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([11, 15, 27,  1, 27, 14, 27, 13,  6, 10,  3, 27, 27, 25, 15, 22, 27, 12,
         3, 25,  8, 27, 27, 26, 27,  7, 27,  5,  0, 20,  0,  8,  0, 27, 27,  4,
        18, 22,  9, 22, 22, 15, 25, 27,  3, 19, 10, 27, 27, 27, 25, 27,  7, 27,
        27, 27, 27, 12, 27,  4, 18, 17, 17, 18], device='cuda:0')
step: 324
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4785, -0.0823,  1.5117],
        [ 1.2520, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2205,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0990,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.5000e-03,  5.7650e-04,  5.6458e-03,  ..., -1.0449e-04,
         -1.1063e-03,  2.7299e-04],
        [ 5.3215e-03, -4.9353e-04, -2.6321e-03,  ...,  5.2185e-03,
         -1.2064e-03,  2.7924e-03],
        [ 2.3174e-03, -7.8440e-05, -1.0729e-03,  ...,  1.2865e-03,
         -9.0027e-04,  3.8624e-04],
        [-6.6757e-04,  9.8038e-04,  3.8052e-03,  ..., -4.9889e-05,
         -6.7568e-04,  2.6226e-03],
        [-5.0211e-04, -6.5565e-07,  1.0586e-03,  ..., -1.2565e-04,
         -4.6611e-04,  4.2391e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9528, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.7090, -2.5840, -4.5508,  ..., -5.4414, -3.9434, -3.2246],
        [-3.7441, -3.3867, -2.7305,  ..., -4.6836, -3.4629, -0.7139],
        [-5.6758, -3.2246, -2.6465,  ..., -4.5195, -4.3320, -0.6309],
        ...,
        [-2.6875, -2.5000, -4.1719,  ..., -5.6406, -5.1250, -3.4688],
        [-4.2383, -3.3184, -2.1465,  ..., -2.8340, -2.1152, -1.9424],
        [-3.8105, -3.2949, -3.7480,  ..., -3.1855, -3.7480, -0.9985]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3,  2, 27, 17, 27, 18,  3, 10, 27, 27, 22,  0, 27, 27, 24,  1, 27, 27,
        10, 26, 18, 15, 27, 15, 15, 27, 27, 27, 27, 27,  7, 27,  7,  8, 27, 27,
         0,  2,  3, 15, 11, 27,  1, 27, 27, 26, 15, 27,  8,  3, 27, 27, 27, 22,
        15,  2, 22,  7, 27,  4,  4, 15,  3, 22], device='cuda:0')
step: 325
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4785, -0.0823,  1.5117],
        [ 1.2520, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2205,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.5783e-04,  1.5287e-03, -1.2112e-03,  ..., -5.9032e-04,
          9.0599e-04, -1.4353e-03],
        [-2.3949e-04,  2.2392e-03,  2.6379e-03,  ...,  1.1902e-03,
         -2.2578e-04, -2.2812e-03],
        [-4.7255e-04,  8.0776e-04,  2.0332e-03,  ...,  1.7703e-05,
         -7.1859e-04, -1.4095e-03],
        [-5.4502e-04,  5.1785e-04, -1.1978e-03,  ..., -1.5855e-04,
         -5.2977e-04,  1.3866e-03],
        [ 7.1526e-04,  8.5831e-04, -3.0470e-04,  ...,  3.0780e-04,
          7.3624e-04, -7.9012e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7451, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4727, -4.0508, -3.3770,  ..., -5.2227, -4.4258, -1.7061],
        [-1.0781, -3.5312, -4.9688,  ..., -6.0781, -3.9531, -3.5781],
        [-1.5381, -4.0859, -5.0234,  ..., -6.3359, -5.7422, -4.5234],
        ...,
        [-1.7002, -2.9492, -3.4648,  ..., -3.0449, -3.8555, -2.6367],
        [-1.7979, -2.4062, -4.2969,  ..., -5.4375, -3.5625, -4.1562],
        [-0.8584, -3.6074, -3.7168,  ..., -5.8906, -2.5449, -3.0625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25,  0, 18, 27, 24, 18, 27, 27, 15, 27, 27,  8, 18, 27, 18, 27,  0, 18,
        26, 27, 18, 15, 27,  3, 27, 27, 27, 27, 25, 23, 14,  0, 18, 27, 27, 27,
        22, 26, 26,  3, 25,  1,  6, 27,  7, 27, 15, 10,  0, 24,  0, 27, 27, 27,
        26, 27, 27, 24,  3, 10, 27, 15, 15,  0], device='cuda:0')
step: 326
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4785, -0.0823,  1.5117],
        [ 1.2520, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2205,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.4673e-04,  1.5533e-04,  1.1959e-03,  ...,  7.2861e-04,
          4.3178e-04, -8.0919e-04],
        [-9.1219e-04,  2.9683e-05, -1.0996e-03,  ..., -1.8549e-03,
          5.6934e-04,  1.4639e-03],
        [ 3.1257e-04, -3.5453e-04, -7.5960e-04,  ..., -2.1553e-03,
          3.5310e-04,  7.1335e-04],
        [-1.1053e-03,  6.9571e-04, -6.9427e-04,  ..., -6.4075e-05,
         -8.4519e-05,  3.2687e-04],
        [-5.5885e-04, -1.4901e-06,  2.3699e-04,  ...,  3.4475e-04,
         -2.5463e-04,  3.7146e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6675, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.2305, -3.6367, -3.0742,  ..., -5.1523, -3.5273, -0.7612],
        [-6.2969, -5.8281, -6.9219,  ..., -8.7188, -5.4062, -3.7656],
        [-1.3076, -3.7910, -4.6367,  ..., -6.0117, -4.7773, -2.5723],
        ...,
        [-2.9082, -3.2969, -4.2500,  ..., -5.9375, -5.2031, -1.3760],
        [-2.1328, -3.1797, -3.9453,  ..., -5.9453, -4.2578, -2.0703],
        [-1.0947, -3.3906, -3.5801,  ..., -5.5156, -3.8438, -2.1113]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22,  7, 18,  1, 26, 18, 27,  4, 27, 17, 18, 25, 27, 27,  0, 24, 27, 15,
        22,  1, 15, 27,  7,  2, 27, 27, 26, 22, 27, 27, 18, 27, 13, 27, 25, 27,
         7, 27, 27, 27,  3,  0,  9, 27,  1,  3,  4,  9,  6, 27,  1, 17, 27, 15,
        27, 27,  4, 27,  7, 27, 27, 10, 27,  0], device='cuda:0')
step: 327
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4785, -0.0823,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2205,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.3154e-04,  8.9884e-04, -3.1900e-04,  ..., -1.9515e-04,
         -4.4107e-04, -9.4032e-04],
        [-6.1607e-04,  9.7370e-04, -3.5763e-04,  ..., -6.9237e-04,
          1.5984e-03,  4.7624e-05],
        [-2.6989e-04,  4.0865e-04, -1.6479e-03,  ...,  1.3781e-03,
          6.1703e-04, -1.4610e-03],
        [-3.3784e-04,  3.5667e-04, -1.0948e-03,  ..., -1.5402e-04,
         -2.4915e-04,  7.7820e-04],
        [-3.3045e-04,  8.4400e-04, -3.2234e-04,  ...,  4.9412e-05,
          5.4300e-05, -5.1022e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7474, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.9639, -3.4785, -4.7773,  ..., -4.9805, -4.2461, -3.0879],
        [-3.2910, -4.9648, -4.9961,  ..., -7.9492, -5.7305, -0.1821],
        [-5.7891, -2.6309, -3.6797,  ..., -5.9453, -2.6641, -1.0693],
        ...,
        [-7.8281, -4.4062, -3.8301,  ..., -5.9688, -6.2344, -0.1263],
        [-0.2632, -3.3730, -6.0273,  ..., -7.9492, -5.4336, -3.6543],
        [-1.8213, -3.4922, -4.1328,  ..., -5.9922, -4.8516, -2.7266]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 27, 27,  7, 20, 27,  0,  7,  7,  0,  1, 27, 27, 10, 27,  1, 13, 27,
        27, 27, 25, 27, 11,  2, 18, 27,  5, 10, 18, 15, 11, 27,  2, 14, 27, 27,
        27,  0, 27, 25, 27, 27, 11, 27,  6, 27,  2,  4, 27,  1, 27,  7, 27, 18,
         4,  7, 27, 15, 27, 27, 17, 10,  0, 18], device='cuda:0')
step: 328
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4785, -0.0822,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2205,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.1567e-04, -1.8187e-03, -8.0061e-04,  ..., -2.1815e-05,
          2.3139e-04,  3.5477e-04],
        [-5.3465e-05, -2.7275e-04,  1.9407e-04,  ..., -1.7576e-03,
         -1.5774e-03,  2.9624e-05],
        [ 4.1676e-04, -9.2411e-04, -8.5592e-04,  ..., -1.3037e-03,
          6.2370e-04,  6.9046e-04],
        [-7.5817e-04, -3.5191e-04,  6.3658e-04,  ...,  2.8348e-04,
          5.4073e-04, -3.0804e-04],
        [ 2.9469e-04,  1.2279e-04, -5.1737e-04,  ...,  5.9366e-05,
          5.6076e-04, -6.2084e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9351, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2656, -3.0312, -3.2344,  ..., -6.7969, -5.1719, -0.7808],
        [-2.9648, -3.7305, -5.0586,  ..., -4.3711, -3.7305, -2.0742],
        [-3.2676, -2.9551, -3.8301,  ..., -4.6094, -5.0312, -0.7979],
        ...,
        [-4.0312, -2.2969, -2.8906,  ..., -3.1875, -4.0781, -0.8589],
        [-1.7764, -3.2285, -4.3086,  ..., -4.6680, -4.5586, -3.3223],
        [-6.7070, -4.0039, -4.2852,  ..., -5.9414, -7.6445, -0.1453]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 13, 10, 27,  5, 22, 15,  6, 17, 24,  4, 27, 27,  0, 25, 15, 27, 20,
        27, 26, 22, 22, 15, 26,  0, 27, 27, 27, 27,  2,  7, 27, 27, 27, 25, 23,
        27,  6,  2, 27, 25, 27, 17,  0, 15,  0, 17, 18, 15, 10, 27,  1, 15,  5,
        27, 15, 27, 13, 27,  3, 27, 10, 20, 27], device='cuda:0')
step: 329
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4785, -0.0822,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2205,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.1410e-04,  1.5154e-03,  8.6308e-04,  ..., -8.1873e-04,
         -8.2314e-05,  1.2093e-03],
        [ 7.3719e-04, -6.3467e-04,  6.5279e-04,  ...,  6.6996e-05,
         -1.2331e-03,  2.9325e-04],
        [ 3.2043e-04,  1.8430e-04,  7.2896e-05,  ..., -2.9969e-04,
          3.7456e-04,  9.4700e-04],
        [ 1.4901e-06, -2.5368e-04,  4.1938e-04,  ...,  2.4438e-06,
          3.3998e-04,  4.1628e-04],
        [ 4.5037e-04,  1.3328e-04, -6.0940e-04,  ...,  1.5569e-04,
          2.8419e-04, -4.1485e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6658, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.5312, -4.1367, -3.6699,  ..., -6.9180, -4.8555, -0.2325],
        [-1.0703, -3.9922, -5.2734,  ..., -7.1172, -5.2109, -5.2578],
        [-4.8555, -3.9805, -5.0273,  ..., -7.8555, -6.1680, -0.3564],
        ...,
        [-2.7324, -3.2793, -3.1699,  ..., -4.1250, -4.6836, -2.1543],
        [-5.7188, -4.3438, -4.8281,  ..., -6.7812, -3.6875, -1.8271],
        [-5.0781, -3.0020, -2.6270,  ..., -4.3281, -5.0156, -0.6577]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 18,  4,  7, 27, 17,  7,  7, 15, 15, 25, 20,  2, 27, 26, 15, 27, 27,
        18, 27, 20, 25, 27, 27,  0, 15, 27, 27, 27,  0,  9, 17, 18, 11, 27, 18,
         1, 27, 10, 17, 24, 27,  0, 27,  3, 25, 27, 27, 27,  1, 27, 20,  1, 25,
        27, 11,  0,  2, 27,  7, 27, 27,  7, 12], device='cuda:0')
step: 330
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6133,  ...,  0.4785, -0.0822,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2205,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2979, -1.7197,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.9140e-03,  2.9922e-05,  1.4801e-03,  ...,  1.1235e-04,
         -7.1955e-04,  1.4668e-03],
        [ 3.4451e-04, -1.7872e-03,  6.8951e-04,  ..., -2.2471e-04,
         -1.0118e-03,  8.6212e-04],
        [-3.4308e-04,  1.2817e-03,  2.0981e-03,  ..., -2.1477e-03,
          3.5882e-04,  2.6665e-03],
        [-9.2983e-04,  5.9938e-04,  1.6661e-03,  ...,  3.3140e-04,
         -5.8556e-04,  3.5810e-04],
        [-1.6725e-04,  2.4629e-04,  4.9210e-04,  ...,  1.3053e-05,
         -3.6454e-04,  3.0136e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9719, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.1582, -4.0156, -4.8164,  ..., -6.6562, -5.4258, -2.7520],
        [-8.5312, -5.2812, -6.2344,  ..., -7.7656, -6.5000, -0.0625],
        [-5.1016, -3.6035, -4.2734,  ..., -4.6484, -4.2109, -0.3220],
        ...,
        [-5.0234, -3.4141, -1.6475,  ..., -1.0693, -4.3984, -3.4590],
        [-4.3477, -3.2207, -3.6270,  ..., -4.2227, -2.9082, -0.8770],
        [-6.1016, -4.1211, -4.1797,  ..., -6.3047, -5.1680, -0.2444]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18,  5, 27,  3, 27,  5,  6, 27, 27, 27, 27, 27, 27,  6, 25, 24,  6, 15,
        18, 27, 10, 27,  4, 26, 20, 18, 15,  4, 10, 10, 25, 27, 15, 27,  1, 27,
        15, 22, 27, 27, 15,  6, 20, 24, 27,  6, 27, 27,  3, 27, 27,  3, 25, 27,
        18,  1, 17, 27, 15,  6, 20, 25,  7, 27], device='cuda:0')
step: 331
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6138,  ...,  0.4785, -0.0822,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2206,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2981, -1.7197,  ..., -0.6597,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.6485e-04, -2.3544e-05,  8.8596e-04,  ...,  1.2293e-03,
         -3.0577e-05,  7.4446e-05],
        [-2.2471e-04, -1.0738e-03,  2.4319e-04,  ..., -3.8300e-03,
         -3.4866e-03,  2.2068e-03],
        [-5.3048e-05,  8.6260e-04,  7.2765e-04,  ...,  1.4639e-04,
         -6.4039e-04,  7.1192e-04],
        [ 5.7697e-04,  3.9911e-04, -1.9121e-03,  ..., -1.2684e-03,
         -2.4533e-04,  6.7854e-04],
        [-3.1281e-04,  8.8930e-04, -3.7837e-04,  ...,  5.4359e-04,
          1.0306e-04, -1.4687e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3782, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.5312, -4.5469, -4.2500,  ..., -6.2031, -5.1562, -0.1549],
        [-5.0820, -2.7871, -3.3164,  ..., -5.4102, -3.9434, -0.6143],
        [-5.3711, -2.8711, -3.5117,  ..., -5.8086, -4.9180, -0.5122],
        ...,
        [-3.0508, -3.2383, -4.6133,  ..., -6.2383, -4.7070, -2.8164],
        [-5.9570, -3.2695, -3.0371,  ..., -4.5664, -3.3965, -0.9736],
        [-4.1406, -2.9219, -2.9688,  ..., -3.9688, -5.6250, -1.2041]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  7, 27, 27, 24,  4, 27, 27,  8, 27,  6, 22, 15, 15, 27,  4, 27,
         0, 26, 13,  3, 17, 27, 27,  5, 27,  0,  8, 22, 22, 27, 11, 10,  0, 27,
        27,  3, 25, 27,  6,  5, 24, 27, 15,  2,  0,  3,  1, 15,  9,  4, 18, 27,
        25, 24, 22, 15, 27,  2, 24, 20,  9, 20], device='cuda:0')
step: 332
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6138,  ...,  0.4785, -0.0822,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2206,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2981, -1.7197,  ..., -0.6597,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.9945e-04,  1.2217e-03,  2.1887e-04,  ..., -8.8930e-04,
         -5.8115e-05,  1.2264e-03],
        [ 2.5606e-04, -8.0490e-04,  1.4758e-04,  ...,  5.8365e-04,
         -5.8651e-04,  4.7112e-04],
        [ 1.1945e-04, -2.0432e-04,  4.4417e-04,  ...,  1.7328e-03,
         -3.3951e-04, -1.9956e-04],
        [ 1.5945e-03,  3.5143e-04, -4.3941e-04,  ..., -1.4937e-04,
         -6.7282e-04, -1.0509e-03],
        [ 4.7398e-04,  4.8351e-04, -3.7599e-04,  ...,  2.2769e-05,
          1.4186e-04,  1.0514e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2469, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6094, -2.7832, -2.2520,  ..., -3.3145, -3.5488, -1.0957],
        [-4.4219, -3.4531, -3.0938,  ..., -3.2188, -3.4844, -0.9067],
        [-4.5508, -3.6602, -4.7852,  ..., -5.0508, -4.3633, -0.4739],
        ...,
        [-5.2539, -3.1914, -2.7227,  ..., -1.8174, -4.7852, -0.9893],
        [-3.0879, -2.8223, -2.4785,  ..., -3.1816, -3.7129, -1.5566],
        [-4.6914, -2.3945, -2.8008,  ..., -3.9883, -3.9570, -1.4414]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([11, 27,  8, 22,  1, 20,  5,  0,  3, 27, 26, 15, 15, 27,  7,  7, 25,  0,
        27, 15, 27, 27, 27,  4, 20, 15, 27, 13,  4, 17, 26,  0, 10, 22, 14, 15,
         1,  9,  3,  2, 17, 27, 24,  4,  7,  2, 15, 17,  0,  1, 25, 25, 15, 22,
        27, 15,  7, 27,  1, 27, 13,  9, 26,  7], device='cuda:0')
step: 333
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2377, -0.6138,  ...,  0.4785, -0.0822,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2206,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2981, -1.7197,  ..., -0.6597,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.4002e-05,  2.1896e-03,  3.9077e-04,  ...,  4.5633e-04,
         -7.8678e-05,  3.6812e-04],
        [ 5.8651e-04, -9.5034e-04,  3.5954e-04,  ...,  1.8187e-03,
          2.8610e-06,  3.3212e-04],
        [-2.6798e-04, -2.3723e-04,  4.0126e-04,  ...,  1.7464e-05,
         -1.5211e-03, -1.8764e-04],
        [ 4.8923e-04, -1.1271e-04, -4.3535e-04,  ...,  1.9288e-04,
         -2.7871e-04, -1.0042e-03],
        [-2.9564e-04,  1.6642e-04, -1.8656e-05,  ...,  9.5189e-05,
          1.8406e-04,  4.9448e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6037, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.4512, -3.2480, -3.5312,  ..., -5.0469, -1.9053, -1.4834],
        [-5.3047, -4.2734, -4.2422,  ..., -6.1484, -5.5859, -0.1630],
        [-5.9023, -4.0273, -4.5586,  ..., -6.9648, -4.4805, -0.6353],
        ...,
        [-4.9258, -2.8301, -4.0977,  ..., -3.9863, -4.8008, -0.6748],
        [-6.8711, -3.5410, -4.1992,  ..., -7.4961, -6.8555, -0.1821],
        [-6.1562, -4.0781, -3.6719,  ..., -5.9531, -4.9688, -0.2651]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26, 27,  6, 27, 26, 27, 27,  1, 27, 27, 27, 14,  7, 11, 27, 27,  7, 20,
        27,  0, 27, 27, 27, 18, 20, 27,  0, 20,  3, 15, 27,  0, 11, 27, 15,  7,
         4, 27, 27,  9,  0, 18, 27, 27,  9, 25, 25, 27, 15,  1, 27, 27,  9, 27,
        27,  7, 15,  1, 27, 27, 27,  3,  6, 10], device='cuda:0')
step: 334
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6138,  ...,  0.4785, -0.0822,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2206,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2981, -1.7197,  ..., -0.6597,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.4005e-04,  1.1492e-03,  1.8806e-03,  ..., -2.4438e-05,
         -9.0456e-04,  1.0414e-03],
        [-4.1890e-04,  8.6927e-04,  8.6641e-04,  ...,  2.0885e-03,
          7.0047e-04, -1.2741e-03],
        [ 1.1826e-04,  2.7299e-04,  3.1304e-04,  ...,  8.4579e-05,
         -2.2531e-05, -8.7500e-05],
        [-2.7037e-04, -2.2471e-05, -1.0395e-04,  ...,  5.9319e-04,
          9.7573e-05,  4.5443e-04],
        [ 3.6001e-05,  9.0885e-04, -5.0843e-05,  ...,  2.4652e-04,
          2.2328e-04,  3.8803e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8567, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.5293, -2.8887, -3.6855,  ..., -4.5430, -3.1855, -2.3887],
        [-3.5176, -3.9707, -4.2070,  ..., -5.6289, -4.8164, -0.7056],
        [-6.4688, -4.3125, -4.0312,  ..., -4.8750, -4.9531, -0.2798],
        ...,
        [-3.5488, -2.9883, -3.5488,  ..., -4.0664, -3.8613, -0.6279],
        [-5.9062, -3.5488, -3.1582,  ..., -5.4375, -5.7969, -0.4546],
        [-6.5586, -3.5410, -3.5098,  ..., -4.4961, -3.9805, -0.5728]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 27, 27, 15, 15, 15,  0, 27, 18,  7, 18, 11, 25, 27, 18,  5, 13, 10,
        27, 27,  0,  5, 11, 27, 14, 25, 27, 27,  1, 25, 27, 27,  3,  9,  2, 27,
        27, 27, 27,  7, 24, 27, 27,  9, 27, 27,  0,  4, 23,  6, 27, 27, 15,  2,
        27, 27,  4, 22, 27, 27, 27,  1, 10,  9], device='cuda:0')
step: 335
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6138,  ...,  0.4785, -0.0822,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2206,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2981, -1.7197,  ..., -0.6597,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.1212e-04,  9.3699e-04,  1.0357e-03,  ..., -6.9261e-05,
         -3.3426e-04,  9.0885e-04],
        [ 4.9925e-04, -5.4836e-05,  1.1420e-04,  ...,  2.2945e-03,
          2.9993e-04,  4.3094e-05],
        [-2.5868e-05, -3.1614e-04, -2.1684e-04,  ...,  1.5497e-03,
         -1.3149e-04,  2.2578e-04],
        [-7.0953e-04,  6.4135e-04,  8.2111e-04,  ..., -9.9778e-05,
         -2.4843e-04,  4.2772e-04],
        [-5.9843e-04,  7.7724e-04,  5.3644e-06,  ...,  1.8120e-04,
         -9.6560e-06,  4.5061e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8544, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.4678, -3.3750, -4.7031,  ..., -6.3125, -2.2637, -2.9199],
        [-5.3281, -2.9219, -2.2031,  ..., -3.0781, -4.9062, -1.2354],
        [-2.6113, -4.1562, -5.1094,  ..., -6.9688, -5.6250, -5.1719],
        ...,
        [-2.1152, -3.3965, -4.6133,  ..., -5.2852, -1.2871, -2.5215],
        [-5.1484, -3.7266, -4.9453,  ..., -8.0391, -7.1016, -0.1169],
        [-4.7188, -3.4375, -3.5781,  ..., -4.4844, -4.5156, -0.3748]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7,  2, 18, 20,  2, 18, 23, 26, 15, 27,  1, 14, 27, 17, 27,  3, 23, 27,
        18, 11, 27, 27,  2, 18, 24,  9, 27, 26, 27,  0,  2, 15, 27, 27, 27,  0,
        27, 27, 25, 18,  6, 27, 22, 17,  7, 27, 10, 27, 26, 27,  0,  3, 27,  0,
        10, 27,  8, 15, 27, 15, 27,  0, 27,  2], device='cuda:0')
step: 336
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6138,  ...,  0.4785, -0.0821,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2206,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2981, -1.7197,  ..., -0.6597,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.1873e-04,  1.0147e-03,  8.5592e-04,  ...,  1.6665e-04,
         -3.6263e-04, -2.4939e-04],
        [-1.8656e-05,  6.2227e-04, -4.0722e-04,  ..., -1.3943e-03,
          2.5570e-05, -7.1049e-04],
        [-5.6124e-04,  1.3180e-03,  9.5415e-04,  ..., -1.6966e-03,
         -6.1417e-04,  1.1396e-03],
        [-5.5790e-04,  3.7503e-04, -9.6416e-04,  ..., -8.7023e-04,
          4.9686e-04, -2.5606e-04],
        [-4.8041e-04,  1.7142e-04, -3.9339e-04,  ...,  2.5630e-06,
          1.2422e-04, -2.2793e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0424, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.7852, -3.4883, -3.5977,  ..., -3.8164, -3.5195, -0.8008],
        [-3.1719, -3.1426, -4.6875,  ..., -5.5156, -4.9688, -3.4844],
        [-1.5654, -2.5645, -4.4570,  ..., -5.7695, -3.1289, -3.8633],
        ...,
        [-1.9199, -2.9668, -4.5469,  ..., -6.3125, -3.6855, -1.7637],
        [-2.6543, -2.8105, -4.2773,  ..., -5.3555, -4.2656, -2.6855],
        [-1.7178, -3.0625, -4.0781,  ..., -5.1250, -4.7500, -2.9688]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 15, 15, 20, 13, 27,  4, 27, 27, 22, 11, 27,  0, 27, 27,  4, 20,  7,
        18, 20, 27, 27,  6, 26, 27, 24, 27, 15, 10, 27,  9,  4, 27,  2,  7, 13,
         0, 27, 27, 27, 27, 10, 15, 18, 25, 24, 11, 15, 18, 27, 18, 20,  6,  1,
        27, 12, 27, 10, 18, 10,  4,  0, 15, 18], device='cuda:0')
step: 337
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6138,  ...,  0.4785, -0.0821,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2206,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4082, -0.2981, -1.7197,  ..., -0.6597,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0519e-03, -1.2360e-03, -5.8746e-04,  ..., -2.0862e-04,
          8.1480e-05,  2.7061e-04],
        [ 9.6846e-04,  2.1875e-04, -8.7142e-05,  ...,  3.6316e-03,
          2.9259e-03, -9.1410e-04],
        [-2.6870e-04,  7.0667e-04,  7.3481e-04,  ...,  7.9775e-04,
         -6.8665e-05,  2.0814e-04],
        [ 1.7996e-03, -6.4039e-04, -1.3866e-03,  ..., -3.0470e-04,
         -1.3518e-04, -1.8063e-03],
        [ 5.2547e-04, -3.5834e-04, -1.3411e-05,  ..., -1.7953e-04,
          1.3351e-05,  2.7084e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1365, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6523, -3.6348, -4.3086,  ..., -4.6055, -3.8242, -0.4485],
        [-5.7266, -2.4141, -2.1797,  ..., -1.2725, -3.3203, -2.6484],
        [-4.5273, -3.4785, -2.5566,  ..., -3.6191, -2.9629, -0.7446],
        ...,
        [-1.8223, -2.3379, -3.6660,  ..., -3.5723, -3.0098, -2.4004],
        [-5.3516, -3.1328, -3.3047,  ..., -5.3203, -4.1484, -0.5537],
        [-5.4766, -4.0078, -3.3672,  ..., -3.5547, -3.6152, -0.4131]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 20, 27, 15, 27,  7, 27, 15,  1,  6,  7, 27, 27, 11, 18, 27, 27, 27,
        27, 25, 13, 26,  2, 27,  4, 14, 27, 14, 22, 27, 22, 18,  7,  3,  3, 27,
         7,  9, 20, 20, 25, 25, 25, 27,  6, 20,  3,  1, 27,  4,  0, 27,  4, 27,
        27, 17, 27, 10,  7, 27, 10, 15, 27, 27], device='cuda:0')
step: 338
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6138,  ...,  0.4785, -0.0820,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2206,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4082, -0.2981, -1.7197,  ..., -0.6597,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1806e-03,  1.3113e-05,  2.2335e-03,  ...,  6.9237e-04,
         -1.0405e-03, -3.6383e-04],
        [ 4.3869e-04, -5.0926e-04, -1.0223e-03,  ..., -9.5654e-04,
          9.2387e-05, -1.5268e-03],
        [-1.5283e-04,  4.9734e-04,  1.6146e-03,  ...,  4.4644e-05,
          1.9002e-04,  2.0714e-03],
        [-4.5061e-04,  7.8869e-04,  1.7414e-03,  ...,  1.8907e-04,
         -5.9605e-04,  1.4944e-03],
        [-4.0078e-04,  2.5082e-04,  9.5844e-04,  ...,  2.6941e-04,
         -4.9353e-05, -2.0206e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9336, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.6250, -3.1074, -3.5605,  ..., -3.7637, -4.0938, -0.8579],
        [-4.2695, -2.5957, -3.0039,  ..., -2.8789, -5.6602, -1.4248],
        [-5.2930, -3.5723, -3.6973,  ..., -5.4336, -2.9941, -0.9790],
        ...,
        [-2.9551, -4.7188, -5.3438,  ..., -7.0938, -6.2344, -5.6562],
        [-6.2188, -3.6875, -3.7969,  ..., -5.6094, -5.1250, -0.5786],
        [-3.7129, -2.6348, -4.4297,  ..., -6.6953, -5.2422, -0.7280]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 11,  2, 27, 20,  6,  9, 10,  4,  8, 27, 27, 15, 15, 11, 27, 26, 25,
        27,  2,  6, 20, 27, 27, 25,  4, 22,  7, 18,  0,  4, 27, 27, 27,  7,  1,
        18, 17, 27, 23, 17, 10, 20, 26, 20, 26, 18, 15, 15, 15, 27,  7, 20, 27,
         5,  6,  0, 22, 27, 22,  3, 18, 27,  4], device='cuda:0')
step: 339
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6138,  ...,  0.4785, -0.0820,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2207,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4082, -0.2981, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.9431e-04, -7.4005e-04,  1.0967e-03,  ...,  4.5896e-04,
         -5.4312e-04, -2.8419e-04],
        [ 4.3678e-04,  5.5313e-04,  1.9588e-03,  ...,  5.5656e-03,
          2.2984e-03, -1.3695e-03],
        [ 3.0422e-04, -6.0749e-04,  7.7677e-04,  ...,  1.8382e-04,
          1.1241e-04, -3.0732e-04],
        [ 7.1573e-04,  5.2452e-05,  4.5633e-04,  ...,  2.0695e-04,
          8.0585e-05,  6.8665e-04],
        [ 1.8048e-04,  1.6212e-04, -6.8367e-05,  ...,  3.6979e-04,
         -2.4414e-04, -1.9193e-04]], device='cuda:0', dtype=torch.float16)
Progress 50.07%, loss: 2.9625685937264388, time 116.03s
loss: tensor(2.3020, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.7773, -3.2012, -3.7480,  ..., -5.6055, -4.9961, -0.2949],
        [-5.4336, -3.2773, -3.9648,  ..., -6.2461, -4.5898, -0.5439],
        [-5.6641, -3.6797, -3.7578,  ..., -4.3203, -4.4922, -0.5239],
        ...,
        [-5.6055, -2.7305, -2.2910,  ..., -3.8398, -4.4648, -1.2607],
        [-4.9102, -3.7383, -3.6758,  ..., -3.5352, -3.5664, -1.0195],
        [-5.3516, -2.8828, -3.7422,  ..., -3.8516, -3.6797, -0.8364]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  4,  3, 11, 27, 13, 27,  0, 27, 15, 27, 25, 20, 15,  6, 20, 16, 27,
         7, 10, 27,  3, 13, 22, 20, 22,  4, 27,  2, 27,  1,  7,  4,  2,  0,  3,
        27, 11, 15, 27,  3, 17, 15, 25, 10, 18,  3, 15,  0, 27, 14, 18,  4,  6,
         3, 18, 27, 27, 27, 26, 27, 27, 11, 27], device='cuda:0')
step: 340
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6138,  ...,  0.4785, -0.0820,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2207,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4082, -0.2981, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.6001e-04, -1.5616e-04, -5.4502e-04,  ...,  6.6459e-05,
         -1.7333e-04,  3.3188e-04],
        [-6.1655e-04, -7.5197e-04,  1.0433e-03,  ..., -1.5745e-03,
         -3.6278e-03,  8.3685e-04],
        [-6.1607e-04,  1.3530e-05,  1.1559e-03,  ...,  2.0790e-03,
         -4.9019e-04, -1.9848e-04],
        [ 2.1839e-04,  4.3774e-04,  8.4400e-04,  ...,  7.3099e-04,
          1.1206e-04,  4.3464e-04],
        [ 1.8537e-05,  4.2105e-04, -3.7432e-04,  ...,  7.1168e-05,
          1.7071e-04, -2.9111e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1802, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.9453, -3.6484, -4.2578,  ..., -5.7422, -6.1797, -0.2883],
        [-4.7773, -2.8555, -3.5742,  ..., -3.1836, -4.2461, -1.1836],
        [-5.0391, -3.5391, -3.6035,  ..., -3.9922, -3.6035, -0.5869],
        ...,
        [-4.4609, -2.6934, -3.0996,  ..., -3.0371, -3.3027, -1.0527],
        [-5.7266, -2.7578, -3.1016,  ..., -3.5859, -3.3047, -1.3359],
        [-6.1250, -4.1562, -4.0508,  ..., -4.5469, -4.6094, -0.4241]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22, 14, 27, 27, 10,  1, 27, 27, 27, 25, 17, 17,  1, 10,  1, 22, 27, 27,
        27, 27, 27, 27, 27, 27, 18,  9, 27, 27,  4, 10,  2, 27, 20,  7, 27,  2,
        23, 27,  0, 20, 15, 15, 18,  7,  0, 27, 27, 10,  3,  1, 26, 27, 24,  4,
        18, 27, 25, 22,  0, 27,  6, 27, 25,  6], device='cuda:0')
step: 341
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6138,  ...,  0.4785, -0.0820,  1.5117],
        [ 1.2510, -1.8916, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2207,  0.7944,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2981, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.0472e-05,  1.2994e-04,  1.2722e-03,  ..., -1.1520e-03,
         -1.1320e-03,  2.2471e-05],
        [-6.3276e-04, -4.2844e-04, -4.5156e-04,  ...,  2.5558e-03,
          3.0289e-03,  1.4191e-03],
        [-3.5763e-07,  8.6641e-04, -2.7180e-04,  ...,  8.4734e-04,
          4.1652e-04, -3.8028e-04],
        [-6.0225e-04,  5.4359e-04,  9.4843e-04,  ..., -5.6744e-04,
         -6.8378e-04,  1.2131e-03],
        [ 5.0128e-05,  1.1520e-03,  2.2089e-04,  ...,  4.1175e-04,
          2.8181e-04, -1.8406e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9159, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.2109, -3.1504, -3.9785,  ..., -5.1484, -3.4004, -0.8843],
        [-2.1777, -2.2871, -4.5977,  ..., -5.9727, -4.1914, -1.7236],
        [-6.2695, -3.2227, -2.1777,  ..., -2.9570, -5.6445, -2.2539],
        ...,
        [-5.6445, -3.4863, -3.7676,  ..., -6.2695, -4.2070, -0.6279],
        [-5.4219, -3.0605, -3.2637,  ..., -5.6094, -3.4668, -1.2646],
        [-3.4648, -2.8730, -3.5605,  ..., -2.9512, -3.2012, -1.6064]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22,  0, 25, 22, 26, 27, 26,  1,  9, 27, 27,  0, 27,  2,  3, 11,  3,  6,
        18, 25,  6,  9,  4, 27, 15, 27, 15, 11, 27,  0, 27,  7, 27, 27, 27,  4,
         2, 27, 27, 10, 27, 10,  3, 10, 22,  0, 26,  7,  2,  7, 11, 27,  6,  0,
        27, 15,  2,  7, 24, 27, 15,  7, 27,  1], device='cuda:0')
step: 342
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6143,  ...,  0.4785, -0.0819,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2207,  0.7939,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.5173e-03,  1.4620e-03, -5.4598e-04,  ..., -3.0756e-04,
         -2.3758e-04,  5.1880e-04],
        [ 3.3212e-04,  1.6212e-04, -1.5068e-04,  ...,  2.6035e-03,
          1.1988e-03,  9.8419e-04],
        [ 4.8399e-04,  1.3340e-04, -1.6737e-04,  ...,  1.7567e-03,
          7.2002e-05,  5.2261e-04],
        [ 8.9264e-04, -3.0279e-04, -5.0545e-04,  ...,  1.8191e-04,
         -9.9945e-04, -7.0333e-04],
        [ 7.4530e-04,  7.5960e-04, -3.5691e-04,  ...,  5.8985e-04,
          3.5739e-04,  5.8222e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2940, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3789, -2.8301, -2.6113,  ..., -3.7207, -2.3770, -1.3301],
        [-4.9336, -3.2012, -3.5918,  ..., -3.4355, -5.6680, -0.9194],
        [-4.8750, -3.6855, -4.1719,  ..., -5.8906, -4.5625, -0.7642],
        ...,
        [-3.9121, -3.0371, -3.8809,  ..., -1.2393, -4.2539, -2.3965],
        [-3.0703, -3.1641, -3.1797,  ..., -2.1172, -3.5703, -2.6641],
        [-6.8516, -3.6953, -3.8047,  ..., -5.1328, -3.3984, -2.8047]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  7, 15,  1,  4, 27, 26,  6, 27,  3,  1, 27, 25, 27,  1,  4, 27,
        27,  3, 10,  4, 26,  2, 27, 22, 18, 22, 27, 27, 22,  1, 11, 27,  4, 14,
         0,  4,  3, 25, 22, 20,  3,  2,  1, 27, 27, 20, 18,  0, 27, 17, 27, 11,
         5,  7, 27, 27, 27,  2, 25,  9, 27,  7], device='cuda:0')
step: 343
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6143,  ...,  0.4785, -0.0819,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2207,  0.7939,  ..., -1.9043, -1.3477, -0.6021],
        [-0.4084, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.7813e-04,  1.0004e-03,  4.4203e-04,  ...,  1.1797e-03,
          7.5245e-04, -6.7377e-04],
        [ 2.2101e-04, -1.0757e-03, -3.8242e-04,  ..., -2.1801e-03,
         -1.1473e-03, -1.9562e-04],
        [ 1.9574e-04,  1.8930e-04,  5.8532e-05,  ..., -1.2131e-03,
         -3.1853e-04,  6.1512e-04],
        [-4.0197e-04, -9.3222e-05, -6.0081e-04,  ...,  1.2684e-04,
         -4.0412e-05, -3.8218e-04],
        [-3.0851e-04, -1.0452e-03,  6.2943e-04,  ..., -3.2425e-05,
         -2.6941e-04,  6.3896e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7458, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.4062, -4.1406, -4.5938,  ..., -6.7188, -5.5625, -0.2351],
        [-1.0947, -3.2676, -3.7363,  ..., -5.3750, -3.3906, -3.0801],
        [-3.2207, -3.6113, -4.8789,  ..., -6.2852, -3.7676, -0.9712],
        ...,
        [-3.2520, -3.6582, -6.3008,  ..., -5.9102, -4.4727, -4.6758],
        [-4.4023, -2.9805, -3.6680,  ..., -7.7773, -5.0742, -0.8252],
        [-4.6094, -2.7793, -4.1562,  ..., -4.7344, -3.9355, -0.7329]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0, 27,  5, 18, 27, 26, 18,  9, 15, 24, 27, 15,  0, 22, 27, 20, 27,
        27, 27, 27,  7,  6, 27, 27, 15, 27, 24, 27, 25, 13,  9, 20, 27, 27,  3,
         9,  3, 27, 27,  0, 27,  3, 10, 27,  8, 10, 17,  1, 22,  4, 27, 11,  7,
        15,  3, 11, 27, 25,  5, 27, 15, 27, 27], device='cuda:0')
step: 344
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6143,  ...,  0.4785, -0.0819,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2207,  0.7939,  ..., -1.9043, -1.3477, -0.6025],
        [-0.4084, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.1076e-04,  4.8494e-04,  9.5797e-04,  ...,  3.4976e-04,
         -6.9141e-04, -7.0953e-04],
        [-1.4486e-03,  9.6941e-04,  5.0116e-04,  ...,  2.5330e-03,
          9.1076e-04, -6.9189e-04],
        [-2.4223e-04,  8.3447e-04, -3.1519e-04,  ...,  4.8018e-04,
          7.3767e-04, -2.8229e-04],
        [-8.8358e-04,  1.1463e-03, -8.3065e-04,  ...,  2.5809e-05,
         -4.9877e-04,  5.9223e-04],
        [-4.0698e-04,  6.1846e-04,  1.7643e-04,  ...,  4.1866e-04,
         -1.5795e-05,  3.0327e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2166, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.1719, -3.5488, -4.6094,  ..., -6.4219, -4.0352, -0.5796],
        [-3.0195, -4.0352, -4.8320,  ..., -5.8164, -4.2539, -2.3320],
        [-5.2109, -2.3672, -3.3359,  ..., -1.7266, -5.1484, -2.8828],
        ...,
        [-4.3750, -1.5322, -2.4707,  ..., -1.6885, -4.7188, -2.4844],
        [-5.7188, -3.5312, -3.8125,  ..., -4.4219, -3.7812, -0.5625],
        [-5.2227, -2.0977, -3.0664,  ..., -3.6133, -3.8164, -0.9731]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  2,  8, 27, 27, 27, 22, 18, 11, 27,  7, 10,  7, 25, 22, 22, 22,
        20, 27,  2, 27, 27, 14,  0,  3,  9, 27, 27, 14,  0, 27, 13, 15, 10, 27,
        20, 27, 27, 27, 27,  8,  2, 27,  0,  9, 16,  7, 18, 27,  2, 27,  4, 27,
        27, 26, 26,  3, 27,  0, 11, 11, 27, 27], device='cuda:0')
step: 345
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6143,  ...,  0.4785, -0.0818,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2207,  0.7939,  ..., -1.9043, -1.3477, -0.6025],
        [-0.4084, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6572,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1683e-03, -2.6226e-04,  1.3180e-03,  ..., -3.0184e-04,
         -6.6662e-04,  6.4659e-04],
        [-5.2309e-04,  8.5068e-04, -7.7343e-04,  ..., -9.7847e-04,
          4.2391e-04, -5.4693e-04],
        [ 1.7965e-04,  6.1750e-04, -6.2180e-04,  ..., -4.3821e-04,
          5.6267e-04, -8.6021e-04],
        [-9.5463e-04,  2.0766e-04,  1.9312e-03,  ..., -1.3089e-04,
          1.9443e-04,  2.0332e-03],
        [-3.2139e-04,  2.0123e-04,  6.8903e-04,  ...,  2.0981e-05,
         -4.4990e-04,  1.6022e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1775, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.9375, -3.2207, -2.5645,  ..., -2.5645, -3.3613, -3.3770],
        [-3.5508, -2.9102, -3.2383,  ..., -2.8789, -2.2383, -1.6455],
        [-6.1602, -3.0801, -4.3945,  ..., -5.8008, -5.4570, -0.7681],
        ...,
        [-4.8047, -3.2715, -4.0703,  ..., -5.2266, -4.1641, -1.1006],
        [-3.3164, -3.0664, -2.2852,  ..., -2.4727, -3.4102, -1.3643],
        [-2.5996, -2.7871, -4.2383,  ..., -5.3984, -3.9590, -1.4902]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([14,  1, 10,  7,  4,  1, 27, 22, 24, 10,  0, 27, 27,  3, 27,  1, 27, 15,
        10, 20, 17,  3,  4, 27,  0, 26, 15,  4, 10, 27,  7, 27, 26, 10, 15,  4,
        25, 23,  9,  0, 13, 10,  4, 27, 27, 27, 27, 26,  2, 27, 27, 27, 27, 27,
         3,  7, 27, 27,  4,  1, 15,  6,  5, 27], device='cuda:0')
step: 346
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6143,  ...,  0.4785, -0.0818,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2207,  0.7939,  ..., -1.9043, -1.3477, -0.6025],
        [-0.4084, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.4543e-04, -1.0548e-03,  5.4121e-04,  ...,  1.2274e-03,
         -6.2990e-04,  1.1182e-04],
        [-5.1546e-04, -1.2026e-03, -5.2404e-04,  ..., -1.7118e-04,
          6.1178e-04,  6.0129e-04],
        [ 6.5041e-04, -4.8232e-04, -2.5392e-05,  ..., -1.2341e-03,
          7.5817e-04,  1.7490e-03],
        [-9.3937e-04,  4.9210e-04,  1.2636e-03,  ..., -2.8110e-04,
         -4.6349e-04,  1.7853e-03],
        [-1.3053e-04,  3.2663e-04,  6.2108e-05,  ...,  1.4210e-04,
         -2.8181e-04, -6.2585e-06]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8554, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.8467, -2.8613, -4.6875,  ..., -5.3906, -4.6289, -4.6875],
        [-4.0156, -3.3281, -5.7656,  ..., -5.9062, -6.3750, -5.3906],
        [-1.0918, -2.9668, -4.9062,  ..., -5.8906, -4.2500, -1.9678],
        ...,
        [-1.1738, -3.6113, -4.2500,  ..., -5.1875, -5.2344, -1.6895],
        [-5.4609, -2.9453, -3.6953,  ..., -3.9609, -4.3047, -1.4766],
        [-2.9434, -3.3027, -5.7227,  ..., -7.1445, -5.3320, -5.1172]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15, 15,  0,  8,  0, 27, 25,  2, 27, 13, 27, 24,  4, 17, 27, 12, 20, 27,
        26, 27,  9,  0,  4,  4,  8, 27, 18, 27, 20, 10,  0,  4,  7,  4, 27, 20,
         4, 11, 27,  0,  4,  0, 27, 13, 10,  7, 27, 18, 27, 18, 15,  0, 10, 15,
        15,  7, 27, 27, 27,  4,  7, 27, 27, 15], device='cuda:0')
step: 347
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6143,  ...,  0.4785, -0.0818,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2208,  0.7939,  ..., -1.9043, -1.3477, -0.6025],
        [-0.4084, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.3896e-04, -8.6880e-04, -6.0415e-04,  ...,  4.0293e-04,
         -2.0552e-04, -9.2840e-04],
        [ 7.8917e-04,  1.1635e-03,  1.7433e-03,  ...,  1.1778e-03,
         -7.6294e-04, -1.2579e-03],
        [-2.4962e-04, -1.9026e-04,  6.3133e-04,  ...,  1.1806e-03,
         -8.6832e-04,  1.4615e-04],
        [-2.2566e-04,  3.1662e-04, -8.1253e-04,  ..., -4.1199e-04,
          2.1458e-06,  3.2783e-04],
        [ 1.5569e-04, -3.6407e-04, -4.2915e-05,  ..., -2.3818e-04,
         -1.0669e-04,  2.1195e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0020, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.4297, -3.2891, -4.2422,  ..., -3.4766, -1.6484, -2.7266],
        [-5.9570, -3.0215, -3.7695,  ..., -7.0273, -4.9570, -0.5049],
        [-5.8516, -2.9785, -3.6191,  ..., -5.1484, -3.6973, -1.3848],
        ...,
        [-4.8984, -3.3496, -3.7871,  ..., -5.7578, -4.3984, -0.7100],
        [-4.5859, -3.9297, -3.5996,  ..., -3.9141, -3.8965, -0.6162],
        [-4.1875, -3.7500, -3.3281,  ..., -4.0781, -3.7656, -1.7178]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26, 27,  6, 18, 17,  1,  9, 27, 18, 27, 12,  7, 27,  4,  0,  1, 15, 20,
        15,  9, 25,  6,  5, 27, 27,  3,  4, 26, 27, 27, 15, 20, 10,  0, 26,  1,
         6, 27, 27, 15, 27,  0,  9, 27, 27,  1, 27, 27, 27, 26, 27,  5, 18, 22,
         4, 27, 10,  3, 27, 22, 27,  1,  3, 27], device='cuda:0')
step: 348
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6143,  ...,  0.4785, -0.0818,  1.5117],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2208,  0.7939,  ..., -1.9043, -1.3477, -0.6025],
        [-0.4084, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.8222e-04, -1.2445e-03, -5.2452e-04,  ..., -1.1272e-03,
          3.5822e-05,  5.7840e-04],
        [ 4.1127e-04,  5.2261e-04, -4.3488e-04,  ..., -3.2592e-04,
         -1.6165e-04, -1.2665e-03],
        [ 9.5308e-05,  9.0504e-04,  4.0603e-04,  ..., -1.4782e-03,
          7.5483e-04,  1.4133e-03],
        [ 1.3268e-04, -1.0586e-03, -4.2534e-04,  ..., -5.9557e-04,
         -4.1914e-04, -1.6651e-03],
        [ 2.6417e-04, -3.6812e-04, -4.8113e-04,  ..., -3.7241e-04,
         -1.5187e-04, -2.7442e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1822, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.7070, -3.0664, -3.4551,  ..., -4.0508, -4.6758, -1.1748],
        [-5.8164, -2.3477, -2.8633,  ..., -1.2998, -4.3945, -2.9258],
        [-4.4844, -3.5781, -2.8438,  ..., -3.8301, -4.0312, -0.5786],
        ...,
        [-7.1914, -3.9863, -4.3164,  ..., -5.3477, -5.3320, -0.2369],
        [-3.9785, -3.1816, -4.2266,  ..., -6.7266, -5.0391, -0.5557],
        [-4.9531, -2.1094, -3.1738,  ..., -2.1562, -3.6719, -1.7979]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 24,  4, 27, 27, 27, 27, 11,  1, 10, 20, 27, 27, 27, 17, 27, 15, 27,
        27,  3, 25, 15, 12, 13,  7, 17, 27,  5, 15, 27,  4, 27, 22,  5, 14, 22,
        23, 15, 27, 27, 20, 27,  3, 10, 27, 27, 27, 27, 17,  3, 15,  9,  2, 27,
         3, 27, 17, 20, 27, 27,  0, 27, 27,  2], device='cuda:0')
step: 349
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6143,  ...,  0.4785, -0.0818,  1.5107],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2208,  0.7939,  ..., -1.9043, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.3515e-04, -1.8196e-03, -8.8739e-04,  ..., -1.0796e-03,
         -5.3215e-04,  1.5688e-04],
        [-1.2865e-03,  1.7443e-03,  9.1362e-04,  ..., -2.9678e-03,
         -2.6512e-03, -3.2258e-04],
        [-7.2718e-04,  1.2503e-03,  2.7680e-04,  ...,  2.7752e-03,
          1.4076e-03, -1.4820e-03],
        [-6.7711e-04, -5.6648e-04, -2.9778e-04,  ..., -3.3331e-04,
          8.9598e-04, -8.3303e-04],
        [-3.1614e-04, -6.0678e-05, -3.8600e-04,  ..., -5.9605e-05,
         -1.4031e-04, -5.8317e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0058, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.7930, -2.9980, -3.2168,  ..., -3.8730, -4.6523, -1.0918],
        [-5.0234, -2.7734, -3.1172,  ..., -2.9453, -3.2578, -1.4453],
        [-7.7500, -6.4375, -6.1875,  ..., -3.7637, -6.0469, -4.5938],
        ...,
        [-5.4375, -4.0781, -4.3125,  ..., -5.0938, -5.1094, -0.2981],
        [-3.8809, -3.1152, -4.2891,  ..., -5.1016, -4.9453, -1.2725],
        [-3.4688, -3.1406, -5.9688,  ..., -5.9062, -4.8438, -5.2812]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  4,  9, 18, 14, 27, 17, 20, 20, 17, 15, 10,  6,  0, 27, 27,  0, 27,
         7,  6,  3, 27, 22, 15,  7, 27, 10, 27, 27, 27, 27, 15,  0, 24, 27,  1,
         7, 27, 11, 18,  4, 27, 27, 27,  8, 10,  3,  5, 11, 27, 27,  7, 27,  1,
        10, 27, 27,  1,  2,  0, 27, 27,  0, 15], device='cuda:0')
step: 350
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6143,  ...,  0.4785, -0.0817,  1.5107],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2208,  0.7939,  ..., -1.9043, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.5797e-04,  2.9063e-04,  1.0576e-03,  ..., -1.5771e-04,
         -1.2894e-03, -6.8665e-04],
        [-3.4189e-04,  1.2732e-03, -4.9400e-04,  ...,  1.4009e-03,
          2.0084e-03, -1.0157e-03],
        [ 1.1849e-04,  1.0300e-03, -1.2407e-03,  ..., -3.6621e-04,
          9.2411e-04, -8.9407e-04],
        [-6.7616e-04,  3.1281e-04,  6.6042e-04,  ...,  4.2582e-04,
          5.9319e-04,  4.3058e-04],
        [-8.7559e-05,  6.7711e-04,  1.6260e-04,  ..., -2.1815e-05,
         -2.8419e-04, -1.5163e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1536, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.6719, -4.2031, -4.1406,  ..., -6.3906, -4.3750, -0.3452],
        [-5.4297, -4.0703, -4.5078,  ..., -4.9922, -4.4141, -0.3970],
        [-2.8926, -3.4082, -5.3164,  ..., -4.9570, -3.6582, -3.7051],
        ...,
        [-6.2422, -3.6953, -3.8359,  ..., -4.0547, -4.3828, -0.4453],
        [-3.7070, -2.9414, -4.2383,  ..., -5.9102, -3.4570, -0.9570],
        [-5.2266, -3.6797, -2.6641,  ..., -3.1953, -4.5234, -1.1953]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 13,  5, 25, 27,  7, 25, 27, 27, 27, 27, 27, 17, 27, 27,  1,  3,
        25,  7, 24, 27, 14, 27, 27, 18,  1, 15, 27, 10, 27, 27, 27, 18, 18,  7,
        15, 27, 14,  0,  7,  5, 27, 27, 27, 25, 17, 10, 26,  3, 17, 22, 17, 17,
         2, 10, 22,  3, 20,  5,  3, 27, 15, 22], device='cuda:0')
step: 351
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6143,  ...,  0.4785, -0.0817,  1.5107],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2208,  0.7939,  ..., -1.9043, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.9778e-04,  1.3103e-03, -1.1082e-03,  ..., -8.9884e-04,
          6.0606e-04,  9.0742e-04],
        [ 9.7156e-06, -5.3406e-04, -1.4982e-03,  ..., -2.1267e-03,
         -1.1797e-03,  1.1282e-03],
        [ 3.6550e-04, -7.6771e-04, -2.8276e-04,  ...,  6.3753e-04,
          1.1959e-03,  9.0885e-04],
        [ 5.2929e-04, -5.5599e-04, -2.2526e-03,  ..., -3.4666e-04,
          1.0023e-03, -1.5059e-03],
        [ 7.8499e-05, -4.0007e-04, -2.8229e-04,  ..., -8.6427e-06,
          1.5903e-04,  3.2187e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2197, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.5312, -3.1875, -3.4688,  ..., -4.5312, -5.5312, -1.0312],
        [-5.7422, -4.0703, -3.8379,  ..., -5.3359, -5.5391, -0.3840],
        [-4.8164, -2.8613, -2.5332,  ..., -2.3301, -4.1289, -1.1738],
        ...,
        [-0.6235, -4.0312, -5.4062,  ..., -7.0156, -3.4824, -4.8906],
        [-2.0508, -3.3164, -4.0977,  ..., -5.8945, -2.8164, -1.7383],
        [-5.1992, -3.7637, -3.8418,  ..., -4.7344, -3.0137, -1.3887]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2, 27, 27, 27,  3,  7,  7,  2,  1,  5,  0, 27, 27, 27, 25, 10,  5,  6,
        15, 22, 27,  6, 27,  3, 27, 27,  6, 12, 27, 27,  9,  5, 21,  4,  7, 27,
         7,  4, 27, 18, 27, 18, 10, 14, 27, 17,  8, 12,  7, 27, 22,  9,  2,  1,
        27, 10, 27, 27, 19,  6, 27,  0,  0,  6], device='cuda:0')
step: 352
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6143,  ...,  0.4785, -0.0817,  1.5107],
        [ 1.2510, -1.8916, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2208,  0.7939,  ..., -1.9043, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.5960e-04, -1.6584e-03,  1.5926e-03,  ..., -2.8729e-04,
         -1.5879e-03,  1.3380e-03],
        [-3.2902e-05,  1.9083e-03,  9.7942e-04,  ...,  3.5076e-03,
          1.2760e-03, -2.0447e-03],
        [-3.8242e-04, -7.3814e-04,  3.4189e-04,  ...,  1.2236e-03,
         -1.1005e-03, -3.5048e-04],
        [-1.0996e-03, -1.2553e-04,  8.4019e-04,  ...,  9.1314e-04,
          7.2432e-04,  1.4029e-03],
        [-3.9363e-04,  1.2946e-04,  1.6856e-04,  ..., -2.1338e-04,
         -3.3665e-04,  3.4857e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9601, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.5625, -3.2500, -3.6094,  ..., -3.2500, -3.5938, -1.1104],
        [-1.1279, -2.8457, -4.2852,  ..., -5.2852, -2.7227, -3.4883],
        [-0.7090, -2.8496, -4.7852,  ..., -6.4609, -3.2090, -3.5684],
        ...,
        [-3.9512, -2.9199, -3.6074,  ..., -3.9668, -2.6230, -1.9053],
        [-6.2852, -3.5664, -3.7383,  ..., -4.6758, -3.9727, -0.7847],
        [-5.3008, -2.4277, -2.9121,  ..., -3.1465, -3.8184, -1.3965]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0,  0, 27, 25,  5, 20,  7,  7,  3,  4, 18, 20,  0, 11, 20, 27, 27,
        17, 15,  0, 18, 13, 15, 15, 27,  8,  9, 27, 27, 27,  7,  7, 22, 18,  0,
        27, 27,  7, 27, 24, 15, 27, 17,  7, 12,  5, 25, 23, 15,  2, 26, 27, 27,
        17, 27, 11, 12, 20, 26, 15, 27,  4,  3], device='cuda:0')
step: 353
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6143,  ...,  0.4785, -0.0817,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2208,  0.7939,  ..., -1.9043, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.5034e-04, -9.7179e-04, -6.2513e-04,  ...,  5.4264e-04,
          6.2466e-04, -1.5748e-04],
        [ 1.1307e-04, -1.1520e-03,  1.8835e-04,  ..., -1.7214e-03,
         -8.2159e-04,  1.1339e-03],
        [ 1.0109e-04, -4.1544e-05,  1.1075e-04,  ..., -1.3046e-03,
         -2.8992e-04,  6.0368e-04],
        [ 3.9339e-05, -2.2936e-04, -2.6035e-04,  ..., -2.4080e-05,
         -7.6234e-05,  5.9080e-04],
        [ 2.5344e-04, -1.7290e-03,  2.2840e-04,  ..., -5.8746e-04,
         -1.0383e-04, -3.8326e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8388, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.5234, -3.1797, -3.5527,  ..., -2.9121, -3.7559, -0.9287],
        [-5.2148, -3.5742, -3.5898,  ..., -3.5898, -3.4629, -2.6211],
        [-5.0703, -2.8516, -3.7734,  ..., -6.4609, -5.2109, -0.6162],
        ...,
        [-5.4883, -3.1133, -3.3320,  ..., -2.2695, -2.9727, -1.5498],
        [-3.9473, -3.0254, -3.3066,  ..., -3.4023, -1.8535, -1.6670],
        [-6.3359, -3.1484, -2.3516,  ..., -4.9297, -3.1328, -1.2900]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  6, 27,  0, 27, 27, 15, 12, 27, 22, 15,  0, 11, 18, 27, 27, 15, 10,
        27,  2, 14, 27, 27, 10,  5, 27,  1, 11, 20, 18, 11,  6, 27, 10, 27, 24,
        27, 27, 25, 11, 27, 27,  3, 25, 27, 27, 27, 10, 27, 18,  8, 27, 17,  5,
         0, 27, 27, 17, 27,  1, 26, 27, 27,  6], device='cuda:0')
step: 354
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6143,  ...,  0.4785, -0.0817,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2208,  0.7939,  ..., -1.9043, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.1383e-04,  8.6927e-04,  2.1896e-03,  ..., -6.6161e-05,
         -4.2486e-04, -3.2926e-04],
        [ 7.7009e-04, -3.3045e-04, -1.8520e-03,  ..., -1.6012e-03,
          1.9979e-04,  1.2760e-03],
        [ 8.7833e-04, -2.6655e-04, -2.8954e-03,  ...,  1.3685e-03,
          1.1749e-03,  1.8978e-03],
        [-2.8753e-04, -2.0885e-04,  2.1801e-03,  ..., -1.7858e-04,
          4.0829e-05, -3.1400e-04],
        [-1.6606e-04,  6.9523e-04,  6.8665e-04,  ...,  8.4877e-05,
          4.1461e-04, -1.6022e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2199, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.7539, -4.3320, -4.4102,  ..., -6.3242, -4.8945, -0.1903],
        [-1.9424, -4.3477, -5.0195,  ..., -6.8945, -6.2070, -4.8945],
        [-3.4238, -2.3926, -3.8770,  ..., -4.5938, -4.1719, -1.3613],
        ...,
        [-0.9878, -3.4414, -5.0195,  ..., -4.9883, -3.4414, -3.0508],
        [-7.1562, -5.7344, -5.1250,  ..., -6.5312, -5.5000, -0.0787],
        [-5.9883, -4.1758, -3.0820,  ..., -3.6445, -4.5352, -0.5825]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 18, 13, 18, 10,  5,  0, 26, 17,  0, 27, 27,  7,  4, 27,  0,  6, 27,
         0,  9, 27, 27, 26, 13, 17,  0, 26, 14,  0, 24, 22, 20, 27, 22, 27, 27,
        27, 27, 27, 20, 27,  1,  7, 27, 15, 10, 20, 10, 27, 18, 27,  0, 27, 27,
         1,  0,  9, 23, 27, 15, 27,  1, 27,  9], device='cuda:0')
step: 355
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6143,  ...,  0.4785, -0.0816,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2208,  0.7939,  ..., -1.9043, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.9658e-04,  6.9857e-05,  3.6669e-04,  ...,  5.1212e-04,
          8.4996e-05, -2.0516e-04],
        [-8.5163e-04,  9.4700e-04,  1.2197e-03,  ...,  4.1084e-03,
          1.6413e-03, -1.6899e-03],
        [-2.5082e-04,  2.4736e-05,  1.6899e-03,  ...,  1.1854e-03,
         -5.8365e-04,  5.1928e-04],
        [ 1.0853e-03,  3.9315e-04, -2.5988e-04,  ..., -4.9591e-05,
         -3.9315e-04, -1.4143e-03],
        [ 5.0735e-04, -1.4007e-05, -1.4782e-04,  ...,  4.5419e-04,
          6.5899e-04,  2.6822e-06]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0050, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.9434, -4.2695, -5.0508,  ..., -6.8164, -6.2695, -5.5352],
        [-1.4824, -2.9980, -5.4336,  ..., -5.5586, -4.2148, -4.1094],
        [-4.6562, -3.0605, -3.6074,  ..., -2.2793, -3.9043, -1.1230],
        ...,
        [-4.1523, -3.5586, -3.1211,  ..., -4.6836, -3.3711, -1.0264],
        [-5.0938, -3.2031, -3.9531,  ..., -4.3438, -2.5156, -0.7334],
        [-3.3008, -3.1289, -2.8320,  ..., -2.5664, -3.9414, -2.3164]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 15, 27, 20, 27, 26,  3, 20, 27,  4, 27, 27,  0, 27, 17, 27, 27, 17,
        27, 20, 22, 27, 27, 11, 20, 10,  0,  0, 27, 15,  3, 18, 11, 26, 27, 27,
        27,  9, 10, 27,  7, 27, 10,  4, 27,  4, 13, 27, 12, 20, 27,  4, 18,  0,
        27,  6, 27, 22, 27,  2, 10,  2, 27,  0], device='cuda:0')
step: 356
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6143,  ...,  0.4785, -0.0816,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2208,  0.7939,  ..., -1.9043, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.3148e-05,  1.6661e-03, -1.6510e-05,  ...,  3.6788e-04,
         -4.0030e-04,  1.4687e-04],
        [-5.1689e-04,  1.4801e-03, -2.0957e-04,  ...,  7.2670e-04,
          9.8765e-05, -1.2100e-04],
        [ 6.4182e-04,  1.3227e-03,  1.5259e-03,  ..., -1.0605e-03,
         -5.8031e-04,  9.6560e-04],
        [ 9.4652e-05,  1.8692e-04, -2.3985e-04,  ..., -1.5235e-04,
          7.3254e-05,  7.7915e-04],
        [ 1.9479e-04, -8.4305e-04,  3.8362e-04,  ...,  1.3614e-04,
         -7.6199e-04,  4.3964e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9030, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.1875, -3.5957, -3.5332,  ..., -5.3281, -3.6582, -0.7832],
        [-5.7227, -3.6465, -4.5039,  ..., -6.4570, -6.8008, -0.4109],
        [-5.4648, -3.8242, -4.1211,  ..., -4.9648, -4.5430, -0.4810],
        ...,
        [-3.8906, -4.0938, -4.5781,  ..., -6.1875, -4.7031, -0.7334],
        [-3.8477, -3.5488, -3.7520,  ..., -5.9727, -5.0352, -0.3621],
        [-5.0430, -2.2754, -2.5254,  ..., -1.8232, -4.9961, -2.3691]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27, 27, 26,  4, 10,  3, 27, 26, 18, 27,  0, 27, 26, 23, 27, 10,
        14, 15,  6, 27, 20, 27,  3, 27, 27, 27,  0, 25, 27, 27,  1, 17, 22,  0,
        26,  0, 27, 11,  3, 27,  6, 10, 27,  0,  9,  5, 27,  1,  3, 27, 11, 27,
        17,  1, 15, 27, 27,  0, 10, 27, 27,  2], device='cuda:0')
step: 357
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6143,  ...,  0.4785, -0.0816,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2208,  0.7939,  ..., -1.9043, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.7592e-05,  1.3447e-03, -6.0701e-04,  ...,  2.7919e-04,
         -2.7418e-04,  1.4620e-03],
        [-7.1192e-04, -2.1095e-03, -9.0837e-04,  ..., -1.8024e-03,
         -1.3819e-03,  2.2087e-03],
        [-8.2874e-04,  9.5558e-04,  1.3351e-03,  ...,  4.5133e-04,
          4.0591e-05,  2.0542e-03],
        [-3.8171e-04, -6.2943e-05, -4.3678e-04,  ..., -7.5579e-04,
         -7.1716e-04,  3.3402e-04],
        [ 2.3305e-05, -3.8528e-04, -2.5558e-04,  ...,  4.8935e-05,
          6.7890e-05,  3.1281e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0803, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.2051, -4.0781, -4.7031,  ..., -7.2031, -6.1094, -4.7031],
        [-4.8594, -3.0957, -2.4863,  ..., -3.7207, -3.7520, -0.7202],
        [-4.0977, -3.5352, -4.4258,  ..., -7.1602, -3.0352, -2.2852],
        ...,
        [-0.4338, -3.8711, -5.0273,  ..., -6.0273, -3.4336, -3.5117],
        [-6.1797, -3.5078, -3.7266,  ..., -6.3516, -4.2422, -1.3984],
        [-2.3477, -4.3008, -5.8164,  ..., -5.7070, -4.8164, -4.1289]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 18,  0, 15, 26,  3, 22, 27, 27,  4, 10, 10,  1,  0, 11, 27,  1, 27,
        20, 27, 14,  2, 20,  4,  7, 27, 27, 27, 17, 27,  4, 27, 27, 15, 27, 14,
        24, 26,  5, 26, 27, 27,  3, 25, 27, 26,  6, 27, 27,  0, 27, 27, 13,  8,
         8, 27,  5, 11,  3, 27, 27,  0,  6, 20], device='cuda:0')
step: 358
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6143,  ...,  0.4785, -0.0815,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.2197e-03, -1.6899e-03, -1.1606e-03,  ..., -7.2861e-04,
          1.1587e-03, -7.4387e-04],
        [ 3.3677e-05, -1.2321e-03, -1.2760e-03,  ...,  8.5235e-05,
          7.8106e-04,  6.5994e-04],
        [-5.9938e-04, -1.8692e-04, -9.6607e-04,  ..., -2.5272e-04,
          3.1590e-06,  2.4414e-04],
        [ 1.0529e-03, -7.3671e-04, -7.9679e-04,  ...,  6.7830e-05,
          1.8919e-04, -1.8864e-03],
        [ 1.1051e-04,  2.9206e-04, -6.2132e-04,  ..., -1.0401e-04,
          5.8460e-04,  1.1921e-07]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9304, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.3633, -5.4102, -5.0820,  ..., -5.0664, -5.6602, -0.1125],
        [-5.0312, -3.1250, -2.9375,  ..., -4.1094, -2.6719, -0.7651],
        [-4.5000, -4.0156, -3.6895,  ..., -4.6719, -3.8926, -0.4700],
        ...,
        [-6.5547, -4.1641, -4.3047,  ..., -5.2266, -5.1484, -0.3669],
        [-1.2627, -3.6523, -3.9492,  ..., -5.1367, -1.6689, -2.9492],
        [-3.9492, -3.9648, -4.5586,  ..., -4.7773, -3.8398, -0.3867]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4,  2, 27, 27, 26, 27, 27, 17, 27, 18, 17,  4, 27,  7, 17, 27,  3, 15,
        27, 15,  1, 25, 27,  0, 18, 27, 27, 27,  9, 22,  4, 15, 22,  4, 21, 13,
        20, 27,  1, 27,  8, 27,  3,  7,  6, 27, 15, 27, 20,  3, 15, 27, 15,  8,
        22, 18, 27, 27, 27, 27,  3, 10, 13, 14], device='cuda:0')
step: 359
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6143,  ...,  0.4785, -0.0815,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.0790e-04,  8.1873e-04,  4.8280e-04,  ...,  2.0421e-04,
          1.6689e-05, -1.7679e-04],
        [ 1.1313e-04,  1.2035e-03, -9.7692e-05,  ...,  3.2640e-04,
          4.0555e-04, -3.0422e-04],
        [ 3.6263e-04, -6.9714e-04, -1.9288e-04,  ...,  1.6141e-04,
          6.8903e-04,  1.1170e-04],
        [ 3.8683e-05,  5.3549e-04,  3.6693e-04,  ...,  4.8470e-04,
         -7.9870e-04,  4.2391e-04],
        [ 7.9095e-05,  4.0197e-04, -3.7432e-04,  ...,  2.8729e-04,
          2.4676e-05,  3.6299e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8203, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.9609, -4.1328, -4.9453,  ..., -7.1484, -6.0391, -0.2418],
        [-6.2266, -4.3047, -4.3672,  ..., -4.5391, -4.9141, -0.2722],
        [-2.1777, -4.6758, -5.0820,  ..., -7.5664, -5.1758, -4.1289],
        ...,
        [-1.9229, -3.5645, -5.1875,  ..., -7.1250, -5.4062, -1.8916],
        [-4.7031, -4.4375, -3.8281,  ..., -4.4844, -4.3125, -0.4065],
        [-2.0605, -3.8574, -3.0449,  ..., -2.9043, -3.0449, -1.9814]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  3, 18, 27, 27,  1, 27,  3, 25, 27, 25, 20, 15, 27, 27, 18, 27, 25,
         7, 27,  7,  0, 18, 10, 13, 10, 25,  9,  0, 14,  4, 27,  0,  0, 27, 27,
        27, 27,  0, 11,  7, 27,  0, 27,  2,  1, 27, 26, 20,  0,  1, 27, 18, 27,
        27,  0, 27, 22,  1,  1,  7, 18, 27, 27], device='cuda:0')
step: 360
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6143,  ...,  0.4785, -0.0815,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.4734e-04, -9.2685e-05,  1.7989e-04,  ...,  4.4537e-04,
         -2.9612e-04, -3.8481e-04],
        [-3.8087e-05, -6.8426e-04, -6.8331e-04,  ..., -1.6108e-03,
         -1.1330e-03,  7.0000e-04],
        [ 2.4962e-04, -4.9233e-05, -1.2159e-05,  ..., -7.6008e-04,
         -9.5844e-04,  3.7670e-05],
        [ 1.0103e-04,  1.4675e-04,  4.4298e-04,  ...,  3.1161e-04,
         -9.1374e-05,  2.4235e-04],
        [-1.8263e-04, -3.1996e-04,  5.2071e-04,  ..., -1.7738e-04,
         -3.1614e-04,  2.4462e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0050, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.7891, -3.7891, -4.4453,  ..., -5.9766, -4.3359, -0.6807],
        [-1.4248, -3.3301, -4.6758,  ..., -6.0352, -3.6582, -3.0801],
        [-4.9414, -3.0977, -3.3164,  ..., -2.4102, -5.8633, -2.6914],
        ...,
        [-4.1680, -3.4512, -4.0273,  ..., -6.1211, -3.8887, -0.9980],
        [-2.2891, -5.1328, -5.3203,  ..., -5.1016, -2.5684, -3.4746],
        [-4.5586, -3.3066, -3.5098,  ..., -4.3398, -2.5410, -1.4014]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  3,  2,  0, 18,  0, 27, 20, 18, 22, 18,  8, 27,  0,  0, 18, 15, 18,
         6, 18, 27, 27, 20, 27, 27,  7, 27, 27, 27, 27, 26,  6,  4, 15,  7, 13,
         7,  3, 25,  1, 25,  3,  0, 27, 27, 27, 12,  0, 18, 27, 27, 18,  1, 18,
        27, 15,  3, 27,  3, 27,  4, 27, 17, 22], device='cuda:0')
step: 361
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2378, -0.6147,  ...,  0.4785, -0.0815,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.0156e-04,  4.2582e-04, -1.2064e-03,  ...,  1.4725e-03,
          5.2595e-04,  3.1137e-04],
        [-2.0170e-04,  1.9817e-03,  2.7800e-04,  ..., -8.2064e-04,
         -1.7653e-03,  1.3804e-04],
        [ 3.4237e-04, -3.3677e-05, -8.9025e-04,  ..., -8.8072e-04,
          9.9242e-05,  5.0354e-04],
        [ 1.5039e-03,  1.2279e-04, -1.0357e-03,  ...,  1.1644e-03,
         -2.0447e-03, -5.4693e-04],
        [ 2.4331e-04, -7.5960e-04, -1.4582e-03,  ...,  3.9530e-04,
         -3.6049e-04,  5.7411e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8189, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.8867, -3.6074, -3.1680,  ..., -2.8887, -3.9512, -0.8877],
        [-4.3672, -4.3984, -4.5391,  ..., -7.1328, -4.1172, -0.7583],
        [-6.3086, -4.4180, -4.5742,  ..., -6.9492, -4.9648, -0.2004],
        ...,
        [-5.9062, -4.2031, -3.5000,  ..., -4.5781, -3.0938, -0.8589],
        [-4.4492, -3.6211, -4.1680,  ..., -4.4961, -3.4336, -0.6216],
        [-0.7002, -3.7148, -5.0273,  ..., -7.6211, -4.4180, -3.5273]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 27, 27, 27,  4, 27, 27, 15, 18, 27, 25, 27, 17, 27, 27, 27, 27,  1,
        27,  3, 22,  4, 27, 27,  4,  0,  0,  5, 15, 17,  7, 27, 26,  7, 27, 27,
        27, 25,  7, 10, 27, 18,  9,  0, 22, 18, 13,  6, 27, 27, 18, 22,  0, 15,
        13, 27, 27, 27, 10, 15,  4, 14, 27,  0], device='cuda:0')
step: 362
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0815,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.7851e-04,  3.2902e-04,  1.1559e-03,  ...,  2.4748e-04,
         -3.9625e-04,  1.2951e-03],
        [-5.5850e-05,  1.0719e-03,  9.0599e-04,  ..., -2.6345e-04,
         -6.8951e-04, -1.0586e-03],
        [ 3.6836e-05, -4.3797e-04,  4.0150e-04,  ..., -5.4502e-04,
         -5.0545e-05,  1.0567e-03],
        [-1.1671e-04, -3.1614e-04,  1.2035e-03,  ..., -5.1880e-04,
         -3.4237e-04,  6.7520e-04],
        [-1.3328e-04, -2.4462e-04,  3.6311e-04,  ..., -3.6478e-04,
         -5.2023e-04,  1.4663e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1531, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.8047, -4.5547, -4.6016,  ..., -5.6641, -5.8203, -0.2576],
        [-4.8086, -3.5273, -3.4180,  ..., -3.2461, -3.0273, -1.6680],
        [-1.0488, -3.5176, -4.6602,  ..., -4.0977, -4.1602, -2.2988],
        ...,
        [-4.5352, -3.4863, -3.1270,  ..., -4.3633, -4.3320, -1.7998],
        [-3.5020, -3.5957, -4.2500,  ..., -4.9258, -5.1914, -1.0645],
        [-0.6587, -3.0020, -4.9414,  ..., -6.6602, -4.5820, -3.8770]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 14,  1, 27, 27,  4, 25,  7, 22,  0,  2, 24,  6,  3, 15,  8, 27, 27,
         0,  3, 27,  9, 27,  3,  0, 15, 14, 27, 27,  3,  0,  0,  2, 10, 25,  5,
         9, 25, 27,  4, 26,  1, 27,  4,  1,  4, 18,  0,  8,  2,  6, 18,  0,  0,
        15, 27, 22, 26, 27, 27, 20, 27,  5, 17], device='cuda:0')
step: 363
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0815,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.2370e-04,  8.6832e-04, -1.0908e-05,  ...,  2.5213e-05,
          9.1970e-05,  8.5068e-04],
        [ 4.8470e-04,  3.9959e-04, -3.8052e-04,  ..., -1.4429e-03,
         -5.2547e-04, -1.8291e-03],
        [ 2.3007e-04, -1.1826e-03, -2.2292e-04,  ..., -2.7561e-04,
          4.5419e-04,  7.1955e-04],
        [-3.0899e-04, -1.0920e-04, -3.6573e-04,  ..., -9.9719e-05,
          4.9114e-04, -8.6021e-04],
        [-2.7752e-04,  2.1791e-04, -2.7370e-04,  ...,  3.7026e-04,
         -5.8591e-05,  1.1206e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1581, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.7227, -3.3633, -2.8496,  ..., -2.4570, -2.8320, -1.9111],
        [-4.1406, -3.0156, -3.1562,  ..., -3.8125, -6.2031, -2.1875],
        [-3.9590, -3.5059, -4.1289,  ..., -5.0664, -5.1289, -0.7397],
        ...,
        [-4.9570, -3.7070, -3.2090,  ..., -3.8945, -4.5820, -0.6143],
        [-4.9297, -3.8047, -4.5078,  ..., -5.3203, -4.5703, -0.6172],
        [-5.4883, -4.1914, -3.7051,  ..., -4.3945, -3.8301, -0.6587]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([14, 25, 27, 27,  9,  3, 15,  4, 27,  1, 25,  0, 12, 19, 27, 15, 22, 27,
        27, 27, 27,  3, 27,  3, 27,  0, 14, 27, 10, 18, 26, 27,  5,  2,  1, 12,
        10,  2, 27, 17, 26,  2,  2,  7, 20, 26,  1, 27, 17, 18, 27, 22, 15, 17,
         9, 14, 15,  3, 26, 10,  3,  3, 27, 27], device='cuda:0')
step: 364
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0815,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.7752e-04,  1.5688e-03,  1.8883e-04,  ...,  2.5630e-05,
          8.2374e-05,  1.8263e-03],
        [-3.4070e-04, -4.2129e-04, -9.0981e-04,  ..., -5.7030e-03,
         -3.4027e-03,  8.7929e-04],
        [-5.7697e-04,  2.4819e-04,  6.8998e-04,  ...,  3.5572e-04,
         -4.5919e-04,  9.1934e-04],
        [-3.9279e-05, -7.4208e-05,  1.3123e-03,  ...,  3.2616e-04,
          2.1172e-04, -5.6386e-05],
        [ 5.5373e-05,  8.6498e-04, -1.5998e-04,  ...,  5.4657e-05,
          5.2214e-05, -2.9469e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0710, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.0820, -3.9121, -1.4736,  ..., -5.4883, -5.1289, -0.6143],
        [-0.8594, -4.3438, -5.9062,  ..., -7.1094, -5.1250, -4.1875],
        [-2.3438, -3.2500, -4.5156,  ..., -5.1719, -2.3438, -4.5469],
        ...,
        [-3.2168, -2.9668, -4.5312,  ..., -5.9844, -4.8438, -1.4365],
        [-4.5156, -3.1719, -3.0469,  ..., -1.7969, -3.2656, -1.4062],
        [-3.0176, -4.4375, -7.1562,  ..., -5.8906, -5.8906, -5.8438]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 27, 13, 27,  4, 17, 27, 27,  3, 27,  4, 15, 13,  1,  7, 22, 22, 14,
         7,  7, 27, 27, 25, 15,  6,  4, 27, 27, 24, 27, 27, 14, 27, 20,  0,  4,
        27,  0, 27, 27,  2, 27,  5, 19,  7, 18, 27, 27,  0,  1, 27, 26, 27,  1,
        27,  7, 21, 20, 13,  6, 27,  4, 10, 20], device='cuda:0')
step: 365
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0815,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.4421e-04, -1.1969e-03, -1.8013e-04,  ..., -8.7070e-04,
         -8.5890e-05, -1.3714e-03],
        [-2.8753e-04,  1.0347e-03, -4.4608e-04,  ...,  1.6966e-03,
          2.5082e-03,  1.5652e-04],
        [ 4.4489e-04,  1.1325e-05, -1.3371e-03,  ..., -2.7990e-04,
          3.6645e-04, -6.5947e-04],
        [-3.0375e-04,  6.2990e-04,  1.4734e-03,  ..., -1.9932e-04,
          3.5620e-04,  7.7581e-04],
        [-1.6367e-04,  6.5088e-04,  2.1398e-04,  ...,  2.5928e-05,
         -1.3649e-05,  2.7180e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9333, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6445, -2.7695, -2.7070,  ..., -1.2080, -4.7227, -2.4258],
        [-4.4648, -4.2148, -5.1016,  ..., -4.5273, -4.9648, -0.3220],
        [-3.9453, -3.7246, -4.1953,  ..., -4.7578, -3.5684, -0.3970],
        ...,
        [-2.5098, -3.8379, -4.4180,  ..., -4.8242, -3.3691, -1.7139],
        [-3.6230, -4.3750, -4.6250,  ..., -4.6406, -3.8574, -0.9360],
        [-5.9961, -5.7461, -7.1523,  ..., -8.4609, -4.7305, -4.6836]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25, 10, 27,  2,  1,  3, 27, 22, 15,  1, 25,  7,  6, 15,  0,  2, 27, 12,
         0,  4, 27, 27, 26, 27, 27, 27, 17, 10,  4, 27, 10, 13, 24, 15, 27, 27,
        18,  7, 27, 27,  6, 27, 11, 27, 27, 17, 15, 27,  4, 10, 10, 25,  3,  3,
         0, 18, 27, 27,  7, 27,  4,  6,  4,  7], device='cuda:0')
step: 366
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0815,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.6532e-04,  1.4706e-03,  1.3456e-03,  ..., -7.6675e-04,
         -8.3160e-04, -9.4175e-04],
        [-1.0872e-04, -2.6822e-04,  1.4627e-04,  ..., -5.7697e-04,
         -3.8075e-04,  5.6219e-04],
        [ 2.2340e-04, -8.5402e-04, -1.5726e-03,  ...,  5.3215e-04,
          1.6699e-03, -3.8242e-04],
        [-6.2084e-04,  6.7234e-04,  1.4286e-03,  ...,  7.4863e-04,
          2.0325e-04,  1.2159e-03],
        [-6.6221e-05,  5.4502e-04,  2.2566e-04,  ...,  2.9039e-04,
         -8.0109e-05, -6.4194e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6227, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9707, -3.5176, -3.0801,  ..., -2.3145, -3.0664, -1.5186],
        [-3.9961, -3.6836, -3.7305,  ..., -6.2305, -4.1211, -0.4189],
        [-3.5625, -3.2656, -3.2188,  ..., -3.7656, -3.0312, -1.3125],
        ...,
        [-1.8643, -4.5820, -5.5039,  ..., -5.2539, -4.8789, -5.1133],
        [-5.8789, -2.8301, -4.4102,  ..., -6.7695, -5.8008, -0.2363],
        [-2.1426, -4.2695, -4.0938,  ..., -5.9570, -5.7852, -0.5337]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  4, 27, 27, 18,  5, 27,  1, 27, 27, 27, 27,  9,  3, 27,  5, 27, 27,
         7, 27, 25,  5,  8, 27, 18, 27, 27, 18, 20, 20, 19,  1, 27, 27, 27, 27,
        20, 27,  0,  8, 15, 27, 27, 27,  0, 27, 27,  4, 10,  4, 27, 10, 27, 27,
        27,  7, 14,  0, 16, 10, 25, 20, 27, 27], device='cuda:0')
step: 367
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0814,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.6284e-04, -1.0033e-03, -7.3290e-04,  ...,  8.9705e-05,
          3.1209e-04, -1.9550e-05],
        [-1.0386e-03,  1.0270e-04, -2.5606e-04,  ..., -1.7843e-03,
         -1.3399e-03,  1.5202e-03],
        [-9.8169e-05,  3.3522e-04,  2.9802e-07,  ..., -4.3416e-04,
          2.6512e-04,  1.0049e-04],
        [-4.3154e-04,  5.9128e-04, -9.5034e-04,  ..., -6.9046e-04,
         -2.0909e-04, -1.0862e-03],
        [-1.6212e-04, -2.7585e-04, -4.5156e-04,  ..., -1.4687e-04,
         -3.3283e-04, -3.3474e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9813, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.7500, -3.3438, -5.2188,  ..., -6.2344, -2.3281, -3.7656],
        [-2.5859, -3.6797, -5.6797,  ..., -6.2734, -4.9297, -5.3047],
        [-2.3516, -3.8672, -6.2266,  ..., -7.1953, -5.9141, -5.3672],
        ...,
        [-3.2715, -4.4297, -4.6328,  ..., -7.6562, -2.2871, -3.7715],
        [-4.4414, -3.4102, -4.2227,  ..., -3.9414, -4.7383, -0.4717],
        [-2.7031, -3.1875, -2.8125,  ..., -3.9844, -4.6406, -0.7974]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26, 15, 15, 20, 20,  4,  3,  4, 18, 27, 27, 22,  7,  1, 15, 18, 17, 27,
        15, 27, 27, 12,  3,  0, 17,  7, 25,  7, 11, 20,  3, 27, 27,  1,  7, 27,
        17, 27, 26,  1,  1, 10,  7, 27, 27, 27,  4, 10,  0, 27,  7, 27,  0, 27,
        27,  0, 27,  1, 10,  0, 27, 13, 27, 27], device='cuda:0')
step: 368
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0814,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.0177e-04,  1.1673e-03,  8.2445e-04,  ..., -6.8331e-04,
         -5.5742e-04,  8.7261e-04],
        [-4.0555e-04,  1.0595e-03,  4.7207e-04,  ...,  3.4952e-04,
          4.1246e-04, -3.3319e-05],
        [-2.7299e-05,  8.7881e-04,  1.3266e-03,  ...,  7.9060e-04,
          1.8084e-04,  2.4366e-04],
        [-2.4986e-04,  9.4116e-05, -1.0023e-03,  ..., -1.2279e-04,
          2.4247e-04, -1.8835e-05],
        [-4.8637e-05, -2.0790e-04, -4.5121e-05,  ..., -2.8706e-04,
         -6.0260e-05,  5.8889e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.4483, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.9785, -3.8223, -3.5566,  ..., -4.6055, -2.7598, -0.9478],
        [-5.5352, -3.1270, -3.7363,  ..., -5.3164, -4.5820, -0.5337],
        [-1.4785, -3.7129, -4.0742,  ..., -5.4961, -2.1035, -1.2764],
        ...,
        [-2.1934, -3.4570, -5.8789,  ..., -6.4570, -5.2227, -3.3496],
        [-1.1494, -4.1328, -5.2578,  ..., -7.4297, -5.5234, -5.1953],
        [-4.5664, -3.3145, -4.4258,  ..., -4.7695, -5.2227, -0.4399]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 27, 27, 20,  1, 10, 22, 22,  7, 27, 14, 27,  4,  5, 20,  5,  5, 15,
         9, 18, 15,  3, 20, 18, 27, 15, 20, 22, 27,  4, 13, 20,  2, 10,  6, 13,
        25, 13, 27, 27, 10,  0,  0,  6, 27,  1,  8, 26,  9,  4, 20, 27, 14, 27,
         0, 27,  1, 27, 14,  0, 27, 20, 18,  4], device='cuda:0')
step: 369
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0814,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.9087e-04,  1.2827e-04,  1.5497e-04,  ..., -1.6384e-03,
          4.8494e-04, -1.1244e-03],
        [-2.4843e-04, -1.8644e-04, -1.9002e-04,  ..., -1.0738e-03,
          1.2856e-03,  2.8253e-05],
        [ 6.7663e-04,  6.4468e-04,  1.2655e-03,  ..., -2.2125e-03,
         -3.2711e-04,  9.0122e-04],
        [ 1.1082e-03, -9.0408e-04, -7.3576e-04,  ...,  2.2483e-04,
          1.1578e-03, -1.3628e-03],
        [ 5.8651e-04, -4.8304e-04,  7.1585e-05,  ..., -2.4247e-04,
          5.5504e-04, -3.7646e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8778, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.1719, -2.6543, -2.9668,  ..., -1.4521, -4.7969, -2.8105],
        [-2.0117, -3.7461, -4.2930,  ..., -5.1523, -4.1836, -2.3555],
        [-3.0195, -3.5039, -4.0195,  ..., -5.7227, -2.5508, -0.7847],
        ...,
        [-2.5840, -3.7090, -4.6016,  ..., -6.2422, -4.0234, -0.9282],
        [-2.8203, -3.5234, -3.8203,  ..., -4.7734, -2.2891, -1.3047],
        [-2.7129, -4.3047, -5.2266,  ..., -5.1953, -4.9297, -0.4778]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 20, 27, 27, 27, 27, 27, 27, 27,  7,  7,  4,  4, 27, 13,  9, 10,  0,
         7, 27,  0, 27, 18, 27,  0,  0,  4,  2,  0, 17, 27, 27, 27,  6, 27, 10,
        13,  6, 18, 27,  7, 22, 27, 27, 14, 15, 27, 22, 20, 27, 27, 27, 11, 27,
        17, 13, 27, 27, 25, 27, 27, 20, 22,  2], device='cuda:0')
step: 370
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0814,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0990,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.5558e-04,  8.5831e-04, -2.2173e-04,  ...,  6.3610e-04,
          5.0068e-04, -1.1587e-03],
        [ 2.9898e-04,  3.0956e-03, -2.8706e-04,  ..., -1.8406e-03,
          5.2595e-04, -2.4395e-03],
        [ 2.6226e-04,  1.2436e-03,  1.1450e-04,  ..., -5.7638e-05,
          4.9877e-04, -4.1413e-04],
        [ 8.6927e-04,  2.8610e-04,  7.5054e-04,  ...,  8.1396e-04,
         -6.5899e-04,  2.4176e-04],
        [ 3.0732e-04, -2.8729e-05, -1.8144e-04,  ...,  7.8487e-04,
          5.6839e-04, -6.3610e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7974, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.1357, -3.7910, -5.4336,  ..., -4.0742, -2.9180, -3.6055],
        [-4.7188, -2.7520, -2.0176,  ..., -3.2656, -3.9531, -1.1572],
        [-4.5742, -2.9824, -3.5293,  ..., -3.8418, -5.2188, -0.8105],
        ...,
        [-2.8203, -3.4922, -6.1328,  ..., -6.9922, -5.4922, -5.9453],
        [-4.0859, -3.4141, -4.1953,  ..., -5.0547, -3.4922, -0.9927],
        [-4.8828, -3.5078, -2.6953,  ..., -4.3203, -4.2891, -1.8359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 25,  5, 25, 10, 27, 27,  4, 27, 27, 10, 27,  0, 20, 22, 27, 27, 14,
        17,  8, 10,  7,  4, 20, 25,  4,  4, 27, 27, 18, 18, 27, 26, 27, 11,  0,
        27, 27, 15,  0, 27, 27,  6, 27, 13,  2,  7, 27,  7, 15,  9, 15, 25, 18,
        18, 17,  6, 13, 27, 27,  5, 15, 20,  3], device='cuda:0')
step: 371
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0814,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0990,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.4867e-04,  3.1614e-04, -9.0957e-05,  ..., -5.6076e-04,
         -4.8184e-04,  4.2677e-04],
        [ 1.7071e-04, -3.5191e-04,  3.6454e-04,  ...,  9.1505e-04,
         -6.5267e-05,  1.5678e-03],
        [ 4.2725e-04, -1.6785e-04,  2.0647e-04,  ..., -8.3983e-05,
         -1.9503e-04,  5.2166e-04],
        [ 9.1028e-04,  7.1526e-06,  1.9145e-04,  ...,  2.0325e-05,
          2.4009e-04,  1.3828e-05],
        [ 1.5676e-05,  7.1859e-04, -1.8597e-05,  ...,  1.3757e-04,
          8.4579e-05, -9.1195e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9485, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.1172, -3.8828, -5.1484,  ..., -5.9141, -3.0547, -5.6953],
        [-1.4941, -2.8535, -3.4941,  ..., -2.6973, -4.0742, -1.8389],
        [-5.4883, -3.9727, -4.0820,  ..., -6.1445, -3.8008, -1.2217],
        ...,
        [-5.2734, -3.6504, -5.0078,  ..., -5.7891, -5.5703, -0.3999],
        [-0.5498, -4.2539, -5.2070,  ..., -5.6914, -2.0195, -4.7070],
        [-3.7617, -2.6855, -3.2793,  ..., -5.7461, -5.8555, -0.5278]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([13,  0,  7, 27, 27,  5,  2, 27,  1, 27,  4, 22, 15, 27, 18, 27, 27, 26,
         1, 27, 17, 27, 27, 27, 11, 27, 27, 27,  9, 25, 17, 25, 27, 27,  2,  4,
        27, 27, 27, 27, 18,  2,  1, 20, 13,  9,  0, 27, 24, 26,  7, 22, 27, 15,
        10, 25,  4, 17, 27, 27, 27, 27,  0,  0], device='cuda:0')
step: 372
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0814,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0990,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.3126e-04,  1.2150e-03,  6.9904e-04,  ...,  5.0592e-04,
         -1.3161e-04, -1.0389e-04],
        [-1.1510e-04,  1.1959e-03,  3.6263e-04,  ..., -8.0395e-04,
          6.3658e-05, -9.2697e-04],
        [-6.1750e-05, -1.2231e-04,  2.0969e-04,  ...,  6.4659e-04,
          2.2221e-04,  3.1662e-04],
        [ 2.4629e-04,  7.5912e-04, -2.3556e-04,  ..., -5.2977e-04,
         -5.7697e-04, -1.1024e-03],
        [-1.0312e-04, -4.2129e-04,  8.5163e-04,  ...,  8.4460e-05,
         -2.6989e-04,  2.9802e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8865, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.7656, -4.1406, -4.5781,  ..., -4.8594, -5.2188, -1.2344],
        [-2.7402, -3.3027, -3.8496,  ..., -2.8965, -1.6152, -2.1152],
        [-2.0605, -4.5117, -4.5586,  ..., -5.7461, -3.5293, -0.6846],
        ...,
        [-2.1152, -3.0996, -5.7734,  ..., -6.4883, -4.5039, -4.3008],
        [-4.0430, -3.3223, -5.1055,  ..., -4.0742, -4.0586, -4.8242],
        [-4.5234, -3.5684, -4.2578,  ..., -3.2715, -3.4434, -0.9595]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 8,  0, 27, 22, 27, 27,  1,  9, 10, 15, 25, 27, 27, 25, 25, 15, 25, 27,
         1, 17, 27, 27, 27, 27, 14,  0, 27, 26, 15, 27, 15, 15,  3,  7, 20, 18,
        15, 27, 27, 27, 24, 25,  3, 27, 10, 27, 27, 11,  7, 25, 24, 27,  6, 18,
         7, 26, 27, 27, 27, 18, 27, 15, 17, 10], device='cuda:0')
step: 373
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0814,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0990,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.5960e-04, -4.2295e-04, -8.3745e-05,  ...,  1.5593e-04,
         -5.2547e-04,  4.3964e-04],
        [-1.0614e-03, -2.7108e-04,  1.1539e-03,  ..., -2.2736e-03,
         -2.9030e-03,  1.7052e-03],
        [-8.0299e-04,  7.3099e-04,  6.5660e-04,  ...,  1.3618e-03,
          4.2737e-05,  1.0481e-03],
        [ 4.9067e-04,  5.7030e-04,  3.8195e-04,  ...,  7.3862e-04,
         -4.3416e-04,  1.2165e-04],
        [-4.0698e-04,  6.1274e-04,  7.0572e-05,  ...,  2.0766e-04,
         -1.9836e-04, -1.6606e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0467, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.7676, -4.0664, -7.5977,  ..., -7.2539, -6.7812, -5.4570],
        [-4.8984, -3.7891, -3.7109,  ..., -4.7109, -4.0859, -0.6016],
        [-3.6328, -3.3984, -4.1797,  ..., -5.3047, -3.8516, -2.1953],
        ...,
        [-2.0527, -3.3340, -4.5820,  ..., -4.4570, -2.8184, -1.6455],
        [-5.2773, -4.1836, -4.9805,  ..., -7.2578, -4.6680, -1.2598],
        [-2.8613, -3.2832, -6.0977,  ..., -4.8320, -3.7988, -3.5176]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15, 10, 20, 11, 15, 10, 27, 11,  4, 27,  2, 25,  4,  0, 13, 12, 27,  1,
        27, 27, 14,  1, 27,  0, 15, 12,  3,  6, 27, 27,  3, 27,  4,  3,  9,  0,
        15, 17,  9, 15, 15, 27, 15, 10,  1, 25, 27,  4,  1,  2,  3,  4, 18, 27,
         7, 19, 27, 22, 27, 18, 11, 27, 27, 15], device='cuda:0')
step: 374
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0814,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6025],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.2643e-04,  1.9255e-03,  8.0156e-04,  ...,  4.6825e-04,
         -2.4486e-04,  7.5674e-04],
        [-1.7905e-04, -9.3651e-04,  6.2943e-04,  ...,  2.6207e-03,
          8.4162e-04,  4.0221e-04],
        [ 2.9540e-04, -1.1158e-03, -7.3195e-04,  ...,  6.0844e-04,
          7.8392e-04, -1.1963e-04],
        [-2.6417e-04,  4.8399e-04, -1.4782e-05,  ...,  2.0170e-04,
         -2.2888e-04,  5.4598e-04],
        [-1.5831e-04,  1.2674e-03, -3.0661e-04,  ...,  6.0987e-04,
          5.0306e-04,  2.5463e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8803, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.6758, -2.7383, -3.5820,  ..., -4.4102, -3.4727, -1.3320],
        [-2.1934, -3.2402, -4.4609,  ..., -5.4922, -1.9121, -1.8350],
        [-5.4219, -3.8438, -3.4844,  ..., -3.7500, -4.7969, -0.3135],
        ...,
        [-4.2969, -3.4062, -4.1094,  ..., -4.5469, -4.4062, -1.3906],
        [-5.1133, -3.1270, -3.8926,  ..., -4.3008, -2.9082, -0.8462],
        [-5.9258, -4.8008, -5.4570,  ..., -0.2690, -5.1289, -2.6445]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 26, 27, 27,  2,  0, 15, 17, 18, 18, 20, 11, 27,  2,  8,  0, 27, 20,
         2, 15,  1, 27, 13,  8,  9, 17, 10, 27, 27,  3, 27, 14, 27, 20,  8, 22,
        27, 27,  7,  4, 18,  1, 20, 15, 24, 10,  7, 27, 20,  4, 27, 27, 27, 27,
        27, 27,  7, 20,  4, 27, 27,  6, 27, 17], device='cuda:0')
step: 375
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0813,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.7820e-04,  1.2245e-03,  1.6575e-03,  ...,  2.9516e-04,
          9.1970e-05, -1.7443e-03],
        [ 6.6376e-04,  1.3733e-03, -2.7347e-04,  ...,  1.4200e-03,
          8.6451e-04, -3.8862e-04],
        [ 2.7895e-04, -5.4026e-04, -1.3132e-03,  ...,  1.4191e-03,
          5.6076e-04, -8.1110e-04],
        [ 5.0497e-04,  1.9150e-03,  1.1091e-03,  ...,  1.1044e-03,
         -9.6941e-04,  6.8092e-04],
        [-1.2755e-05,  6.3467e-04,  4.5395e-04,  ...,  7.0381e-04,
         -3.0780e-04,  3.7551e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7497, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.0898, -3.1992, -4.4961,  ..., -4.6211, -1.3721, -2.2773],
        [-2.5684, -4.1797, -4.0859,  ..., -4.5234, -3.5996, -0.4905],
        [-3.0332, -3.5020, -4.2500,  ..., -5.1562, -3.7832, -2.1426],
        ...,
        [-4.4531, -3.0000, -3.6250,  ..., -5.2812, -3.1270, -1.0791],
        [-3.5430, -3.1836, -2.9492,  ..., -2.2305, -3.9023, -1.2920],
        [-4.6758, -2.9590, -2.5059,  ..., -5.1484, -1.8809, -1.0059]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26, 27, 13,  0, 27,  5, 27, 27, 27, 27, 26, 27, 18, 18, 27, 23, 25, 11,
         9,  0, 20, 26,  3, 17, 27, 27,  7, 17, 27,  3, 27, 27, 27, 18, 24, 10,
         8, 17, 27, 18, 15, 15, 27, 10,  4,  2, 18, 27, 20,  0, 25, 22, 27, 27,
         1, 10,  0, 27, 27, 20,  0,  9, 24, 27], device='cuda:0')
step: 376
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0813,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.8031e-04, -1.0672e-03, -3.8815e-04,  ..., -7.8154e-04,
         -1.3888e-04, -1.9283e-03],
        [ 1.0729e-03,  9.8991e-04, -3.7813e-04,  ..., -3.5973e-03,
         -4.9829e-04, -2.7218e-03],
        [-4.0054e-04,  9.0504e-04,  6.7711e-04,  ..., -3.7551e-04,
          1.6618e-04,  3.4928e-04],
        [-3.4499e-04, -3.7956e-04,  2.0294e-03,  ...,  1.1330e-03,
          1.1168e-03, -1.4019e-03],
        [ 2.8014e-05, -1.3256e-04,  3.8743e-04,  ..., -1.4591e-04,
         -9.1434e-05, -1.5903e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0385, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.6309, -4.2695, -4.4609,  ..., -0.8804, -3.9121, -1.3809],
        [-4.6875, -4.4062, -4.5625,  ..., -5.5625, -2.1250, -1.3281],
        [-7.0781, -3.6426, -4.4844,  ..., -7.9062, -4.9375, -0.4700],
        ...,
        [-4.3867, -3.1992, -3.3242,  ..., -3.6211, -5.8242, -0.8857],
        [-3.6035, -4.3086, -4.3867,  ..., -4.4805, -3.0586, -0.6509],
        [-3.2910, -3.1660, -4.5078,  ..., -3.5723, -1.7441, -1.3535]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25, 27,  7,  0,  0, 27,  3,  3,  4,  2, 27, 14,  1, 27,  0,  0,  2,  1,
        27, 27, 27, 18, 27, 27, 10, 27, 17,  2, 27, 27, 15, 27, 10, 27, 27, 27,
        27,  2,  6, 15, 15,  1, 18, 18, 27,  1, 25, 24,  1, 10, 13, 27,  7,  7,
         3,  1, 27, 27, 17,  8, 27,  3, 27, 26], device='cuda:0')
step: 377
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0813,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4082, -0.2983, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.7248e-05,  1.4000e-03, -4.2725e-04,  ...,  1.1539e-03,
         -5.0116e-04, -8.0919e-04],
        [ 1.0490e-03,  1.9875e-03,  7.0858e-04,  ...,  4.0174e-04,
         -8.8835e-04, -2.4967e-03],
        [ 6.0081e-04, -1.3018e-03, -1.8663e-03,  ...,  9.0313e-04,
          3.0184e-04, -1.5249e-03],
        [ 2.6894e-04,  5.6219e-04,  4.8780e-04,  ...,  1.4448e-04,
         -2.2340e-04, -5.3215e-04],
        [-1.7178e-04,  1.1072e-03, -4.3201e-04,  ...,  5.8794e-04,
          1.1283e-04, -1.8251e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9058, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.4102, -3.7070, -4.0195,  ..., -4.7227, -3.0527, -1.3799],
        [-3.0820, -4.5039, -5.3633,  ..., -4.7539, -5.2539, -3.8320],
        [-4.6875, -3.2500, -3.3770,  ..., -3.3594, -5.3594, -1.0166],
        ...,
        [-3.1914, -2.7695, -3.1289,  ..., -1.6123, -4.0508, -1.7061],
        [-4.4805, -4.0273, -4.7930,  ..., -5.2617, -2.7773, -1.6201],
        [-3.9668, -3.0898, -3.0742,  ..., -3.1992, -4.9180, -1.6221]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 20, 27,  6, 27, 27,  4, 14,  8, 20, 18,  4, 26,  0, 17,  4, 27,  4,
        27,  4, 20, 15, 25,  0,  7, 15, 27, 17, 27, 27, 10, 27,  4,  3, 15, 26,
         4, 12, 27, 27,  0,  0, 20, 27, 15, 15, 18, 10, 18, 27,  8,  7,  4, 26,
        27, 18,  4,  3,  4, 27,  2, 12,  7,  5], device='cuda:0')
step: 378
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0813,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0147e-03,  1.1978e-03,  7.5388e-04,  ..., -5.0592e-04,
         -4.3941e-04,  6.4468e-04],
        [ 3.4046e-04,  1.3220e-04, -4.7827e-04,  ...,  2.9802e-04,
          7.1430e-04,  4.4608e-04],
        [-2.0862e-04,  5.0688e-04, -3.0661e-04,  ..., -7.5340e-04,
          2.7061e-04, -6.4313e-05],
        [ 2.8372e-04,  1.4901e-05,  1.2903e-03,  ...,  4.6778e-04,
         -6.4564e-04, -4.5562e-04],
        [ 1.7440e-04,  6.8092e-04, -2.0051e-04,  ...,  1.7881e-07,
          2.3842e-07, -7.4327e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0689, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.7949, -4.6992, -6.5898,  ..., -7.9336, -5.9336, -5.6836],
        [-4.5078, -2.7266, -3.4277,  ..., -4.5391, -5.9141, -1.1318],
        [-1.8711, -4.3242, -5.1680,  ..., -6.2148, -4.4180, -2.1836],
        ...,
        [-2.8125, -3.7031, -4.3125,  ..., -5.9219, -4.8438, -0.8291],
        [-6.2227, -3.6133, -4.0039,  ..., -5.5508, -3.5039, -1.0508],
        [-4.7578, -4.0703, -4.8203,  ..., -4.8672, -4.8203, -0.6001]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18,  5, 27,  2,  6, 10, 27, 27,  8, 27, 27,  3, 27, 17, 27,  3,  3,  3,
        25,  3, 12, 27,  4, 27,  0, 25,  4, 15, 27, 20, 18, 27, 24,  4, 22, 18,
        26,  2, 27,  7, 27, 27, 22,  1,  5, 27, 27, 27,  3, 27, 27,  4, 27, 27,
         0, 17,  2,  3,  1, 20, 26, 27,  6, 20], device='cuda:0')
step: 379
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2379, -0.6147,  ...,  0.4785, -0.0812,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.3780e-04, -1.1177e-03, -8.4734e-04,  ..., -4.2021e-05,
          4.4441e-04,  1.4772e-03],
        [ 3.2043e-04, -3.1567e-04, -2.3735e-04,  ..., -5.4836e-04,
         -8.0490e-04, -8.5545e-04],
        [ 1.9622e-04,  2.5225e-04,  6.4969e-06,  ..., -2.9826e-04,
          5.5647e-04,  4.7851e-04],
        [ 8.0681e-04, -6.0558e-04, -1.0328e-03,  ..., -2.5654e-04,
         -4.5347e-04, -1.7214e-03],
        [ 4.5252e-04,  2.4796e-04, -8.9884e-04,  ..., -7.1824e-05,
          1.5318e-04, -4.8876e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1338, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.6680, -4.2031, -4.5625,  ..., -4.1523, -4.3281, -0.3579],
        [-2.5957, -4.3281, -5.1875,  ..., -5.3750, -4.3750, -4.6562],
        [-5.9062, -4.6211, -5.0742,  ..., -7.0781, -5.1367, -0.1385],
        ...,
        [-2.9961, -3.2930, -3.0898,  ..., -3.1680, -4.5898, -0.9019],
        [-3.7266, -3.3672, -4.7109,  ..., -5.7578, -3.9609, -0.3826],
        [-4.9805, -3.6699, -4.1680,  ..., -4.0586, -4.9180, -0.4968]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 25, 27, 27, 27,  0, 25, 27, 25,  9, 18,  9,  1,  0,  2, 27,  5, 26,
        18,  2, 24, 20, 10,  4,  0, 27, 26,  4, 11, 27, 27, 15, 27, 20, 27, 15,
        27,  0, 15, 27, 18,  6, 27, 27, 14, 25,  4, 10,  4, 15, 27, 27, 26,  5,
        27, 20, 20, 27, 18, 26, 18, 15, 27,  4], device='cuda:0')
step: 380
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2380, -0.6147,  ...,  0.4785, -0.0812,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.2343e-04,  2.4486e-04,  1.1454e-03,  ...,  7.1287e-04,
         -1.4877e-04,  5.7888e-04],
        [ 4.3368e-04, -3.6407e-04,  2.6751e-04,  ..., -9.1195e-05,
         -1.1539e-03, -4.7708e-04],
        [-2.0385e-04, -9.0599e-04,  5.4359e-04,  ..., -5.9605e-04,
         -1.5364e-03,  9.8801e-04],
        [-1.0366e-03,  2.6870e-04,  1.3552e-03,  ...,  1.0338e-03,
          1.5533e-04,  5.1165e-04],
        [-5.5647e-04,  4.9877e-04,  3.4070e-04,  ...,  4.9210e-04,
         -4.3297e-04,  3.5191e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7078, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.6934, -4.3477, -5.3164,  ..., -4.7852, -4.9727, -0.5049],
        [-3.7109, -4.0859, -5.0234,  ..., -5.4922, -5.6797, -0.8208],
        [-3.0820, -4.0820, -4.2070,  ..., -5.3320, -5.3164, -5.8633],
        ...,
        [-4.2578, -3.2109, -4.4297,  ..., -4.1328, -3.3203, -1.5391],
        [-4.1562, -3.6270, -3.5176,  ..., -2.1895, -2.4551, -0.8452],
        [-4.1328, -4.2539, -4.0977,  ..., -4.9727, -4.7695, -3.1934]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 18, 27, 27, 27, 27, 23, 10,  6, 11, 27, 20,  0,  0, 22,  6,  5,
        25, 18,  1,  0,  7, 27,  4, 15, 17,  7,  3,  0, 27, 27, 14, 27, 15, 27,
        25, 17,  0, 27, 26, 22, 14,  7,  0,  0, 27, 27, 27, 15, 27,  0, 27, 18,
        15, 17, 12, 27, 17, 22,  0,  7, 27,  8], device='cuda:0')
step: 381
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2380, -0.6147,  ...,  0.4785, -0.0812,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.2445e-03,  5.8174e-04,  4.8113e-04,  ..., -8.8930e-04,
          7.3373e-05,  7.4959e-04],
        [-1.0312e-04, -2.9278e-04, -8.2064e-04,  ..., -1.5569e-04,
          9.8705e-04,  5.9700e-04],
        [-7.1573e-04,  5.4312e-04,  8.3160e-04,  ...,  6.3002e-05,
         -5.1928e-04,  1.4095e-03],
        [ 3.0637e-05, -5.0068e-04, -1.0735e-04,  ..., -5.1451e-04,
         -1.7595e-04, -2.8515e-04],
        [-1.5283e-04,  7.5698e-06, -7.4744e-05,  ...,  2.9731e-04,
          1.5831e-04,  3.8838e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2654, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.1855, -3.3262, -4.9531,  ..., -5.0781, -3.8262, -1.2168],
        [-3.7832, -3.5801, -3.7988,  ..., -0.8931, -3.0957, -2.3301],
        [-5.9297, -3.9629, -5.5430,  ..., -5.7422, -4.9648, -0.3220],
        ...,
        [-4.6758, -3.4414, -3.2695,  ..., -1.2217, -4.4258, -2.5332],
        [-0.4211, -4.0781, -5.7500,  ..., -7.5781, -3.9062, -4.2500],
        [-2.9355, -4.3867, -4.1836,  ..., -4.0938, -3.5449, -0.4661]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 24, 27,  3, 27, 25, 18,  0, 22, 20, 26,  6, 26, 27, 22, 27, 27, 27,
         2, 15, 27, 13, 27, 15, 12, 20,  3,  7, 13,  1, 15, 27, 17,  6,  4, 20,
        22,  8, 27,  1,  0, 10,  2, 25, 25, 13, 27, 27,  7,  0,  5,  1, 15, 27,
        13, 27, 10, 13, 18, 27,  6,  9,  0, 21], device='cuda:0')
step: 382
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2380, -0.6147,  ...,  0.4785, -0.0812,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.6280e-04,  1.3232e-04, -3.8719e-03,  ...,  1.0657e-04,
          6.4373e-04, -2.6855e-03],
        [ 1.3475e-03, -1.9140e-03, -2.0278e-04,  ..., -1.4458e-03,
         -3.7155e-03,  6.2227e-04],
        [ 4.3631e-05, -1.8940e-03, -1.3709e-06,  ...,  1.0592e-04,
         -7.2765e-04, -1.0449e-04],
        [-4.1723e-04,  9.5963e-06,  3.9160e-05,  ..., -1.1146e-05,
         -3.2187e-04, -1.4315e-03],
        [-3.5453e-04, -6.8569e-04,  2.0075e-04,  ...,  4.2415e-04,
          1.3328e-04, -3.1996e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0618, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6328, -3.5078, -4.1641,  ..., -6.3984, -3.3672, -0.7896],
        [-4.0000, -4.4062, -6.1875,  ..., -4.5625, -2.7324, -3.2188],
        [-4.9648, -3.5430, -3.7461,  ..., -4.7148, -3.4180, -1.0898],
        ...,
        [-6.6406, -4.5156, -5.4531,  ..., -7.2656, -6.2500, -0.1561],
        [-3.7109, -2.4453, -4.1016,  ..., -2.2891, -4.1172, -2.1016],
        [-6.1914, -4.4727, -4.7070,  ..., -6.8164, -6.9414, -0.1431]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 13,  7, 11, 27, 27,  1,  0, 27,  7,  5,  1, 22, 11, 27, 26, 27,  0,
        27, 27, 27,  1, 13, 15,  4,  2,  0, 27,  4, 15, 27, 15,  1, 10, 10,  5,
        27,  0, 12,  0, 27,  3, 13,  3,  7, 27, 20, 27, 11, 27,  5,  1, 17, 27,
        26, 14, 27,  4, 10, 14,  0, 27,  3, 27], device='cuda:0')
step: 383
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2380, -0.6147,  ...,  0.4785, -0.0812,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.5947e-04,  9.4509e-04,  5.6696e-04,  ..., -5.6791e-04,
         -1.4782e-04,  1.4811e-03],
        [ 8.3447e-04, -2.7657e-03, -3.7479e-04,  ..., -2.6665e-03,
         -1.7233e-03,  2.1935e-03],
        [ 2.6464e-04, -6.6376e-04,  8.2350e-04,  ..., -1.3113e-03,
         -1.4429e-03,  1.4277e-03],
        [ 4.4155e-04, -2.5463e-04, -9.5904e-05,  ..., -1.0834e-03,
          2.0516e-04,  1.2627e-03],
        [-4.2975e-05, -5.6744e-05,  3.7551e-04,  ..., -1.4424e-04,
         -3.4142e-04,  6.3181e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9995, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9668, -3.0293, -3.1387,  ..., -3.5918, -3.9668, -0.7803],
        [-3.9102, -4.2852, -3.8613,  ..., -6.5352, -6.5820, -2.1602],
        [-5.5078, -3.9902, -4.9727,  ..., -5.5859, -4.9883, -0.3181],
        ...,
        [-1.3604, -2.5625, -4.3594,  ..., -5.1562, -2.1875, -2.9062],
        [-1.5928, -2.8105, -3.9980,  ..., -4.7500, -2.2324, -2.4043],
        [-4.3984, -2.7734, -3.6309,  ..., -2.6621, -4.5234, -1.2256]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1,  8, 27, 27, 14,  7, 27,  6,  3, 27, 20, 27,  8, 15, 27, 27,  0,  3,
        15,  2, 20, 21, 10,  3, 27,  7,  0, 27, 27, 27,  1, 11,  9, 25,  0,  4,
        15,  2, 27, 17, 15, 26,  5, 22,  3,  1, 27, 22,  7, 19,  0, 27,  0, 15,
        27,  2, 27,  0, 10, 10, 27,  0, 26, 10], device='cuda:0')
step: 384
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2380, -0.6147,  ...,  0.4788, -0.0812,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.9026e-04, -3.4022e-04,  3.0041e-04,  ...,  3.2067e-04,
          1.6904e-04, -3.2401e-04],
        [ 2.5129e-04, -4.1533e-04,  1.1187e-03,  ...,  3.3188e-03,
         -3.8767e-04, -8.0252e-04],
        [ 1.8740e-04, -7.2002e-04, -8.0824e-04,  ...,  2.5296e-04,
          6.7520e-04,  2.3556e-04],
        [-2.2376e-04, -6.4993e-04, -1.1289e-04,  ..., -2.7919e-04,
          3.4070e-04, -4.7517e-04],
        [ 2.5034e-04,  1.5712e-04, -8.5831e-06,  ...,  1.0252e-05,
          1.5044e-04, -2.6226e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1741, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.7402, -2.9746, -3.8340,  ..., -4.1641, -3.5059, -1.0371],
        [-4.2578, -2.6348, -2.3848,  ..., -2.1973, -3.5254, -1.6191],
        [-2.8496, -3.5840, -4.5977,  ..., -6.0664, -4.3320, -6.0039],
        ...,
        [-1.7334, -1.9062, -4.4219,  ..., -5.6250, -4.2656, -1.5615],
        [-4.6211, -2.8086, -3.6055,  ..., -3.7129, -5.9023, -0.7607],
        [-2.9375, -4.6094, -5.3906,  ..., -4.9688, -4.8438, -3.6895]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 27, 18, 10,  9, 10, 27,  5,  0, 24, 26,  2, 27, 20, 27, 18,  1,  1,
         7,  2, 11, 27, 27, 27,  1, 10, 27, 19, 27,  4,  6, 27,  6, 17, 27, 20,
         0, 27, 26, 10, 26, 27, 27, 27, 27, 22, 27, 17,  7, 27, 27, 27, 27, 27,
        27, 18, 27, 20, 10,  0,  5,  4,  5, 27], device='cuda:0')
step: 385
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2380, -0.6147,  ...,  0.4788, -0.0812,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.5039e-03, -2.1915e-03, -4.1962e-04,  ...,  2.5368e-04,
         -7.4148e-04, -5.6505e-04],
        [-3.5906e-04,  8.5640e-04,  4.8399e-04,  ...,  6.8998e-04,
          5.2881e-04,  1.6880e-04],
        [ 8.7261e-04, -1.1510e-04, -7.7105e-04,  ..., -3.7313e-04,
          1.4839e-03, -5.3740e-04],
        [-2.3115e-04, -4.8780e-04,  1.0223e-03,  ...,  2.1148e-04,
          3.2830e-04, -4.1389e-04],
        [ 7.3528e-04,  6.6996e-04, -9.1362e-04,  ..., -7.6890e-06,
          1.8764e-04, -1.0424e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0621, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.1738, -3.2832, -4.3633,  ..., -4.7383, -5.2852, -1.4873],
        [-5.0078, -3.4590, -4.0078,  ..., -5.1016, -3.5840, -1.1465],
        [-3.5859, -1.8369, -2.9922,  ..., -3.5234, -2.3984, -2.3516],
        ...,
        [-5.1602, -4.6758, -4.4570,  ..., -4.5977, -5.9570, -0.1613],
        [-2.9941, -3.0098, -3.6816,  ..., -5.0703, -5.5703, -1.6348],
        [-1.6270, -3.8926, -6.5312,  ..., -5.4414, -4.1289, -5.4531]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4,  7, 27, 18, 10, 25,  1,  3, 26, 27, 15, 17, 27, 27,  4, 27,  0,  9,
        25,  0, 25, 27,  9, 15, 27, 18, 24, 22,  8,  3, 27,  8,  7, 27,  7, 27,
        18,  3,  9, 15,  9,  7, 18, 27,  7, 10, 13, 27,  4,  1, 27,  1, 27,  5,
        27, 25,  0, 10, 15,  6, 12, 27, 27,  0], device='cuda:0')
step: 386
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2380, -0.6147,  ...,  0.4788, -0.0812,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.8590e-04, -1.1091e-03, -7.8583e-04,  ...,  7.0763e-04,
         -5.6326e-05, -1.4648e-03],
        [ 7.2241e-04,  1.3123e-03,  1.1339e-03,  ...,  1.4200e-03,
         -9.6560e-04, -2.1420e-03],
        [-2.6751e-04,  1.0099e-03, -2.8157e-04,  ..., -4.9472e-06,
          7.5817e-04, -5.0068e-04],
        [-7.0095e-04,  3.7098e-04,  2.2335e-03,  ...,  2.1815e-04,
         -7.1883e-05,  1.7309e-03],
        [-6.2895e-04, -1.1692e-03,  8.2207e-04,  ...,  1.2565e-04,
         -3.0422e-04, -2.2674e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8859, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.4766, -2.4922, -3.8359,  ..., -5.8047, -6.3516, -1.7578],
        [-4.4180, -2.9961, -2.8242,  ..., -3.8398, -6.3398, -2.7773],
        [-5.2695, -3.8320, -4.1758,  ..., -3.5352, -4.8945, -0.8320],
        ...,
        [-4.0312, -2.7500, -2.5605,  ..., -1.2021, -5.6094, -2.5781],
        [-4.6680, -3.0723, -2.5254,  ..., -1.5264, -4.8711, -2.4785],
        [-2.9395, -3.4238, -4.6289,  ..., -5.5977, -5.3477, -0.7520]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27, 27, 15, 18,  4,  4, 27, 17, 20, 10,  2, 20, 27, 17,  2, 20,
        27, 27, 15, 27, 10, 27, 27, 13,  2, 18, 27, 27, 27, 27, 15,  0, 18, 19,
         7, 24,  4, 26, 27, 27, 17,  4, 27,  4, 15, 10, 26,  8,  6, 27, 27,  9,
         7, 27,  4, 14, 27, 27, 11,  2, 11,  4], device='cuda:0')
step: 387
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6147,  ...,  0.4788, -0.0812,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.4605e-04, -3.2024e-03,  2.4986e-04,  ...,  4.8208e-04,
         -9.5701e-04, -3.5667e-04],
        [-4.9973e-04,  2.4681e-03,  9.9468e-04,  ..., -1.0008e-04,
         -3.1352e-04, -8.5974e-04],
        [-6.7568e-04,  1.3475e-03,  6.3992e-04,  ...,  1.7061e-03,
          9.0790e-04,  2.9254e-04],
        [-1.2684e-04,  1.6689e-04,  1.5097e-03,  ...,  4.3082e-04,
         -6.3133e-04, -2.6989e-04],
        [-2.3556e-04,  1.0452e-03, -5.3120e-04,  ...,  3.4571e-04,
         -7.7426e-05, -9.0933e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0260, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6289, -3.2715, -4.3164,  ..., -4.9258, -3.7559, -0.6924],
        [-4.5430, -2.8418, -3.2012,  ..., -3.2324, -2.5918, -1.2314],
        [-3.6387, -2.2480, -2.6543,  ..., -1.9355, -4.5273, -2.9355],
        ...,
        [-3.2656, -2.9531, -4.0938,  ..., -5.6250, -3.2188, -0.8438],
        [-1.8779, -4.0195, -7.1914,  ..., -7.4258, -7.0039, -7.2383],
        [-4.4766, -3.0215, -3.7402,  ..., -5.9297, -4.4141, -0.4600]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22, 27, 10, 27, 12, 27,  7,  2,  3,  1, 10,  0,  7, 26, 18,  6, 27, 27,
         7, 22, 27, 10, 14, 22,  0, 27,  4,  0, 10,  6, 13, 15, 27,  3, 27,  2,
        11,  5,  1, 27, 15, 13, 27, 27,  0,  5, 20,  1, 27, 20, 27, 15, 10,  4,
        20, 24, 10,  9, 27, 27, 27, 27,  0, 27], device='cuda:0')
step: 388
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6147,  ...,  0.4788, -0.0812,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.8181e-04, -1.8854e-03, -3.0208e-04,  ...,  4.8018e-04,
          2.0862e-05,  7.2479e-04],
        [ 5.5790e-05,  3.7503e-04,  7.1907e-04,  ..., -4.2224e-04,
         -1.0462e-03, -2.0885e-04],
        [ 5.3072e-04,  4.4990e-04, -4.8637e-04,  ..., -7.7486e-04,
          8.4162e-04, -5.9557e-04],
        [ 7.9584e-04, -2.6321e-04,  9.3818e-05,  ..., -4.7684e-06,
          8.8215e-04, -3.1114e-04],
        [ 4.1962e-04,  9.8038e-04, -4.3535e-04,  ...,  1.0246e-04,
          2.0742e-04, -6.2943e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9446, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.6953, -3.5078, -6.2266,  ..., -6.0391, -5.2891, -5.3672],
        [-2.1934, -3.0371, -3.0059,  ..., -3.4121, -4.0977, -2.4746],
        [-4.9609, -2.7422, -3.2109,  ..., -5.0547, -4.2578, -0.6484],
        ...,
        [-4.6953, -2.4277, -3.5371,  ..., -2.6621, -3.9902, -1.2725],
        [-2.3711, -4.4492, -5.6523,  ..., -8.0938, -6.4336, -5.7305],
        [-4.5664, -2.4727, -3.0059,  ..., -2.6758, -5.1914, -2.7090]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15, 18, 27,  4, 27, 27, 26, 27, 27, 10, 19, 22, 18,  4, 27, 13, 15, 27,
        10,  9, 27,  1, 13,  0,  1, 17,  0, 27, 27, 11, 27, 10,  4, 20, 15, 27,
        27,  3,  0, 27, 15, 27, 27, 27, 27, 10, 27,  8, 27, 17, 27, 22,  1, 17,
        27, 25, 20, 10,  9,  2, 27, 10, 18, 25], device='cuda:0')
step: 389
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6147,  ...,  0.4788, -0.0812,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.8215e-04,  5.1856e-06,  1.0672e-03,  ...,  1.1511e-03,
         -6.6280e-05, -2.3670e-03],
        [ 4.1819e-04, -3.3307e-04, -3.7074e-04,  ...,  2.0618e-03,
          1.7633e-03,  1.4524e-03],
        [-4.3154e-04, -1.4830e-04, -7.3318e-03,  ...,  7.5493e-03,
          3.8033e-03, -3.1204e-03],
        [-4.8876e-04,  3.8147e-04, -1.4563e-03,  ...,  1.3208e-03,
          1.8716e-04, -5.2166e-04],
        [-4.7946e-04,  4.9019e-04,  5.3549e-04,  ...,  4.7874e-04,
          2.2352e-05,  8.0681e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9504, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.7891, -2.8965, -3.5840,  ..., -6.0547, -4.2578, -0.5381],
        [-1.2598, -3.4160, -4.2148,  ..., -4.3828, -2.9004, -2.0098],
        [-4.9336, -2.5430, -3.6523,  ..., -5.5742, -5.0898, -0.9648],
        ...,
        [-4.6914, -3.3008, -3.6758,  ..., -4.5820, -4.5508, -0.8950],
        [-2.3008, -2.5527, -4.1133,  ..., -3.8633, -5.3320, -1.6924],
        [-4.4688, -3.6230, -3.7168,  ..., -3.5137, -4.0781, -0.7637]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 18, 22,  9, 27, 27, 27, 27, 15,  9,  1,  9, 22, 18,  0, 27, 25, 27,
         4,  3,  4,  8, 27, 27, 20, 15, 27, 18, 11, 18, 25,  0,  1, 15, 27, 10,
         5, 26,  2, 15, 11, 27, 27, 20, 27,  4, 27, 25, 10, 26, 20, 27,  7,  6,
         7, 15, 27,  7, 27, 10, 22, 27, 17, 27], device='cuda:0')
step: 390
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6147,  ...,  0.4785, -0.0812,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.1832e-04, -1.2999e-03, -7.2432e-04,  ...,  3.0136e-04,
          5.6839e-04, -9.0075e-04],
        [ 1.7834e-04,  9.3043e-05, -4.0340e-04,  ..., -1.1272e-03,
          6.0463e-04,  1.2169e-03],
        [ 4.8828e-04,  1.5783e-04,  2.2817e-04,  ...,  5.2929e-04,
         -1.9085e-04, -2.8789e-05],
        [ 1.1997e-03,  2.2256e-04, -8.9407e-04,  ..., -8.3745e-05,
          4.7708e-04, -5.2118e-04],
        [ 1.9741e-04, -2.9087e-04, -3.8719e-04,  ...,  2.6417e-04,
         -9.7811e-05, -6.1214e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0526, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.4824, -3.8730, -4.5625,  ..., -4.3594, -5.3281, -0.6392],
        [-5.0195, -5.6914, -6.6328,  ..., -7.6914, -4.4609, -3.0996],
        [-4.7695, -3.9277, -2.3652,  ..., -2.4902, -3.8340, -1.8340],
        ...,
        [-3.6777, -2.7559, -3.5996,  ..., -5.9766, -6.6758, -1.2090],
        [-3.1777, -2.8496, -2.8340,  ..., -3.4121, -4.1172, -1.4277],
        [-4.4805, -3.8398, -3.9180,  ..., -3.4824, -4.2305, -1.0127]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 5,  7,  9, 15,  6, 27, 15,  0, 27, 18,  9,  5, 15, 17,  4, 27, 23,  7,
        27, 13, 25, 17, 14, 27, 27, 27, 27, 15, 27, 22, 22,  5,  1, 15, 24, 11,
        22, 27,  1,  4, 27,  9, 18, 27, 24, 15, 27, 20, 27, 17, 27, 27, 25, 27,
        25, 18, 14,  6, 22, 27, 27, 27,  1, 27], device='cuda:0')
step: 391
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4785, -0.0812,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.8459e-05, -2.3174e-03,  4.0030e-04,  ..., -1.7548e-03,
         -1.4000e-03, -1.9562e-04],
        [ 9.1612e-05,  1.5965e-03,  1.0242e-03,  ...,  1.9588e-03,
          1.3685e-03, -5.2166e-04],
        [-5.1546e-04,  1.9989e-03,  8.2779e-04,  ...,  9.3842e-04,
          1.0061e-03,  4.9019e-04],
        [-4.2343e-04, -1.8334e-04,  1.8463e-03,  ...,  4.8399e-04,
          6.8617e-04,  6.5041e-04],
        [ 2.1839e-04, -1.0091e-04,  3.7289e-04,  ..., -2.8682e-04,
         -1.7357e-04, -3.1567e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8230, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2422, -4.4766, -4.4297,  ..., -5.4766, -4.8984, -0.3516],
        [-2.5762, -3.3574, -3.5762,  ..., -3.7637, -3.6699, -1.8428],
        [-3.2871, -2.9434, -3.7871,  ..., -3.3184, -3.2402, -1.1621],
        ...,
        [-3.6348, -2.7441, -3.1035,  ..., -2.0723, -4.5078, -2.4941],
        [-2.8965, -3.1309, -5.0859,  ..., -4.4297, -3.6309, -5.2422],
        [-1.6406, -3.0000, -4.5312,  ..., -4.7188, -2.2656, -2.6094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0, 27,  0, 18,  1, 27,  4, 20, 17, 27, 22, 27, 14, 27,  7, 27, 27,
        17, 27, 27, 20,  0, 27, 26,  0, 20,  0,  3, 13, 11,  2, 15, 27,  4, 25,
        27, 27,  2, 27, 27, 27, 27, 15, 13, 27,  7, 15, 22,  7, 27, 25,  5, 10,
         0,  2,  0,  4, 24, 27, 27,  3, 23,  1], device='cuda:0')
step: 392
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4785, -0.0812,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.5490e-04, -8.1873e-04, -3.0279e-04,  ...,  1.3475e-03,
          5.1165e-04, -9.8991e-04],
        [ 7.9060e-04,  8.8632e-05, -1.6427e-04,  ...,  1.8663e-03,
          7.0381e-04,  1.2189e-04],
        [ 1.5330e-04, -3.0255e-04,  4.7064e-04,  ...,  3.1757e-04,
         -5.4693e-04,  3.8099e-04],
        [-3.9577e-04,  2.5511e-04, -3.8981e-04,  ..., -1.5998e-04,
          4.3726e-04,  7.2002e-04],
        [-4.1842e-05, -1.8978e-04,  2.1124e-04,  ...,  3.6097e-04,
          2.0123e-04,  2.0373e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0291, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.9688, -3.5781, -4.2031,  ..., -5.0156, -4.1250, -0.8589],
        [-5.3906, -3.0781, -3.5938,  ..., -5.6562, -4.0000, -1.0781],
        [-6.3555, -3.7461, -4.9180,  ..., -4.8711, -5.8242, -0.3247],
        ...,
        [-3.1777, -3.6152, -3.8789,  ..., -4.6758, -3.2227, -0.8174],
        [-1.9980, -3.1074, -5.0117,  ..., -4.6211, -3.7168, -1.4199],
        [-3.7109, -4.3828, -4.6484,  ..., -5.8359, -4.1484, -2.0078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 27, 27, 26, 27, 20, 27, 14, 26, 27,  3, 27, 18, 27, 15,  8, 10,  3,
         0, 17,  1, 27, 27, 25,  3,  3, 10,  3, 27,  0, 27,  0,  7, 15,  7, 22,
         8, 27, 14,  4, 15, 27, 27,  1, 27, 27,  7, 27, 15, 17, 15, 18, 26, 27,
         0,  0, 23,  7, 20,  4, 27,  4,  1, 20], device='cuda:0')
step: 393
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4785, -0.0812,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.2999e-03, -3.9530e-04,  1.3847e-03,  ..., -8.1730e-04,
         -4.3631e-04, -1.2082e-04],
        [-1.4186e-04, -7.4053e-04,  4.5300e-04,  ...,  1.9150e-03,
          1.9569e-03,  2.4796e-04],
        [ 1.4853e-04, -8.2016e-04, -6.1083e-04,  ...,  2.4128e-04,
          3.2353e-04, -2.6155e-04],
        [-1.0532e-04, -1.6737e-04,  1.6713e-04,  ..., -1.0580e-04,
          1.1330e-03, -6.6698e-05],
        [-1.2577e-05,  2.2912e-04,  2.8920e-04,  ...,  3.3140e-04,
          3.9339e-04, -3.7861e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7480, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.7852, -3.2227, -4.1914,  ..., -4.4570, -4.5977, -0.8174],
        [-4.3086, -3.4023, -3.9805,  ..., -5.7148, -3.9180, -0.5894],
        [-3.8145, -2.9707, -4.2969,  ..., -4.0625, -4.8125, -1.0010],
        ...,
        [-5.2031, -3.5020, -3.7988,  ..., -5.0195, -5.6094, -0.5488],
        [-4.5703, -2.8652, -2.6152,  ..., -2.7871, -4.9141, -1.0693],
        [-2.8945, -2.8789, -6.2852,  ..., -6.1914, -5.0352, -5.3320]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 26,  0, 27,  3, 26, 27,  2, 27, 20, 27, 15,  3, 27, 27, 10,  3,  6,
        27, 15, 10,  0, 27, 27, 18,  0, 17,  2, 17, 25, 15,  1,  3,  7, 18, 27,
        27, 27, 27, 27, 27, 27, 27,  2, 27,  6, 27, 23,  8, 15,  0, 27,  0, 27,
        13,  4, 10, 18, 27, 27,  0,  3,  3, 15], device='cuda:0')
step: 394
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4785, -0.0811,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.4390e-04, -2.7156e-04,  8.1348e-04,  ...,  8.5831e-06,
         -6.4754e-04, -4.4703e-06],
        [ 6.2895e-04, -2.2650e-04,  2.9898e-04,  ...,  2.3575e-03,
          1.4696e-03,  2.3210e-04],
        [ 4.1699e-04, -6.9141e-04, -6.2513e-04,  ...,  9.9778e-05,
          8.5163e-04,  5.9605e-04],
        [-5.6982e-04,  4.8041e-04, -5.6839e-04,  ..., -1.7834e-04,
          3.0899e-04,  7.6103e-04],
        [-1.8215e-04,  1.3618e-03, -3.7909e-04,  ...,  6.2943e-04,
          1.4067e-04, -2.3258e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9116, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.4336, -2.5449, -3.8242,  ..., -2.0117, -4.4961, -1.9346],
        [-3.4160, -4.2891, -4.9922,  ..., -4.1484, -4.2891, -3.6348],
        [-4.8555, -3.5293, -3.7480,  ..., -5.3398, -6.0898, -0.5762],
        ...,
        [-1.7510, -3.6406, -3.5957,  ..., -3.9082, -2.1562, -2.0312],
        [-4.8516, -3.2715, -2.4746,  ..., -3.9746, -3.3965, -2.5840],
        [-3.7305, -2.7129, -2.5586,  ..., -2.7617, -3.2461, -1.8086]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 23,  5, 27,  3,  5, 17, 27, 18, 20,  7, 27, 27, 27, 27, 27,  4,  3,
        18, 27, 13, 18, 27, 27, 22,  5, 27,  5, 15, 27, 17, 12,  9,  4,  7, 15,
        23, 15, 18,  4, 27, 17, 27, 18, 27,  7, 27, 27,  1, 25, 27, 27, 15,  4,
        27, 15, 27,  0, 15,  4, 27,  9, 14, 12], device='cuda:0')
step: 395
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4785, -0.0811,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.4005e-04,  9.5320e-04, -1.0586e-03,  ..., -4.2772e-04,
          8.0824e-04,  5.5313e-04],
        [ 2.2483e-04, -6.2847e-04, -4.8971e-04,  ..., -1.6556e-03,
         -3.6335e-04,  8.8787e-04],
        [ 4.1580e-04,  1.6809e-05,  1.6904e-04,  ...,  2.3007e-04,
         -3.6120e-04, -4.2248e-04],
        [ 5.3120e-04, -6.6376e-04, -2.4548e-03,  ..., -1.5030e-03,
          2.8396e-04, -1.3590e-03],
        [-2.2113e-05, -1.6861e-03,  3.5477e-04,  ..., -9.7942e-04,
         -4.4513e-04,  5.1641e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.4416, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0820, -3.5664, -2.5195,  ..., -2.8477, -4.0195, -3.4258],
        [-6.3359, -4.5703, -4.2734,  ..., -5.4453, -6.5859, -0.1487],
        [-2.0996, -2.3652, -5.2578,  ..., -4.5234, -5.4922, -2.2715],
        ...,
        [-4.2500, -3.6562, -3.5781,  ..., -3.2812, -3.6875, -0.7969],
        [-4.1523, -4.0586, -4.5586,  ..., -6.3594, -6.1250, -0.4355],
        [-6.3320, -4.7695, -5.1133,  ..., -6.5352, -6.3789, -0.1274]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 27,  0, 27, 20, 27, 27, 15, 27, 22, 20,  0, 26, 15, 15, 27, 10, 27,
         7, 27, 25, 27, 27, 17,  4,  0, 18, 27,  9, 27, 27, 15, 18, 15,  1,  4,
        25, 27, 25, 14, 27, 27, 27, 18, 10, 27,  2, 15,  0, 27,  4, 15,  0, 27,
        10, 15,  0,  3, 18,  6,  3, 27, 27, 27], device='cuda:0')
step: 396
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4785, -0.0811,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.5654e-04,  1.5211e-03,  7.3528e-04,  ..., -6.9475e-04,
          1.6737e-04, -7.7534e-04],
        [ 3.0565e-04, -1.3852e-04, -4.1413e-04,  ...,  1.0166e-03,
          1.4172e-03, -7.4768e-04],
        [-5.5194e-05, -8.6164e-04, -3.6001e-05,  ...,  5.2214e-04,
          6.0368e-04,  1.1349e-03],
        [-5.1022e-04, -4.6921e-04,  5.6887e-04,  ..., -4.8399e-04,
          2.9492e-04, -4.0007e-04],
        [ 1.4043e-04, -1.0818e-04,  5.0497e-04,  ..., -7.2718e-06,
          2.8133e-04,  4.2391e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1835, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9590, -4.1484, -5.0078,  ..., -5.8203, -5.3359, -0.3350],
        [-4.9219, -3.9844, -4.2031,  ..., -6.8125, -4.7969, -0.3281],
        [-4.5078, -4.3047, -3.7441,  ..., -3.7285, -4.5078, -0.2910],
        ...,
        [-4.9297, -3.9609, -4.4297,  ..., -6.4766, -5.4609, -0.4453],
        [-5.3398, -4.6680, -5.3242,  ..., -6.4336, -6.5430, -0.1044],
        [-5.3555, -3.0586, -3.0273,  ..., -3.4805, -3.3555, -0.7461]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  7, 16, 18,  0, 27, 24,  7, 10,  1, 27,  8, 10, 13, 27,  7, 13, 10,
         7,  4, 27, 15,  9,  2,  1, 10,  4, 26, 27, 10, 27,  7, 27, 27,  0,  9,
        27, 20,  1, 15,  8, 24, 27,  3,  0,  4, 27, 27, 27,  3, 27, 27, 17, 27,
        12, 15, 15, 17,  7, 22, 27,  6,  5,  3], device='cuda:0')
step: 397
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4785, -0.0811,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.6689e-03,  2.9993e-04, -1.5326e-03,  ..., -3.8695e-04,
          7.1096e-04,  2.4438e-06],
        [-1.9991e-04, -1.0662e-03, -1.0653e-03,  ..., -7.0715e-04,
         -8.3447e-07,  7.5245e-04],
        [ 5.9509e-04, -1.7571e-04, -8.6689e-04,  ..., -1.1759e-03,
          1.8275e-04,  3.3069e-04],
        [ 7.5960e-04,  1.7476e-04, -3.0351e-04,  ..., -2.2233e-04,
         -8.8644e-04,  2.5105e-04],
        [ 7.1764e-04, -1.4925e-04, -1.1234e-03,  ..., -2.6369e-04,
         -4.9782e-04, -2.9707e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7216, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.5957, -3.6582, -5.5312,  ..., -7.7031, -5.1406, -2.6270],
        [-4.2109, -2.9922, -4.1172,  ..., -5.6797, -3.6797, -0.9141],
        [-4.7188, -3.1543, -3.8262,  ..., -4.5469, -4.0781, -0.6392],
        ...,
        [-3.6953, -2.6328, -3.3516,  ..., -2.0859, -4.5703, -2.8047],
        [-6.7227, -4.4727, -5.3164,  ..., -6.6914, -6.6289, -0.2222],
        [-1.7061, -3.1914, -4.6289,  ..., -6.0039, -6.0195, -1.3633]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  7, 27,  1, 27,  0,  9, 18, 27, 27,  0, 25,  8, 27, 27, 27, 22,  2,
        27, 27,  4,  0, 27, 27, 27, 22, 27, 15, 27,  0,  4, 27, 15, 27,  0, 10,
         7,  4, 10, 26,  0, 27, 27, 27, 27, 27, 17,  2,  7, 27, 27,  3, 18, 27,
        17, 15,  4, 10,  0,  4, 27, 11, 27,  0], device='cuda:0')
step: 398
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4785, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.6035e-04, -4.0936e-04, -3.4690e-04,  ...,  1.4029e-03,
          2.0921e-05,  2.4617e-05],
        [-2.0087e-05,  6.3562e-04,  8.2731e-04,  ..., -1.3809e-03,
         -2.5196e-03, -3.5286e-04],
        [ 8.1897e-05,  2.8515e-04, -2.1291e-04,  ..., -5.6219e-04,
         -4.3917e-04,  4.8923e-04],
        [ 2.4509e-04, -3.3426e-04, -5.6219e-04,  ...,  2.9945e-04,
          2.5749e-04,  7.4387e-04],
        [ 2.8682e-04, -3.2711e-04, -4.3583e-04,  ..., -3.2163e-04,
         -2.2531e-04,  1.4925e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7337, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.3594, -3.9707, -4.5781,  ..., -7.6875, -5.1406, -0.2040],
        [-3.2598, -3.3691, -3.7129,  ..., -4.3828, -2.6973, -1.1973],
        [-4.9961, -2.4512, -3.2793,  ..., -2.4980, -5.2188, -1.3574],
        ...,
        [-0.8823, -3.5859, -4.6484,  ..., -7.7266, -5.0547, -3.6953],
        [-5.1680, -3.5762, -4.1680,  ..., -5.2148, -4.9180, -0.7007],
        [-4.5508, -2.4395, -2.6895,  ..., -2.0332, -4.0938, -1.7051]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 15, 25, 15, 27,  0, 27, 27, 14,  0,  5, 20,  1, 27,  7, 27, 27, 18,
        27, 10, 27,  3,  9, 18, 27, 27, 18,  0, 17,  7, 27, 27, 26, 18,  6, 15,
        27, 18,  0, 11, 27,  0, 27, 15, 27,  4, 27, 27, 27,  0, 27, 15,  0, 27,
        10, 27, 10, 24, 15, 18, 27, 18, 10, 12], device='cuda:0')
step: 399
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4785, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.1509e-04, -1.4782e-04, -5.4836e-04,  ..., -3.5214e-04,
          5.8556e-04, -5.5838e-04],
        [-3.4046e-04,  1.7500e-04, -5.5742e-04,  ..., -1.7128e-03,
         -3.3736e-04,  3.3903e-04],
        [ 1.3912e-04, -4.3750e-05, -4.1103e-04,  ...,  3.7336e-04,
          2.1553e-04, -9.1362e-04],
        [ 2.1815e-04,  5.2869e-05, -1.5469e-03,  ...,  2.5773e-04,
         -3.3331e-04, -3.1114e-04],
        [ 1.2255e-04,  3.7193e-05, -3.8481e-04,  ...,  3.6907e-04,
          2.8539e-04,  1.0580e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9023, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.5781, -4.2969, -7.8281,  ..., -8.3125, -7.5312, -5.5469],
        [-3.9805, -3.4492, -4.7305,  ..., -5.6523, -4.6367, -0.4492],
        [-4.1055, -4.9805, -8.0469,  ..., -7.4961, -8.0547, -6.9805],
        ...,
        [-5.8242, -4.2617, -4.3555,  ..., -5.9648, -5.1055, -0.3088],
        [-3.9688, -4.2969, -3.0000,  ..., -5.0000, -5.3750, -0.2494],
        [-3.3633, -3.2695, -4.3945,  ..., -5.8633, -4.7070, -0.7231]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15,  4, 15, 22, 27, 25, 27, 20,  1,  0,  0, 10, 20, 18, 27,  3, 27,  0,
         3,  4, 27, 18, 27,  1, 27, 27, 27,  1, 27, 27, 13, 22,  7, 26, 20,  1,
         9, 27,  3, 27,  8, 27, 27,  3, 27, 27,  4, 27, 27,  5, 22, 27, 27, 10,
         0, 11,  0, 10, 27, 18, 24, 27, 27, 23], device='cuda:0')
step: 400
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4785, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.9468e-04, -1.2875e-03, -2.4033e-04,  ..., -2.1362e-03,
          1.0669e-05, -3.7241e-04],
        [ 7.3862e-04, -1.4842e-05,  3.3689e-04,  ...,  7.7677e-04,
          9.4604e-04, -1.2026e-03],
        [-4.9686e-04, -2.3305e-04,  4.2176e-04,  ...,  1.9181e-04,
          1.0699e-04, -2.6107e-04],
        [-5.0664e-06, -1.3285e-03,  9.3937e-04,  ...,  5.3406e-04,
          4.6039e-04, -1.0958e-03],
        [-1.3292e-05, -6.0511e-04,  9.1457e-04,  ..., -4.4346e-04,
         -4.4417e-04,  4.3774e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1161, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.7461, -3.5605, -3.6230,  ..., -6.7500, -4.9961, -0.8418],
        [-4.8359, -3.9941, -5.2109,  ..., -7.8984, -5.5391, -0.3376],
        [-0.3181, -3.7871, -6.3633,  ..., -7.4258, -5.0039, -2.8965],
        ...,
        [-4.2344, -3.8125, -4.4219,  ..., -4.5625, -5.7344, -0.6865],
        [-5.1758, -4.8633, -4.7383,  ..., -7.1914, -5.9102, -0.2058],
        [-1.6025, -4.8516, -6.2266,  ..., -8.0547, -6.0703, -5.1328]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([14,  4,  0,  1, 27,  0, 22, 27, 27, 20,  4,  4, 10, 22,  1,  4,  2,  1,
         5,  0, 27,  7, 24, 10, 20, 27, 27, 18, 17, 27,  1, 26, 20, 27, 27, 27,
        27, 27, 13, 27,  4,  4, 27, 27, 27, 27, 27,  1,  3,  3, 27, 10, 18, 27,
         7, 14,  5,  6, 27, 26, 18,  5, 27, 18], device='cuda:0')
step: 401
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4788, -0.0812,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.6372e-04, -7.6115e-05,  7.8917e-04,  ..., -1.5807e-04,
          8.6486e-05,  1.2112e-03],
        [-3.5548e-04,  7.0477e-04, -5.0735e-04,  ..., -6.2466e-04,
         -6.0081e-04, -8.9598e-04],
        [ 3.4785e-04, -1.3614e-04,  6.1941e-04,  ..., -5.1594e-04,
          2.1529e-04,  1.8144e-04],
        [ 8.1730e-04,  5.5408e-04, -1.3697e-04,  ..., -5.9319e-04,
          2.6298e-04,  8.2254e-05],
        [-2.7323e-04,  1.5450e-04, -2.3508e-04,  ..., -2.3961e-05,
         -2.6464e-05, -2.1744e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7043, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.5215, -3.7715, -4.5820,  ..., -8.0234, -4.8008, -1.1309],
        [-3.2656, -3.4668, -4.8125,  ..., -3.9355, -4.2969, -0.6401],
        [-2.4453, -3.7422, -5.6328,  ..., -6.6328, -5.5547, -5.5703],
        ...,
        [-4.6328, -3.1465, -4.0391,  ..., -1.1787, -3.8027, -1.9287],
        [-5.3359, -3.6152, -3.1934,  ..., -4.5078, -3.6934, -1.5850],
        [-5.2070, -3.6133, -4.7070,  ..., -6.2383, -5.2539, -0.4890]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4,  4, 15, 26, 10, 26, 18,  1, 13, 10, 22, 17,  6,  0,  4, 27, 27,  2,
        27, 17, 20, 27,  7, 27,  1, 27, 27, 15, 27, 27,  1, 27, 27, 27, 27, 27,
        27, 27, 18, 27,  4, 17,  7, 15, 27, 15,  0, 27, 11,  5, 27, 27, 25,  9,
         0, 27, 11, 25,  4, 14, 10, 27,  1, 27], device='cuda:0')
step: 402
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4788, -0.0812,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6567,  0.7061, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.3818e-04,  6.6698e-05, -8.0967e-04,  ...,  2.2483e-04,
          5.7125e-04,  4.6897e-04],
        [-8.1956e-05,  6.2895e-04,  2.9778e-04,  ...,  7.7391e-04,
         -1.0926e-04,  3.1471e-04],
        [-2.9945e-04,  1.1883e-03,  3.3450e-04,  ...,  2.1291e-04,
          6.0940e-04,  3.8719e-04],
        [ 4.5919e-04, -2.2411e-04, -1.0920e-03,  ..., -5.4502e-04,
         -9.6977e-05, -1.3423e-04],
        [ 1.5330e-04,  7.6711e-05, -6.5041e-04,  ...,  2.2578e-04,
         -4.2677e-05, -1.5438e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8671, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.2891, -3.8223, -4.4453,  ..., -7.6016, -5.2891, -0.2438],
        [-5.1289, -2.9395, -2.7695,  ..., -3.7832, -3.1270, -2.1113],
        [-2.2227, -3.3789, -4.3477,  ..., -5.4727, -4.1602, -0.8315],
        ...,
        [-3.2949, -3.6855, -4.4648,  ..., -1.9658, -3.5605, -2.0449],
        [-3.9902, -3.3965, -3.0840,  ..., -5.3828, -3.4902, -0.9751],
        [-3.5742, -4.3867, -5.0742,  ..., -6.1367, -3.6211, -3.6074]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  3,  4, 27, 25, 27, 11, 27,  0,  6,  8, 27, 27, 27, 27,  0, 24, 27,
        15,  4, 27, 27, 27, 15, 27, 24, 10, 17,  5, 20, 10,  0,  3, 27,  1, 27,
         9, 27, 20,  3, 27,  1,  4, 27,  7,  1,  0,  1, 18, 13,  3, 27, 27, 27,
        18, 27, 27,  7, 22, 27, 15,  1, 12, 13], device='cuda:0')
step: 403
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4788, -0.0812,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2986, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0262e-03, -1.8702e-03,  3.4189e-04,  ..., -5.2261e-04,
         -1.0252e-03, -9.4461e-04],
        [-5.6314e-04,  1.6189e-04, -8.0729e-04,  ..., -2.2352e-05,
          1.0242e-03, -2.3162e-04],
        [ 1.9932e-04, -8.9824e-05,  5.7161e-05,  ..., -3.1173e-05,
          2.7537e-04, -2.0885e-04],
        [-4.8542e-04, -5.3823e-05, -4.0150e-04,  ..., -8.6117e-04,
         -4.2367e-04, -6.8092e-04],
        [-5.7268e-04, -2.4855e-05,  3.4070e-04,  ..., -3.9995e-05,
         -5.1355e-04,  1.4186e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9729, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4102, -4.0977, -4.8477,  ..., -6.5508, -5.4727, -0.3162],
        [-3.1582, -3.6113, -4.2227,  ..., -6.3164, -3.9395, -0.5181],
        [-4.0586, -3.7285, -2.6504,  ..., -0.9634, -5.2305, -3.1348],
        ...,
        [-4.5898, -4.0586, -5.2930,  ..., -6.7930, -3.9004, -0.8696],
        [-4.4062, -4.5000, -4.6250,  ..., -7.9219, -6.2344, -0.1263],
        [-4.9531, -2.8750, -4.2656,  ..., -6.2344, -6.0469, -0.4058]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 22, 24,  4, 27, 27, 10, 27, 19, 27, 27, 20, 27,  5,  3, 27,  4, 27,
         6,  3, 20,  2, 27,  8, 27,  4,  7, 27, 27, 27, 27, 27,  6,  0, 18,  0,
         0, 27, 27, 22,  4,  2, 27, 18, 27, 27, 13, 22, 17, 26, 27,  4, 10, 27,
        27,  9, 25,  0, 26, 27, 22, 27, 14,  4], device='cuda:0')
step: 404
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4788, -0.0812,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.6468e-04,  4.5586e-04,  7.0429e-04,  ..., -3.1161e-04,
          1.9968e-04, -6.2656e-04],
        [-1.2708e-04,  2.7618e-03, -6.7997e-04,  ..., -1.4305e-03,
          4.1580e-04, -1.2808e-03],
        [ 1.2994e-04,  4.4084e-04, -8.8215e-04,  ...,  1.9741e-03,
          1.5364e-03, -8.7547e-04],
        [-3.9244e-04,  4.2772e-04, -1.1116e-04,  ...,  3.4809e-04,
         -4.8804e-04,  2.4271e-04],
        [-1.5748e-04, -2.3723e-04, -1.4126e-04,  ..., -8.1539e-05,
         -3.1519e-04,  1.3518e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8613, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.2490, -4.4062, -5.5938,  ..., -7.0469, -4.2031, -4.7031],
        [-2.8574, -3.9043, -3.1680,  ..., -6.2461, -6.2930, -1.9033],
        [-1.6025, -3.8535, -4.7109,  ..., -5.3828, -4.1172, -0.7744],
        ...,
        [-5.9062, -4.1406, -4.2500,  ..., -8.0469, -4.4375, -1.8584],
        [-2.3652, -3.7090, -4.5195,  ..., -6.4727, -3.3496, -0.8647],
        [-3.4023, -4.4805, -7.3242,  ..., -7.3711, -7.4023, -5.9805]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17,  8, 27,  1, 27, 27, 17, 24,  4, 27,  3, 27, 10, 27,  6, 11,  6, 26,
        27, 26, 17, 15, 15,  7, 18, 27, 27,  8, 10,  3, 27,  2,  3,  2, 10, 27,
        20, 27, 27, 26, 27, 27,  0,  0, 27,  5,  0, 12,  1,  6,  0,  5, 26, 24,
         1, 27, 15, 10, 27, 26, 27,  7,  4, 15], device='cuda:0')
step: 405
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4788, -0.0812,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.1826e-03,  1.1320e-03, -9.0599e-04,  ...,  1.1215e-03,
          3.9291e-04, -2.5916e-04],
        [ 5.5361e-04,  1.2255e-04, -9.9087e-04,  ..., -1.5926e-03,
         -3.8981e-04, -3.1471e-05],
        [-7.8917e-05, -8.1396e-04, -2.8896e-04,  ..., -3.0780e-04,
         -7.7486e-04,  1.1263e-03],
        [ 8.4829e-04, -1.9073e-04, -7.3338e-04,  ..., -8.0526e-05,
         -9.5558e-04, -1.6856e-04],
        [-1.7750e-04, -1.9944e-04, -4.1485e-04,  ...,  5.1796e-05,
         -1.7822e-05,  3.8505e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9953, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.7207, -3.6426, -4.6289,  ..., -6.5508, -4.4102, -0.7676],
        [-0.4978, -3.7793, -5.5117,  ..., -6.3555, -5.1055, -2.1855],
        [-0.5093, -3.3828, -5.3984,  ..., -7.8359, -5.0547, -3.4609],
        ...,
        [-3.6406, -2.8438, -3.2812,  ..., -3.3906, -3.8594, -1.0312],
        [-1.3887, -3.3262, -6.0117,  ..., -6.7305, -3.9980, -4.1250],
        [-5.0234, -2.1172, -2.3359,  ..., -1.8516, -4.1484, -1.8828]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  0,  0,  0, 15, 25, 26, 27, 18, 20, 27, 18, 19,  3,  1, 27, 18,
        27, 27, 17, 11, 27,  1,  3,  0, 26,  1, 26, 27,  6,  7, 27,  4, 14, 25,
        20, 27, 27, 15,  6, 17,  6, 18,  6, 27, 27,  7,  6, 13, 27,  4, 10, 27,
        19, 22,  0, 27, 27,  1,  1,  9, 17,  9], device='cuda:0')
step: 406
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0812,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9111, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.1160e-05, -2.1858e-03,  1.2388e-03,  ...,  1.5907e-03,
         -2.1505e-04,  1.4362e-03],
        [ 1.0204e-03,  4.0674e-04, -7.8535e-04,  ..., -6.1512e-04,
         -1.3657e-03,  1.6832e-03],
        [ 5.0497e-04,  7.7724e-04,  6.7568e-04,  ..., -4.9591e-04,
          1.6761e-04,  1.5059e-03],
        [ 1.0872e-03, -2.8086e-04, -7.1478e-04,  ..., -8.7261e-04,
         -4.7231e-04, -7.7784e-05],
        [ 2.1207e-04,  1.2283e-03, -1.4429e-03,  ...,  6.8474e-04,
         -2.9027e-05, -8.2016e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9184, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6406, -3.5957, -4.1094,  ..., -6.4375, -5.2500, -0.3459],
        [-1.6641, -3.5391, -4.9766,  ..., -7.0391, -5.4922, -1.4922],
        [-4.0781, -3.9531, -4.2344,  ..., -3.8301, -4.3906, -0.5166],
        ...,
        [-5.8281, -4.3438, -5.0156,  ..., -8.2656, -5.8750, -0.1874],
        [-2.5645, -3.2988, -3.7051,  ..., -3.8145, -4.1562, -0.7676],
        [-5.5000, -3.3594, -4.3438,  ..., -5.4844, -6.3906, -0.4546]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27, 27,  1, 11,  1,  9, 27, 27, 17, 25, 27,  2, 26, 27, 27, 26,
         5, 27, 27, 13,  3, 27, 18, 27, 27, 15, 17,  7, 10, 27, 26, 24, 17, 26,
        10, 27,  7, 17, 27, 10, 22, 27, 22,  2,  4, 27, 27,  4, 27, 21, 20, 15,
         0, 22,  3, 15,  7, 17,  9, 27,  1, 27], device='cuda:0')
step: 407
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0812,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.1559e-03, -7.8022e-05, -5.6028e-04,  ...,  4.2224e-04,
          1.1325e-04,  3.9768e-04],
        [ 1.8644e-04,  1.4746e-04,  2.7943e-04,  ..., -9.4223e-04,
         -8.6308e-04,  6.2799e-04],
        [ 1.9002e-04, -1.2598e-03, -1.0290e-03,  ..., -3.0804e-04,
         -7.5293e-04, -2.0373e-04],
        [ 1.2445e-03, -6.6757e-06, -4.8828e-04,  ...,  6.9678e-05,
          4.7326e-04, -4.5824e-04],
        [ 3.6645e-04,  1.5080e-05, -2.2721e-04,  ...,  2.1052e-04,
          1.4186e-05,  1.2493e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0737, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.0859, -3.5566, -4.0703,  ..., -6.7891, -3.8223, -0.5718],
        [-2.2324, -3.6230, -4.7461,  ..., -6.9336, -5.1211, -1.2314],
        [-4.0820, -4.0508, -5.0977,  ..., -5.6758, -4.9414, -4.5977],
        ...,
        [-3.1738, -3.0176, -3.5020,  ..., -3.6113, -2.4863, -1.6416],
        [-2.8203, -3.4141, -5.7422,  ..., -7.1172, -5.1172, -4.1016],
        [-4.1406, -3.5449, -4.5156,  ..., -4.7500, -4.1562, -0.6548]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6,  4, 23,  1, 27,  0,  3,  1, 27, 27,  0, 14, 11, 27, 27, 27,  0,  5,
        27,  7, 27, 27, 27,  3,  7, 15, 18, 27,  3, 13, 27, 20, 27, 22,  4,  4,
        27, 21, 27,  4,  1, 21, 27, 25, 27, 27, 10, 27,  7,  0,  1,  3, 15, 20,
         0, 27, 27, 22, 10, 27,  0, 27, 22,  5], device='cuda:0')
step: 408
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0812,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.5133e-04, -7.3552e-05, -5.0211e-04,  ...,  6.3276e-04,
         -3.6430e-04,  1.2219e-05],
        [-6.1512e-04, -1.0042e-03,  4.8018e-04,  ..., -1.0043e-04,
         -1.5345e-03,  1.7262e-03],
        [ 1.3089e-04,  2.2626e-04,  3.9434e-04,  ...,  2.0504e-05,
         -1.3471e-04, -5.1880e-04],
        [-8.5640e-04,  1.4782e-04,  5.7316e-04,  ..., -1.4758e-04,
         -4.9114e-04, -1.6713e-04],
        [-1.4663e-04,  2.9731e-04,  1.2159e-04,  ..., -7.5102e-05,
         -1.0812e-04, -1.1635e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0506, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4453, -2.6484, -1.9297,  ..., -3.5234, -4.7266, -1.2900],
        [-4.5117, -4.0117, -4.5273,  ..., -4.4180, -3.2793, -0.5913],
        [-5.1719, -3.8926, -4.7031,  ..., -4.2969, -5.7031, -0.3762],
        ...,
        [-1.8115, -3.3750, -4.9219,  ..., -5.9375, -4.5000, -2.2656],
        [-2.7324, -4.5938, -5.8906,  ..., -8.0000, -6.2969, -5.9375],
        [-1.3828, -2.9922, -4.8359,  ..., -7.2578, -4.6797, -2.5859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2, 27,  5,  0,  3, 25,  4, 27,  1, 27,  1,  8, 14, 11, 15, 17, 27, 18,
        18, 20, 10,  7, 20, 20,  0, 25, 27,  0, 27, 27, 27, 20, 15, 27,  4,  4,
        27, 10, 27, 27, 13, 15, 24, 27, 23, 27, 27, 27,  1,  3, 14,  5, 27, 27,
        27, 27, 27, 11, 27, 18,  0,  0, 18, 18], device='cuda:0')
step: 409
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0812,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.3876e-03,  4.1008e-05,  9.9945e-04,  ..., -2.0294e-03,
         -1.0614e-03,  1.3227e-03],
        [ 6.3419e-05, -2.0289e-04,  2.8038e-04,  ..., -2.7466e-03,
         -9.8228e-04,  1.9622e-04],
        [-9.9564e-04,  2.6226e-04,  6.2609e-04,  ...,  3.1662e-04,
          1.9097e-04, -1.1921e-03],
        [-2.7919e-04,  1.6236e-04,  1.6985e-03,  ..., -1.2636e-03,
          2.4929e-03, -3.3474e-04],
        [-1.6689e-04,  4.0317e-04,  1.1635e-03,  ..., -1.3638e-04,
          4.4703e-04, -1.7560e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0933, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.0352, -3.1133, -3.4414,  ..., -4.9883, -2.7695, -1.3799],
        [-5.1328, -5.0547, -5.9922,  ..., -8.0703, -6.7891, -0.1153],
        [-2.6250, -3.7656, -4.9531,  ..., -7.0781, -5.2500, -0.2976],
        ...,
        [-1.8164, -2.9102, -4.1445,  ..., -4.3945, -2.0664, -2.1133],
        [-2.7754, -2.9316, -4.2734,  ..., -4.6992, -3.2285, -3.0723],
        [-3.6035, -3.1797, -3.1172,  ..., -1.9775, -3.4609, -1.0869]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 27,  0, 27,  3, 24, 25, 27,  6,  3,  5, 17, 17, 27, 17, 14,  5,  3,
         3,  9, 27, 27, 27, 20, 27, 15, 17,  4,  1, 14, 15, 15, 15, 13, 15,  4,
        15, 27, 27, 27,  1, 26,  2,  0, 12, 27, 21, 10, 27, 27, 11, 13, 27,  1,
        27, 10,  4, 17, 24,  8, 20, 26, 17,  2], device='cuda:0')
step: 410
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.6164e-04,  1.5135e-03,  1.0653e-03,  ..., -2.7466e-04,
         -9.8705e-05,  1.9236e-03],
        [ 2.3186e-05, -5.4407e-04,  1.9753e-04,  ...,  1.5926e-04,
         -1.0443e-03,  8.3208e-04],
        [-3.8028e-05, -1.7557e-03,  2.3961e-04,  ...,  4.1342e-04,
         -1.5574e-03,  5.9414e-04],
        [-2.5272e-04,  7.0453e-05,  6.2561e-04,  ...,  1.9252e-04,
         -2.0826e-04,  1.9431e-04],
        [-3.3092e-04,  6.2227e-04, -3.8218e-04,  ...,  2.7847e-04,
          4.2975e-05, -1.2994e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9851, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.3633, -3.7051, -4.4727,  ..., -5.5820, -5.5352, -0.2988],
        [-3.1816, -3.1504, -4.8516,  ..., -7.8516, -5.4766, -0.5874],
        [-3.0977, -2.4570, -3.3770,  ..., -3.4551, -2.2695, -1.5186],
        ...,
        [-2.7402, -2.6465, -4.6953,  ..., -6.9922, -1.5537, -2.5371],
        [-1.7520, -4.0156, -4.7070,  ..., -5.4102, -3.3926, -1.7207],
        [-2.1387, -3.7324, -4.6875,  ..., -7.2344, -3.6074, -4.2656]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 11,  0, 27,  3, 27,  4,  4,  3, 10, 15, 27, 27,  9, 20, 27, 27,
        27,  9,  6,  0, 15,  0, 10, 11,  0,  3, 27, 27, 27, 25,  4, 27,  4,  4,
         7,  2, 27, 27, 27,  7, 27, 27, 25, 11,  5, 27, 18,  2, 10, 27, 10, 27,
         1,  0,  0, 15,  1,  1, 15, 13, 21, 27], device='cuda:0')
step: 411
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.4816e-04,  9.0027e-04, -2.8467e-04,  ...,  4.6182e-04,
          4.0960e-04,  2.7108e-04],
        [-9.6321e-05,  1.9436e-03,  3.1877e-04,  ..., -1.9417e-03,
         -1.3428e-03, -8.5258e-04],
        [ 1.8263e-04,  1.0242e-03,  1.1945e-04,  ...,  6.0737e-05,
          2.9278e-04,  3.7456e-04],
        [ 2.7514e-04, -1.5211e-04,  2.8467e-04,  ...,  1.9526e-04,
          8.6427e-06,  4.9782e-04],
        [ 4.9591e-04,  1.5175e-04, -5.0879e-04,  ...,  4.2439e-04,
         -3.5763e-07, -3.9744e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8017, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.5410, -3.2598, -5.3828,  ..., -5.3203, -3.6191, -4.2734],
        [-5.0820, -2.9863, -3.3301,  ..., -5.6875, -3.5020, -1.6426],
        [-4.4609, -1.9609, -3.0859,  ..., -3.5234, -4.0078, -1.6953],
        ...,
        [-2.8105, -2.7012, -7.0156,  ..., -6.2969, -5.8438, -6.3906],
        [-2.3691, -4.8047, -5.4453,  ..., -6.9766, -5.3711, -4.6836],
        [-2.3555, -2.2285, -4.0742,  ..., -6.6211, -3.5098, -2.4316]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17,  7, 27,  1, 27,  7, 27,  2,  4, 27, 15, 15, 27,  0, 22,  1,  2, 25,
        25,  7, 13,  2,  7, 22, 20, 27, 10, 20, 27, 17,  8,  4,  8, 27,  9,  9,
         0, 10, 24, 27,  0, 27, 15,  5,  7,  7, 27,  4, 27, 11,  6, 27, 27,  0,
         3, 25, 15,  7, 27,  1, 27, 15,  3, 18], device='cuda:0')
step: 412
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.1498e-04,  2.0730e-04,  3.5286e-05,  ..., -8.2302e-04,
         -3.7336e-04,  4.0436e-04],
        [ 1.2226e-03, -7.9250e-04,  5.0545e-04,  ...,  1.8826e-03,
          1.3247e-03, -2.7657e-04],
        [-1.3804e-04, -4.3821e-04, -4.4918e-04,  ...,  2.1791e-04,
         -4.4322e-04,  3.9995e-05],
        [-4.6587e-04,  2.2864e-04,  1.0996e-03,  ...,  6.9523e-04,
         -8.7678e-05,  4.3249e-04],
        [ 4.0293e-05, -4.7827e-04,  9.0694e-04,  ..., -4.5919e-04,
         -4.8876e-04,  1.2541e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1202, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.1113, -2.5957, -5.2500,  ..., -6.0469, -4.9688, -5.3906],
        [-2.5859, -3.3672, -4.9141,  ..., -6.2266, -5.5547, -4.2109],
        [-4.1797, -2.4785, -3.4941,  ..., -4.9180, -2.9785, -1.5723],
        ...,
        [-4.3281, -2.9824, -4.3281,  ..., -3.7793, -1.8574, -0.9673],
        [-5.1289, -2.3340, -3.2246,  ..., -2.3789, -4.0664, -1.2236],
        [-5.5234, -3.6621, -3.8965,  ..., -5.6797, -2.4121, -1.5215]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15, 20, 27, 27, 26, 25, 19,  1, 27, 10,  6, 15, 25, 11, 26, 15,  6,  7,
         7, 27, 25,  7,  3, 14, 13,  4, 11, 27, 10, 27, 27,  4, 27,  0, 24,  3,
        27, 11,  3, 15,  3, 27, 17, 17, 10, 27, 20, 22, 27, 22, 27, 10, 27, 26,
        26, 27, 20,  1, 27, 15, 27,  1, 22,  4], device='cuda:0')
step: 413
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.6369e-04,  2.7800e-04, -5.3406e-04,  ..., -2.7418e-06,
          3.5882e-04, -2.6822e-05],
        [-1.0854e-04,  2.0075e-04,  5.8317e-04,  ..., -3.9101e-04,
         -1.2732e-03, -7.4959e-04],
        [-9.6321e-05, -9.0182e-05,  5.4026e-04,  ..., -5.3596e-04,
          4.4513e-04, -5.9187e-05],
        [-5.7888e-04,  4.2295e-04,  4.7398e-04,  ...,  8.6010e-05,
         -2.7323e-04,  1.0653e-03],
        [ 1.4412e-04, -4.4942e-04,  4.3106e-04,  ...,  2.7514e-04,
          1.6761e-04,  4.3726e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0082, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.2109, -3.4160, -4.7891,  ..., -4.2266, -2.9766, -2.3516],
        [-2.4395, -3.5020, -3.5176,  ..., -4.0977, -3.4707, -0.9087],
        [-2.1309, -3.1777, -5.7539,  ..., -5.8008, -4.7227, -4.3008],
        ...,
        [-4.6406, -3.7988, -4.3281,  ..., -6.2031, -3.0645, -0.6270],
        [-4.3359, -2.6973, -3.2754,  ..., -3.0879, -3.4004, -1.6348],
        [-0.9072, -3.2363, -4.3438,  ..., -5.2031, -3.2656, -3.5645]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22, 27, 15,  1, 27, 13, 17, 18,  2,  2, 27, 15,  0, 27,  1, 27,  2, 27,
         8, 10,  3, 15, 27,  2, 13, 27, 25, 18, 15,  2,  1, 15, 15,  1,  2, 26,
         1, 25,  0,  0,  2,  8, 26, 27, 27, 27, 11,  3, 17, 15, 27, 26,  1,  7,
        27, 27,  1,  5,  4, 27, 18,  1, 22,  0], device='cuda:0')
step: 414
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6040, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.1424e-04,  8.9073e-04, -2.5439e-04,  ...,  1.7920e-03,
          5.2118e-04,  8.4925e-04],
        [ 7.7724e-04, -2.1534e-03, -1.2100e-05,  ...,  1.5430e-03,
         -1.4410e-03,  1.5903e-04],
        [ 2.1875e-04, -1.4067e-03, -2.4486e-04,  ..., -4.9114e-04,
         -7.0572e-05,  1.4553e-03],
        [-3.1376e-04, -7.0000e-04, -1.2989e-03,  ..., -3.6693e-04,
         -2.3818e-04, -5.6171e-04],
        [-8.8274e-05, -2.7323e-04, -9.4295e-05,  ..., -7.3314e-06,
          1.9395e-04,  4.6420e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8923, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.5859, -3.2734, -3.9473,  ..., -2.9004, -4.6641, -2.4941],
        [-3.3281, -2.8594, -4.4375,  ..., -4.6250, -5.2031, -4.9219],
        [-3.3516, -2.4297, -2.9922,  ..., -2.8047, -2.5566, -2.0078],
        ...,
        [-1.9092, -2.2520, -2.6133,  ..., -3.0332, -2.5645, -3.4238],
        [-2.2812, -2.8125, -4.2031,  ..., -6.1719, -4.4688, -1.3115],
        [-2.3945, -2.6309, -4.1445,  ..., -5.7070, -2.0820, -2.6758]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 9, 20,  1, 27, 27, 10, 27, 11,  0, 22, 27, 27, 27, 27, 27,  7,  3, 13,
         9, 27, 10,  0, 27, 27, 27, 27, 15, 27, 25,  3,  7, 25, 27,  3, 14, 18,
         7, 17, 27,  0, 27,  8, 27, 27, 15, 22, 17, 27,  1,  2, 27,  5, 27,  6,
        10, 20,  3, 27,  1, 10, 10, 11,  4,  1], device='cuda:0')
step: 415
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.6890e-06, -3.4273e-05,  1.3037e-03,  ..., -1.8835e-04,
         -2.4915e-04,  1.1272e-03],
        [-1.3494e-04,  1.2064e-03,  8.3256e-04,  ...,  2.0409e-03,
          1.3828e-03, -2.3956e-03],
        [-1.0598e-04,  6.1655e-04, -1.6177e-04,  ..., -1.3762e-03,
          2.3556e-04, -8.6117e-04],
        [ 1.8454e-04, -4.3535e-04,  5.4550e-04,  ..., -3.5739e-04,
          3.3855e-04,  5.8365e-04],
        [-2.2459e-04, -3.5548e-04,  9.0182e-05,  ..., -5.7411e-04,
         -5.8365e-04, -2.0027e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8983, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.5176, -3.1426, -2.8613,  ..., -1.5801, -3.5332, -3.5332],
        [-5.1094, -3.1582, -3.0801,  ..., -6.2031, -4.4844, -1.5947],
        [-4.3281, -2.7949, -3.0449,  ..., -4.8281, -4.0156, -0.8110],
        ...,
        [-5.6523, -2.6699, -3.5137,  ..., -4.7461, -3.6074, -1.4199],
        [-2.4062, -2.7969, -5.8906,  ..., -6.0781, -4.2656, -4.8906],
        [-3.1152, -3.0996, -3.4277,  ..., -3.7402, -3.8809, -2.5527]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25, 27,  9,  3, 17, 27, 27, 19, 26, 26, 20, 15, 27, 27, 15, 27,  9, 27,
        15, 27, 14,  2,  4,  0,  1, 26, 27, 13, 17, 27, 27, 27, 20, 10, 10,  0,
         4,  4,  2,  0, 15,  0,  9, 26, 15,  7, 25, 27, 26, 10,  1, 27,  8,  4,
        15,  1,  7,  6, 25, 27,  0,  7, 15, 27], device='cuda:0')
step: 416
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.7084e-04,  1.1463e-03, -9.6703e-04,  ...,  1.0529e-03,
          2.3854e-04, -1.2693e-03],
        [ 3.9864e-04, -2.7428e-03,  1.1027e-04,  ..., -2.7466e-04,
         -9.2793e-04,  1.0090e-03],
        [-1.2851e-04, -7.7248e-04,  1.2274e-03,  ..., -2.7008e-03,
         -2.0943e-03,  2.0618e-03],
        [-7.2384e-04, -1.1140e-04,  1.3885e-03,  ..., -1.8096e-04,
         -4.7803e-05, -2.2960e-04],
        [-2.5177e-04, -1.0958e-03,  8.9931e-04,  ..., -7.1383e-04,
         -1.0151e-04,  3.6144e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8889, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.2061, -2.5820, -4.8945,  ..., -6.2539, -4.5508, -3.9570],
        [-4.2148, -3.3691, -3.9004,  ..., -4.8203, -4.9180, -0.4167],
        [-4.3555, -2.7305, -3.4492,  ..., -5.2461, -4.9961, -0.8232],
        ...,
        [-3.9062, -2.7188, -4.4531,  ..., -3.4219, -1.8281, -1.6562],
        [-1.2051, -1.5479, -5.4531,  ..., -6.2344, -5.5000, -2.6738],
        [-4.0469, -3.0625, -2.6562,  ..., -1.0156, -3.4062, -3.5781]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 27, 18, 11, 15,  0, 18,  4,  4, 27, 20, 27,  4,  9, 27, 25,  7, 15,
        27, 27, 27, 27,  7, 27, 13, 18,  0, 27, 27, 27,  2,  9, 27, 20, 10, 25,
        27, 27,  0, 24, 26, 10, 27,  1,  5, 15,  6,  1, 17,  4, 27, 22, 20, 27,
        24,  0,  4, 27, 10,  4,  1,  0,  0, 25], device='cuda:0')
step: 417
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.2131e-03, -1.2264e-03,  1.4372e-03,  ..., -8.7070e-04,
         -9.8228e-04, -2.6822e-04],
        [-4.4107e-04,  1.7128e-03,  1.2696e-05,  ...,  8.2159e-04,
          1.3285e-03,  4.1771e-04],
        [ 6.6876e-05,  1.2274e-03,  1.7900e-03,  ..., -3.7551e-04,
          1.5032e-04,  5.3501e-04],
        [-1.3304e-04,  8.5592e-05,  2.1286e-03,  ...,  5.6982e-04,
         -1.7929e-04,  1.0252e-03],
        [ 1.7405e-04, -8.2779e-04,  6.5088e-04,  ...,  2.8682e-04,
         -3.9577e-04, -3.0279e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7361, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.7109, -3.0703, -3.7402,  ..., -6.7734, -5.3984, -2.6777],
        [-4.5820, -2.6602, -3.0977,  ..., -4.9258, -3.9258, -0.6924],
        [-3.8887, -2.5449, -4.3125,  ..., -2.8887, -2.3887, -1.7959],
        ...,
        [-5.0977, -2.6309, -3.5840,  ..., -3.9121, -4.2695, -1.2402],
        [-1.4170, -2.2617, -4.0430,  ..., -5.4805, -2.2461, -3.1523],
        [-4.2070, -2.9570, -3.5820,  ..., -4.8945, -2.5195, -1.5664]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 8, 27, 10, 25,  7,  3,  4, 10, 27,  9,  4, 27, 12, 18,  6, 27, 22, 27,
         6, 27, 27, 10, 27,  6, 18, 14, 10, 27,  5, 22, 18, 26,  1,  4,  1, 27,
        27, 27, 27,  7, 27, 27, 15, 15,  1, 13,  3,  7, 14,  2,  7, 18,  4, 27,
        17, 27, 27, 11, 18, 27, 27, 27,  0,  9], device='cuda:0')
step: 418
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.9097e-04,  3.3140e-04,  5.7983e-04,  ..., -6.8855e-04,
         -2.1720e-04,  4.4298e-04],
        [-4.3440e-04,  5.6982e-04, -1.4615e-04,  ..., -5.3692e-04,
          7.4434e-04,  2.1517e-04],
        [-2.1577e-05, -4.0674e-04,  1.1146e-04,  ..., -1.1444e-03,
          4.7684e-04,  5.9462e-04],
        [-4.2892e-04, -1.5521e-04, -3.0708e-04,  ...,  1.3900e-04,
          5.2166e-04,  2.5034e-06],
        [ 1.5426e-04, -1.0815e-03,  5.6124e-04,  ..., -4.9353e-04,
         -2.9969e-04,  2.4843e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7479, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.5352, -3.0039, -3.1914,  ..., -3.3789, -5.2383, -0.8013],
        [-2.3301, -2.7344, -5.0625,  ..., -6.1719, -4.5156, -3.2832],
        [-4.9375, -3.0312, -3.2988,  ..., -3.9707, -3.6406, -1.4229],
        ...,
        [-1.8721, -2.6074, -5.0898,  ..., -5.9023, -2.5762, -2.3887],
        [-3.5586, -3.4492, -5.9023,  ..., -6.0430, -5.4336, -5.6367],
        [-2.9473, -2.6973, -2.5566,  ..., -2.3223, -2.9160, -2.7285]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 20,  6, 15, 27, 27, 27,  1, 27, 22, 25,  3, 11,  3, 27, 27,  0,  2,
        18,  5, 27, 27,  1,  3, 27, 27, 27, 25,  0, 15, 18,  4, 14, 20,  6, 18,
        25, 27, 27,  9,  4, 27, 27,  4,  0,  1,  0,  0, 26,  0, 18, 15, 26, 27,
         7, 18, 24, 10, 25, 22, 26,  0, 15,  9], device='cuda:0')
step: 419
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.6042e-05, -4.3821e-04, -1.3752e-03,  ..., -3.9268e-04,
          5.0497e-04, -1.4677e-03],
        [ 7.9298e-04, -8.6546e-05,  5.4455e-04,  ...,  1.1387e-03,
          1.2159e-04, -7.7820e-04],
        [-1.1915e-04, -3.5477e-04, -8.3971e-04,  ..., -1.5478e-03,
          5.5552e-05,  5.2118e-04],
        [ 6.2525e-05,  1.7571e-04, -5.2547e-04,  ..., -2.5439e-04,
         -4.9114e-04, -5.7030e-04],
        [-3.8207e-05, -8.3637e-04,  5.8079e-04,  ..., -1.2112e-04,
          3.9101e-04,  5.1260e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8739, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4297, -2.2871, -3.0840,  ..., -1.8818, -3.4121, -2.2090],
        [-5.5078, -2.6797, -2.7109,  ..., -2.0234, -4.2734, -3.4609],
        [-3.4238, -3.5645, -3.8457,  ..., -3.5488, -3.8613, -1.0801],
        ...,
        [-3.6367, -2.6055, -4.6211,  ..., -5.8867, -5.0898, -1.7773],
        [-3.9336, -3.2461, -4.3242,  ..., -5.6680, -5.6211, -0.5747],
        [-1.6475, -4.0703, -5.5391,  ..., -7.5078, -4.9609, -5.0391]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 9, 25,  0, 27, 22, 20,  0, 27, 13, 22, 27,  1,  5, 15, 20, 27, 27, 27,
        27, 15, 17, 27, 27,  9, 15, 15,  9, 27,  6, 18, 15, 27,  7, 27,  0, 15,
        27, 27, 26, 25,  1, 25,  9,  0,  6, 15, 18,  4, 22, 27,  7, 27,  9, 27,
         0, 27, 18,  0,  6, 17, 27,  4, 13, 18], device='cuda:0')
step: 420
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0991,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.7853e-03, -3.9268e-04,  1.4744e-03,  ...,  2.3007e-05,
         -2.4223e-04, -1.1921e-04],
        [-1.2074e-03,  1.5688e-03,  7.5674e-04,  ..., -7.9012e-04,
         -3.8719e-04,  7.9274e-06],
        [-3.1900e-04,  1.2932e-03,  1.5411e-03,  ..., -8.8406e-04,
          4.1223e-04,  1.2455e-03],
        [-3.9315e-04,  0.0000e+00,  1.2665e-03,  ..., -4.8566e-04,
         -4.3571e-05,  9.9277e-04],
        [-4.6039e-04, -9.1076e-04,  1.0767e-03,  ..., -3.3236e-04,
         -1.4210e-04,  8.6498e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8466, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.7285, -3.7910, -4.2578,  ..., -5.4922, -5.3516, -4.5391],
        [-3.0938, -2.6562, -3.8438,  ..., -4.9531, -4.5781, -1.1104],
        [-4.5156, -3.2500, -4.5781,  ..., -6.7656, -4.8750, -0.3911],
        ...,
        [-4.5508, -3.3770, -4.3789,  ..., -6.9414, -5.3477, -0.5034],
        [-7.3789, -5.0977, -6.4570,  ..., -0.0835, -7.1445, -5.5352],
        [-5.5664, -3.6602, -4.1758,  ..., -6.8945, -4.3477, -0.4717]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20,  5, 27,  3,  0, 20, 27,  3, 18,  1, 25, 27, 27, 22, 27,  3, 27,  4,
        15,  9, 27,  0, 27, 27, 27, 17, 15, 27,  1, 24,  7,  0, 18, 13, 15, 27,
        27,  6, 25,  0, 27,  7, 10, 25, 17,  7, 26,  0,  5, 27, 27, 27,  7,  4,
         0, 27,  4, 26,  4, 15, 27,  4, 25, 27], device='cuda:0')
step: 421
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0992,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.8382e-04, -6.9809e-04,  6.0260e-05,  ...,  9.2149e-05,
         -1.2010e-04,  2.0599e-04],
        [-7.3373e-05,  2.6398e-03,  3.7456e-04,  ..., -2.3289e-03,
          1.1712e-04, -1.2751e-03],
        [ 4.2772e-04,  1.3914e-03,  3.5620e-04,  ..., -2.3327e-03,
          8.0013e-04,  1.7376e-03],
        [ 3.0327e-04, -6.2561e-04,  7.3338e-04,  ...,  1.5795e-04,
         -1.6761e-04,  1.1501e-03],
        [-8.8632e-05, -4.5776e-04, -3.6049e-04,  ...,  1.0699e-04,
         -4.2415e-04, -2.5654e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0138, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3281, -2.9688, -4.6406,  ..., -6.3594, -3.7031, -1.1875],
        [-1.6123, -2.8320, -4.8633,  ..., -4.9102, -5.4883, -1.7686],
        [-4.0703, -3.2578, -3.5859,  ..., -4.2734, -3.7891, -0.6323],
        ...,
        [-2.6074, -2.2168, -4.4805,  ..., -6.1875, -6.2656, -1.6230],
        [-5.2812, -2.5938, -3.4688,  ..., -4.5469, -5.1719, -0.9688],
        [-4.4492, -3.9043, -4.1680,  ..., -3.8730, -4.2148, -0.7314]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 17,  1,  6,  7,  3, 27,  3, 10, 17, 27,  1, 27, 27, 15,  1, 15, 27,
        27, 10,  0, 20,  4, 27,  0, 17, 17,  1, 11,  9,  4, 25, 17,  2, 10, 13,
         8, 18, 27, 18, 27, 17, 27, 10, 27, 13,  4, 13,  0, 27, 14, 27, 27, 27,
        27,  4, 26, 17, 27,  4, 27, 27, 11, 27], device='cuda:0')
step: 422
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0992,  ...,  0.6562,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.7299e-04, -9.1219e-04, -1.3723e-03,  ..., -1.0958e-03,
          4.7421e-04, -4.3631e-04],
        [-6.2704e-04,  9.4271e-04, -1.4601e-03,  ..., -3.2215e-03,
         -1.4782e-05,  1.2817e-03],
        [ 4.3368e-04,  1.0805e-03,  2.5177e-04,  ..., -1.3552e-03,
          4.8065e-04,  7.2479e-04],
        [-3.3116e-04, -8.0585e-04,  8.4448e-04,  ...,  5.8746e-04,
          1.1387e-03,  4.4513e-04],
        [ 1.1742e-05, -4.6945e-04, -4.8935e-05,  ..., -4.8780e-04,
         -3.2949e-04, -4.5347e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0300, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.4648, -3.3711, -2.9805,  ..., -0.8872, -6.0742, -5.4336],
        [-5.3633, -3.7852, -5.0977,  ..., -7.3789, -6.3477, -0.1909],
        [-5.8242, -3.8066, -3.0586,  ..., -0.9170, -4.8711, -3.0742],
        ...,
        [-1.9775, -3.8047, -3.8047,  ..., -5.5234, -3.6484, -0.7583],
        [-3.3613, -2.9863, -4.1289,  ..., -4.0195, -2.1113, -2.1113],
        [-4.6914, -2.5820, -4.4102,  ..., -5.3633, -3.5039, -2.5020]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25, 27, 24, 22, 27,  6, 27, 20,  4, 17, 27, 27, 27, 18, 27, 27, 15, 27,
        11,  7, 18, 27, 27, 20, 13,  8, 27, 18,  8,  0, 10, 20,  5, 27, 27,  3,
         0, 27, 14,  6, 27, 22,  4, 17,  7, 10, 27, 20, 10,  8, 27, 27,  6,  7,
        27,  4,  7, 22, 18, 14, 27, 27, 20,  7], device='cuda:0')
step: 423
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0992,  ...,  0.6567,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.5029e-05, -1.3046e-03,  2.4390e-04,  ...,  2.9993e-04,
         -5.5122e-04, -2.8801e-04],
        [-8.7118e-04, -5.7650e-04, -8.9884e-04,  ..., -1.5807e-04,
          1.5755e-03,  9.7609e-04],
        [-9.9719e-05,  3.2449e-04,  4.8876e-04,  ..., -8.9788e-04,
         -3.8445e-05, -1.5819e-04],
        [-6.5994e-04,  1.0031e-04,  1.4086e-03,  ..., -1.6665e-04,
          2.8205e-04,  1.4534e-03],
        [-1.7285e-05, -1.2636e-03,  1.0347e-03,  ..., -5.6648e-04,
         -3.7575e-04,  2.2411e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8596, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2695, -2.7715, -3.3965,  ..., -4.0820, -3.9277, -0.7085],
        [-3.0742, -3.7305, -4.7773,  ..., -6.6055, -4.2461, -0.6528],
        [-4.1484, -2.7715, -3.0371,  ..., -4.2734, -2.2715, -1.2256],
        ...,
        [-4.8867, -3.2441, -3.9629,  ..., -5.6211, -4.0430, -0.4316],
        [-6.5117, -3.0723, -4.1016,  ..., -4.5547, -4.8203, -0.7749],
        [-5.0352, -1.8643, -2.9902,  ..., -2.1758, -4.7383, -1.8643]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  2,  4,  7, 22, 27, 27, 15,  6,  6, 27, 27,  2,  1, 27,  0,  4,
        25, 10,  0,  7, 25,  7,  5,  6, 25, 26, 25,  3,  7, 27, 27, 17, 27, 27,
        27, 18, 27, 27, 10,  5,  6, 27,  4, 27, 15, 17, 10,  4, 27, 27, 27,  1,
         1, 13, 27, 17,  9,  5, 10,  1, 27,  1], device='cuda:0')
step: 424
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0992,  ...,  0.6567,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1597e-03, -8.0776e-04,  1.8063e-03,  ...,  1.7462e-03,
         -9.3746e-04,  1.7748e-03],
        [ 5.0850e-03,  1.9283e-03, -3.4046e-03,  ...,  2.7466e-03,
         -3.6573e-04, -2.9678e-03],
        [ 7.7200e-04,  1.6737e-03,  3.7861e-04,  ..., -1.4153e-03,
          1.0312e-05,  1.3590e-03],
        [ 1.1009e-04, -1.6689e-04, -1.0502e-04,  ..., -3.3689e-04,
          1.0276e-04,  4.8280e-04],
        [ 1.0681e-04,  5.2214e-04, -2.9469e-04,  ...,  3.2234e-04,
         -2.1422e-04, -1.2445e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8160, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.7852, -3.9414, -2.5508,  ..., -0.8159, -5.0508, -4.6602],
        [-4.9922, -3.1016, -1.5859,  ..., -1.1953, -6.0703, -4.3516],
        [-6.3672, -3.7246, -3.8496,  ..., -4.5547, -5.8047, -2.6621],
        ...,
        [-3.2129, -4.2930, -5.6523,  ..., -7.6523, -6.3711, -5.7773],
        [-4.9258, -3.8301, -5.0664,  ..., -6.0469, -5.7539, -0.4241],
        [-6.1445, -4.0195, -4.7227,  ..., -5.1758, -4.8008, -0.3008]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 9,  2, 11, 27,  0, 18, 27,  2, 25,  6,  7, 18, 25, 27, 19, 27, 27, 10,
        10, 27,  0, 12, 27,  4, 25,  6,  4, 27,  2,  3, 27,  3, 27,  2, 27, 27,
         6,  3,  4, 22, 26, 18,  4,  3, 15, 27, 17, 27, 14, 22, 27, 27,  0, 27,
        15, 17, 15, 14,  4, 17, 25, 18,  4, 27], device='cuda:0')
step: 425
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2209,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0992,  ...,  0.6567,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.6856e-04,  1.1444e-03,  2.7275e-04,  ..., -7.5197e-04,
          5.6505e-04,  1.8692e-04],
        [-2.5201e-04, -2.7442e-04, -1.2960e-03,  ...,  9.7990e-05,
          1.2624e-04, -2.8205e-04],
        [ 3.9744e-04, -1.1292e-03, -2.3670e-03,  ...,  1.2913e-03,
          1.7214e-03, -1.1358e-03],
        [-3.3283e-04,  3.3021e-05, -3.0208e-04,  ..., -6.4945e-04,
          2.7633e-04, -6.1798e-04],
        [-1.3685e-04,  4.3154e-04, -1.2493e-04,  ..., -6.0022e-05,
          2.7323e-04, -1.1414e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0538, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.2461, -3.0742, -4.3398,  ..., -5.3086, -3.0898, -1.5576],
        [-6.4766, -3.0566, -3.2598,  ..., -4.5078, -3.7441, -1.8213],
        [-4.0430, -3.1055, -3.6367,  ..., -4.6992, -4.1211, -0.9961],
        ...,
        [-4.8125, -3.2812, -3.9688,  ..., -3.9688, -3.8281, -0.5771],
        [-5.9570, -3.0664, -3.7520,  ..., -3.5488, -5.0977, -0.8784],
        [-6.0898, -3.8730, -4.5586,  ..., -7.1680, -3.4023, -0.8569]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10,  6, 27,  5, 27, 27, 27,  0, 17, 27,  4, 27, 27, 27, 27,  4, 27, 25,
         6, 27,  4, 27, 20, 17, 27, 27, 27, 27, 27,  2, 26, 14, 10, 27, 18, 27,
        27, 18, 24, 10,  4, 24, 15, 27,  3, 27, 18, 15, 27, 22, 22,  0, 20,  7,
         4,  9, 25,  1,  0,  4,  8,  1, 10,  7], device='cuda:0')
step: 426
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2983, -1.7207,  ..., -0.6597,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6567,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.3440e-04, -9.6273e-04, -7.1669e-04,  ...,  2.4772e-04,
          3.5226e-05, -8.3089e-05],
        [-2.9421e-04,  1.1673e-03,  1.2770e-03,  ...,  4.9293e-05,
         -2.0390e-03, -1.8530e-03],
        [ 2.0695e-04,  7.5459e-05, -9.2030e-04,  ...,  9.1600e-04,
          7.0763e-04, -5.7364e-04],
        [ 5.4836e-04, -7.5293e-04,  4.1604e-04,  ...,  4.6253e-04,
          6.2943e-04, -7.3671e-04],
        [ 3.7861e-04,  1.0169e-04, -5.3501e-04,  ..., -7.1526e-07,
         -1.7536e-04, -3.3712e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7211, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.6875, -3.6719, -4.3906,  ..., -5.7812, -4.4375, -0.2500],
        [-2.2051, -3.7520, -4.6914,  ..., -6.0195, -4.9258, -3.4414],
        [-3.9746, -3.3496, -3.0371,  ..., -4.8789, -5.3477, -2.3965],
        ...,
        [-5.5664, -2.3965, -1.8174,  ..., -3.0371, -4.2852, -1.8486],
        [-4.0156, -4.4062, -6.0469,  ..., -6.7500, -5.7656, -5.9688],
        [-6.6367, -4.2461, -4.5273,  ..., -5.9492, -5.1836, -0.2148]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([13, 27,  8, 27, 27, 27,  3, 27,  0, 27, 10,  7, 27, 27, 22, 10, 27, 27,
        27,  7, 13,  2,  7,  7,  0, 27, 27, 14,  3, 27,  2, 27, 27, 27, 27,  9,
         8,  7, 22, 18,  3, 10,  7, 27, 27, 26, 27,  1, 15,  0, 27, 10,  0, 13,
        27, 15, 17, 13, 27,  7, 15,  2, 18, 27], device='cuda:0')
step: 427
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2983, -1.7207,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6567,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.0558e-05, -1.8539e-03, -8.8072e-04,  ...,  1.8620e-04,
         -1.7440e-04,  6.8378e-04],
        [ 5.4693e-04,  5.3644e-06,  1.7524e-04,  ..., -3.3283e-03,
         -1.2465e-03,  1.0973e-04],
        [ 1.3089e-04, -5.1260e-04, -1.5259e-04,  ..., -2.1482e-04,
         -5.6934e-04,  2.2233e-04],
        [ 1.2093e-03, -9.2888e-04,  3.0589e-04,  ..., -4.9925e-04,
         -2.8348e-04, -6.0129e-04],
        [ 2.0719e-04, -1.6050e-03, -5.0211e-04,  ..., -3.4857e-04,
         -6.1178e-04, -2.9302e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8780, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.1055, -3.3887, -4.1680,  ..., -4.6055, -4.6680, -0.4500],
        [-6.7695, -2.8320, -2.8164,  ..., -4.3477, -5.3789, -0.6924],
        [-2.6621, -3.6152, -3.9590,  ..., -3.4121, -4.2891, -1.7402],
        ...,
        [-2.3320, -2.8633, -4.6289,  ..., -5.5508, -4.3320, -1.9727],
        [-4.7266, -2.9609, -3.0859,  ..., -2.6504, -3.7129, -2.7422],
        [-4.5234, -3.1016, -4.0234,  ..., -4.4141, -2.1016, -0.9604]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22, 27, 27, 27,  2, 27,  7, 10,  2, 20, 17,  7,  1, 27, 11, 27, 27,  0,
         9, 18, 27, 15,  5, 27, 27, 12, 18, 15,  9,  9,  3,  0, 11,  6, 27, 27,
        24, 26, 15, 27, 27,  7, 10, 24, 27, 27, 15, 20, 27,  7, 11, 14, 18,  6,
        27, 18,  0, 27, 15,  3, 10, 27,  3, 11], device='cuda:0')
step: 428
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2981, -1.7207,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6567,  0.7065, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.8954e-05,  3.5667e-04,  3.0994e-04,  ..., -3.3736e-05,
         -1.3888e-04, -1.4901e-06],
        [ 1.8525e-04, -5.8460e-04, -2.4223e-04,  ...,  1.8692e-03,
          7.3910e-04, -8.3447e-04],
        [-2.6751e-04,  4.4394e-04,  1.8930e-04,  ...,  1.1784e-04,
         -2.6155e-04,  1.1563e-04],
        [-2.1577e-04,  2.0027e-04, -5.1212e-04,  ..., -4.1842e-04,
         -4.2439e-04, -1.0669e-05],
        [-3.4630e-05, -2.0814e-04,  3.8886e-04,  ..., -9.2387e-06,
         -2.7633e-04,  4.8089e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9510, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0508, -3.0332, -3.4395,  ..., -3.0957, -3.0645, -1.4707],
        [-5.8789, -3.3789, -4.0820,  ..., -5.6289, -4.3633, -0.7222],
        [-4.2734, -3.4277, -2.9434,  ..., -2.5684, -4.5234, -1.4277],
        ...,
        [-3.8477, -3.3164, -3.1445,  ..., -1.0654, -3.8320, -3.7227],
        [-4.2227, -2.8809, -4.2070,  ..., -7.1758, -4.0977, -0.5522],
        [-5.2539, -3.3945, -4.0664,  ..., -5.7070, -3.9102, -1.0977]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  6, 27,  1,  7,  7,  0, 27, 27,  8, 27, 27, 18, 17, 27, 27, 12, 16,
        14, 27, 17,  0, 27,  1, 27,  4, 15,  6,  1, 15, 13, 27, 15, 27, 22,  0,
         3, 27, 13, 17,  1,  4, 27, 25, 27, 18, 18, 16, 18, 27, 15, 27,  2, 27,
        27,  0,  2, 27, 15, 27, 27, 25, 27, 27], device='cuda:0')
step: 429
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2981, -1.7207,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.4019e-04,  3.1996e-04, -5.2834e-04,  ...,  1.5080e-05,
          5.8508e-04,  1.6809e-05],
        [ 9.0981e-04, -8.6689e-04,  4.8804e-04,  ..., -4.0531e-04,
         -1.2093e-03,  1.0242e-03],
        [ 4.0102e-04, -5.8031e-04, -8.5974e-04,  ..., -1.4992e-03,
         -4.7827e-04,  7.2813e-04],
        [-2.7359e-05, -4.3511e-04, -1.0376e-03,  ..., -7.7343e-04,
          3.5620e-04, -9.4938e-04],
        [ 2.2650e-04, -1.6439e-04,  4.5109e-04,  ..., -3.6836e-05,
         -1.0741e-04,  2.8229e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1290, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.4238, -2.8926, -3.6113,  ..., -3.3770, -2.9883, -2.0488],
        [-4.8125, -4.6719, -5.1406,  ..., -6.4688, -6.2656, -0.2345],
        [-3.4082, -3.5488, -4.7188,  ..., -6.6562, -4.7812, -1.8613],
        ...,
        [-4.9766, -2.7441, -2.8535,  ..., -3.5723, -3.1816, -1.2744],
        [-2.9863, -3.9082, -3.7363,  ..., -5.0977, -2.7832, -0.9551],
        [-6.8672, -4.3203, -4.5547,  ..., -6.6484, -6.1641, -0.1335]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1,  4, 20,  0, 10,  1, 27,  4, 18, 27, 27, 18, 24, 27, 22,  7, 27, 27,
         0,  0, 13,  9, 27, 18, 22, 10, 24,  1,  1, 27, 20,  5,  7, 15, 20,  3,
        10,  4,  3, 27, 22, 13,  1,  0, 27, 26,  9, 27,  4, 27, 27, 17,  4,  4,
        11, 14, 15, 17,  0, 27, 27,  2, 19, 27], device='cuda:0')
step: 430
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0811,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2981, -1.7207,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.4727e-04,  2.7218e-03,  1.7490e-03,  ..., -1.2112e-03,
         -2.4080e-04,  1.6842e-03],
        [ 5.1451e-04,  8.2350e-04,  1.0357e-03,  ...,  1.5717e-03,
          8.4460e-05, -4.4370e-04],
        [-6.2883e-05, -8.9169e-05,  4.0913e-04,  ...,  9.0504e-04,
          4.5562e-04,  1.3328e-04],
        [-4.6110e-04,  3.8981e-05,  1.5008e-04,  ..., -3.6764e-04,
          4.2677e-05,  8.8072e-04],
        [-7.9870e-06,  6.2132e-04,  3.0041e-04,  ...,  3.3307e-04,
         -2.3174e-04,  6.2418e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8984, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.3828, -2.4766, -4.7266,  ..., -5.7109, -4.2578, -3.3047],
        [-3.8164, -3.1621, -3.6465,  ..., -2.9746, -3.6465, -1.0518],
        [-5.0195, -3.7227, -3.0977,  ..., -1.3945, -4.8320, -1.3945],
        ...,
        [-3.9082, -3.3457, -3.7520,  ..., -4.3438, -4.0781, -0.7666],
        [-0.4507, -3.7480, -5.1367,  ..., -7.9336, -5.1367, -3.8262],
        [-3.8262, -3.5137, -3.7930,  ..., -5.0273, -4.0586, -0.5601]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15, 17, 25, 11, 25, 17, 26,  7,  0, 15, 14, 15,  4, 18,  0, 20,  3, 27,
        27,  0, 20, 11,  1,  4, 27,  0,  5, 12, 27, 10, 22, 27, 13, 11,  7,  0,
        10,  3, 27,  4,  1,  4,  6, 15, 27, 17, 11, 27,  7, 20, 27,  3, 18, 27,
        17, 27, 27, 15, 10, 27, 25, 27,  0, 27], device='cuda:0')
step: 431
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0810,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2981, -1.7207,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.6185e-04, -1.0738e-03,  5.0259e-04,  ...,  8.2493e-04,
          2.0385e-04,  4.4656e-04],
        [ 2.8014e-04,  1.0008e-04,  1.5669e-03,  ..., -6.0701e-04,
         -2.0943e-03, -2.0771e-03],
        [-3.6144e-04,  4.8447e-04,  1.9026e-03,  ..., -1.8191e-04,
         -8.2016e-04,  5.9891e-04],
        [ 3.1924e-04, -9.4843e-04, -7.4244e-04,  ..., -4.6730e-04,
          1.2577e-05,  5.4240e-06],
        [ 3.8505e-05, -1.8864e-03,  1.0738e-03,  ..., -4.9257e-04,
         -3.4094e-04,  8.3923e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9079, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.3015, -4.1133, -5.9102,  ..., -8.3203, -5.0664, -2.9102],
        [-3.9727, -2.2852, -3.0508,  ..., -2.2070, -3.0977, -1.9883],
        [-4.0820, -3.3652, -4.5195,  ..., -4.7383, -6.5508, -0.6455],
        ...,
        [-3.8359, -3.6953, -4.3203,  ..., -5.0547, -4.1328, -0.5073],
        [-5.8359, -3.0527, -3.8965,  ..., -5.8008, -5.6758, -0.2871],
        [-3.3008, -3.0352, -5.3945,  ..., -5.2227, -5.0820, -5.1758]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  7,  5,  1, 13, 27,  5, 18, 27, 14,  8,  0, 25, 15,  5, 27,  3, 15,
         3,  1,  4, 11, 27,  4, 15, 27, 27, 27,  1, 27,  1, 15, 27,  0, 27, 10,
        22, 27, 27, 25,  1,  4, 27, 27, 13,  1, 13,  7, 27,  1, 15, 27,  0, 27,
         8,  4,  0,  7, 27,  1,  4, 27, 27, 15], device='cuda:0')
step: 432
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0810,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2981, -1.7207,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.1212e-04, -1.5326e-03,  4.3058e-04,  ...,  4.7636e-04,
         -2.3985e-04,  1.5736e-03],
        [-2.5320e-04, -5.1022e-04,  1.8430e-04,  ...,  2.3041e-03,
          3.0422e-04,  3.8862e-05],
        [ 3.7479e-04,  1.1454e-03,  8.8215e-04,  ..., -8.9550e-04,
          1.4699e-04,  8.8453e-04],
        [-7.5436e-04, -3.6049e-04,  7.0477e-04,  ..., -1.9383e-04,
         -8.1360e-05,  1.0328e-03],
        [-4.1902e-05, -1.3924e-03,  3.3855e-05,  ...,  2.2662e-04,
         -4.5443e-04,  3.4332e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1782, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.7305, -2.3867, -3.6055,  ..., -3.1367, -2.9180, -1.4492],
        [-2.7285, -3.3516, -5.7578,  ..., -7.6953, -5.2578, -4.5234],
        [-5.8867, -3.5449, -3.8418,  ..., -6.1367, -3.9043, -1.1074],
        ...,
        [-1.9238, -3.1113, -3.5957,  ..., -3.0020, -2.4707, -1.1582],
        [-2.7793, -2.6855, -2.9512,  ..., -2.9043, -4.2148, -1.5293],
        [-5.1680, -3.2793, -3.6543,  ..., -3.9980, -3.8418, -0.5132]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 18, 27, 25, 18,  0, 27, 27, 25, 15, 15,  1,  8,  5,  1, 10, 15, 27,
        27, 26,  7,  6, 27, 18, 27, 12, 27,  7, 27,  2, 27, 17, 17, 27, 27, 20,
         1,  3, 27,  2, 27, 22, 27, 27, 20, 27, 27, 27, 11, 27, 27,  7, 27,  8,
         7,  3, 23,  3, 24,  3,  7,  1,  2,  1], device='cuda:0')
step: 433
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0810,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2981, -1.7207,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0290e-03, -3.0112e-04,  1.7176e-03,  ..., -1.2150e-03,
         -2.2182e-03,  1.1177e-03],
        [-5.1689e-04,  1.1587e-03,  1.1981e-05,  ..., -1.4544e-03,
          8.6427e-06, -3.1948e-04],
        [-6.1691e-05,  2.7895e-04,  1.7605e-03,  ..., -4.9353e-05,
         -1.6079e-03,  4.3929e-05],
        [-1.3351e-03,  3.9887e-04,  1.8377e-03,  ...,  9.0361e-04,
          9.7036e-05,  2.4910e-03],
        [-9.4056e-05,  3.6454e-04,  1.0109e-03,  ...,  7.4911e-04,
         -6.0654e-04, -9.9123e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9559, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6055, -2.7324, -3.7168,  ..., -1.5762, -3.5293, -1.7949],
        [-4.8711, -2.2754, -3.1035,  ..., -2.3379, -3.6504, -1.8701],
        [-1.9844, -3.3906, -4.8281,  ..., -6.1094, -5.6562, -2.8438],
        ...,
        [-6.1602, -3.2383, -4.1602,  ..., -4.7070, -5.5352, -0.3481],
        [-3.6230, -2.4980, -3.9043,  ..., -5.1367, -3.3418, -0.7324],
        [-2.1875, -2.8438, -5.1094,  ..., -5.9062, -3.8438, -3.5469]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([12, 25,  5, 11, 27,  1, 25, 27,  1,  0, 27, 20, 27,  0,  0, 26, 27, 27,
        27, 12, 10, 20,  0, 27, 26, 15, 27, 27, 10, 27, 27, 15, 27, 13, 27,  1,
         4, 24, 18,  7,  7, 27,  0, 10, 27,  0, 27, 27,  3, 27, 27,  6, 24, 17,
         6,  3, 27, 27, 15, 22,  4, 27, 10, 15], device='cuda:0')
step: 434
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0810,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2981, -1.7207,  ..., -0.6597,  0.6523,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.5810e-04,  2.2531e-05,  1.0138e-03,  ...,  4.6754e-04,
         -6.6948e-04, -1.8477e-04],
        [ 1.5783e-04, -5.5504e-04,  7.6008e-04,  ..., -1.1129e-03,
         -1.6203e-03, -1.6475e-04],
        [ 3.9577e-05,  5.3215e-04,  4.5896e-05,  ...,  1.0624e-03,
          1.0318e-04, -3.9101e-04],
        [ 1.4305e-04,  2.0874e-04, -5.6648e-04,  ...,  1.0908e-05,
         -3.1853e-04,  5.8222e-04],
        [-1.0264e-04, -3.4034e-05,  1.9646e-04,  ...,  6.4969e-06,
          1.1480e-04,  4.7994e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9569, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.1211, -2.7480, -3.7949,  ..., -3.4199, -2.1699, -2.1387],
        [-4.8594, -2.2168, -3.7324,  ..., -4.7344, -2.2949, -1.0293],
        [-3.7578, -2.2578, -3.6797,  ..., -5.2891, -3.9453, -1.1953],
        ...,
        [-5.9102, -4.2539, -4.7070,  ..., -5.4883, -6.4258, -0.2537],
        [-6.7344, -3.7168, -4.2969,  ..., -6.2344, -5.7656, -0.2639],
        [-2.4336, -3.7793, -4.0117,  ..., -5.6523, -4.6211, -4.2930]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20,  4,  4, 24, 27, 10, 27,  2, 18,  4, 27, 27, 10,  8, 23,  0, 27, 27,
        27, 27,  4, 26, 13,  3, 10, 10, 27,  1,  3,  2,  3, 26, 27,  4,  4, 27,
        27, 20, 27, 25,  1, 10, 27, 17, 27, 10, 27,  7, 27, 11, 18, 27, 18, 27,
        27, 10, 17,  5, 27, 27, 10,  4, 27, 18], device='cuda:0')
step: 435
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0810,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2981, -1.7207,  ..., -0.6592,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.1294e-04,  1.2350e-03, -1.0276e-04,  ..., -8.0705e-05,
         -1.9550e-05,  2.0266e-04],
        [-6.8188e-04, -1.1282e-03,  5.3358e-04,  ...,  1.6975e-03,
          5.6362e-04,  6.3467e-04],
        [-3.6335e-04, -7.1049e-04, -1.6856e-04,  ...,  7.0381e-04,
          3.3665e-04, -2.0874e-04],
        [ 5.8591e-05,  2.5034e-04, -4.8637e-04,  ...,  1.4591e-04,
         -5.3644e-04, -4.4632e-04],
        [ 5.3585e-05,  1.2093e-03, -2.1875e-04,  ...,  2.0897e-04,
          3.6573e-04,  6.1035e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7818, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.4004, -3.0098, -4.4961,  ..., -6.6211, -2.3691, -3.9492],
        [-4.2812, -3.1855, -3.1855,  ..., -2.5137, -3.9199, -0.9204],
        [-4.6758, -2.3789, -2.9727,  ..., -3.6914, -5.1289, -1.4570],
        ...,
        [-5.2773, -3.6367, -4.1680,  ..., -6.0430, -5.7773, -0.2930],
        [-5.8828, -3.8047, -5.2422,  ..., -7.2578, -6.4766, -0.1476],
        [-3.5176, -2.7832, -4.1445,  ..., -6.2539, -3.1895, -0.8149]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 27,  3, 27,  3,  2, 11, 27, 27,  6, 27,  4, 12,  9, 27, 27, 27,  8,
         1, 20, 15,  8, 27,  2, 13, 27, 27, 11, 21,  3,  4, 18, 10, 27, 27, 15,
        11, 27, 27,  3,  3, 27,  0, 18,  1, 27, 14, 27,  3, 10, 27, 27, 27, 27,
         0, 27,  0, 27, 15, 27, 20, 27, 27,  4], device='cuda:0')
step: 436
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0809,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2981, -1.7207,  ..., -0.6592,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.6267e-04, -4.6968e-04, -1.2851e-04,  ..., -5.8830e-05,
         -1.1539e-04, -9.2411e-04],
        [ 2.3103e-04, -2.1768e-04,  5.5170e-04,  ..., -1.1673e-03,
         -5.7042e-05,  1.3151e-03],
        [-3.3438e-05, -6.2370e-04, -4.0436e-04,  ...,  3.4094e-04,
          1.1520e-03, -4.4417e-04],
        [-1.1053e-03,  3.8505e-04,  9.1076e-04,  ...,  2.1982e-04,
          4.4346e-04,  4.9782e-04],
        [-4.6253e-05,  4.5276e-04,  4.1533e-04,  ...,  1.0580e-04,
          2.3139e-04, -3.9577e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0895, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.1035, -3.2910, -5.0547,  ..., -6.5703, -5.8047, -0.6343],
        [-2.4766, -3.3047, -4.2266,  ..., -5.7422, -4.5234, -2.9766],
        [-3.6777, -2.9590, -3.2402,  ..., -2.1465, -4.0977, -1.3184],
        ...,
        [-2.7754, -4.1484, -4.8867,  ..., -5.3984, -4.0703, -1.8066],
        [-3.8027, -3.4590, -4.9258,  ..., -6.4258, -5.3984, -0.3655],
        [-1.8086, -2.9961, -4.8242,  ..., -4.7461, -4.5273, -3.5586]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 18, 27, 22, 27,  7,  6, 27,  5, 27,  1, 22,  1, 27,  7, 14, 10,  4,
        22,  4, 27, 27, 27,  7, 27, 27, 27, 27, 20,  0,  1, 27,  3, 20, 20, 27,
         0, 27,  0, 17, 27,  7,  0, 27,  4,  9, 12,  1, 27, 25,  7, 27, 27,  5,
        26, 27, 22, 27,  6, 26,  5, 22, 27, 15], device='cuda:0')
step: 437
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4788, -0.0809,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2981, -1.7207,  ..., -0.6592,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.1301e-04,  1.7405e-04,  4.2534e-04,  ...,  2.0003e-04,
         -1.1992e-04,  9.9301e-05],
        [ 5.5504e-04,  4.2367e-04,  9.8801e-04,  ...,  9.8407e-05,
         -1.2732e-03, -3.0613e-04],
        [ 1.7285e-06, -3.6860e-04,  7.0429e-04,  ..., -2.5558e-04,
         -7.9584e-04,  5.9652e-04],
        [ 4.3273e-05, -6.1274e-04,  1.0481e-03,  ..., -2.4247e-04,
         -3.8433e-04,  7.3814e-04],
        [-2.5845e-04, -7.4339e-04,  4.3011e-04,  ..., -1.4532e-04,
          1.7309e-04,  4.0054e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3131, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.5977, -2.0195, -2.8477,  ..., -1.5508, -5.2852, -3.6758],
        [-3.4688, -3.6250, -3.7344,  ..., -4.0156, -3.0469, -1.3271],
        [-2.0371, -2.7871, -4.5039,  ..., -6.6914, -4.4727, -1.4902],
        ...,
        [-6.4297, -3.1953, -4.3672,  ..., -8.5859, -5.9297, -0.6016],
        [-2.8125, -3.2188, -4.4844,  ..., -7.4219, -5.6562, -1.7803],
        [-5.5117, -2.1836, -3.4336,  ..., -3.5273, -3.9492, -0.9185]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([12,  7,  9, 27, 27, 20, 27, 10, 27, 27, 15,  2, 18,  0, 12, 27, 27, 18,
        18,  7, 27, 27, 18,  1, 27, 12, 14,  4, 23,  2, 27, 27,  4, 22,  6, 27,
        27, 17, 27, 17,  9, 26, 27, 27, 27,  4,  9, 10, 27, 17, 17,  4,  2,  5,
        25, 10,  1, 27, 15,  7, 22, 22, 27, 25], device='cuda:0')
step: 438
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4788, -0.0809,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2981, -1.7207,  ..., -0.6592,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0767e-03, -1.8311e-03, -9.8896e-04,  ...,  1.8253e-03,
          9.2793e-04,  2.7561e-04],
        [-2.3866e-04, -1.8358e-04,  2.1973e-03,  ..., -2.0924e-03,
         -2.6875e-03,  4.7874e-04],
        [ 2.4152e-04, -5.7817e-06, -2.8253e-04,  ...,  2.5320e-04,
         -4.5002e-05,  8.2850e-05],
        [-1.7285e-04, -2.1148e-04,  9.0218e-04,  ...,  7.0333e-06,
         -6.4969e-05,  2.4986e-04],
        [ 2.8610e-04, -7.0190e-04,  1.0914e-04,  ..., -1.8883e-04,
         -3.6120e-04,  1.9228e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9655, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.0605, -3.4199, -4.1836,  ..., -7.2461, -6.8711, -1.4355],
        [-6.4648, -3.5098, -3.5723,  ..., -5.3711, -6.6211, -0.5410],
        [-5.0430, -2.5430, -2.9180,  ..., -5.1836, -4.7617, -0.7778],
        ...,
        [-2.0371, -3.6309, -5.3516,  ..., -4.6016, -2.2402, -2.8340],
        [-5.6289, -2.7246, -3.0840,  ..., -4.3789, -4.9414, -0.6768],
        [-3.2246, -2.9277, -3.7402,  ..., -6.6016, -4.0703, -1.5068]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27,  3,  2, 27, 18,  7,  4, 27, 25, 10, 27,  7,  2, 18,  0, 27,
         0, 13, 10, 27, 27,  2, 22, 11, 27, 24, 10,  7, 18,  0, 27,  1, 27,  4,
         4, 27, 27, 17, 27,  0, 27,  4, 27,  3, 18, 14, 27, 17,  0, 27, 27,  0,
         6, 27, 27, 24, 13,  7, 27, 26, 25, 13], device='cuda:0')
step: 439
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4788, -0.0809,  1.5107],
        [ 1.2500, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2981, -1.7207,  ..., -0.6592,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.7336e-04, -2.8610e-04, -5.6076e-04,  ..., -1.5211e-03,
         -5.5695e-04, -4.3058e-04],
        [-2.3746e-04, -2.0695e-04, -7.1001e-04,  ...,  1.5936e-03,
          1.8406e-03, -4.6396e-04],
        [-4.9233e-05, -2.0504e-04,  2.7037e-04,  ..., -4.2319e-04,
         -1.3065e-04,  3.4475e-04],
        [-2.2733e-04,  3.5477e-04,  2.5725e-04,  ..., -1.2732e-04,
          1.4138e-04,  3.1042e-04],
        [-1.8179e-05, -6.8808e-04,  2.4939e-04,  ..., -4.5109e-04,
         -1.4234e-04,  1.5688e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0769, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3984, -2.6816, -3.1191,  ..., -3.8691, -3.6348, -1.6973],
        [-6.0938, -4.0781, -4.8594,  ..., -3.2988, -6.1406, -0.2664],
        [-5.3164, -2.7383, -3.6289,  ..., -5.2383, -6.0977, -0.7388],
        ...,
        [-0.4324, -3.7129, -5.5586,  ..., -7.4648, -5.9961, -3.3555],
        [-5.7617, -2.4336, -2.9199,  ..., -1.9971, -4.0586, -2.0117],
        [-6.5742, -3.6035, -3.6191,  ..., -6.7578, -4.2148, -1.4004]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 27, 25, 11, 25, 27,  0, 27, 17, 24,  1, 10, 15, 18,  4, 15, 25, 27,
        27,  2, 17,  3, 25, 27,  0, 27, 15, 25, 21,  0, 27, 27, 18, 22, 15, 27,
         4, 14,  0, 27,  6,  9,  7, 25,  0, 24, 27, 18, 11,  0, 27, 27, 27, 26,
        27,  3, 18, 26, 24, 27, 24,  4,  3,  6], device='cuda:0')
step: 440
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4788, -0.0809,  1.5107],
        [ 1.2500, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2981, -1.7207,  ..., -0.6592,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.2135e-05,  2.3425e-04, -1.7986e-03,  ..., -5.4741e-04,
          5.4073e-04, -9.3269e-04],
        [ 1.0443e-03,  2.2125e-04,  2.3901e-04,  ...,  3.8509e-03,
          1.0996e-03, -7.9870e-04],
        [-2.1243e-04,  1.9503e-04, -1.3161e-03,  ...,  5.0783e-04,
          5.6458e-04, -7.9632e-04],
        [ 7.6354e-05,  4.6015e-05, -2.1839e-03,  ..., -1.2131e-03,
         -5.4932e-04, -8.5926e-04],
        [ 3.8767e-04,  6.2799e-04, -5.1355e-04,  ..., -1.2374e-04,
          4.8137e-04, -5.3167e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6077, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.6631, -3.3809, -5.3203,  ..., -7.9922, -5.3828, -4.7266],
        [-2.3730, -3.8887, -5.5586,  ..., -8.6094, -6.9492, -4.6992],
        [-3.8242, -3.6367, -6.9805,  ..., -6.2930, -5.6680, -6.3711],
        ...,
        [-4.8867, -3.6855, -3.7148,  ..., -6.1523, -5.4492, -0.3411],
        [-4.5781, -2.9043, -3.8887,  ..., -4.7500, -3.9199, -0.5918],
        [-5.5898, -2.6191, -3.0723,  ..., -5.5273, -3.4180, -1.0420]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 18, 15, 17, 27, 27, 15, 15, 27, 11,  4, 27, 27, 27,  4,  3,  7, 27,
         2, 27, 20,  7, 27, 18,  3,  3,  5, 27,  4,  5,  7,  5,  1,  0, 13, 22,
         0,  5,  4, 15, 15,  7,  0,  4, 26, 14,  2,  0, 18, 27, 10, 17, 18,  2,
        27, 27, 27, 10, 27, 18,  8, 27, 27,  2], device='cuda:0')
step: 441
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4788, -0.0809,  1.5107],
        [ 1.2500, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4082, -0.2981, -1.7207,  ..., -0.6592,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.6246e-04,  3.1700e-03, -3.4690e-05,  ..., -9.7466e-04,
          1.7691e-04,  2.7108e-04],
        [ 4.4322e-04, -9.7466e-04, -1.8549e-04,  ...,  2.5082e-04,
          4.3726e-04,  3.0565e-04],
        [ 3.3808e-04,  1.4615e-04,  2.0237e-03,  ..., -2.3956e-03,
         -8.8930e-04,  1.3971e-03],
        [-5.4312e-04,  2.4390e-04,  5.5122e-04,  ..., -7.6413e-05,
         -2.7847e-04,  3.1161e-04],
        [ 7.9930e-05, -5.1451e-04, -1.1072e-03,  ...,  3.7527e-04,
         -3.2163e-04, -1.8477e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9174, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.8398, -4.1328, -4.6523,  ..., -6.4961, -7.2305, -0.1038],
        [-4.4961, -3.1836, -4.6211,  ..., -6.3711, -6.1211, -0.4167],
        [-1.2568, -2.6016, -4.3516,  ..., -6.1172, -2.7734, -2.1953],
        ...,
        [-3.7891, -3.2891, -2.5234,  ..., -2.3203, -4.1641, -2.6797],
        [-4.2227, -3.7852, -4.6758,  ..., -8.2734, -6.5039, -0.5513],
        [-5.2461, -2.8242, -3.3242,  ..., -4.7461, -5.3711, -1.4033]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  1, 27,  4, 15,  3, 27, 27,  8,  2,  1,  5, 24,  7, 17, 18,  4,
        13, 18, 10,  9,  5,  4,  4, 20, 20, 27,  3,  9,  3,  7, 10, 27, 22, 15,
         2, 10, 27,  2,  7, 15,  6, 27,  1,  0, 18, 27, 27, 27, 15, 17, 15, 22,
        17,  0, 27,  7,  6, 15, 27, 24, 27, 27], device='cuda:0')
step: 442
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4788, -0.0809,  1.5107],
        [ 1.2500, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9043, -1.3477, -0.6030],
        [-0.4082, -0.2981, -1.7207,  ..., -0.6592,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0452e-03,  6.3324e-04,  2.1420e-03,  ..., -3.9434e-04,
         -4.2534e-04,  1.3237e-03],
        [-1.2426e-03, -1.8940e-03, -1.6937e-03,  ..., -1.0061e-03,
          1.2436e-03,  2.2583e-03],
        [-9.8348e-05,  8.6498e-04,  1.3247e-03,  ..., -1.3485e-03,
          2.5272e-04,  2.3422e-03],
        [ 1.1063e-03, -1.6356e-04,  1.1253e-03,  ...,  1.0800e-04,
         -1.9217e-04, -2.8324e-04],
        [-1.3375e-04, -3.6740e-04,  8.8310e-04,  ...,  2.2554e-04,
         -4.8876e-06,  6.7997e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8306, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2188, -2.3262, -2.6543,  ..., -3.6074, -4.0156, -1.1699],
        [-2.7734, -3.8984, -5.0391,  ..., -5.4453, -4.5547, -3.2266],
        [-1.1729, -3.0938, -4.6094,  ..., -5.9531, -3.8750, -1.5635],
        ...,
        [-4.7773, -3.1836, -3.8242,  ..., -4.5117, -4.5273, -0.3875],
        [-2.0332, -2.6270, -4.4727,  ..., -6.0156, -4.2500, -1.0488],
        [-1.3027, -1.9902, -4.1133,  ..., -4.0977, -3.2871, -2.5684]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22, 17,  0, 27,  1, 27, 11,  0, 27, 27, 27, 27, 25, 14,  1, 27, 10, 10,
        27, 20, 19, 11, 27, 11, 24,  0, 20,  7, 17,  2,  1, 27, 27, 11,  1,  1,
         1, 27, 27, 17,  4, 15, 27, 27, 18, 26, 27,  3, 25,  0, 27,  6,  0, 27,
         4, 27, 15,  7, 27,  3, 27,  3, 17, 27], device='cuda:0')
step: 443
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4788, -0.0809,  1.5107],
        [ 1.2500, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9043, -1.3477, -0.6030],
        [-0.4082, -0.2981, -1.7207,  ..., -0.6592,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0996,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.5102e-05, -7.1287e-05, -1.4186e-04,  ..., -5.6124e-04,
         -2.4343e-04,  1.8454e-04],
        [-4.4298e-04,  3.6907e-04,  5.7173e-04,  ...,  3.0518e-03,
          1.0300e-03, -6.3038e-04],
        [-8.2016e-05,  1.4007e-05,  6.8188e-04,  ...,  5.4026e-04,
         -9.8705e-04, -2.0742e-04],
        [ 1.6999e-04, -3.9935e-05, -1.2913e-03,  ..., -3.4189e-04,
         -2.2304e-04,  1.9217e-04],
        [ 3.8385e-04, -7.4625e-04,  5.9557e-04,  ..., -1.5211e-04,
          1.1885e-04,  5.5218e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.4961, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9883, -3.8789, -6.6914,  ..., -7.1758, -7.0508, -6.4258],
        [-2.9023, -3.3867, -4.7617,  ..., -5.2305, -4.7148, -4.6367],
        [-5.3203, -3.2246, -3.5996,  ..., -4.0859, -5.1953, -1.3818],
        ...,
        [-4.6289, -3.4395, -4.1914,  ..., -4.6445, -4.2695, -0.7056],
        [-6.5312, -2.6719, -3.9062,  ..., -5.2500, -4.6250, -0.5938],
        [-5.4102, -3.5801, -3.8613,  ..., -3.7832, -3.1270, -1.6895]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15, 17,  4,  2,  0, 27, 27, 15, 27,  7, 27,  0, 27,  7,  0, 18, 17, 27,
        27, 20,  1,  7, 26, 13,  1,  9, 20, 27, 27,  0, 27, 27, 10,  6, 15,  7,
        15,  1, 18, 12, 27, 27,  1, 18,  8, 27, 27, 27, 27, 14, 27, 27,  0, 15,
         2, 18, 27, 27, 18,  4, 27, 27, 22,  6], device='cuda:0')
step: 444
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4788, -0.0808,  1.5107],
        [ 1.2500, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9043, -1.3477, -0.6030],
        [-0.4082, -0.2981, -1.7207,  ..., -0.6592,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0996,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.2469e-04, -3.7956e-04, -2.5845e-04,  ..., -6.1035e-05,
         -4.6039e-04, -3.9172e-04],
        [-1.4544e-04,  1.2636e-04, -3.2401e-04,  ...,  6.3467e-04,
          8.6737e-04, -6.3956e-05],
        [-5.8830e-05,  1.1663e-03,  4.3130e-04,  ..., -1.0616e-04,
         -5.7817e-05,  3.4475e-04],
        [-3.9673e-04,  6.0940e-04, -6.3515e-04,  ..., -4.7851e-04,
         -2.7800e-04,  2.6488e-04],
        [-8.9407e-06, -1.1101e-03,  4.1580e-04,  ..., -4.9293e-05,
         -5.4836e-04,  2.6798e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7032, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.5996, -2.3027, -3.9902,  ..., -5.1328, -3.7090, -1.7256],
        [-6.2539, -3.3457, -3.6270,  ..., -6.4414, -4.4102, -1.7998],
        [-2.0723, -2.5723, -6.4141,  ..., -7.5703, -5.3984, -4.4141],
        ...,
        [-4.1406, -3.9238, -5.5625,  ..., -7.0312, -2.3457, -4.2031],
        [-2.7832, -2.7051, -3.7988,  ..., -4.3281, -3.7988, -1.0645],
        [-3.7070, -2.5664, -3.2695,  ..., -3.0352, -3.9883, -1.1133]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 27, 15, 15,  4, 15, 20, 15, 27, 18, 27, 18, 27, 17, 27, 27, 18, 23,
         3, 10,  0, 25, 27, 27, 27, 27, 27,  6,  9,  0,  0,  3,  3, 15, 27,  9,
        10,  4, 15, 20, 27, 27, 27,  1, 25,  6, 27,  0, 27,  4,  7, 27,  4,  5,
        27, 15, 27,  7,  8,  7, 25, 13, 17,  4], device='cuda:0')
step: 445
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4788, -0.0808,  1.5107],
        [ 1.2500, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9043, -1.3477, -0.6030],
        [-0.4082, -0.2981, -1.7207,  ..., -0.6592,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.1989e-04, -6.9189e-04,  1.1063e-04,  ..., -3.0494e-04,
         -9.1553e-04,  1.9817e-03],
        [-6.1750e-04, -5.4216e-04,  7.4768e-04,  ...,  5.2786e-04,
         -1.2245e-03,  7.3195e-04],
        [-3.8981e-04,  1.2074e-03,  1.9388e-03,  ..., -9.2983e-04,
         -7.5912e-04,  8.9741e-04],
        [-2.2793e-03, -2.4557e-04,  5.9938e-04,  ...,  4.7493e-04,
          1.6069e-04,  1.0118e-03],
        [-5.3644e-04, -8.4996e-05,  1.7214e-04,  ..., -2.2650e-06,
         -4.1485e-04,  1.1533e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0108, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.3594, -4.3906, -5.4219,  ..., -6.8281, -7.3281, -4.6719],
        [-1.0615, -2.8105, -5.3906,  ..., -7.1719, -6.3438, -2.4375],
        [-3.1719, -2.5156, -2.7500,  ..., -3.7168, -4.1562, -1.8740],
        ...,
        [-7.0781, -3.7168, -4.0586,  ..., -6.0117, -7.0781, -0.2482],
        [-3.9023, -2.9961, -3.3711,  ..., -4.3086, -5.6523, -1.0898],
        [-6.2227, -2.9238, -3.5488,  ..., -6.1758, -4.6758, -1.1279]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18,  5,  8,  8, 13, 14,  6,  2, 25, 27, 26, 27, 27, 27,  7, 11, 27, 27,
        27, 27, 20, 27, 10, 27,  3, 15,  2,  3, 27, 10,  7,  1, 15, 20,  4, 27,
        27, 15, 15, 27, 17,  8, 24, 13, 20, 26, 10, 15,  0, 10, 27, 27,  7, 27,
        25,  0,  4, 17,  3, 10, 16, 10, 27,  6], device='cuda:0')
step: 446
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4788, -0.0808,  1.5107],
        [ 1.2500, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9043, -1.3477, -0.6030],
        [-0.4082, -0.2981, -1.7207,  ..., -0.6592,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.6335e-04,  4.6492e-04,  4.4966e-04,  ..., -5.6744e-05,
         -4.9591e-04,  3.3951e-04],
        [ 5.2929e-04, -1.3380e-03, -4.5800e-04,  ...,  7.6342e-04,
         -1.3399e-04,  3.3474e-04],
        [ 5.0724e-05, -1.4222e-04,  7.2479e-04,  ..., -4.7326e-04,
         -6.3515e-04,  9.2077e-04],
        [ 2.0742e-05, -2.0242e-04,  1.4439e-03,  ...,  2.2507e-04,
         -4.7517e-04, -7.7248e-04],
        [-3.1257e-04, -3.4785e-04,  4.7088e-05,  ...,  2.1231e-04,
         -2.7871e-04,  3.7491e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8251, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.6953, -2.5703, -3.4141,  ..., -4.7578, -4.7891, -2.2109],
        [-5.0664, -4.4258, -4.0508,  ..., -5.1758, -6.5039, -0.1602],
        [-5.6523, -3.2949, -4.3750,  ..., -6.5938, -3.9355, -1.0293],
        ...,
        [-5.0586, -3.7617, -4.4492,  ..., -6.8398, -5.8398, -0.5752],
        [-7.0820, -3.2070, -3.6445,  ..., -4.9883, -5.8008, -0.3628],
        [-4.0469, -2.3418, -3.5137,  ..., -5.8906, -4.3750, -0.8110]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 27,  7, 17, 27, 27,  4,  3,  1,  9, 15,  9, 18, 18,  0, 27, 27, 10,
         0,  7, 17, 13,  2, 10, 27, 27, 13, 27, 27, 26,  4, 27,  4,  0, 26, 27,
         1,  0,  2,  3,  6, 27, 27, 27, 27, 15,  4, 20, 27,  7, 27,  0,  4, 27,
         9, 23, 17,  0, 27, 21, 27, 27, 27, 13], device='cuda:0')
step: 447
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6152,  ...,  0.4788, -0.0807,  1.5107],
        [ 1.2500, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9043, -1.3477, -0.6030],
        [-0.4082, -0.2981, -1.7207,  ..., -0.6592,  0.6523,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.5899e-04, -1.6463e-04, -1.0228e-04,  ...,  1.0836e-04,
         -1.8573e-04,  3.8290e-04],
        [ 5.1498e-05,  4.4036e-04,  1.3170e-03,  ...,  9.7942e-04,
         -1.2579e-03, -1.0376e-03],
        [-6.3419e-04,  2.1267e-04, -4.6873e-04,  ..., -4.5395e-04,
         -2.3663e-05, -1.0794e-04],
        [-9.2983e-04, -2.0432e-04,  1.9684e-03,  ...,  2.7847e-04,
          1.4782e-05, -1.3180e-03],
        [-3.5667e-04,  4.7565e-04, -1.1826e-04,  ..., -7.8738e-05,
         -1.8978e-04, -4.6396e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9701, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.3848, -2.3379, -3.7910,  ..., -5.2578, -3.8223, -0.8994],
        [-4.1484, -2.6191, -3.3848,  ..., -4.7578, -3.6973, -2.5410],
        [-2.3340, -3.0840, -4.8516,  ..., -7.0859, -4.3828, -4.2422],
        ...,
        [-3.0234, -3.8340, -3.8809,  ..., -4.6172, -5.7266, -1.4131],
        [-5.1445, -3.5820, -4.1445,  ..., -6.4570, -6.1602, -0.3000],
        [-3.1582, -3.4082, -3.5801,  ..., -5.0312, -5.7344, -0.6895]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  1,  1, 27, 15, 27, 15, 25, 15, 27,  6, 25,  7,  7, 27, 22, 18, 27,
         4, 27, 11, 27, 26, 20, 27,  7, 20,  4, 11, 11,  0, 27, 27, 18, 27,  0,
         1,  4, 26, 27, 27,  7, 17,  7, 21, 27,  0, 27,  4,  9,  7, 27, 11,  7,
        27, 10, 27,  1, 11, 15, 27, 18, 27, 17], device='cuda:0')
step: 448
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6152,  ...,  0.4788, -0.0807,  1.5107],
        [ 1.2500, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2213,  0.7939,  ..., -1.9043, -1.3477, -0.6030],
        [-0.4082, -0.2981, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.2411e-03,  3.0470e-04, -9.9754e-04,  ...,  5.8889e-04,
          6.3610e-04, -1.0843e-03],
        [-7.4005e-04, -6.5756e-04, -1.3914e-03,  ...,  1.9703e-03,
          1.7252e-03,  1.6384e-03],
        [-1.5962e-04,  8.8263e-04,  6.5386e-05,  ...,  4.0460e-04,
         -7.7724e-04, -4.5180e-04],
        [ 1.0214e-03,  7.0620e-04, -1.8930e-03,  ...,  1.2188e-03,
         -1.3084e-03,  4.3607e-04],
        [ 1.6963e-04, -7.9250e-04, -6.0654e-04,  ...,  6.1941e-04,
          2.7514e-04,  2.0576e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8825, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.8672, -2.7109, -2.9766,  ..., -1.2109, -4.5391, -1.6328],
        [-0.3804, -3.6465, -5.4258,  ..., -6.8008, -5.1289, -3.8809],
        [-2.4297, -2.8203, -3.2441,  ..., -2.1191, -3.5410, -2.1328],
        ...,
        [-1.4727, -3.3945, -4.0977,  ..., -6.4727, -5.4258, -5.6758],
        [-3.8945, -3.6270, -6.3789,  ..., -6.1602, -6.3945, -6.2695],
        [-1.2646, -3.7812, -3.6719,  ..., -4.3594, -5.5938, -3.5156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0, 14,  8, 15,  3,  7, 27, 27, 15, 27, 27, 15, 27,  1,  1,  6, 15,
         7, 27, 15, 17,  4, 20,  6,  0, 27, 27, 27,  1, 22, 27, 18, 25, 27,  9,
        10, 10, 12,  2, 13, 27, 27,  6,  0, 27, 22, 18, 18,  9, 27,  1, 14,  7,
         4, 14, 27, 10, 27, 17, 25, 18, 15,  8], device='cuda:0')
step: 449
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6152,  ...,  0.4788, -0.0807,  1.5107],
        [ 1.2500, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2213,  0.7939,  ..., -1.9043, -1.3477, -0.6030],
        [-0.4082, -0.2981, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.2711e-04, -1.0777e-03, -2.4009e-04,  ...,  7.2145e-04,
          2.3341e-04, -8.0824e-04],
        [-5.3978e-04, -2.6643e-05,  4.9293e-05,  ...,  4.6062e-04,
          2.5988e-04, -8.0824e-04],
        [ 1.1849e-04,  1.3199e-03,  9.1076e-04,  ..., -9.5463e-04,
          2.4736e-05,  7.2765e-04],
        [-2.6655e-04, -6.5041e-04,  2.0397e-04,  ..., -5.6314e-04,
         -8.5056e-05, -8.1825e-04],
        [ 1.4079e-04, -1.1289e-04, -4.8971e-04,  ...,  1.7095e-04,
          6.3777e-05, -1.1902e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6049, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.8906, -3.3887, -4.0469,  ..., -5.4023, -7.0781, -0.4363],
        [-6.4531, -6.2383, -6.7500,  ..., -9.1875, -5.1406, -4.6406],
        [-2.2070, -2.9590, -4.5195,  ..., -5.2852, -5.4102, -1.3174],
        ...,
        [-5.2305, -3.6816, -4.4023,  ..., -5.7930, -6.3555, -0.3235],
        [-3.7637, -4.3555, -7.5898,  ..., -7.2461, -6.6680, -6.5898],
        [-2.7812, -2.4688, -4.0469,  ..., -4.5000, -4.4531, -1.4209]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  7,  0,  4, 27, 10, 27, 27,  0,  9, 27,  3,  4, 27, 27,  1,  1, 27,
         2, 27, 15, 27,  7, 15, 27, 27, 27, 27,  9, 25, 18,  4, 27,  3,  4, 27,
        27, 15,  1, 27, 27,  0, 22,  9, 15, 27,  3, 10, 10, 18, 27, 27, 27, 27,
        27,  0, 27, 17, 27, 18,  6, 27, 26,  4], device='cuda:0')
step: 450
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6152,  ...,  0.4788, -0.0807,  1.5107],
        [ 1.2500, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2213,  0.7939,  ..., -1.9043, -1.3477, -0.6030],
        [-0.4082, -0.2981, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.3746e-04, -1.6441e-03, -7.6485e-04,  ...,  3.7217e-04,
         -2.2721e-04,  7.8058e-04],
        [-1.6060e-03,  1.7333e-04,  1.0121e-04,  ..., -2.9087e-03,
         -3.6240e-03,  8.0884e-05],
        [-2.8229e-04,  1.4248e-03, -1.8072e-04,  ..., -5.3346e-05,
          2.4891e-04, -7.4911e-04],
        [-1.0452e-03, -1.7476e-04, -6.9523e-04,  ..., -2.4605e-04,
          1.9789e-04,  2.1434e-04],
        [-8.4817e-05, -3.7718e-04, -1.7214e-04,  ..., -3.9816e-04,
         -3.6645e-04, -1.6975e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9190, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.5586, -2.9824, -3.1230,  ..., -3.8574, -5.3594, -0.9980],
        [-2.9023, -1.9336, -2.0117,  ..., -2.4336, -4.5273, -2.0898],
        [-5.2578, -3.7109, -4.0703,  ..., -5.6172, -6.9453, -0.3208],
        ...,
        [-2.8574, -2.7168, -3.5312,  ..., -4.5938, -5.1875, -1.8428],
        [-3.6699, -2.5293, -3.3418,  ..., -4.9336, -4.1094, -1.6387],
        [-3.3047, -3.0703, -3.3809,  ..., -4.6172, -4.9766, -2.1016]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 12, 10, 27, 27,  4,  1,  7,  4,  5,  3, 25, 15, 27,  1, 27,  2, 27,
        15, 27, 27, 18, 17,  4, 27, 27,  1, 27,  9,  9, 17,  4, 27,  2,  4, 17,
        27, 13, 27, 27,  6,  4,  0, 25,  0,  4, 18, 27, 27, 27, 27, 27, 17,  0,
        22, 27,  0, 27, 27,  0, 27,  3, 27, 18], device='cuda:0')
step: 451
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6152,  ...,  0.4788, -0.0807,  1.5107],
        [ 1.2500, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2213,  0.7939,  ..., -1.9043, -1.3477, -0.6030],
        [-0.4082, -0.2981, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.5715e-04,  1.5736e-04,  7.7152e-04,  ..., -7.9012e-04,
         -4.3941e-04,  9.5749e-04],
        [-2.2423e-04,  4.7016e-04,  1.4362e-03,  ...,  6.5231e-04,
         -4.8089e-04,  4.5109e-04],
        [-1.3757e-04,  6.3038e-04, -6.3848e-04,  ...,  3.9387e-04,
          1.0138e-03, -3.1376e-04],
        [-4.7159e-04, -1.5855e-05,  7.8154e-04,  ...,  2.2602e-04,
          5.9319e-04,  5.6553e-04],
        [-1.1760e-04,  5.0831e-04, -2.0385e-05,  ..., -2.9945e-04,
          1.9312e-04,  1.2589e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8542, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.1875, -3.6406, -2.8438,  ..., -1.5312, -5.8281, -2.5938],
        [-4.8164, -2.2383, -2.1602,  ..., -1.5967, -7.3320, -3.1445],
        [-4.4062, -4.2344, -4.7500,  ..., -4.9688, -0.2507, -2.8750],
        ...,
        [-6.2383, -3.0352, -3.0352,  ..., -5.7539, -4.8789, -1.0977],
        [-4.9805, -2.4492, -2.3848,  ..., -2.4180, -6.2930, -2.0098],
        [-3.4336, -2.8086, -2.3867,  ..., -4.1211, -4.2461, -1.3555]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25,  3, 26,  1,  3, 18, 27, 19, 27, 18, 15, 14, 20, 20,  5, 24,  7,  6,
        20, 11,  0, 27, 27, 14, 27, 27, 20, 13, 17,  2, 22, 20,  1, 18,  4, 18,
        15,  8, 22, 27, 26, 25, 27,  5, 15,  0, 20,  9,  2, 18,  0, 27,  9, 27,
         1, 27,  1,  7, 15,  3, 27, 10, 27, 27], device='cuda:0')
step: 452
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6152,  ...,  0.4788, -0.0807,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2214,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2981, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.0701e-04, -1.4889e-04,  5.3024e-04,  ..., -9.0265e-04,
         -2.8968e-04,  8.5258e-04],
        [ 8.9169e-05, -2.6393e-04, -1.2517e-06,  ...,  2.8610e-04,
         -6.6936e-05, -8.2207e-04],
        [-2.8586e-04, -4.6968e-05,  2.0370e-03,  ..., -5.8746e-04,
         -6.2227e-04,  1.0929e-03],
        [-3.3975e-04, -2.8181e-04, -2.6369e-04,  ..., -3.3665e-04,
         -2.2864e-04, -1.0653e-03],
        [-6.7770e-05, -1.3018e-03,  1.1325e-05,  ...,  1.4603e-05,
         -4.2748e-04, -4.5955e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9175, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.1445, -3.0645, -2.8145,  ..., -2.2676, -5.3633, -1.1270],
        [-2.7031, -2.7344, -3.0625,  ..., -4.6250, -4.5156, -0.8130],
        [-5.3086, -3.1387, -3.9824,  ..., -5.1367, -4.9180, -0.3254],
        ...,
        [-3.7773, -3.4180, -4.3242,  ..., -5.6836, -4.8867, -0.5430],
        [-5.6211, -4.3398, -3.1035,  ..., -0.4324, -6.2461, -5.3242],
        [-5.7578, -3.0859, -2.5391,  ..., -5.1016, -3.7578, -1.0225]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 14, 15, 27, 27, 25, 27,  5,  0, 27, 15,  9,  0, 22, 25, 15,  3,
        10,  1, 27, 27,  7,  4,  5, 20, 19, 27, 25, 27, 17, 18, 27, 27,  4, 27,
         0, 27, 18, 27,  0,  7, 22, 20, 17, 27, 15,  7,  9,  6,  9, 27, 27, 27,
         0, 27, 15,  0,  0,  6,  4,  4, 25,  9], device='cuda:0')
step: 453
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6152,  ...,  0.4788, -0.0807,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2214,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2981, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.1274e-04,  1.1015e-03, -1.2655e-03,  ...,  1.5330e-04,
          3.1853e-04, -1.2655e-03],
        [ 3.2353e-04, -8.8882e-04, -3.0923e-04,  ...,  1.8418e-04,
         -3.3283e-04,  5.9509e-04],
        [-9.2268e-05, -1.0443e-03, -6.0177e-04,  ...,  7.8392e-04,
         -1.5759e-04,  3.6335e-04],
        [ 5.7554e-04, -5.1165e-04, -1.5154e-03,  ...,  2.7299e-05,
         -8.9169e-04, -1.7300e-03],
        [ 9.2685e-05, -3.6860e-04, -1.9789e-05,  ...,  2.2447e-04,
          4.7565e-05,  3.0518e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2199, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.5977, -3.3164, -2.6445,  ..., -1.0967, -5.5195, -2.2832],
        [-5.6367, -3.2148, -3.5254,  ..., -5.1992, -2.9336, -1.9014],
        [-5.4883, -4.1758, -3.9863,  ..., -7.3164, -6.1758, -0.5337],
        ...,
        [-2.4863, -2.9863, -4.0195,  ..., -5.8164, -4.6602, -1.6895],
        [-5.0820, -3.3633, -4.2852,  ..., -6.1914, -5.1289, -0.4258],
        [-3.1191, -2.8535, -3.1660,  ..., -1.1816, -3.9473, -4.0586]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([24, 26,  7, 27, 27, 18, 15,  4, 19, 27, 10,  1,  3, 18,  6, 19,  3,  9,
         7, 27,  0, 27, 27, 17, 27, 27, 25,  1,  3,  4,  0, 15,  7, 15, 18, 11,
         5,  3, 27, 27, 24,  2, 27, 27,  4, 27, 10,  2,  9, 27,  7, 26,  4,  0,
         7, 18,  4,  3, 11,  6, 27,  4, 18,  3], device='cuda:0')
step: 454
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6152,  ...,  0.4788, -0.0807,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2214,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.6073e-04, -4.1664e-05,  9.5463e-04,  ..., -2.6035e-04,
          8.4686e-04, -4.2820e-04],
        [ 1.4791e-03,  2.2674e-04,  3.7479e-04,  ...,  1.1456e-04,
         -2.6073e-03, -8.0395e-04],
        [ 5.2786e-04, -1.2836e-03, -4.6468e-04,  ...,  8.5688e-04,
         -3.7265e-04,  2.7299e-04],
        [ 5.7411e-04, -7.2956e-04, -1.4868e-03,  ..., -1.2350e-04,
         -3.4380e-04, -1.0452e-03],
        [-1.5402e-04, -4.2725e-04, -5.5313e-04,  ..., -1.2314e-04,
          2.0051e-04,  1.1921e-06]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0161, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.3008, -3.2207, -4.3789,  ..., -4.6133, -5.3945, -0.4707],
        [-5.0859, -3.1348, -3.8828,  ..., -4.9922, -4.0078, -1.0713],
        [-2.2773, -3.4023, -4.9492,  ..., -5.9648, -7.4648, -0.8394],
        ...,
        [-4.1914, -2.5352, -2.5039,  ..., -3.6914, -2.5977, -1.6602],
        [-4.5078, -3.0703, -3.4453,  ..., -4.8516, -3.0859, -0.6953],
        [-3.4043, -2.7012, -3.3887,  ..., -4.9219, -5.1094, -0.9834]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  0, 22,  7, 27, 27,  2, 10,  6, 27, 22, 22, 20, 15, 27, 20, 27,
        18, 27, 27, 27, 15,  1,  1, 22, 27,  4, 27, 27, 11, 22, 27, 27,  4, 14,
        15, 15, 25, 27, 20, 27,  1,  0, 15,  1, 27, 27, 27,  1,  3, 27, 27, 20,
        17,  5, 22, 27,  1, 27,  1, 27, 27, 27], device='cuda:0')
step: 455
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6152,  ...,  0.4788, -0.0807,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2214,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0614e-03,  1.2636e-03,  1.5154e-03,  ...,  8.1253e-04,
          6.4516e-04, -4.5276e-04],
        [-1.2388e-03,  8.7547e-04, -3.4165e-04,  ..., -1.9550e-03,
         -1.7204e-03,  8.0872e-04],
        [ 4.7493e-04,  2.1553e-04,  6.4993e-04,  ..., -2.2829e-05,
         -4.8018e-04,  6.4731e-05],
        [ 1.0691e-03,  8.8501e-04, -3.2330e-03,  ...,  2.8801e-04,
         -8.7357e-04,  1.1673e-03],
        [ 3.4153e-05, -4.6563e-04, -2.6178e-04,  ..., -2.6059e-04,
          3.2139e-04, -2.1219e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8267, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-7.2461, -4.4492, -3.3574,  ..., -2.7012, -6.5586, -5.1367],
        [-5.9883, -3.3613, -2.8613,  ..., -0.5337, -4.9102, -3.2832],
        [-1.6807, -2.7754, -4.5859,  ..., -6.3359, -4.1484, -2.1504],
        ...,
        [-6.3398, -3.7773, -3.8555,  ..., -5.6992, -5.8555, -0.2148],
        [-6.5195, -3.5664, -3.6602,  ..., -5.4102, -5.0664, -0.3640],
        [-4.1289, -3.4727, -6.8477,  ..., -8.4297, -7.5820, -6.5977]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 24,  0, 15,  6, 27,  8, 25,  0, 27, 27,  0,  6, 27,  2, 27, 27,  0,
         9, 27, 25, 27, 18,  0, 17,  1, 15,  0, 25,  4, 27, 27, 15, 27, 13,  2,
         4, 27, 27, 27, 27,  3, 27, 10, 26, 27, 17, 13, 15, 27, 11, 27, 25, 25,
         0, 10, 10, 27, 27, 27, 10, 27, 22, 15], device='cuda:0')
step: 456
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6152,  ...,  0.4788, -0.0807,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2214,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.0333e-04,  5.2166e-04, -1.1148e-03,  ...,  8.6975e-04,
          5.8746e-04, -4.2629e-04],
        [ 1.2195e-04,  1.5230e-03,  6.9022e-05,  ..., -3.2825e-03,
         -1.6708e-03, -1.0147e-03],
        [-7.8678e-04,  1.0223e-03, -7.7069e-05,  ...,  2.0885e-03,
         -4.6062e-04, -8.4686e-04],
        [-4.2295e-04,  4.0829e-05, -5.4407e-04,  ..., -2.1458e-04,
          5.0640e-04, -4.5347e-04],
        [ 7.1704e-05, -1.6809e-04, -4.1389e-04,  ...,  7.4744e-05,
          5.4932e-04, -4.0591e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7904, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.8320, -3.1289, -3.2695,  ..., -3.8164, -5.4258, -1.0205],
        [-4.0195, -2.8945, -3.0039,  ..., -2.7852, -4.6289, -1.1768],
        [-2.5117, -3.6211, -4.5273,  ..., -6.2305, -5.1367, -3.8867],
        ...,
        [-2.7070, -3.7227, -4.6914,  ..., -6.4727, -6.1914, -0.5034],
        [-4.1914, -4.0508, -4.5352,  ..., -5.3945, -6.0664, -0.1903],
        [-3.2324, -3.1387, -3.4512,  ..., -4.6211, -3.9980, -0.8101]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 27, 18,  0, 10, 15,  7, 27, 18,  2, 27, 11, 15, 27, 25, 17, 15,  1,
        27, 22, 13, 18,  7, 25, 27, 27, 17,  4, 25, 24, 17, 27, 27, 26, 10, 13,
         6, 27, 27, 15,  7,  5, 10, 27, 27, 18, 22,  7, 10, 27,  9, 15,  3, 27,
        25,  1, 11, 25, 27,  4,  3, 27, 27,  1], device='cuda:0')
step: 457
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6152,  ...,  0.4788, -0.0807,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2214,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.1648e-04,  2.3293e-04, -2.8801e-04,  ..., -4.3452e-05,
         -3.0994e-06, -8.6594e-04],
        [-3.9005e-04,  3.9458e-04, -3.2234e-04,  ..., -1.0366e-03,
          1.1053e-03,  1.6584e-03],
        [-5.0485e-05,  5.9605e-04,  7.2384e-04,  ..., -1.8549e-04,
         -2.1791e-04,  6.6423e-04],
        [ 1.3275e-03, -3.1531e-05, -8.2731e-04,  ...,  1.6296e-04,
          3.4451e-04,  1.2970e-03],
        [-9.5069e-05, -7.6354e-05, -4.2915e-04,  ..., -1.7500e-04,
         -1.1319e-04, -4.0174e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9299, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0625, -3.6094, -4.2344,  ..., -5.8750, -6.4531, -0.3584],
        [-6.0039, -3.7539, -4.2539,  ..., -6.4102, -4.6445, -1.3008],
        [-1.6094, -3.1406, -3.6875,  ..., -3.4844, -4.2969, -1.3115],
        ...,
        [-5.6797, -3.4746, -3.4121,  ..., -7.2891, -5.2109, -1.4131],
        [-4.3125, -2.8438, -3.3438,  ..., -4.2188, -2.7500, -1.7188],
        [-1.7529, -4.1289, -5.7539,  ..., -8.2500, -6.0195, -4.5352]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  1, 27, 26, 15, 18, 12, 15,  6,  0, 27, 10, 10, 27, 22, 27, 15,
         1, 20, 27, 27, 13, 27, 17,  5, 22, 24, 27,  0, 27,  6, 12,  6,  9, 18,
        18, 10, 27, 11, 27,  1, 15,  0,  0, 27, 17, 18,  6, 20, 27, 17,  9,  5,
        27,  0, 17, 17, 27, 27,  0, 27,  1, 18], device='cuda:0')
step: 458
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6152,  ...,  0.4788, -0.0807,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2216,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.2589e-04, -1.1263e-03, -6.9094e-04,  ..., -2.7504e-03,
          3.5977e-04, -1.1187e-03],
        [ 1.4281e-04, -1.5616e-05, -3.7346e-03,  ...,  3.3569e-03,
          3.6354e-03,  6.2704e-04],
        [ 2.0099e-04,  2.0289e-04, -9.0027e-04,  ...,  7.1573e-04,
          1.3199e-03, -5.3215e-04],
        [ 1.6689e-04, -4.4847e-04, -1.1692e-03,  ...,  1.0896e-04,
          1.8072e-04, -8.3780e-04],
        [-2.6822e-04,  7.1049e-04,  8.2970e-04,  ..., -2.8324e-04,
          3.6669e-04,  9.4891e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6020, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.6289, -3.9570, -4.0977,  ..., -5.6289, -6.0352, -0.4890],
        [-5.6211, -2.8711, -3.8086,  ..., -3.9648, -3.3867, -0.9180],
        [-2.9238, -4.0469, -6.7656,  ..., -6.7812, -6.2969, -5.3906],
        ...,
        [-5.6172, -3.7891, -3.8359,  ..., -5.0391, -4.9766, -0.3372],
        [-2.4824, -2.2949, -4.1367,  ..., -6.0117, -2.4824, -3.0762],
        [-4.6953, -2.8984, -3.4590,  ..., -3.0078, -5.2109, -1.2725]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  6, 15, 27, 27, 15, 14, 18,  0, 27, 10, 27, 20, 26, 27, 27,  1, 18,
         0, 27, 10,  7, 18,  4, 27, 27, 15, 10,  9,  0, 20, 27,  5, 27, 15,  9,
         0,  3,  1,  0, 18,  3, 25,  7, 27, 26,  2, 14,  3,  3, 14,  0, 27, 27,
        27, 27, 27, 27, 11,  7, 18, 20, 15, 27], device='cuda:0')
step: 459
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0807,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2216,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.7253e-05, -3.8958e-04, -5.9700e-04,  ...,  9.5725e-05,
         -1.0449e-04, -5.8174e-04],
        [ 4.3106e-04, -1.5306e-04, -5.5885e-04,  ...,  1.5717e-03,
          1.4524e-03, -7.9632e-04],
        [-7.4506e-05,  4.0817e-04,  5.8031e-04,  ..., -3.2163e-04,
         -1.1510e-04,  7.3910e-04],
        [ 3.6216e-04,  1.8048e-04, -5.0449e-04,  ...,  1.6940e-04,
         -2.3186e-04, -3.9577e-04],
        [-4.4048e-05, -1.4722e-04, -1.8048e-04,  ..., -2.5773e-04,
          5.3585e-05,  2.6011e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9933, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.8867, -3.1367, -5.3867,  ..., -6.9336, -6.5273, -0.7783],
        [-4.4141, -2.5859, -2.5254,  ..., -3.1348, -2.6973, -2.7422],
        [-2.7168, -3.5918, -4.4219,  ..., -4.0938, -5.1406, -0.4043],
        ...,
        [-4.6484, -3.3379, -2.6660,  ..., -3.1191, -5.8672, -1.7754],
        [-2.9570, -3.2070, -3.1133,  ..., -2.8477, -3.4727, -1.1611],
        [-4.0977, -3.3945, -4.4883,  ..., -5.2227, -3.8320, -0.6748]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4,  2, 27, 26, 17,  6, 27,  7, 18, 27,  0,  5,  7, 27,  1,  5, 18, 25,
         8, 27, 27, 27, 20,  0, 27, 27, 12,  4, 26, 26,  4, 10, 14, 27, 10,  4,
        11, 27,  7, 27,  7, 12, 13, 20, 27, 27, 18, 27, 27,  0, 27, 22,  1, 17,
        20, 24, 27, 27, 22, 27, 10, 27,  9, 27], device='cuda:0')
step: 460
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0807,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2216,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0538e-03,  1.2417e-03, -8.0061e-04,  ...,  4.4703e-04,
          2.4164e-04, -1.6379e-04],
        [-6.3133e-04, -5.9986e-04, -4.0483e-04,  ..., -2.6016e-03,
         -1.9445e-03,  2.3403e-03],
        [-1.9407e-04, -2.8300e-04, -8.0538e-04,  ...,  5.9462e-04,
         -4.7970e-04, -1.2693e-03],
        [-9.0301e-05,  6.7115e-05, -1.9283e-03,  ..., -1.0977e-03,
         -5.2166e-04, -2.1095e-03],
        [ 1.6212e-04, -4.3559e-04, -1.8013e-04,  ..., -1.8406e-04,
          4.2486e-04, -4.9412e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8507, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.0996, -3.2871, -4.5664,  ..., -5.9102, -6.3203, -1.2881],
        [-4.3086, -3.4023, -2.6211,  ..., -3.3242, -4.8086, -1.1201],
        [-2.6055, -3.4629, -6.8086,  ..., -6.1992, -6.8867, -4.2773],
        ...,
        [-5.9961, -3.9336, -4.3398,  ..., -3.9336, -7.4336, -0.6362],
        [-3.4570, -4.2383, -4.8945,  ..., -5.4883, -4.2852, -0.4731],
        [-4.7422, -2.9609, -3.6328,  ..., -5.0703, -4.7266, -0.6797]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6,  3, 20,  5, 18, 27, 27,  3, 27,  7, 27, 27, 18, 27, 27, 27, 15, 18,
        26, 10, 20, 23, 10, 25, 27, 17,  9, 10,  4, 27,  6,  1, 27,  0, 27, 27,
        27, 10, 27,  6,  2, 27, 15,  0,  7, 27, 10, 27, 25, 10,  1,  0, 18,  2,
        18,  0, 27, 25, 10, 24, 27, 27, 27, 13], device='cuda:0')
step: 461
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0807,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2216,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.1539e-04, -9.9850e-04,  7.5531e-04,  ..., -1.2054e-03,
         -1.0138e-03,  5.6458e-04],
        [-7.5340e-04,  9.2030e-04,  3.6049e-04,  ...,  3.2368e-03,
          1.6575e-03, -1.8063e-03],
        [-4.0007e-04, -6.9976e-05,  1.0500e-03,  ...,  1.0757e-03,
          3.3498e-04, -2.9707e-04],
        [-8.7595e-04,  5.2404e-04,  1.5163e-03,  ...,  4.8876e-04,
          5.2261e-04,  4.3631e-04],
        [-4.2200e-04,  6.7568e-04, -2.9755e-04,  ..., -3.1853e-04,
          1.4675e-04, -7.8535e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9702, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.3535, -2.8223, -4.3242,  ..., -5.9766, -4.8516, -1.3691],
        [-4.6133, -3.3477, -3.8320,  ..., -3.8633, -5.0195, -0.6914],
        [-1.9570, -3.4883, -3.3945,  ..., -4.5039, -5.6602, -0.8794],
        ...,
        [-4.3789, -3.8145, -4.2695,  ..., -6.6602, -6.0039, -0.3467],
        [-5.4609, -3.7285, -3.9785,  ..., -3.6973, -5.7266, -0.4006],
        [-3.9102, -3.5508, -3.3008,  ..., -4.8633, -4.4258, -0.3940]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([23, 27, 13, 27, 27,  2,  0, 27, 27, 27,  2, 27, 27, 20, 15, 27,  8, 27,
        20, 27, 18,  4, 18,  4, 14,  6,  3, 27, 27,  5,  9,  0, 27, 27, 27, 27,
         9,  4, 15, 23,  4, 15,  7, 11, 27, 27, 13, 27,  3,  4, 27, 27, 27,  5,
        27,  4, 27,  0, 27, 27, 17, 13, 27,  2], device='cuda:0')
step: 462
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4790, -0.0807,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2216,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.5878e-04, -1.4935e-03,  1.6785e-04,  ..., -4.2701e-04,
         -2.7895e-04, -6.1989e-04],
        [-5.6744e-04,  8.4519e-05,  5.6267e-04,  ..., -9.2506e-04,
         -5.5599e-04, -4.5156e-04],
        [ 1.7691e-04,  8.0299e-04,  1.0471e-03,  ..., -1.3981e-03,
          4.9067e-04,  8.1110e-04],
        [-1.3571e-03, -8.4102e-05,  1.2503e-03,  ...,  2.3055e-04,
          1.1435e-03, -1.1152e-04],
        [-8.3208e-05,  1.3769e-04,  3.6359e-06,  ..., -4.5013e-04,
          3.9053e-04, -5.3787e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9474, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.7891, -2.9160, -1.8838,  ..., -3.2285, -3.9004, -1.7910],
        [-3.1562, -4.0625, -6.7969,  ..., -6.1406, -6.2656, -5.6250],
        [-5.3320, -4.1602, -4.2852,  ..., -6.0039, -6.7695, -0.1302],
        ...,
        [-1.0654, -3.2676, -5.0820,  ..., -6.7227, -4.5664, -1.2373],
        [-0.1521, -4.6680, -5.6992,  ..., -6.0586, -5.4336, -4.2461],
        [-2.0078, -3.0703, -4.3984,  ..., -4.8359, -4.7266, -0.5396]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([24, 15, 27,  0, 27,  2,  4, 14,  4, 22,  0, 27, 15,  2,  7, 25, 27, 27,
        27, 27,  6,  7,  3, 22,  3, 15, 15,  8, 27, 18, 22,  7, 25, 15, 15, 21,
        27,  1, 20,  4, 25, 18,  0,  3,  0, 27,  7,  2, 15, 25, 27, 27, 19,  7,
        27,  3, 11, 27, 20,  4, 15,  0,  0, 22], device='cuda:0')
step: 463
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4790, -0.0807,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2216,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0996,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.1732e-05,  1.8501e-03, -4.5681e-04,  ..., -1.3590e-04,
          2.6178e-04, -7.5340e-04],
        [ 2.6798e-04, -1.3447e-03,  8.1635e-04,  ...,  8.6308e-04,
         -8.2779e-04,  1.4102e-04],
        [-5.5122e-04, -7.5912e-04, -2.1720e-04,  ...,  8.3399e-04,
         -5.6219e-04, -1.2326e-04],
        [ 8.7738e-04,  7.2539e-05, -1.5182e-03,  ..., -2.2352e-05,
         -2.6751e-04, -7.5102e-04],
        [ 3.0780e-04, -3.4928e-05,  1.6451e-04,  ...,  2.9922e-04,
          9.6023e-05,  4.8304e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7509, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.9023, -4.9023, -4.7773,  ..., -2.8242, -5.6523, -3.7148],
        [-3.8066, -3.7754, -3.9629,  ..., -4.5703, -6.4609, -4.0547],
        [-2.8301, -3.7520, -2.8770,  ..., -1.7363, -1.6582, -2.2988],
        ...,
        [-6.3047, -3.4941, -3.4941,  ..., -6.2617, -4.9766, -1.2129],
        [-5.4062, -3.6406, -3.9219,  ..., -4.1250, -4.3750, -0.6245],
        [-3.8828, -5.0391, -8.0859,  ..., -7.7891, -8.2422, -6.0703]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 9,  3, 26,  0,  4,  9, 25, 27, 27, 27, 27, 24,  5, 21, 27,  0,  2,  2,
        10, 11,  2, 10, 13,  4,  0, 27, 15, 20, 27, 27, 27,  6, 27,  1,  7, 18,
         0, 20,  6, 27, 27, 17,  1, 17, 18,  6,  8, 27, 10, 27, 27, 27, 27, 27,
        15, 20, 15, 27, 24,  6, 27,  7, 10, 15], device='cuda:0')
step: 464
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4790, -0.0807,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2217,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0996,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.8951e-04, -4.5466e-04, -4.6897e-04,  ..., -1.4377e-04,
         -4.2772e-04, -3.8052e-04],
        [ 1.9467e-04,  1.8048e-04,  1.3161e-04,  ...,  5.3883e-04,
         -2.6846e-04, -6.8963e-05],
        [-8.5413e-05, -2.4283e-04, -1.1997e-03,  ...,  1.4076e-03,
          1.3237e-03, -4.7636e-04],
        [-7.3373e-05,  1.1879e-04,  6.0558e-04,  ...,  8.8692e-05,
         -1.1950e-03, -2.6655e-04],
        [ 1.8120e-04,  8.8215e-05, -5.6744e-05,  ...,  4.6790e-05,
         -2.8324e-04, -2.6250e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8395, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6250, -3.0781, -4.1406,  ..., -4.6875, -6.6406, -0.4536],
        [-0.2382, -4.7383, -6.2695,  ..., -7.2539, -4.9883, -2.8633],
        [-1.8154, -3.7676, -4.3320,  ..., -5.7227, -3.7051, -2.2363],
        ...,
        [-1.8818, -3.5059, -5.1172,  ..., -6.2266, -4.9141, -1.1621],
        [-5.3320, -3.9863, -4.7695,  ..., -4.3008, -5.4414, -0.2524],
        [-1.5449, -3.4824, -4.6523,  ..., -5.8398, -4.1680, -0.9038]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10,  0, 18, 27, 10, 25, 27,  3, 25, 27,  3, 27, 13, 27, 27, 27, 27, 27,
         7,  3, 15, 24,  6,  4, 27, 10,  0, 25, 20, 27, 10,  1,  1, 27,  4,  1,
        15, 27, 20,  0, 27, 20, 18, 17, 26, 15,  7,  0, 14, 27,  1, 24, 13, 27,
        27, 27, 26, 20, 17, 27, 17,  4, 10,  0], device='cuda:0')
step: 465
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4790, -0.0807,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0996,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.5440e-03, -1.0910e-03, -2.0576e-04,  ..., -1.6375e-03,
         -1.0748e-03,  2.2840e-04],
        [ 1.0614e-03, -3.1304e-04, -9.3269e-04,  ..., -1.7538e-03,
         -1.3351e-03, -1.8477e-04],
        [-1.2994e-04, -2.4056e-04, -5.4121e-05,  ..., -2.1756e-04,
         -1.7834e-04,  3.7909e-04],
        [-2.1629e-03, -1.4257e-04,  7.5912e-04,  ...,  2.8014e-05,
          1.5211e-04,  3.9721e-04],
        [-5.4026e-04, -1.5545e-04,  5.4359e-04,  ..., -7.4625e-05,
         -6.8474e-04, -8.0049e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1418, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.9570, -5.4102, -6.0156,  ..., -7.8633, -7.4258, -0.0958],
        [-3.6953, -4.7891, -8.8203,  ..., -7.8516, -7.9297, -5.4922],
        [-3.5957, -3.1426, -4.2344,  ..., -5.9375, -4.4844, -0.4080],
        ...,
        [-5.2109, -4.1172, -4.5234,  ..., -5.8203, -6.7422, -0.3677],
        [-5.3945, -3.6934, -3.2402,  ..., -4.0703, -5.3008, -0.6777],
        [-2.7617, -3.0430, -3.2305,  ..., -3.8086, -5.1523, -0.7939]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 15, 27, 17, 27, 27,  4,  2,  3, 27, 10, 20,  0,  0, 27, 27,  2, 27,
         7, 27, 20, 13,  7, 13, 18, 27, 10,  7, 10, 21, 27, 24, 27, 20,  7,  2,
         1, 11, 27,  7, 20, 24, 10,  4,  4,  0, 10,  8,  6,  9, 27,  1,  2,  0,
        18, 15, 27, 27,  0, 11, 27,  4, 20, 27], device='cuda:0')
step: 466
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4790, -0.0807,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0996,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.7977e-03, -2.1305e-03, -1.0252e-03,  ..., -2.1858e-03,
         -3.4475e-04, -7.7152e-04],
        [ 1.0881e-03,  7.0715e-04,  1.6546e-03,  ..., -2.0542e-03,
         -3.2024e-03, -6.4373e-04],
        [-1.9670e-05, -6.1989e-04,  2.3413e-04,  ...,  4.2963e-04,
          1.2147e-04, -8.9598e-04],
        [-1.6088e-03, -5.6553e-04,  5.6877e-03,  ...,  1.3113e-03,
         -1.5342e-04, -1.3800e-03],
        [-3.3832e-04,  5.4455e-04,  1.0128e-03,  ...,  4.5252e-04,
         -5.4264e-04, -3.8266e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9272, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.1953, -2.9922, -3.5547,  ..., -4.1953, -3.1328, -0.8047],
        [-3.3984, -2.5547, -3.6016,  ..., -4.7578, -3.5703, -1.0381],
        [-2.1699, -3.5293, -4.4648,  ..., -5.0898, -2.6699, -2.0449],
        ...,
        [-1.2051, -3.0645, -4.1602,  ..., -4.0664, -3.2363, -3.2988],
        [-5.2305, -3.4316, -3.3223,  ..., -2.8066, -4.7461, -0.9014],
        [-0.8916, -4.0312, -6.0781,  ..., -6.7031, -4.9844, -3.3457]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2, 27,  1, 27, 27, 14, 24,  4, 18,  1, 10, 27,  0,  6,  9,  9, 25, 13,
        27, 18, 17, 15, 20, 27,  1, 27,  3, 27, 15, 17, 20, 27, 27, 18,  8, 25,
        27, 22, 15,  9, 15, 27,  5,  6, 27,  4, 24, 18, 27,  7,  7,  7, 24, 10,
         7,  4,  0, 15, 27,  0, 27,  5,  6,  0], device='cuda:0')
step: 467
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4790, -0.0807,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2666],
        [-1.1729,  2.2285, -0.0996,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.7238e-04, -1.0605e-03, -6.8903e-04,  ...,  2.5940e-04,
          1.0777e-04, -6.2704e-04],
        [-4.2510e-04,  6.5994e-04,  6.8188e-04,  ..., -8.9645e-04,
         -5.8603e-04,  6.8665e-04],
        [-6.0320e-04, -4.8280e-05,  5.1594e-04,  ..., -2.4557e-04,
         -7.7534e-04,  5.9462e-04],
        [ 3.8648e-04,  1.1772e-04,  1.6956e-03,  ..., -7.7844e-05,
         -1.0567e-03, -6.3801e-04],
        [ 3.3319e-05, -8.5068e-04,  6.7997e-04,  ...,  2.7704e-04,
         -5.7316e-04,  6.0654e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8456, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3477, -3.6914, -3.1133,  ..., -1.7217, -3.5195, -1.1436],
        [-1.3789, -4.4414, -5.4258,  ..., -6.4414, -6.1602, -5.2383],
        [-0.5581, -3.6211, -6.0586,  ..., -7.3086, -3.8242, -3.3555],
        ...,
        [-3.1504, -3.3223, -3.7754,  ..., -7.0273, -6.0078, -0.7290],
        [-3.1309, -4.1484, -4.6016,  ..., -5.8984, -5.2734, -0.5073],
        [-0.8369, -3.7734, -4.8828,  ..., -5.3672, -2.7422, -3.9922]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([14, 18,  0, 15, 22, 18,  7, 15, 18,  3, 27, 27, 10,  2,  4, 15, 27, 18,
        25,  2,  4, 23, 27, 18, 27, 17, 27, 18, 27,  9, 18,  4, 27, 11, 27,  4,
        27, 17,  4, 27, 27,  1,  2, 17, 27, 27, 11, 27, 27,  7, 12,  9, 27, 10,
        22, 10, 27,  7, 15, 27, 15, 27, 27, 26], device='cuda:0')
step: 468
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4790, -0.0806,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0996,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.7227e-04,  1.9302e-03,  1.5554e-03,  ...,  6.6400e-05,
         -1.1533e-04,  4.7231e-04],
        [-3.1829e-04,  7.6866e-04, -9.6655e-04,  ...,  1.3914e-03,
          1.2770e-03,  2.7657e-04],
        [-1.4019e-04,  7.3481e-04,  6.3610e-04,  ...,  1.0557e-03,
          1.3466e-03,  7.6199e-04],
        [ 1.7834e-03,  2.4581e-04, -1.7214e-03,  ...,  5.4407e-04,
         -1.1759e-03,  1.2484e-03],
        [-1.5533e-04,  4.1246e-05,  8.4996e-05,  ...,  6.6233e-04,
          2.6107e-04,  3.9673e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9576, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.5938, -3.7656, -4.1719,  ..., -5.0469, -5.6094, -0.2825],
        [-5.9414, -4.3477, -4.0039,  ..., -7.0039, -5.0352, -1.7080],
        [-3.2695, -4.0352, -4.9570,  ..., -5.1758, -4.4727, -0.3948],
        ...,
        [-3.8281, -2.9531, -3.2188,  ..., -3.6562, -2.5469, -1.7500],
        [-4.3086, -4.6523, -4.9805,  ..., -5.4492, -6.4805, -0.2781],
        [-3.0020, -2.8145, -3.7051,  ..., -3.5020, -2.6738, -1.1572]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  6,  3,  9, 25, 27,  4, 18,  7, 12, 27, 17, 27,  7, 13, 15,  0,  4,
        25, 25, 18, 27, 27, 17,  4, 25,  3, 24, 26,  7, 13, 27, 17, 27,  7, 12,
        15, 27, 27,  5, 15,  1, 25, 27, 27,  6, 24, 27, 27, 17, 27, 27, 27, 22,
        15,  4, 27, 18,  2, 27, 27, 25,  4, 27], device='cuda:0')
step: 469
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4790, -0.0806,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.8399e-04, -6.6566e-04, -2.0671e-04,  ...,  1.9670e-04,
          6.8855e-04, -3.6478e-04],
        [-6.8903e-04, -5.4598e-05,  3.1424e-04,  ...,  1.5688e-03,
          2.4366e-04,  4.6372e-04],
        [ 3.8803e-05,  7.8678e-04, -4.2021e-05,  ...,  1.3375e-04,
          4.7922e-04,  1.6475e-04],
        [ 2.8038e-04, -4.9782e-04,  1.1581e-04,  ...,  1.8287e-04,
         -5.8770e-05, -5.0163e-04],
        [ 1.3638e-04, -3.6478e-04,  3.6430e-04,  ...,  4.6301e-04,
          8.9824e-05, -6.1095e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6741, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2148, -3.7305, -4.5430,  ..., -5.5273, -4.8242, -0.6045],
        [-0.6182, -4.7266, -5.2422,  ..., -6.3516, -4.9453, -1.6182],
        [-1.1855, -3.4043, -4.5469,  ..., -4.4805, -2.3262, -3.0449],
        ...,
        [-3.8398, -3.2773, -4.0117,  ..., -3.7305, -3.7930, -0.4961],
        [-2.8066, -4.7305, -5.4180,  ..., -7.3867, -5.1680, -5.1992],
        [-5.3789, -3.4863, -3.4863,  ..., -3.4707, -3.9395, -1.1123]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7,  0, 26,  9,  4, 17,  7,  7, 27,  3, 27, 27, 27,  8,  0, 22, 27, 27,
         7, 27,  9, 11, 20, 27, 22,  5, 15, 13,  0, 27, 20, 18, 25,  4,  3,  0,
        27, 15, 27,  9,  0, 11, 18,  1, 20, 27, 24, 17,  3,  0, 10, 15,  0, 24,
        10,  2, 15,  4,  7, 27, 22, 27, 18,  6], device='cuda:0')
step: 470
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4790, -0.0806,  1.5107],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.9427e-04,  1.7338e-03,  8.4209e-04,  ..., -9.6321e-04,
         -9.0313e-04, -3.8052e-04],
        [-1.7476e-04, -3.3021e-04, -3.5906e-04,  ...,  6.2084e-04,
          3.6287e-04,  3.4809e-04],
        [-3.2425e-05, -7.9298e-04, -3.1805e-04,  ...,  4.5490e-04,
          6.7282e-04,  6.0987e-04],
        [ 5.4789e-04, -5.2261e-04,  1.5287e-03,  ...,  8.8501e-04,
          1.6689e-06,  1.6012e-03],
        [-4.2379e-05,  3.7885e-04,  9.6142e-05,  ...,  3.6812e-04,
         -1.1218e-04,  4.9353e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9733, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.2363, -3.8770, -6.7383,  ..., -6.3164, -6.5977, -5.0352],
        [-3.4473, -2.8848, -3.5098,  ..., -2.9629, -4.3047, -1.1191],
        [-0.3208, -4.1172, -6.4297,  ..., -7.8203, -5.4922, -3.2266],
        ...,
        [-4.5508, -4.0508, -5.5039,  ..., -5.6133, -6.2383, -5.0195],
        [-4.8789, -4.5664, -4.6289,  ..., -7.1133, -6.7695, -0.2089],
        [-4.4492, -3.2793, -4.6367,  ..., -6.1680, -3.2793, -5.9961]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15,  9,  0,  3, 27,  0,  5, 27,  8,  3,  2, 27, 17, 13, 27, 25,  7,  1,
         6,  0, 27, 27,  6,  8, 10, 27,  2,  1,  0, 27, 15, 27, 27,  7, 23,  5,
        27, 26, 20, 27,  3,  0, 27, 12, 10, 27, 18, 27,  1, 27,  7, 27, 27, 10,
        17,  2, 15,  4,  0, 27,  0, 20, 27, 13], device='cuda:0')
step: 471
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4790, -0.0806,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6528,  1.2676],
        [-1.1729,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.0156e-04,  7.1228e-05, -7.4506e-05,  ...,  1.8101e-03,
          1.8444e-03, -1.6460e-03],
        [-6.8569e-04, -2.4748e-04, -1.3962e-03,  ..., -9.2936e-04,
          3.6263e-04, -5.5885e-04],
        [ 2.8753e-04, -2.8825e-04,  1.3876e-03,  ..., -9.1267e-04,
         -2.7514e-04,  1.3227e-03],
        [ 2.3746e-03, -3.3808e-04, -3.8071e-03,  ..., -6.7711e-04,
         -4.5204e-04, -5.0354e-04],
        [-6.9714e-04, -2.7108e-04, -4.4394e-04,  ..., -2.0313e-04,
          2.8610e-04, -2.8658e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7035, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.1953, -3.2422, -3.6641,  ..., -4.3516, -4.6172, -0.9146],
        [-5.3281, -3.1250, -3.2812,  ..., -3.6875, -4.3438, -0.8291],
        [-3.0566, -4.2734, -4.5703,  ..., -6.3359, -4.9453, -0.4614],
        ...,
        [-2.7539, -3.0664, -3.3945,  ..., -1.8789, -7.1602, -4.1133],
        [-3.5098, -3.2285, -3.6660,  ..., -5.0586, -7.1055, -0.6353],
        [-4.0273, -4.1367, -4.9180,  ..., -5.5586, -5.4336, -4.1250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6, 27, 27, 27, 17,  4, 27, 27,  3, 27, 27,  9, 27, 13, 27, 27, 20, 14,
        26,  2,  5, 27,  3, 27,  1,  8, 17,  2, 17, 12, 27, 27,  5, 27,  2,  2,
         7, 15, 27, 11, 10, 17,  0, 27, 27, 27, 11, 27,  0, 27, 20,  7, 27, 27,
        27, 11,  7, 10, 18, 25, 25,  2, 27, 20], device='cuda:0')
step: 472
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4790, -0.0806,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.4829e-04, -1.6479e-03, -3.1090e-04,  ...,  1.7653e-03,
          6.2132e-04, -9.8705e-04],
        [-7.3910e-06, -5.1498e-04, -7.2670e-04,  ..., -1.0376e-03,
         -7.2181e-05,  6.7091e-04],
        [ 6.3705e-04,  9.0301e-05, -1.4567e-04,  ..., -9.7752e-05,
          6.7520e-04, -1.6141e-04],
        [ 1.8139e-03, -1.6534e-04, -4.7684e-04,  ...,  1.5235e-04,
          4.9353e-05, -5.0116e-04],
        [ 2.8062e-04,  8.3506e-05, -6.5279e-04,  ...,  1.4460e-04,
          4.2033e-04, -6.5517e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9948, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6602, -3.7227, -3.7539,  ..., -5.5664, -5.6133, -0.5044],
        [-4.6367, -2.7480, -2.8887,  ..., -3.1543, -5.7930, -0.9194],
        [-6.0234, -2.9473, -2.1973,  ..., -3.2910, -5.1016, -0.7280],
        ...,
        [-2.4941, -3.0098, -2.5254,  ..., -2.9785, -2.6348, -2.2441],
        [-5.0195, -3.9414, -4.3164,  ..., -8.3203, -6.2852, -0.9268],
        [-2.1113, -4.2188, -4.8594,  ..., -5.7812, -5.6406, -4.6094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  3, 27, 16, 15,  8,  4,  4, 27, 27,  3,  4,  4,  0,  4, 25,  0, 27,
        27, 26, 15,  1, 18, 18, 22, 10, 24,  4, 27, 27,  0, 15,  4,  1, 17, 17,
        20, 27, 17, 27, 10,  8, 10, 11,  0, 10, 16, 15, 11, 27, 12, 27, 11,  4,
        17,  7, 26,  4, 27,  7, 27,  3,  4, 20], device='cuda:0')
step: 473
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4790, -0.0806,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4080, -0.2979, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.8324e-04,  8.1158e-04, -2.9278e-04,  ...,  1.2684e-04,
          2.6464e-04, -2.8753e-04],
        [-5.9986e-04, -1.3533e-03, -6.1893e-04,  ..., -2.6751e-04,
          2.8682e-04,  4.7016e-04],
        [ 3.8719e-04, -5.2750e-05, -3.4189e-04,  ..., -1.0711e-04,
          1.8108e-04,  5.9128e-04],
        [ 1.0548e-03,  1.7750e-04, -2.0466e-03,  ..., -6.6233e-04,
         -4.8923e-04,  3.4642e-04],
        [ 3.7885e-04,  6.9141e-04, -6.7711e-04,  ...,  4.5824e-04,
          2.5463e-04, -9.5725e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8313, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.6699, -3.7012, -4.7500,  ..., -6.7500, -4.9375, -3.9512],
        [-5.4766, -2.9922, -2.9004,  ..., -2.7129, -4.9922, -1.4775],
        [-2.8125, -3.1094, -3.2188,  ..., -3.5625, -2.6562, -1.8906],
        ...,
        [-5.2852, -3.4863, -3.5957,  ..., -5.0195, -5.3008, -0.4087],
        [-0.6099, -4.3906, -5.8281,  ..., -6.5312, -4.6562, -3.9219],
        [-3.5996, -3.3340, -4.0977,  ..., -6.3320, -3.7246, -1.1152]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  3,  3, 27, 25, 11, 15, 27,  7,  1,  1, 17, 17, 18,  4, 27, 26, 27,
         7, 27, 27,  3, 25,  0,  7, 27, 16,  1, 12, 15, 26, 27, 27, 27,  9, 24,
         3, 15,  0, 27,  0,  1, 27,  0, 27, 15, 27,  4,  0, 27, 27, 27, 27, 20,
        27,  1,  0, 27,  3, 27, 25, 10,  0, 27], device='cuda:0')
step: 474
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4790, -0.0806,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4082, -0.2979, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.9788e-04,  5.3215e-04, -3.9983e-04,  ..., -1.1387e-03,
         -1.4305e-06, -7.8917e-05],
        [-2.4261e-03,  6.1464e-04,  1.6174e-03,  ..., -1.5497e-05,
          1.7014e-03, -1.6222e-03],
        [-1.4210e-03,  6.6328e-04,  1.2550e-03,  ..., -3.7766e-04,
         -8.3148e-05,  4.6587e-04],
        [-6.3848e-04, -5.9557e-04, -3.8600e-04,  ..., -4.5586e-04,
         -9.4831e-05, -1.0073e-04],
        [-2.2674e-04, -1.3900e-04,  4.4656e-04,  ..., -1.0771e-04,
         -1.9181e-04,  4.6039e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6890, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.2090, -4.1445, -5.0664,  ..., -6.1641, -5.6133, -0.3181],
        [-3.1699, -3.9512, -5.0742,  ..., -7.0430, -6.3867, -0.5132],
        [-0.3918, -3.8770, -6.0312,  ..., -7.8438, -5.6094, -3.7051],
        ...,
        [-3.3105, -4.0469, -6.7500,  ..., -6.4648, -6.5469, -5.8750],
        [-5.9648, -3.8066, -4.5117,  ..., -4.5898, -6.5586, -0.3074],
        [-4.6992, -3.5098, -4.7109,  ..., -6.4336, -5.6523, -1.1035]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  0, 18, 27, 27,  6, 26,  4, 24, 27,  7, 11, 27,  7, 26,  0, 27,
        27, 22,  8, 27,  9, 10,  5,  0, 17, 27, 27, 27, 20, 22, 18, 27, 25, 27,
         4, 20, 27, 27, 27, 24, 27, 17, 27,  6,  5, 27,  1,  1,  3,  9,  1,  9,
        27,  1, 27,  7,  7, 15, 26, 15,  4, 27], device='cuda:0')
step: 475
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4790, -0.0806,  1.5107],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4082, -0.2979, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.3627e-04,  4.0412e-04,  1.1986e-04,  ..., -3.6240e-04,
          1.3447e-04, -7.6818e-04],
        [ 7.3242e-04, -4.4751e-04, -8.5640e-04,  ...,  2.0199e-03,
          1.9121e-03, -2.8014e-05],
        [ 3.4738e-04,  5.9032e-04, -3.1054e-05,  ...,  5.9307e-05,
          1.0270e-04,  5.0783e-05],
        [-3.3164e-04, -3.2616e-04,  1.0853e-03,  ...,  5.7364e-04,
         -2.1899e-04,  4.5586e-04],
        [ 9.1434e-05,  4.2200e-04, -1.8120e-05,  ...,  3.5167e-04,
          5.4479e-05, -3.5667e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9618, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.4688, -3.5000, -2.8750,  ..., -1.8281, -4.9219, -2.5469],
        [-1.9336, -4.0742, -6.4492,  ..., -6.5898, -3.4023, -4.6211],
        [-3.2402, -3.0996, -3.7734,  ..., -6.0078, -5.1172, -0.8506],
        ...,
        [-1.3916, -3.6094, -5.2031,  ..., -5.2344, -4.0781, -1.8447],
        [-2.2383, -3.1445, -3.4883,  ..., -3.1914, -2.2070, -1.8320],
        [-3.3809, -4.7734, -8.2266,  ..., -6.9141, -7.1016, -6.6953]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([24, 26, 27, 27,  2, 27,  1,  1, 10, 27, 15,  0, 20, 10, 27, 11, 17, 17,
         0, 17, 27, 25, 27, 27,  9, 18,  7, 23, 27,  9,  6, 10, 27, 27, 26, 26,
        18, 10, 27, 18,  1,  0, 27, 27,  0,  9,  0, 27, 27,  4, 27, 13, 27,  6,
         2,  0, 25,  5,  3, 27, 27, 27,  9, 15], device='cuda:0')
step: 476
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4790, -0.0806,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4082, -0.2979, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.2691e-04,  3.3259e-04, -7.8869e-04,  ...,  3.0689e-03,
          1.2951e-03, -9.8038e-04],
        [-2.2233e-05,  2.0075e-04, -7.4244e-04,  ...,  1.6203e-03,
         -5.0783e-04,  1.4734e-04],
        [ 2.8343e-03, -4.1771e-04,  1.2550e-02,  ..., -7.8506e-03,
         -5.6076e-03,  3.0155e-03],
        [ 9.4032e-04, -7.8964e-04,  1.2684e-03,  ..., -2.0659e-04,
          9.2924e-05, -1.1873e-03],
        [ 7.9060e-04, -4.3249e-04, -1.2865e-03,  ...,  7.9775e-04,
          8.6641e-04, -1.9093e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7025, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2305, -3.4648, -3.6523,  ..., -4.4180, -6.1523, -0.6372],
        [-4.8867, -2.9023, -2.6211,  ..., -3.0117, -4.4180, -2.4648],
        [-1.9844, -2.9062, -4.0156,  ..., -6.2656, -4.7188, -1.2197],
        ...,
        [-2.4199, -4.3555, -5.2773,  ..., -5.6055, -4.4961, -4.1680],
        [-6.0117, -4.0430, -4.2930,  ..., -5.6055, -5.6367, -0.2786],
        [-1.5918, -5.3438, -9.5000,  ..., -8.1094, -7.3438, -7.8125]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 27,  7, 27, 18, 27, 11,  0, 27, 27, 27, 18,  4,  0, 27,  0, 27, 18,
         7, 27,  9, 22, 10, 11,  7, 17, 27,  9,  4, 27, 18,  4, 27, 26, 27,  3,
         4, 27,  5, 10, 27, 14, 27, 26, 27,  1,  2, 27, 27,  2, 27, 27, 25,  5,
        17, 27, 27, 13,  7, 18,  0, 17, 24,  0], device='cuda:0')
step: 477
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4790, -0.0807,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4082, -0.2979, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.1577e-04, -9.5701e-04, -4.3917e-04,  ...,  6.1178e-04,
          3.9887e-04, -3.7241e-04],
        [-4.7088e-06,  2.3711e-04,  4.0531e-06,  ..., -1.5287e-03,
         -1.6117e-04, -1.7285e-04],
        [ 5.3215e-04,  8.6069e-04, -5.2261e-04,  ..., -3.7289e-04,
          8.8120e-04, -9.3281e-05],
        [-1.6057e-04,  2.1625e-04, -5.1308e-04,  ...,  3.0637e-04,
          7.7581e-04,  2.7847e-04],
        [-8.5235e-05, -9.8228e-05, -2.4402e-04,  ..., -9.3937e-05,
         -1.8036e-04, -6.4433e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9977, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.6211, -3.8711, -4.4180,  ..., -6.4648, -4.3867, -1.2314],
        [-4.4648, -3.6074, -5.1523,  ..., -5.6992, -6.0312, -0.5913],
        [-0.9229, -3.3125, -5.2812,  ..., -6.2344, -3.5625, -2.1406],
        ...,
        [-0.7725, -3.5996, -5.6016,  ..., -7.0078, -5.1484, -2.7402],
        [-2.0879, -3.7754, -3.8223,  ..., -4.1836, -4.4805, -2.9316],
        [-3.1641, -3.2891, -4.6797,  ..., -6.8516, -5.5234, -0.7109]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 27,  0, 27, 27, 27, 27,  8, 27,  3,  9, 27, 27, 27,  4, 18, 26, 27,
        15, 17, 27, 15, 27,  0,  0, 10, 27, 22,  4, 27, 26, 27, 27, 27, 27,  2,
        15, 10,  3,  6,  0, 25,  9,  6, 27, 10,  8, 23,  3,  1, 27, 24,  3, 27,
         0,  3,  1,  9, 15,  3, 11,  0, 25, 15], device='cuda:0')
step: 478
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4790, -0.0807,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4082, -0.2979, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0997,  ...,  0.6567,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0717e-04,  2.4281e-03,  2.8205e-04,  ...,  2.0289e-04,
          3.4428e-04,  9.6083e-04],
        [-5.1165e-04,  5.3787e-04, -5.0259e-04,  ...,  7.0715e-04,
          6.3038e-04,  1.1864e-03],
        [ 4.0436e-04, -2.7251e-04,  3.5191e-04,  ...,  5.8937e-04,
         -7.9012e-04,  7.0047e-04],
        [ 1.5354e-03,  3.4118e-04, -5.6410e-04,  ..., -3.5286e-04,
         -9.8228e-04,  1.8907e-04],
        [-1.8907e-04,  9.7632e-05, -2.2650e-05,  ...,  7.3528e-04,
          3.6550e-04,  4.1533e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0170, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9004, -2.9316, -3.3848,  ..., -3.7285, -3.7598, -1.1816],
        [-1.8936, -4.2070, -5.3633,  ..., -7.3477, -5.5195, -0.8311],
        [-5.6758, -3.7227, -3.9414,  ..., -6.2227, -6.3789, -0.2532],
        ...,
        [-2.5781, -2.6387, -3.3594,  ..., -3.0469, -3.0137, -1.7959],
        [-4.0430, -3.0742, -3.1992,  ..., -4.9023, -4.9961, -0.9492],
        [-4.1484, -3.2891, -3.3047,  ..., -3.9746, -3.3984, -1.3037]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22,  0, 27, 17,  1,  7, 13,  2, 10,  6, 27,  6, 27, 17,  0, 27,  4, 15,
        27, 17, 27, 27,  0,  4, 26, 22, 12, 15, 27, 25, 25, 27,  3, 27,  4, 27,
         7, 14, 27, 27,  4, 22,  0, 27,  5,  6, 27, 27,  4,  0, 25,  0, 27, 27,
        27, 27,  2, 10, 14,  4, 27, 27, 22, 27], device='cuda:0')
step: 479
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4790, -0.0807,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7939,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4082, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0997,  ...,  0.6562,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.1781e-04, -6.5231e-04, -2.2373e-03,  ..., -7.1239e-04,
          6.8855e-04, -1.2217e-03],
        [-5.8365e-04,  2.5582e-04, -2.8896e-04,  ...,  1.3056e-03,
          1.0700e-03, -8.6403e-04],
        [-5.0402e-04,  3.3474e-04,  1.3771e-03,  ..., -7.7367e-05,
         -8.1396e-04,  2.8872e-04],
        [-4.8923e-04,  2.6751e-04, -1.8044e-03,  ..., -1.3323e-03,
          9.0837e-04,  6.5041e-04],
        [-2.1482e-04, -2.5129e-04, -1.6928e-04,  ...,  6.5863e-05,
          1.6654e-04, -2.6679e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8282, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.2070, -1.6934, -2.5684,  ..., -1.9746, -4.1445, -1.8965],
        [-5.1523, -4.2773, -5.0742,  ..., -0.3101, -4.8867, -2.4961],
        [-5.3242, -3.3398, -3.6211,  ..., -2.9492, -5.1367, -1.1221],
        ...,
        [-5.6680, -3.6367, -3.3867,  ..., -1.2314, -3.7773, -2.5898],
        [-5.1992, -4.9961, -5.0117,  ..., -6.8242, -6.1836, -0.1202],
        [-5.3242, -3.2910, -3.5098,  ..., -4.4648, -5.7930, -0.8545]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 25,  2, 27, 27,  4, 20, 25, 27,  7, 26, 18,  0,  0,  2, 17,  3, 27,
        26, 11,  5,  4, 27, 16, 27,  2, 13, 17,  3, 27, 11, 27,  1, 27, 27, 27,
         3, 27, 27, 15,  0, 27, 13, 20, 27, 27, 10, 27,  7,  4, 27, 20, 18, 10,
        27, 18, 27, 22, 15, 25,  2,  3, 27,  5], device='cuda:0')
step: 480
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4790, -0.0807,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4082, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0997,  ...,  0.6562,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1463e-03,  6.7997e-04, -1.8096e-04,  ..., -7.9060e-04,
          6.2037e-04, -1.9140e-03],
        [ 2.7599e-03, -2.2259e-03, -1.4377e-04,  ...,  2.0542e-03,
         -1.7509e-03,  1.7109e-03],
        [ 2.7299e-04, -9.1934e-04, -3.7551e-04,  ...,  1.1377e-03,
         -4.5109e-04, -2.5082e-04],
        [-2.2054e-04, -2.0957e-04,  3.5119e-04,  ..., -3.5191e-04,
         -5.1260e-04, -1.1082e-03],
        [-1.4126e-04, -8.2791e-05, -2.4533e-04,  ...,  1.0109e-04,
          1.5521e-04, -3.6001e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7663, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.5508, -3.6895, -4.0664,  ..., -5.0820, -4.1602, -0.3303],
        [-5.8672, -2.9922, -3.9141,  ..., -3.2578, -6.3672, -2.0547],
        [-2.6602, -3.6465, -4.3008,  ..., -5.3789, -4.0820, -0.4897],
        ...,
        [-2.7734, -2.6641, -4.1953,  ..., -5.0234, -3.6328, -1.1162],
        [-4.7773, -3.7324, -4.7617,  ..., -6.4492, -5.7148, -0.2319],
        [-5.9961, -2.7441, -3.4805,  ..., -5.3398, -3.5098, -0.7920]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 25, 27,  0,  4,  6,  4,  0, 11,  3, 24, 27,  3, 27, 27, 27,  4,  6,
        27,  4, 27, 25, 25, 27, 27, 15, 15, 18, 27, 27, 10, 27, 11, 27,  4, 18,
        27,  3,  4,  3, 27, 27, 24, 27,  1, 27, 27,  6,  7, 27, 27, 27,  7, 20,
         4, 25, 27,  1,  7,  2, 11,  3, 27, 27], device='cuda:0')
step: 481
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2377, -0.6147,  ...,  0.4790, -0.0807,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4082, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0996,  ...,  0.6562,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.2045e-03,  1.6022e-03,  8.9073e-04,  ...,  2.7256e-03,
          7.1621e-04,  1.1339e-03],
        [-1.0195e-03,  1.1406e-03,  1.4515e-03,  ...,  1.4315e-03,
         -6.5029e-05,  3.1710e-04],
        [ 3.3665e-04,  1.1492e-03, -9.5749e-04,  ...,  4.8566e-04,
          6.5422e-04, -7.5340e-04],
        [ 7.2718e-04,  4.7088e-04, -1.1196e-03,  ...,  1.2207e-03,
         -2.1782e-03,  1.4486e-03],
        [ 1.4448e-04, -4.5061e-05, -7.8201e-04,  ...,  9.9182e-04,
          4.5896e-05,  4.3392e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9527, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.1797, -4.0078, -4.1641,  ..., -5.4141, -3.3496, -2.0527],
        [-5.8477, -4.3789, -5.7227,  ..., -7.0195, -7.1758, -0.1619],
        [-3.4473, -3.8535, -3.8691,  ..., -3.8379, -3.0879, -0.6973],
        ...,
        [-6.6562, -3.9824, -4.1719,  ..., -7.0625, -4.0781, -0.7490],
        [-2.8770, -3.9082, -4.6914,  ..., -4.7695, -1.4404, -1.0029],
        [-1.8057, -3.9004, -5.0234,  ..., -6.3828, -4.1172, -4.4141]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22, 27,  0, 18, 26,  0, 11, 18, 27, 10, 27, 27,  2,  4,  9, 15,  4,  1,
         0, 17, 18,  0, 25, 27, 27, 27, 17,  9, 27,  0,  0, 18, 11,  9, 10, 27,
         1,  4,  6,  7, 17, 22, 27, 15,  0, 26,  4, 27, 27,  9, 27,  4, 26, 26,
        22,  3, 18, 27, 27, 27, 27, 27, 21,  0], device='cuda:0')
step: 482
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4790, -0.0808,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4084, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0996,  ...,  0.6562,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.7798e-04,  3.6755e-03, -4.0221e-04,  ...,  1.7948e-03,
          1.8921e-03, -4.6110e-04],
        [-3.8099e-04, -4.8280e-04, -1.3418e-03,  ...,  2.9602e-03,
          1.8530e-03,  1.5507e-03],
        [-1.4925e-04, -9.1195e-05,  5.9891e-04,  ...,  1.1921e-03,
          7.8106e-04,  1.4868e-03],
        [ 3.5629e-03,  8.6975e-04, -5.4970e-03,  ..., -4.1533e-04,
         -6.9714e-04,  1.1606e-03],
        [-2.0742e-04,  6.2752e-04, -9.7609e-04,  ...,  4.0984e-04,
          7.2956e-04,  8.5402e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.5653, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.0801, -3.9395, -6.0039,  ..., -7.2188, -0.4080, -5.0000],
        [-4.2305, -2.5742, -2.8086,  ..., -2.2617, -5.3867, -1.6846],
        [-4.0508, -4.2852, -4.5195,  ..., -4.3008, -4.5039, -0.5488],
        ...,
        [-3.6484, -2.9141, -3.7578,  ..., -5.1484, -4.1016, -0.4446],
        [-0.9526, -3.3125, -5.5625,  ..., -7.7031, -4.3281, -2.4844],
        [-5.7891, -3.3672, -3.4141,  ..., -5.8516, -6.5391, -0.3992]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26, 27, 27,  0,  4, 18,  9, 27, 27, 14, 27, 27, 11, 20, 27, 27,  4, 18,
         6, 27,  0, 14, 27, 25, 27, 18, 17,  3, 20, 20, 27, 27,  7, 27,  9, 18,
        20, 18, 27, 11, 20, 27,  2, 27,  3,  7,  2,  9, 25, 27, 26, 27, 17, 15,
        27,  0,  1, 15, 27, 27,  7, 27,  0,  2], device='cuda:0')
step: 483
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4790, -0.0808,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4084, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0995,  ...,  0.6562,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.2149e-04, -4.4107e-05, -8.4400e-04,  ...,  7.4291e-04,
          2.2531e-04, -1.1082e-03],
        [-7.8249e-04, -2.8801e-04,  4.6706e-04,  ..., -1.4219e-03,
         -1.0576e-03,  3.4451e-04],
        [ 3.3951e-04, -2.7204e-04,  9.9373e-04,  ..., -7.7009e-04,
         -5.0926e-04, -8.5354e-04],
        [ 7.2861e-04,  9.7632e-05, -7.9346e-04,  ...,  7.6866e-04,
          9.2840e-04,  1.7762e-04],
        [ 8.6248e-05, -3.4070e-04, -1.5700e-04,  ..., -1.5783e-04,
          3.4857e-04, -7.0143e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7943, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.4961, -4.8555, -5.3711,  ..., -6.9648, -7.1367, -4.9180],
        [-4.3945, -3.1445, -3.6602,  ..., -4.9258, -4.1289, -0.5972],
        [-2.9043, -3.5137, -3.3887,  ..., -5.0781, -5.1250, -0.4985],
        ...,
        [-7.1328, -5.4180, -4.8867,  ..., -4.3516, -8.1328, -5.0586],
        [-3.9688, -3.7344, -4.2656,  ..., -5.5156, -4.2031, -0.5156],
        [-2.9082, -4.1562, -6.5781,  ..., -6.8750, -5.1562, -5.8125]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 27,  5, 27, 27, 17,  3, 27, 18, 27, 18,  0, 27, 27,  6, 10,  7, 25,
        27, 15, 18, 20,  7,  1,  0,  0, 27, 27,  3,  6, 27, 15, 27,  4, 27, 27,
        15,  9, 27, 27, 27, 15, 26, 18, 27,  2,  7, 27,  1, 27, 27, 14,  0, 26,
        25, 27, 10, 27, 22, 10,  1, 11, 27, 15], device='cuda:0')
step: 484
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4790, -0.0809,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9111, -0.2217,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4084, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0995,  ...,  0.6562,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0967e-03, -2.6131e-03, -2.9659e-03,  ...,  1.4744e-03,
         -5.0497e-04, -5.1260e-04],
        [ 7.8630e-04,  2.3441e-03,  1.3590e-05,  ..., -1.1692e-03,
         -8.3160e-04, -1.4305e-03],
        [ 1.0662e-03,  1.0490e-03,  2.4452e-03,  ...,  9.1887e-04,
         -1.4162e-04,  6.9952e-04],
        [-1.6022e-03, -6.7616e-04,  1.8530e-03,  ..., -1.1247e-04,
          1.6556e-03, -1.0748e-03],
        [ 1.9908e-04, -3.4499e-04, -1.3053e-04,  ...,  4.8637e-04,
         -3.6550e-04, -8.9741e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7219, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.5605, -2.3574, -2.4512,  ..., -2.7637, -2.6992, -1.6533],
        [-2.7246, -4.7578, -5.7070,  ..., -8.5234, -6.4141, -5.3984],
        [-0.9570, -3.8477, -5.1758,  ..., -8.1250, -5.2070, -1.2383],
        ...,
        [-5.1172, -3.3652, -4.4727,  ..., -4.9883, -4.9727, -0.6309],
        [-3.7227, -4.0195, -4.9570,  ..., -6.4570, -7.1133, -0.3015],
        [-2.1504, -3.9785, -5.4023,  ..., -5.7773, -3.8535, -2.2598]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26, 18,  0,  2,  6,  3, 27,  8, 27,  2,  1,  3, 27,  0, 18, 27, 27,  9,
        25, 20,  1, 27, 15, 27, 18,  2, 11, 22, 27, 27, 27, 15, 27,  3, 27, 27,
        20, 25, 18, 22,  0, 10, 18, 11, 17, 27, 24,  6, 27,  8,  9, 10, 20, 13,
        27, 25,  7, 18, 27, 15,  7, 27, 27, 23], device='cuda:0')
step: 485
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4790, -0.0809,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2217,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4084, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0995,  ...,  0.6562,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.1594e-04,  1.6422e-03,  5.7268e-04,  ..., -5.2595e-04,
         -1.5879e-04,  5.1069e-04],
        [-2.2972e-04, -8.6880e-04,  7.8082e-06,  ...,  5.5695e-04,
         -1.1568e-03,  5.2643e-04],
        [-1.2255e-04, -1.8415e-03, -1.1295e-04,  ...,  1.7118e-04,
         -9.3079e-04,  1.8859e-04],
        [-9.6917e-05,  1.2434e-04,  6.4850e-04,  ...,  4.2796e-04,
         -1.5426e-04, -9.9277e-04],
        [-2.4390e-04,  1.3757e-04,  3.7813e-04,  ...,  2.3186e-05,
         -7.9334e-05,  2.5463e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8977, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.8379, -3.5410, -2.5098,  ..., -3.4609, -3.6504, -2.8379],
        [-1.7158, -2.8887, -2.9805,  ..., -3.6523, -4.8242, -1.2939],
        [-6.1680, -4.0586, -4.1523,  ..., -5.5273, -3.5254, -1.7764],
        ...,
        [-5.5742, -3.4316, -3.3223,  ..., -2.9941, -4.7773, -1.5723],
        [-2.0723, -3.9316, -5.0430,  ..., -4.4766, -3.1973, -1.9941],
        [-3.8594, -3.8438, -3.4062,  ..., -5.7656, -4.1875, -0.5781]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 8, 18,  7, 10, 15, 20, 27, 27,  7, 27, 20, 27, 25, 26, 22,  0, 27, 15,
         4,  0,  0,  5, 27, 15, 27, 20, 20, 27, 24, 15, 27, 18,  3, 27, 27, 20,
        25,  6, 20, 10, 15, 27, 26, 25,  7, 18, 20,  0, 27, 15, 27, 27, 22,  1,
        14, 27, 13,  7, 18, 27,  1, 25, 17, 21], device='cuda:0')
step: 486
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4790, -0.0809,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2217,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4084, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0995,  ...,  0.6562,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1292e-03,  8.8024e-04, -3.7599e-04,  ...,  2.1305e-03,
         -2.2364e-04,  6.7413e-05],
        [ 3.3593e-04, -1.1454e-03, -9.6262e-05,  ...,  4.5443e-04,
          2.4772e-04, -5.7602e-04],
        [ 2.1744e-03, -2.6512e-04,  1.2871e-02,  ..., -9.3231e-03,
         -7.6027e-03,  4.2915e-03],
        [ 3.6955e-05, -5.0592e-04,  3.1376e-04,  ..., -5.0211e-04,
          1.0157e-03,  6.6519e-04],
        [ 3.4928e-04, -2.5558e-03, -6.6376e-04,  ...,  1.4782e-04,
         -1.6761e-04, -1.4324e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9555, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -2.4609,  -3.3203,  -2.9609,  ...,  -4.8359,  -4.3672,  -0.8823],
        [ -3.8887,  -3.3887,  -3.7637,  ...,  -3.5605,  -3.3262,  -0.9980],
        [ -6.8945,  -3.7207,  -4.0977,  ...,  -5.5977,  -5.2539,  -0.3774],
        ...,
        [ -3.1738,  -4.4258,  -7.0352,  ...,  -4.6758,  -4.0820,  -3.5801],
        [ -4.8594,  -6.2344, -10.1250,  ...,  -8.2500,  -8.0938,  -6.7344],
        [ -6.0312,  -3.8301,  -4.1406,  ...,  -6.4688,  -3.9863,  -1.0479]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25, 27, 27, 22, 20,  3,  5, 27,  7,  2, 19, 27, 27,  4,  5, 27, 27,  7,
        27, 17, 27, 27, 27, 27,  9, 18,  0,  4, 27, 27,  1, 18, 13, 27, 15,  0,
         1, 25,  1,  3,  3,  7, 15, 25, 27,  3, 15, 27, 25, 27, 10, 17, 12, 27,
         4,  2, 27,  6, 27, 27, 27, 17, 15,  7], device='cuda:0')
step: 487
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4790, -0.0809,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2217,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4087, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0995,  ...,  0.6562,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0003,  0.0008, -0.0019,  ...,  0.0033, -0.0005, -0.0003],
        [-0.0005,  0.0008, -0.0006,  ..., -0.0009,  0.0014,  0.0002],
        [ 0.0023,  0.0021,  0.0087,  ..., -0.0064, -0.0024,  0.0060],
        [ 0.0028,  0.0002,  0.0014,  ..., -0.0004, -0.0002, -0.0005],
        [ 0.0002, -0.0009, -0.0021,  ...,  0.0015,  0.0002, -0.0015]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(1.7381, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.1836, -4.1992, -4.0273,  ..., -6.2461, -6.0430, -1.4014],
        [-1.4824, -3.6387, -4.6719,  ..., -6.1250, -6.2500, -1.0293],
        [-5.8516, -3.8066, -4.3672,  ..., -7.2578, -5.6484, -0.2588],
        ...,
        [-2.8027, -3.5996, -6.0977,  ..., -7.9102, -4.5820, -4.3945],
        [-7.9883, -6.3008, -6.2227,  ..., -6.0039, -6.9414, -6.5039],
        [-4.5312, -3.5781, -4.4688,  ..., -5.5000, -5.3281, -0.8130]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 27, 27,  9, 15, 27, 27, 27,  7,  1, 20, 10,  0,  9, 27, 18, 27,  3,
         2, 27,  1,  6, 19,  4, 27, 18, 27, 18, 27, 27, 27,  0, 18, 15, 27, 27,
         7, 14, 27, 26,  7, 27, 13, 27, 25, 25, 18, 27, 27, 27, 27,  0,  0, 20,
         0,  0,  8, 14, 13, 27, 13, 15,  3, 27], device='cuda:0')
step: 488
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4790, -0.0810,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2217,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4087, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0994,  ...,  0.6562,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.0480e-04, -3.9959e-04, -1.0628e-04,  ...,  3.8695e-04,
         -1.3971e-04,  2.8563e-04],
        [ 4.0174e-05, -1.2875e-03, -9.2936e-04,  ..., -2.7657e-04,
         -6.6090e-04,  6.6280e-04],
        [-5.8985e-04, -6.2227e-04,  6.6996e-04,  ...,  4.4274e-04,
         -1.1883e-03,  8.6451e-04],
        [-2.7466e-04, -5.4598e-04,  2.8467e-04,  ..., -4.9233e-05,
          2.2411e-04, -9.0837e-04],
        [-5.7340e-05, -3.8958e-04,  5.1069e-04,  ..., -2.5535e-04,
         -2.3305e-05,  1.1522e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9127, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.2402, -3.9102, -4.6758,  ..., -6.3789, -5.2383, -0.2240],
        [-5.1094, -3.2656, -3.7812,  ..., -4.8438, -2.0000, -0.8594],
        [-4.9141, -3.5527, -2.4277,  ..., -1.1777, -5.1797, -2.8027],
        ...,
        [-6.3867, -3.6035, -4.1484,  ..., -4.9180, -6.3672, -0.4629],
        [-0.2418, -4.8359, -7.3984,  ..., -7.6484, -5.9453, -3.9766],
        [-2.9746, -3.6641, -4.3672,  ..., -5.5703, -2.5059, -0.9287]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([11, 12, 25,  4,  7, 27,  1, 27, 27,  7, 27,  5,  0,  1,  4, 27, 13,  9,
        27, 27,  1,  1, 18, 24, 27, 27, 27,  3, 27, 27, 27, 27, 27, 27,  4,  3,
        20, 15, 27, 15, 15, 27,  1, 19, 25, 18,  4,  3, 15, 22,  4, 27,  6,  0,
        17, 27,  1,  0,  1,  7, 22, 27,  0,  1], device='cuda:0')
step: 489
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2378, -0.6147,  ...,  0.4788, -0.0810,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2218,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4087, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0994,  ...,  0.6562,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.0531e-04,  3.0251e-03,  5.6076e-04,  ...,  5.3263e-04,
         -2.4652e-04,  1.2064e-03],
        [ 9.1314e-04, -8.5735e-04,  1.0796e-03,  ..., -1.0128e-03,
         -9.8133e-04,  1.8716e-04],
        [ 2.4283e-04, -3.7622e-04,  3.2883e-03,  ..., -3.7823e-03,
         -1.8921e-03,  3.6945e-03],
        [ 1.4391e-03,  3.0041e-04, -5.4741e-04,  ..., -2.9087e-05,
         -5.7697e-04, -3.9053e-04],
        [ 2.6703e-04, -4.2582e-04, -3.9220e-04,  ...,  4.9353e-04,
         -1.8668e-04,  3.2568e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6915, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2109, -3.4590, -3.6465,  ..., -5.8984, -4.0703, -0.4756],
        [-3.9551, -3.9863, -4.7383,  ..., -5.9414, -5.2227, -0.3147],
        [-5.5000, -3.7812, -4.2031,  ..., -0.5615, -5.3750, -2.2031],
        ...,
        [-5.2891, -3.5254, -3.5254,  ..., -4.8203, -3.2441, -1.2744],
        [-2.3340, -4.9727, -5.2695,  ..., -6.1445, -5.3633, -0.3027],
        [-2.9688, -5.1875, -6.1250,  ..., -8.9375, -7.2500, -5.9062]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 27, 25, 27, 17,  3, 27, 15, 27, 27, 26, 20, 18,  4,  3, 15, 20, 27,
         2, 10,  4, 27,  1, 22, 27, 25, 27,  0, 27, 27,  5, 27,  3,  2,  6,  2,
         0, 25,  9, 27,  0, 22, 26,  4,  5,  9,  0, 27, 27, 18, 14, 20, 27, 27,
        27, 27,  8, 15, 27, 25, 27,  6,  4, 18], device='cuda:0')
step: 490
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4788, -0.0810,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2218,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4087, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0994,  ...,  0.6562,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1903e-04,  2.9755e-03,  1.8978e-03,  ..., -1.9627e-03,
         -5.0449e-04, -3.7551e-04],
        [ 1.2863e-04, -5.0354e-04, -4.8280e-05,  ...,  7.4053e-04,
          5.0783e-04,  5.3883e-04],
        [-1.1415e-03, -4.8137e-04, -3.7098e-03,  ...,  2.8992e-03,
          9.4986e-04, -1.5974e-03],
        [ 3.9196e-04, -1.2894e-03, -3.8509e-03,  ...,  1.2693e-03,
          3.0637e-04, -1.0548e-03],
        [ 1.5211e-04,  1.5850e-03,  1.0414e-03,  ..., -1.0109e-03,
          8.3590e-04,  3.4904e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3588, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.9390, -4.0469, -5.8281,  ..., -6.7812, -4.9531, -1.9229],
        [-3.6289, -3.0977, -3.1445,  ..., -3.4414, -3.3477, -0.5513],
        [-4.4961, -2.6074, -3.0605,  ..., -2.9512, -4.9648, -1.9033],
        ...,
        [-6.6523, -4.1680, -4.3047,  ..., -5.3359, -5.2148, -0.1504],
        [-3.1289, -4.4258, -6.9727,  ..., -7.0195, -5.6133, -5.1914],
        [-6.4336, -3.7012, -3.5293,  ..., -5.4180, -4.3242, -0.4827]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 27, 27,  5, 27, 26, 11,  0, 27, 27, 27, 23,  2,  0,  7,  4,  4, 11,
        20, 27,  3, 25, 27, 27,  2,  0, 27,  4,  7, 14, 20,  9, 10, 27, 22,  4,
        27,  1, 10, 20, 27, 23,  2,  9,  3, 27,  7,  5, 18, 14, 22, 22, 22, 15,
         6, 22, 20, 14, 27, 25,  3,  3, 15, 27], device='cuda:0')
step: 491
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4788, -0.0810,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2218,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4087, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0994,  ...,  0.6562,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.0831e-04, -4.5013e-04, -2.3246e-04,  ..., -3.7408e-04,
         -5.0879e-04,  1.9884e-04],
        [ 1.5860e-03, -1.9894e-03, -2.6584e-04,  ...,  7.2670e-04,
          7.1096e-04,  5.2023e-04],
        [-1.7452e-04, -8.4877e-05,  7.6771e-04,  ..., -1.0538e-03,
         -8.3542e-04, -3.1042e-04],
        [-6.7949e-04, -4.4227e-05,  1.3447e-03,  ..., -6.0368e-04,
          3.0041e-04,  2.8849e-04],
        [-9.1136e-05, -7.8678e-05, -1.7977e-04,  ..., -3.0577e-05,
         -4.4250e-04, -1.7512e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7248, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.8667, -3.3047, -5.4297,  ..., -6.3672, -4.0703, -1.2256],
        [-4.7969, -2.7012, -3.1387,  ..., -4.4219, -4.5781, -0.7017],
        [-1.8213, -4.0078, -5.8203,  ..., -4.2422, -3.8359, -4.0391],
        ...,
        [-3.6113, -3.0977, -3.0039,  ..., -3.0352, -4.3789, -1.2842],
        [-2.4102, -4.1602, -4.3945,  ..., -4.9258, -4.6758, -3.2090],
        [-3.0488, -3.7363, -4.6250,  ..., -4.8438, -5.2656, -0.4553]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17,  2, 15, 27, 27, 18, 27, 27, 27, 27, 27, 27, 17,  1, 27,  0, 27, 27,
        27, 20,  0, 27, 27, 27,  3, 27, 17, 20,  7, 27, 17, 27, 15, 20, 27,  7,
         0, 20, 14,  0, 15, 27, 27, 25,  7,  3, 27, 27,  5, 27,  1,  9,  1, 24,
        27, 27, 27,  1,  4, 17, 25,  0, 23, 20], device='cuda:0')
step: 492
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6147,  ...,  0.4788, -0.0810,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2218,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4087, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0994,  ...,  0.6562,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.6229e-04, -2.8839e-03, -2.6965e-04,  ...,  4.5347e-04,
          1.7738e-04,  2.5702e-04],
        [ 1.0121e-04, -4.0829e-05, -6.7425e-04,  ...,  1.3494e-03,
         -9.7942e-04,  2.1327e-04],
        [-1.4191e-03,  1.0204e-03,  1.1206e-03,  ...,  6.3705e-04,
         -3.7432e-04, -3.9959e-04],
        [ 2.1458e-04, -5.4836e-04, -3.3879e-04,  ...,  8.4496e-04,
         -2.8229e-04, -5.8365e-04],
        [-7.5579e-04, -7.4291e-04, -5.2547e-04,  ...,  1.1749e-03,
         -3.5715e-04, -7.9775e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1507, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.4072, -4.6719, -5.2812,  ..., -7.0000, -4.7500, -4.4688],
        [-2.6484, -3.9453, -3.8984,  ..., -3.9922, -3.6641, -0.5073],
        [-6.3281, -4.9844, -4.7812,  ..., -6.5781, -7.3750, -0.0771],
        ...,
        [-3.0762, -4.1055, -6.8398,  ..., -4.9492, -4.1523, -3.1074],
        [-4.2969, -2.9863, -2.2363,  ..., -2.0332, -4.7070, -1.3613],
        [-7.1172, -4.5078, -4.5078,  ..., -6.1328, -3.1172, -1.6494]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 27,  2,  7,  8,  1, 13,  3,  9, 20, 27, 27,  0, 27, 18,  0,  6,  4,
        27, 27, 18,  7, 27, 27, 27,  2, 10,  4, 15, 27,  7, 11, 25,  4, 12,  0,
         3, 10,  1,  6, 18, 23,  0, 27, 22, 26, 26, 27, 13, 15, 27, 27, 20,  5,
        27, 27, 18, 27, 14,  3,  4, 17,  2,  7], device='cuda:0')
step: 493
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6143,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2218,  0.7930,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4089, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0993,  ...,  0.6558,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0010,  0.0024, -0.0053,  ...,  0.0066,  0.0020,  0.0041],
        [ 0.0040,  0.0004,  0.0244,  ..., -0.0258, -0.0144,  0.0073],
        [ 0.0053, -0.0039, -0.0162,  ...,  0.0082, -0.0025, -0.0177],
        [ 0.0027,  0.0001,  0.0045,  ...,  0.0003,  0.0006,  0.0030],
        [ 0.0029, -0.0013, -0.0075,  ...,  0.0017, -0.0008, -0.0040]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(1.8222, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.6406, -4.0312, -3.6719,  ..., -4.5000, -3.2969, -0.5625],
        [-1.6387, -2.7949, -3.8262,  ..., -2.8418, -2.8730, -1.6230],
        [-5.5000, -3.1270, -3.2832,  ..., -4.5938, -5.2656, -0.7671],
        ...,
        [-4.9609, -3.8203, -3.7402,  ..., -4.5078, -5.0391, -0.3818],
        [-0.2600, -3.7754, -6.4023,  ..., -8.6172, -5.4492, -3.7129],
        [-3.3223, -3.3223, -4.2734,  ..., -4.3828, -3.2754, -0.6187]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 20, 20, 25,  0,  4, 23, 27, 18, 27, 27, 26,  4, 27, 17, 27, 27, 15,
        17, 15, 27,  0, 26, 27,  7,  6, 27, 27,  9, 27, 25, 27,  5,  4, 11, 27,
         1,  0, 27, 11,  0,  3, 25,  6, 27, 20, 25, 15,  1,  4, 27, 25, 14,  3,
        27, 27,  3, 15,  7, 15, 25, 27,  0,  1], device='cuda:0')
step: 494
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6143,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2218,  0.7930,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4089, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.1292e-03,  1.4286e-03,  3.9124e-04,  ..., -2.1887e-04,
          3.9434e-04,  1.9741e-03],
        [-1.2189e-04, -1.2684e-03, -2.7537e-04,  ..., -3.6001e-04,
         -3.7813e-04,  9.5797e-04],
        [-5.6696e-04, -1.0414e-03, -1.3626e-02,  ...,  7.7667e-03,
          2.2240e-03, -6.3629e-03],
        [-3.8838e-04,  1.7667e-04, -5.7364e-04,  ..., -5.2881e-04,
         -3.4761e-04, -1.2932e-03],
        [-4.7708e-04,  2.2259e-03,  3.7885e-04,  ..., -8.7357e-04,
         -3.9518e-05,  1.7099e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.5258, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.7852, -3.0508, -3.0195,  ..., -2.4570, -4.9258, -1.5039],
        [-5.1055, -3.1523, -3.6211,  ..., -4.7773, -3.5117, -0.4331],
        [-4.9258, -3.3789, -4.3945,  ..., -5.5039, -5.4258, -0.3789],
        ...,
        [-2.6875, -4.6094, -4.6250,  ..., -5.4219, -1.4697, -2.3613],
        [-3.6602, -4.7383, -5.4570,  ..., -8.4922, -7.1289, -6.5352],
        [-4.3008, -3.4902, -3.9746,  ..., -5.3789, -4.5391, -0.5991]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2,  4, 27,  0, 27, 18, 27, 27,  7,  3,  1,  9, 27, 15,  7, 15, 25, 15,
         7, 27,  1,  5,  7, 20, 27, 27, 27, 20, 25, 27,  0, 12,  0, 13,  4,  0,
        18, 27, 15, 27, 27, 15, 27,  0,  8, 27,  7,  4,  7, 25, 15, 10, 27, 27,
        27, 27, 15, 27, 25, 27, 18, 13, 18, 27], device='cuda:0')
step: 495
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6143,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2218,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4089, -0.2976, -1.7207,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.3903e-04,  1.8301e-03,  2.1286e-03,  ...,  2.8591e-03,
          8.6010e-05,  9.5892e-04],
        [-3.2711e-04, -4.7445e-04,  2.4681e-03,  ...,  3.3021e-05,
         -8.3971e-04,  1.1272e-03],
        [-9.0885e-04,  1.7557e-03,  3.7479e-03,  ...,  9.4986e-04,
          7.4768e-04,  4.3716e-03],
        [-1.1835e-03, -4.3750e-04, -9.2924e-05,  ...,  3.3169e-03,
         -1.6556e-03,  1.8044e-03],
        [-4.8423e-04, -4.0293e-04, -5.8842e-04,  ..., -3.1233e-05,
         -1.0979e-04, -3.5143e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8062, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.1211, -3.7305, -3.3555,  ..., -3.0117, -3.7617, -0.8711],
        [-4.4883, -2.4863, -2.5801,  ..., -3.5801, -5.7812, -2.2207],
        [-5.8398, -4.7422, -4.6836,  ..., -5.6172, -5.8555, -0.2754],
        ...,
        [-1.7285, -3.4492, -5.5898,  ..., -7.8555, -6.1836, -3.5879],
        [-3.9746, -2.9121, -3.1309,  ..., -4.3789, -3.5215, -0.5830],
        [-2.8652, -4.5977, -7.6758,  ..., -6.4727, -5.8945, -5.4414]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 11, 17,  9, 27, 27,  2,  7, 27, 10,  0, 17,  3, 17,  6, 27, 25,  4,
        27,  6, 22, 18, 18, 25, 27, 27,  3, 13, 27,  7, 27, 17, 11, 15, 27, 27,
        18, 24,  1,  0, 27,  7, 27, 27,  0,  3, 15, 27, 11,  1, 17, 18, 25, 27,
         0, 10,  3, 27, 15, 27, 15, 18, 22, 15], device='cuda:0')
step: 496
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6143,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2043,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2218,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4089, -0.2976, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.9032e-04,  1.4043e-04, -9.4604e-04,  ...,  1.6041e-03,
          2.0027e-05, -7.8249e-04],
        [ 1.6880e-03,  3.2310e-03,  9.7580e-03,  ..., -1.2455e-03,
         -8.2016e-04,  6.2227e-04],
        [ 7.7248e-04,  4.9973e-04,  2.5253e-03,  ..., -3.6263e-04,
         -9.4795e-04,  8.1539e-04],
        [ 1.1902e-03, -6.0654e-04,  8.4782e-04,  ..., -8.2374e-05,
         -7.4911e-04,  6.6566e-04],
        [ 8.9979e-04, -1.5259e-03, -5.1785e-04,  ...,  2.9659e-04,
         -1.1325e-06, -3.9005e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1212, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9062, -3.7656, -3.2969,  ..., -3.0625, -5.1250, -0.5474],
        [-5.8516, -4.0586, -4.0898,  ..., -5.8867, -3.4941, -2.1191],
        [-3.3105, -2.8887, -3.5293,  ..., -3.5762, -2.8730, -0.9829],
        ...,
        [-3.2246, -3.4434, -4.1133,  ..., -4.2227, -4.9453, -0.3655],
        [-3.0098, -2.9941, -2.4160,  ..., -3.1973, -4.1680, -0.7290],
        [-1.6826, -3.7441, -4.9023,  ..., -7.3242, -4.7305, -4.5898]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 27, 27, 27, 14,  4,  3, 22, 23, 27, 15, 17, 27, 18,  4, 27,  5,  1,
        27,  0, 10, 27,  0, 27, 22,  0, 27, 10,  7, 22,  5, 27,  7, 27, 15,  4,
        27, 26, 20, 27, 11,  0, 15, 22,  4, 27,  0, 10, 27,  5,  8, 10, 26, 26,
         0,  3,  0, 27,  1, 27,  7, 27,  3, 18], device='cuda:0')
step: 497
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6143,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2218,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4089, -0.2976, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0991,  ...,  0.6558,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.3862e-04, -1.7862e-03, -2.5005e-03,  ...,  2.2125e-03,
          1.0872e-03, -2.9297e-03],
        [ 1.2994e-04,  2.7323e-04,  2.4259e-05,  ...,  3.6297e-03,
          1.9131e-03, -1.6899e-03],
        [ 1.6093e-05,  3.2310e-03,  1.3809e-02,  ..., -8.6288e-03,
         -3.4695e-03,  1.2169e-02],
        [ 1.5621e-03, -9.9468e-04, -7.5936e-05,  ..., -3.3641e-04,
         -4.7398e-04,  4.7350e-04],
        [ 7.3195e-04, -1.8702e-03, -5.2738e-04,  ...,  4.6134e-04,
          7.2861e-04, -1.3285e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2068, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.7578, -4.2578, -4.8828,  ..., -5.3203, -2.3203, -0.9448],
        [-3.6680, -3.6055, -4.7617,  ..., -4.8711, -4.9961, -1.1064],
        [-5.2422, -3.7422, -2.7266,  ..., -1.1787, -4.6328, -2.8828],
        ...,
        [-5.0078, -2.6348, -2.9785,  ..., -2.3516, -4.6328, -1.9619],
        [-4.9414, -2.8926, -3.9551,  ..., -3.2051, -4.0781, -0.5181],
        [-3.7539, -3.8945, -4.5508,  ..., -4.3633, -4.4414, -0.6445]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  5, 24, 17, 24,  0, 10,  6, 18, 27,  3, 27, 18, 15, 22,  2,  1, 18,
        14, 27, 27, 20, 10, 27, 27,  5,  1, 27, 27, 11, 27, 10, 11, 19,  1, 13,
         0, 27, 27,  1, 20,  3, 27, 27, 10, 27,  0, 27, 27,  4, 27, 20, 24, 27,
        18, 27, 27,  4,  9, 26,  3,  2, 27, 27], device='cuda:0')
step: 498
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6143,  ...,  0.4788, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2218,  0.7930,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4089, -0.2976, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0991,  ...,  0.6558,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.7212e-03, -3.4475e-04,  6.5002e-03,  ..., -3.6640e-03,
          1.0757e-03,  4.2953e-03],
        [-9.5844e-04, -7.4148e-04,  8.1940e-03,  ...,  3.7746e-03,
         -2.3766e-03,  5.6801e-03],
        [-8.2855e-03, -5.8632e-03, -4.6478e-02,  ...,  3.0258e-02,
          4.1656e-03, -2.9510e-02],
        [-6.1264e-03,  5.5218e-04, -8.8120e-03,  ..., -2.5768e-03,
         -5.2357e-04, -9.6703e-04],
        [-1.5831e-03,  3.6316e-03,  3.8338e-03,  ..., -2.9583e-03,
          4.1425e-05,  4.9934e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9709, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.9297, -3.6172, -4.1797,  ..., -6.6172, -4.7422, -0.4929],
        [-5.1953, -3.2109, -3.3203,  ..., -4.0859, -6.1172, -0.7578],
        [-5.2148, -3.6816, -3.9004,  ..., -5.9648, -6.8242, -0.3696],
        ...,
        [-1.9795, -3.1035, -5.3711,  ..., -6.5430, -3.8223, -3.8379],
        [-4.3828, -4.4766, -5.6445,  ..., -0.1780, -3.2090, -4.4414],
        [-2.4570, -3.4102, -5.6133,  ..., -5.2695, -5.4570, -2.5684]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 16, 27, 27,  0,  2, 27, 20,  6, 27, 19, 27, 27,  7,  7, 18, 15, 27,
        27, 27, 20,  9, 24, 17, 27, 27,  7,  0,  0,  5, 15, 17,  0,  6, 27,  7,
         9, 27, 12, 27, 10,  1, 27, 25, 27,  4, 27,  0, 22,  1,  6, 25, 27, 26,
        18, 27, 27,  3, 27, 13,  5, 15, 25, 27], device='cuda:0')
step: 499
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6143,  ...,  0.4785, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2045,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2218,  0.7930,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4089, -0.2974, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2305, -0.0991,  ...,  0.6558,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.9855e-03,  1.1845e-03, -2.6608e-04,  ..., -4.6301e-04,
         -1.5268e-03,  1.2131e-03],
        [ 1.4248e-03, -7.7057e-04,  9.5825e-03,  ...,  1.1406e-03,
         -5.3062e-03,  7.5798e-03],
        [ 1.9550e-03, -5.2261e-03, -2.1561e-02,  ...,  3.9024e-03,
         -2.8877e-03, -6.2790e-03],
        [-2.3556e-03,  2.0504e-03, -4.2152e-03,  ..., -4.3411e-03,
          1.9193e-05, -1.4915e-03],
        [-3.8109e-03,  3.1910e-03, -1.4963e-03,  ...,  5.7650e-04,
         -4.9057e-03,  1.6050e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7874, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9512, -3.6230, -4.4531,  ..., -4.5000, -5.5781, -0.4985],
        [-3.9824, -3.2168, -3.7480,  ..., -4.4375, -3.7480, -0.4197],
        [-3.9414, -3.4727, -2.9727,  ..., -1.2861, -4.9570, -2.1602],
        ...,
        [-2.9023, -2.6055, -2.9805,  ..., -3.8242, -2.9648, -1.6367],
        [-2.5605, -3.1387, -3.5137,  ..., -3.5293, -2.9512, -2.5605],
        [-2.3906, -4.5781, -5.9688,  ..., -4.4531, -2.9844, -3.2812]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27,  0, 13, 27, 25, 27, 27, 27, 27,  9,  7, 27, 27,  1, 27, 27,
        14, 17, 10, 22, 10, 27, 27, 26,  1, 27, 10,  4, 17, 18, 27, 22, 17, 10,
        25,  0, 17, 27, 17, 27, 27, 26,  0,  8,  9, 25, 10, 27,  6, 27, 20, 27,
        25, 10, 27, 27, 15,  1, 27, 27, 20, 17], device='cuda:0')
step: 500
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6143,  ...,  0.4785, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2046,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2218,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4089, -0.2974, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1719,  2.2305, -0.0990,  ...,  0.6558,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0009, -0.0041, -0.0090,  ...,  0.0052,  0.0008, -0.0050],
        [-0.0002,  0.0046, -0.0005,  ..., -0.0018,  0.0006, -0.0002],
        [ 0.0136, -0.0106,  0.0371,  ..., -0.0012, -0.0103,  0.0116],
        [ 0.0033,  0.0028,  0.0027,  ..., -0.0121,  0.0003, -0.0012],
        [ 0.0025, -0.0032, -0.0025,  ..., -0.0004,  0.0007, -0.0052]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(2.4242, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.5898, -2.8867, -3.4199,  ..., -2.9805, -4.6055, -1.2002],
        [-2.3984, -2.2109, -2.8516,  ..., -2.4297, -2.1172, -3.1797],
        [-3.1523, -3.1992, -3.2930,  ..., -3.1680, -2.9336, -1.4170],
        ...,
        [-3.5586, -3.3242, -3.6992,  ..., -3.6211, -4.3242, -0.5591],
        [-5.9688, -4.0312, -4.8438,  ..., -5.7500, -3.1406, -3.2188],
        [-4.0391, -2.9141, -4.2734,  ..., -5.3359, -1.9297, -1.0068]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10,  8, 17, 22,  3, 27, 25,  4, 27, 11, 13, 27, 11, 11, 15, 21, 20,  5,
        19, 15,  4, 10,  3, 27,  1,  4, 17, 27, 27, 27, 22,  4,  2, 26, 11,  3,
        15,  9, 27,  2,  4, 18, 27, 26, 10, 27,  0, 22,  7, 15, 27,  4, 27, 27,
         0, 11, 27, 27, 27,  7,  8, 27, 27,  2], device='cuda:0')
step: 501
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6143,  ...,  0.4785, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2046,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2217,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4089, -0.2974, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1719,  2.2305, -0.0989,  ...,  0.6558,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.7479e-04, -1.1129e-03,  5.5552e-04,  ..., -1.7500e-03,
         -2.3699e-04,  1.0662e-03],
        [ 8.1062e-04, -2.2736e-03, -2.1114e-03,  ..., -5.6887e-04,
          1.2136e-04,  2.6093e-03],
        [ 8.4877e-04, -1.5135e-03, -5.6038e-03,  ...,  4.9400e-03,
          1.9512e-03, -3.5782e-03],
        [-6.2406e-05,  1.4663e-04, -3.9196e-04,  ..., -9.0790e-04,
         -3.4857e-04,  2.6131e-04],
        [ 2.4116e-04,  1.6680e-03,  1.7042e-03,  ..., -3.0556e-03,
          3.0088e-04,  4.7455e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0426, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.2900, -3.0410, -4.0391,  ..., -5.2891, -3.6641, -1.2275],
        [-6.1094, -2.2793, -2.1719,  ..., -0.9834, -3.9355, -3.2793],
        [-3.7168, -2.3418, -2.0137,  ..., -2.5137, -4.3750, -2.1074],
        ...,
        [-1.9082, -3.9238, -4.4414,  ..., -4.3320, -2.4082, -0.9399],
        [-2.6035, -4.3203, -5.2422,  ..., -6.6797, -5.9141, -4.6172],
        [-1.6250, -3.0469, -4.5156,  ..., -5.3281, -4.7188, -1.2812]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 25, 12,  3, 27,  0,  1, 14, 27, 15,  7, 27,  1,  0, 27,  7, 10,  4,
         1, 27, 27, 15, 25,  0, 10, 27, 27,  4, 27, 27,  1, 26,  1, 11, 26,  5,
        27,  7,  6, 27, 27, 15, 27,  0, 11, 27,  4,  1, 12, 27,  1, 27,  2,  4,
         6,  0, 24, 18, 27, 10, 17, 27, 18,  0], device='cuda:0')
step: 502
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6143,  ...,  0.4785, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2047,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2217,  0.7935,  ..., -1.9043, -1.3477, -0.6035],
        [-0.4089, -0.2974, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1719,  2.2305, -0.0989,  ...,  0.6558,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.5287e-03, -1.5278e-03, -3.7885e-04,  ..., -2.0828e-03,
         -2.3327e-03,  3.0022e-03],
        [ 3.7646e-04,  1.6165e-03, -4.3907e-03,  ..., -3.9215e-03,
          1.0395e-03, -1.5173e-03],
        [-3.6836e-04, -3.8071e-03, -3.7708e-03,  ...,  1.4668e-03,
         -4.2295e-04, -1.1635e-02],
        [-1.5965e-03,  4.5013e-04,  1.9417e-03,  ...,  1.4439e-03,
         -6.2585e-06,  1.5306e-03],
        [-1.1473e-03,  3.0041e-04, -1.4181e-03,  ...,  1.6708e-03,
         -2.2297e-03,  2.8729e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0262, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.3652, -3.4590, -3.8027,  ..., -1.5996, -3.8809, -3.5840],
        [-4.0938, -3.7480, -3.4980,  ..., -4.3281, -5.5781, -0.7017],
        [-5.7148, -3.3574, -2.7324,  ..., -1.3252, -4.8398, -1.6064],
        ...,
        [-3.8086, -2.8086, -2.3086,  ..., -3.6367, -6.4648, -2.8867],
        [-5.3203, -3.1152, -3.3047,  ..., -2.0234, -3.2090, -1.5225],
        [-4.0391, -2.9141, -4.2734,  ..., -3.3359, -2.5391, -0.8521]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20,  2, 10, 27,  6,  2, 17, 25,  6,  6, 27, 15,  0, 18, 18, 26,  9, 15,
         4,  0, 27,  8, 27, 27, 27, 27,  0, 10,  4, 20, 27, 27, 15,  0, 27,  8,
        25, 22,  7,  0, 27, 27, 24, 15, 27,  4,  0, 27, 27,  2, 27, 27, 27,  8,
        27, 15, 12, 25, 27,  3, 11, 27, 25,  4], device='cuda:0')
step: 503
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6143,  ...,  0.4785, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2047,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2217,  0.7935,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2974, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0989,  ...,  0.6558,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.6907e-04, -2.3537e-03, -8.5831e-04,  ...,  2.1410e-04,
         -3.3951e-04, -2.6054e-03],
        [-1.9312e-05,  1.1997e-03, -2.1057e-03,  ..., -6.0844e-04,
          2.4719e-03, -4.6778e-04],
        [-9.0075e-04,  1.2693e-03,  1.1375e-02,  ..., -7.4463e-03,
          1.0662e-03,  7.7782e-03],
        [ 9.3365e-04, -5.5885e-04,  2.6855e-03,  ...,  2.4166e-03,
         -1.3673e-04,  2.3293e-04],
        [ 7.1812e-04, -4.9257e-04, -8.0919e-04,  ...,  7.3338e-04,
          3.4857e-04, -1.8244e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6146, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.5288, -3.9961, -5.9961,  ..., -4.7461, -2.8730, -4.1680],
        [-5.0508, -4.0977, -3.7363,  ..., -4.4258, -3.8301, -0.7061],
        [-3.7773, -2.4336, -3.6680,  ..., -4.6211, -5.0117, -1.2773],
        ...,
        [-2.6406, -2.5312, -3.3750,  ..., -5.6562, -3.9531, -1.1094],
        [-2.7520, -2.6895, -3.4551,  ..., -5.3164, -2.4863, -1.1582],
        [-3.7266, -4.5859, -7.9141,  ..., -5.8359, -6.8984, -5.8516]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 27,  4, 26, 27, 17, 27, 10,  3, 18, 27,  7,  4, 27, 27, 15, 22,  1,
        10, 10, 27,  7, 20,  0, 27, 27,  3, 27, 13,  4, 27,  2,  0, 27, 20, 27,
         7,  4,  4, 20, 27, 10,  3, 25,  1, 25, 15, 10, 14, 27,  6, 27,  7,  3,
         3, 27, 27, 27, 27, 27, 25, 27, 22, 15], device='cuda:0')
step: 504
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6143,  ...,  0.4785, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2047,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2217,  0.7935,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2974, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0989,  ...,  0.6558,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.6171e-04,  8.2636e-04,  1.5192e-03,  ..., -6.9809e-04,
         -5.4693e-04, -3.9518e-05],
        [ 6.4659e-04,  1.1148e-03, -1.1549e-03,  ...,  1.0166e-03,
          6.3229e-04, -3.9172e-04],
        [ 2.4033e-04,  8.5068e-04, -1.3981e-03,  ...,  2.1667e-03,
          2.6836e-03,  1.0271e-03],
        [-4.6670e-05,  1.2875e-04, -3.9196e-04,  ..., -2.9707e-04,
         -2.8491e-04, -1.8442e-04],
        [ 9.7275e-05,  5.5981e-04,  1.0920e-03,  ..., -6.8188e-04,
         -2.0123e-04,  1.5650e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7822, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.7539, -3.2539, -3.4258,  ..., -3.3945, -3.2852, -0.6597],
        [-4.9141, -2.3027, -2.8809,  ..., -3.1621, -2.7559, -0.9751],
        [-4.1641, -2.8535, -3.3535,  ..., -1.9160, -3.2441, -1.6973],
        ...,
        [-3.1797, -3.4277, -4.5859,  ..., -5.5234, -2.8184, -0.5850],
        [-2.9180, -4.6523, -4.9805,  ..., -8.0312, -4.5117, -4.6680],
        [-0.7490, -4.5469, -5.4688,  ..., -6.2969, -3.3281, -1.5303]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  9, 27,  1, 27, 27,  0, 18,  7,  6,  0, 27, 27, 27, 27, 15, 20, 27,
         0, 27,  5, 27, 15, 27,  7, 27,  2,  6, 17,  9, 27,  5,  3, 15,  2, 25,
        27,  3,  7,  8, 27, 10, 27, 22,  7,  1, 17, 27, 27, 25,  6, 15, 27, 15,
        27, 27, 27, 15,  3, 27,  9, 27, 27,  0], device='cuda:0')
step: 505
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6143,  ...,  0.4785, -0.0811,  1.5117],
        [ 1.2510, -1.8926, -0.2047,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2216,  0.7935,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2974, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0989,  ...,  0.6558,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.6241e-03, -2.1496e-03, -2.1572e-03,  ...,  6.5184e-04,
          2.9683e-05,  1.0042e-03],
        [-4.9305e-04,  1.1883e-03, -1.6975e-03,  ..., -4.6005e-03,
         -6.5041e-04, -5.3346e-05],
        [-2.2864e-04,  1.8196e-03,  1.4153e-02,  ..., -7.6523e-03,
         -6.8474e-03,  3.5172e-03],
        [-2.2488e-03, -1.3947e-04,  3.3493e-03,  ..., -1.5917e-03,
          7.1287e-04,  1.9302e-03],
        [-7.5817e-05, -1.7910e-03, -1.2064e-03,  ...,  6.0749e-04,
          4.0889e-04, -4.2496e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8910, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4727, -2.4414, -2.5977,  ..., -2.9121, -5.0820, -1.3477],
        [-4.0117, -4.6836, -4.8711,  ..., -6.5742, -5.5742, -0.2456],
        [-0.8618, -4.0664, -5.1289,  ..., -6.1445, -5.0352, -1.3779],
        ...,
        [-4.7031, -4.3281, -5.0000,  ..., -4.8594, -5.1875, -0.2345],
        [-5.2461, -3.6523, -3.6211,  ..., -5.2773, -6.3711, -0.5581],
        [-6.8125, -4.2969, -4.6875,  ..., -6.7188, -5.4844, -0.1868]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2,  7, 21, 13, 18, 27,  2,  3, 20, 10, 18, 22,  5, 27, 27,  7, 27, 27,
         8,  4,  7, 27, 27, 27, 27,  9, 10,  4,  3, 20, 15, 27, 20, 18, 27, 26,
         4,  1,  8,  4, 27, 27, 21, 24, 18, 27, 27, 25,  5, 27, 27,  0, 20,  0,
         1, 18, 27, 15,  6, 27, 27, 27,  5, 27], device='cuda:0')
step: 506
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6143,  ...,  0.4785, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2216,  0.7935,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2974, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0988,  ...,  0.6558,  0.7070, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.4868e-03,  5.5809e-03, -4.7646e-03,  ...,  9.9754e-04,
         -1.7703e-05, -2.7065e-03],
        [-8.5783e-04,  4.2801e-03,  6.3858e-03,  ..., -1.6678e-02,
         -1.4236e-02,  5.1079e-03],
        [ 1.6594e-04, -6.4468e-04,  1.6289e-03,  ..., -1.2789e-03,
          4.3774e-04,  6.1655e-04],
        [ 7.4577e-04, -4.3535e-04, -3.4847e-03,  ..., -1.7786e-03,
         -5.1212e-04, -1.0462e-03],
        [ 7.0953e-04, -1.4133e-03, -2.5349e-03,  ...,  2.6965e-04,
          3.7193e-04, -2.9869e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7836, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.1172, -2.8828, -3.4922,  ..., -2.9453, -4.3516, -1.2109],
        [-1.2090, -3.5371, -5.3516,  ..., -6.0195, -5.5078, -1.0215],
        [-4.1328, -3.3203, -4.2109,  ..., -5.3672, -4.6484, -0.3984],
        ...,
        [-4.0586, -4.0430, -4.7617,  ..., -5.7461, -3.2168, -0.6226],
        [-3.1406, -4.1719, -4.6719,  ..., -5.6562, -4.6094, -0.4219],
        [-3.1270, -4.4688, -5.6406,  ..., -7.4062, -6.2969, -5.3906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 9,  0, 27,  0, 27, 18,  0,  7,  8, 27,  0, 25,  7, 27,  7, 22, 27,  7,
        27,  0, 20, 10, 27, 17, 17,  4, 10, 27, 27, 15,  4, 15,  1, 27,  4, 27,
         1, 14,  0,  0, 27, 27, 15, 18,  5,  1, 25, 27, 27, 25, 27, 14,  2,  3,
        18, 27, 18, 18,  4, 27,  1, 27, 22, 18], device='cuda:0')
step: 507
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2379, -0.6143,  ...,  0.4785, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6045, -0.8247],
        [ 1.9102, -0.2216,  0.7935,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2974, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0988,  ...,  0.6558,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0015, -0.0020,  0.0059,  ..., -0.0080, -0.0015,  0.0038],
        [ 0.0029, -0.0065, -0.0021,  ..., -0.0031, -0.0018,  0.0055],
        [-0.0040, -0.0023, -0.0113,  ...,  0.0074,  0.0092,  0.0049],
        [-0.0006, -0.0001,  0.0037,  ...,  0.0047,  0.0024,  0.0023],
        [-0.0008,  0.0015,  0.0022,  ..., -0.0035, -0.0017,  0.0040]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(1.7441, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.7754, -3.7129, -5.5273,  ..., -6.4336, -3.5098, -2.4785],
        [-2.8242, -3.4492, -4.6680,  ..., -6.6055, -4.9336, -2.6211],
        [-4.4375, -3.5000, -4.7344,  ..., -4.9219, -6.1250, -0.4072],
        ...,
        [-5.2148, -4.3867, -4.6992,  ..., -6.8555, -5.9805, -0.2319],
        [-6.2930, -5.3086, -6.4336,  ..., -7.6836, -4.3398, -2.5273],
        [-6.3984, -3.6953, -3.9297,  ..., -4.3672, -5.1328, -0.6484]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26, 27,  4, 27,  7,  7, 15, 14,  3, 27, 15, 19, 27,  3, 24,  2,  5, 27,
        27,  3, 27,  4, 27, 27, 27, 25, 13, 17, 27,  0,  4, 20,  7, 20, 25,  1,
        27, 19, 27, 27, 15,  9, 13, 27, 27, 18, 27,  3, 15, 27, 27,  3, 22, 18,
        27, 27, 18, 17,  7, 18,  1, 27, 27, 22], device='cuda:0')
step: 508
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4785, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6045, -0.8252],
        [ 1.9102, -0.2216,  0.7935,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2974, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0988,  ...,  0.6558,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.8551e-05,  7.4005e-04,  3.9005e-03,  ..., -3.2711e-03,
         -1.9646e-03, -3.6812e-03],
        [-3.3455e-03,  5.2872e-03, -1.3443e-02,  ...,  3.2349e-03,
          7.5951e-03, -4.1485e-04],
        [-2.5368e-04, -6.8235e-04, -1.0246e-02,  ..., -6.1095e-05,
          2.8458e-03,  6.4087e-04],
        [-2.8687e-03,  1.2112e-03, -3.5286e-03,  ...,  3.0403e-03,
          3.9387e-04,  1.9991e-04],
        [-3.7766e-03,  3.3875e-03, -1.6937e-03,  ...,  1.9178e-03,
         -2.9240e-03,  2.1946e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9846, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.1758, -2.5352, -3.3477,  ..., -2.7227, -3.7852, -1.7549],
        [-5.3438, -4.8906, -5.1406,  ..., -7.3594, -4.5938, -0.3289],
        [-4.6602, -2.9902, -3.3965,  ..., -2.8027, -3.7246, -1.1611],
        ...,
        [-5.2227, -3.8477, -3.6445,  ..., -5.8320, -3.8164, -1.2070],
        [-2.1289, -2.7363, -3.5195,  ..., -2.3789, -1.7529, -2.2695],
        [-1.0781, -3.3125, -5.7500,  ..., -7.4844, -3.4219, -2.8281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3,  7,  1, 15,  3, 27, 27,  3, 27, 11, 17,  3, 25,  4, 27,  1, 27, 26,
        20, 27, 27,  7, 15, 27,  3, 27,  0, 13, 27,  8, 12,  6,  0, 25,  7, 27,
         4,  1, 20, 10,  3, 27,  6, 25,  7,  7, 16, 15,  8,  1, 10, 15, 22, 26,
        15, 27, 25, 27, 27, 27, 17,  6, 22, 17], device='cuda:0')
step: 509
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4785, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6045, -0.8252],
        [ 1.9102, -0.2214,  0.7935,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2974, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0988,  ...,  0.6558,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0031,  0.0033,  0.0014,  ..., -0.0010, -0.0004,  0.0015],
        [-0.0013, -0.0009, -0.0026,  ...,  0.0015,  0.0016,  0.0040],
        [ 0.0020, -0.0034, -0.0251,  ...,  0.0050, -0.0013, -0.0121],
        [-0.0016,  0.0018, -0.0049,  ..., -0.0028,  0.0023, -0.0002],
        [-0.0017,  0.0027,  0.0016,  ...,  0.0010, -0.0022,  0.0019]],
       device='cuda:0', dtype=torch.float16)
Progress 75.11%, loss: 2.619109242102679, time 151.32s
loss: tensor(1.7504, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.3718, -3.2637, -5.7617,  ..., -7.4492, -5.4180, -3.7148],
        [-2.2344, -3.5312, -4.7656,  ..., -6.7656, -4.4219, -0.9688],
        [-2.3789, -3.2852, -4.0352,  ..., -5.9883, -3.9727, -0.8315],
        ...,
        [-5.4258, -4.0508, -4.3477,  ..., -4.3320, -6.4258, -0.4583],
        [-4.1055, -3.8398, -5.1367,  ..., -5.3555, -4.1992, -0.7461],
        [-2.0195, -3.9883, -5.3477,  ..., -7.2070, -4.5664, -0.6133]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  4,  0, 18, 27, 27,  3,  9,  3, 27, 27, 27, 22,  7,  4, 27, 22, 27,
        27, 27, 27, 15, 17,  7, 27, 27, 26,  7,  5,  7,  3, 27, 11, 27, 10, 27,
        15,  7, 20,  4, 27, 27,  1,  9,  7,  1, 22,  0, 20, 27,  1,  2, 10, 27,
        20, 22, 17, 27,  2,  1,  6, 10, 27, 27], device='cuda:0')
step: 510
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4785, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6045, -0.8252],
        [ 1.9102, -0.2214,  0.7935,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2974, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0988,  ...,  0.6558,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.7760e-03,  5.6839e-03,  1.3123e-02,  ..., -1.2398e-02,
         -2.7847e-04,  4.5471e-03],
        [ 1.0862e-03, -3.3512e-03,  8.7509e-03,  ...,  1.0094e-02,
         -4.2839e-03,  7.8812e-03],
        [-1.3077e-02, -7.3624e-03, -6.9702e-02,  ...,  4.8859e-02,
          1.8066e-02, -2.2766e-02],
        [-3.4008e-03,  4.0245e-04, -1.2390e-02,  ...,  7.3767e-04,
         -1.9491e-05, -7.7019e-03],
        [-1.8234e-03,  6.6032e-03,  1.6251e-02,  ..., -3.6850e-03,
          7.1192e-04,  1.6861e-02]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8502, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.5957, -3.2520, -3.8301,  ..., -2.4395, -3.8750, -1.0322],
        [-1.0459, -3.3105, -4.8281,  ..., -5.3281, -2.3887, -1.9053],
        [-4.7109, -4.2422, -4.8203,  ..., -6.5703, -4.7422, -0.1659],
        ...,
        [-3.4980, -4.2188, -3.3418,  ..., -0.5771, -4.5781, -3.0918],
        [-5.6445, -2.6113, -3.4395,  ..., -4.2383, -5.8945, -0.4399],
        [-3.5039, -3.4102, -4.4102,  ..., -4.8477, -5.0039, -1.4570]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 26, 27,  4, 27,  0,  4, 17,  0,  2, 18, 15, 27, 15,  5,  0, 11,  0,
        27,  9, 27, 18, 13,  8, 18, 27, 26,  9, 27,  1,  1, 27,  7, 15, 22, 20,
        10,  3, 18, 27,  4, 15, 25, 27, 17, 15, 27, 20,  4, 13,  3,  1, 27,  5,
         7, 13, 27, 17, 25, 26, 27, 25, 11, 27], device='cuda:0')
step: 511
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4785, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2214,  0.7935,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2976, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0988,  ...,  0.6558,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0021,  0.0012,  0.0034,  ..., -0.0041,  0.0015, -0.0025],
        [-0.0037,  0.0065, -0.0173,  ...,  0.0063,  0.0102, -0.0030],
        [-0.0005,  0.0006, -0.0014,  ..., -0.0011,  0.0025,  0.0010],
        [-0.0018, -0.0013, -0.0060,  ...,  0.0023,  0.0008, -0.0042],
        [-0.0021,  0.0029,  0.0074,  ..., -0.0007, -0.0001,  0.0025]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(1.7784, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.3242, -3.6348, -4.4648,  ..., -6.3555, -6.4805, -0.2444],
        [-5.2227, -3.0664, -3.7539,  ..., -5.7227, -5.9258, -0.6133],
        [-5.1328, -4.1484, -4.3047,  ..., -6.5430, -5.7109, -0.2283],
        ...,
        [-6.3906, -3.5605, -3.8574,  ..., -4.7656, -5.1719, -0.3896],
        [-4.8516, -4.0703, -4.7891,  ..., -4.5234, -5.8203, -0.3818],
        [-6.7461, -3.5879, -3.9160,  ..., -5.9648, -6.3398, -0.3228]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27, 20,  1,  5, 10, 27, 10,  3, 27, 26, 10, 27, 27, 15, 23, 27,
         1, 27,  7, 27, 18, 11, 27,  2,  7, 27, 18, 27, 21,  2,  0, 27, 10, 27,
        11, 27, 27,  0, 27, 15, 19,  1, 27,  1, 27, 25, 27, 15,  4, 15,  1, 27,
        25, 11, 27, 15,  0,  1, 27, 10,  4, 27], device='cuda:0')
step: 512
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4788, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2214,  0.7935,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2976, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0988,  ...,  0.6558,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.6171e-04,  1.6890e-03,  2.4490e-03,  ..., -1.9236e-03,
         -1.7281e-03,  3.0470e-04],
        [ 7.6962e-04, -9.0122e-04,  8.8263e-04,  ...,  2.6627e-03,
         -4.2892e-04,  1.3046e-03],
        [ 4.8828e-04, -2.4700e-03, -3.8643e-03,  ...,  5.9738e-03,
          1.7328e-03, -5.8441e-03],
        [-9.5463e-04, -3.5763e-04,  1.2770e-03,  ...,  1.4963e-03,
         -8.1301e-04, -8.6403e-04],
        [-9.9719e-05,  7.0620e-04,  1.4973e-04,  ...,  1.2751e-03,
         -2.9802e-04,  1.0080e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0246, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.1504, -3.6172, -4.1328,  ..., -4.5703, -3.6641, -0.5244],
        [-4.5547, -3.4004, -4.0586,  ..., -5.1367, -4.9648, -0.3848],
        [-5.7891, -3.5840, -4.6953,  ..., -5.3672, -5.6953, -0.1630],
        ...,
        [-5.3125, -3.1875, -3.7344,  ..., -5.7031, -4.3594, -0.4685],
        [-2.6816, -4.6055, -4.9180,  ..., -6.9961, -4.6992, -3.6816],
        [-4.4258, -2.9883, -3.6758,  ..., -3.8945, -5.1758, -1.0195]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  2, 27,  5, 27, 26,  6, 14, 27, 18,  1,  5,  6, 22, 22, 26, 27, 15,
        27,  5, 25,  2, 27,  7, 10,  1, 27, 10,  2,  0,  7, 27, 27,  1,  4, 27,
         7,  7, 27,  7,  3,  7, 17, 25, 20, 27, 27, 27,  6, 27, 27, 27, 25, 27,
         2,  9, 10, 27, 27, 10, 27,  6, 18, 10], device='cuda:0')
step: 513
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4788, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2213,  0.7935,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2976, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0988,  ...,  0.6558,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0012,  0.0020, -0.0003,  ..., -0.0034,  0.0008, -0.0014],
        [ 0.0001,  0.0007,  0.0009,  ..., -0.0009, -0.0015,  0.0011],
        [-0.0004, -0.0012, -0.0169,  ...,  0.0068,  0.0060, -0.0063],
        [-0.0008, -0.0003, -0.0033,  ..., -0.0009,  0.0010, -0.0005],
        [-0.0001,  0.0022,  0.0013,  ..., -0.0026,  0.0005,  0.0022]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(1.6089, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.2539, -3.6895, -4.3945,  ..., -5.7070, -5.8906, -0.2209],
        [-4.7695, -3.4414, -4.3164,  ..., -6.1445, -6.4727, -0.2852],
        [-2.0410, -3.0723, -5.0391,  ..., -7.8672, -5.7422, -1.9932],
        ...,
        [-3.1152, -2.6621, -4.3047,  ..., -4.6953, -3.7715, -2.2559],
        [-5.4102, -3.7070, -3.9883,  ..., -4.0508, -3.9102, -0.5044],
        [-0.5830, -3.2402, -4.5664,  ..., -6.4883, -3.0996, -3.2090]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  0, 15, 27, 18,  6,  7, 18,  0, 27, 15,  0, 25, 27,  9, 10, 18,
        12, 15, 17, 27, 27, 17, 10, 27,  0, 27, 27, 27, 25, 15,  0,  1, 27, 27,
        10, 18, 27, 27, 27, 27, 15,  5,  9, 26, 25,  4, 27,  4,  4, 27, 27, 27,
        27,  5, 27, 27,  4,  4, 27, 22, 22,  0], device='cuda:0')
step: 514
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4788, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2213,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2976, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0989,  ...,  0.6562,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.3632e-03,  1.5850e-03,  3.2139e-03,  ..., -6.6948e-04,
         -1.1832e-04,  2.1019e-03],
        [ 1.1206e-04, -2.4629e-04, -7.7200e-04,  ..., -5.1260e-04,
         -1.9264e-04, -5.0211e-04],
        [-3.9744e-04, -1.1778e-04,  1.3912e-04,  ...,  4.3797e-04,
          5.9128e-04, -8.3351e-04],
        [-4.4870e-04, -6.1131e-04,  1.2245e-03,  ...,  2.6512e-04,
          9.3460e-04,  1.4410e-03],
        [-2.8539e-04, -5.1928e-04,  3.5381e-04,  ...,  3.0851e-04,
         -6.5804e-04,  7.8082e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9496, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.5938, -3.0176, -2.1738,  ..., -0.7197, -5.0625, -4.5625],
        [-1.7500, -3.0000, -4.3594,  ..., -5.0156, -4.0938, -1.5781],
        [-3.3047, -3.7422, -3.6797,  ..., -4.2578, -2.9766, -0.5229],
        ...,
        [-6.3516, -4.0078, -4.7109,  ..., -6.4297, -6.3203, -0.1465],
        [-4.4727, -3.8770, -4.4727,  ..., -4.1289, -5.9727, -0.4707],
        [-6.5469, -3.5918, -4.8125,  ..., -7.8125, -7.6328, -0.1082]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([24, 17, 27, 27, 27,  6, 27, 20, 27, 27, 13, 21, 13, 18,  3, 21,  0, 15,
        24, 20, 27, 27,  6,  7, 11, 27,  7, 27,  0,  4, 20, 15, 25,  0, 10, 22,
        27,  5, 18, 27, 20, 26, 26, 27, 27, 27, 15, 27,  8,  0, 27, 27, 27, 11,
        27, 27, 27, 27,  1, 11,  0, 27, 18,  2], device='cuda:0')
step: 515
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4788, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2213,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4087, -0.2974, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0989,  ...,  0.6562,  0.7075, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.4274e-04,  1.4162e-03,  1.2994e-05,  ..., -1.3218e-03,
         -5.5122e-04, -2.2945e-03],
        [ 1.0271e-03,  9.6130e-04,  3.7670e-05,  ...,  5.3101e-03,
          2.3632e-03, -8.0919e-04],
        [ 8.0013e-04,  5.0592e-04,  3.0541e-04,  ..., -2.5129e-04,
         -6.7329e-04, -3.5954e-04],
        [ 9.8228e-04, -1.4722e-04, -2.1124e-04,  ...,  5.2214e-04,
         -6.7806e-04, -6.6185e-04],
        [ 1.7095e-04, -5.9557e-04,  9.4116e-05,  ...,  3.1161e-04,
         -2.2233e-04,  5.1022e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7686, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.8848, -3.2441, -4.4961,  ..., -6.2148, -3.7910, -1.5889],
        [-5.6719, -3.5469, -4.2031,  ..., -2.9219, -5.3594, -2.4062],
        [-6.0391, -4.0195, -4.6953,  ..., -6.3633, -4.3945, -1.2090],
        ...,
        [-5.1523, -3.2305, -3.7930,  ..., -2.2617, -5.9961, -2.0273],
        [-5.1289, -3.1465, -3.6914,  ..., -4.3320, -2.5820, -1.0830],
        [-1.5967, -3.6426, -5.4414,  ..., -7.0195, -5.3164, -1.2998]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 22,  7,  9, 27,  4, 11,  7,  7, 27,  4, 15,  4,  7, 27, 27,  6, 13,
        22, 27, 18,  0, 11,  9, 20, 20, 27,  3,  1, 27,  4, 27, 27, 27,  6,  3,
        27, 14, 27, 26, 20, 27, 27, 11, 15, 27,  9, 13, 27, 20,  4,  9, 27, 27,
        10, 27, 23, 27,  3, 14,  3, 10, 27, 27], device='cuda:0')
step: 516
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6147,  ...,  0.4788, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4087, -0.2974, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0989,  ...,  0.6562,  0.7080, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.0051e-04,  6.1870e-05, -3.3140e-04,  ...,  1.4567e-04,
          5.8317e-04, -1.1654e-03],
        [-1.8692e-04,  1.1978e-03,  8.0347e-04,  ..., -1.9684e-03,
         -6.7949e-06,  1.7583e-04],
        [ 7.8201e-04,  5.5313e-04,  3.1929e-03,  ..., -2.0370e-03,
         -1.1587e-03,  8.9550e-04],
        [ 1.0669e-04,  9.3102e-05,  1.5450e-03,  ..., -2.3723e-05,
          5.0926e-04,  4.3035e-04],
        [ 4.0936e-04, -2.8896e-04, -8.4925e-04,  ..., -1.0788e-04,
          5.5361e-04, -1.0662e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7410, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.6484, -2.7090, -2.3027,  ..., -2.3184, -5.4922, -1.9902],
        [-1.6836, -3.1680, -4.4336,  ..., -6.4648, -4.5273, -1.3555],
        [-4.3672, -3.7109, -4.1797,  ..., -6.0391, -5.7734, -0.3508],
        ...,
        [-3.4395, -2.3613, -2.4082,  ..., -1.7686, -5.8008, -2.6738],
        [-1.8389, -4.0273, -6.0430,  ..., -7.8555, -5.8711, -4.4023],
        [-5.0312, -3.2793, -4.0938,  ..., -4.1250, -4.1719, -0.5464]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25, 18, 27, 27, 20, 27, 27,  4,  1,  5, 18,  5, 10, 27, 27,  1,  6, 27,
        20, 15, 13, 18, 27, 27, 17, 27, 27, 10, 27, 27,  3, 11, 27, 27,  3, 20,
        27, 27,  4,  4, 27, 27, 27, 25, 20, 24, 27, 27, 27, 20, 27, 27, 11, 27,
        27, 27, 27,  4, 27, 22,  0,  2, 18, 14], device='cuda:0')
step: 517
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4788, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4087, -0.2974, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0990,  ...,  0.6562,  0.7080, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.0075e-04, -7.7200e-04,  4.2582e-04,  ...,  1.2827e-03,
         -3.0041e-04,  3.2282e-04],
        [-5.4693e-04,  8.3542e-04, -2.4354e-04,  ...,  6.8998e-04,
          1.0033e-03, -6.4802e-04],
        [ 3.5286e-04,  2.8014e-04,  4.9934e-03,  ..., -4.7302e-03,
          4.3273e-04,  4.1161e-03],
        [ 6.0558e-04,  8.6486e-05,  8.2302e-04,  ..., -3.4237e-04,
         -3.4261e-04, -1.9288e-04],
        [ 9.1612e-05,  2.6631e-04, -6.4564e-04,  ...,  1.2865e-03,
          2.1088e-04, -1.0443e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9235, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.3457, -3.0020, -3.2207,  ..., -4.5000, -3.6270, -1.0176],
        [-3.3633, -3.2539, -4.4258,  ..., -5.9727, -6.9570, -0.5195],
        [-4.3555, -3.9004, -4.5586,  ..., -6.6367, -6.2773, -0.1827],
        ...,
        [-2.1191, -4.6016, -7.3203,  ..., -5.7266, -4.9141, -4.6953],
        [-4.5898, -4.2773, -4.4023,  ..., -5.1992, -5.6523, -0.3411],
        [-5.0430, -2.8398, -4.1211,  ..., -4.8867, -3.5430, -0.9487]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0, 27, 11, 22, 18,  7,  3, 15, 18, 27,  6,  5, 24, 25, 15, 20,  9,
        27, 19,  2,  4, 22,  7, 15,  5, 11, 20, 20, 20, 27, 14,  0, 14, 20, 27,
        10, 27, 10, 25, 26, 27, 20, 26, 27, 17, 26, 27,  7, 27, 27,  3, 27, 27,
         0, 27, 27, 27, 26, 10,  7, 15, 27,  1], device='cuda:0')
step: 518
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4790, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4087, -0.2974, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0990,  ...,  0.6562,  0.7080, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.2922e-03, -5.5504e-04, -9.3746e-04,  ..., -9.1076e-04,
          1.1625e-03, -1.3227e-03],
        [ 4.5156e-04, -4.5967e-04, -1.2165e-04,  ...,  2.2829e-05,
         -5.6505e-04, -3.8147e-05],
        [ 2.5010e-04, -1.4019e-03, -1.4524e-03,  ...,  4.3201e-04,
          1.1158e-04, -2.9039e-04],
        [ 1.2703e-03, -6.8426e-04, -1.9050e-04,  ..., -1.9383e-04,
          2.3723e-04, -1.5914e-04],
        [ 4.5395e-04, -5.7173e-04, -2.4235e-04,  ...,  5.4240e-05,
          5.1260e-04,  2.5034e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7782, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.2148, -4.7422, -6.1953,  ..., -0.1191, -5.2617, -3.9004],
        [-5.3516, -2.9004, -3.1797,  ..., -3.8047, -5.8984, -0.6650],
        [-4.8125, -4.0938, -4.3906,  ..., -5.1719, -4.1719, -0.4692],
        ...,
        [-4.6758, -2.4902, -2.6465,  ..., -3.7090, -4.0664, -1.1299],
        [-5.5469, -2.3613, -2.0801,  ..., -1.9229, -4.5039, -2.6895],
        [-3.4355, -2.3574, -3.0293,  ..., -4.9180, -3.4043, -1.2480]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25,  3, 26, 27,  4, 27, 17, 15,  7, 27, 25, 27, 27, 18, 27,  7,  0,  4,
         9, 18, 10, 22, 27,  9, 15,  8, 25,  3, 27, 25,  3, 15,  3, 15, 27, 26,
         0, 17, 27, 25, 27, 27,  0, 15, 15, 20, 27,  7, 27, 27, 27, 25, 25,  0,
        26, 26, 17, 10, 27,  1, 27, 27, 14, 18], device='cuda:0')
step: 519
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4790, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4087, -0.2974, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0991,  ...,  0.6562,  0.7080, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.3344e-04, -1.3905e-03, -1.3494e-03,  ..., -5.2309e-04,
         -1.1414e-04, -2.1839e-03],
        [ 7.8058e-04, -2.2144e-03,  6.7425e-04,  ...,  9.1648e-04,
         -1.2169e-03,  1.0471e-03],
        [ 3.9876e-05, -1.6670e-03,  2.2340e-04,  ..., -1.1826e-03,
         -4.4703e-04,  4.1175e-04],
        [-2.1915e-03, -2.4390e-04,  2.7218e-03,  ...,  1.2312e-03,
         -1.7858e-04, -3.1412e-05],
        [-1.7953e-04, -1.7715e-04,  5.7411e-04,  ...,  3.5739e-04,
         -3.9363e-04, -4.4346e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1291, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2773, -3.5586, -4.4180,  ..., -5.0898, -5.6836, -0.2307],
        [-0.5566, -4.0547, -6.1953,  ..., -7.6953, -6.0547, -4.8359],
        [-5.2578, -4.1328, -4.6641,  ..., -6.8672, -4.8984, -0.8042],
        ...,
        [-5.5781, -4.2344, -4.6875,  ..., -7.1875, -4.9844, -1.5605],
        [-3.2793, -2.7930, -2.8086,  ..., -2.2305, -3.9492, -0.9653],
        [-2.4570, -2.5977, -3.6602,  ..., -2.2070, -3.7070, -2.0664]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 9, 27, 27, 22, 24, 27, 27, 27, 25, 17,  7, 27,  9, 27,  0, 27,  7, 18,
        17,  8,  1, 26, 15, 27, 27, 10,  0,  1, 20, 27, 22, 15, 24,  5,  0,  1,
        25,  3,  7, 25, 27, 27, 27, 27, 27,  3,  3, 27, 10, 15,  4, 15,  0, 10,
        10,  0,  4, 11, 25, 24, 27, 27, 18,  3], device='cuda:0')
step: 520
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4790, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4087, -0.2974, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0991,  ...,  0.6562,  0.7080, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.6679e-04, -8.9025e-04, -5.6934e-04,  ..., -7.4387e-04,
         -4.7755e-04, -7.3814e-04],
        [ 9.4414e-04,  4.5538e-04,  4.7326e-04,  ..., -2.6417e-03,
         -1.4782e-03, -1.4782e-03],
        [-2.3413e-04, -1.7834e-04, -7.4720e-04,  ...,  1.2093e-03,
          1.0767e-03, -2.0933e-04],
        [-1.4153e-03,  4.9496e-04,  2.1534e-03,  ...,  3.7408e-04,
          3.0327e-04,  9.4354e-05],
        [ 2.4533e-04,  3.8624e-04, -1.4472e-04,  ...,  1.9789e-04,
          1.7977e-04, -2.4939e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7741, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.8672, -4.1328, -5.2891,  ..., -7.4297, -5.1484, -4.7422],
        [-3.2832, -3.7988, -4.0820,  ..., -5.2070, -5.8945, -2.6738],
        [-5.3516, -2.2734, -4.0391,  ..., -4.6172, -5.6016, -0.8042],
        ...,
        [-5.4805, -4.2617, -4.8867,  ..., -6.0430, -4.1523, -1.2773],
        [-2.7246, -4.9102, -6.2852,  ..., -7.4883, -6.3477, -5.2852],
        [-1.2549, -3.4883, -6.1445,  ..., -7.8789, -5.7852, -1.6914]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 27, 27, 22,  0, 27,  4, 24,  9, 27, 13, 18,  3, 27, 10, 27, 27,  4,
        13,  4,  8,  7, 27, 18,  1, 12, 14, 26, 27,  4, 26,  0,  9, 27, 20, 26,
         3,  0,  4,  0, 25,  7, 26,  7, 27,  3, 27,  7, 14, 25, 27, 18, 20, 27,
        26, 27, 20, 22, 15,  4, 15, 27, 18,  0], device='cuda:0')
step: 521
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4790, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2211,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4087, -0.2974, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0991,  ...,  0.6562,  0.7080, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.5197e-04,  6.0439e-05, -1.8144e-04,  ...,  1.2505e-04,
          3.3092e-04, -1.0405e-03],
        [ 1.4076e-03, -1.2064e-03,  1.5461e-04,  ...,  1.4954e-03,
          1.9050e-04, -3.7694e-04],
        [ 7.1669e-04, -1.8549e-03, -5.1403e-04,  ..., -1.5860e-03,
          7.0667e-04,  6.3038e-04],
        [-9.0170e-04, -2.2459e-04,  2.0657e-03,  ...,  2.6798e-04,
         -1.4496e-04,  1.0413e-04],
        [-1.9777e-04, -2.2411e-04, -7.8022e-05,  ...,  8.3387e-05,
         -3.8671e-04, -1.2708e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8829, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.3711, -2.7324, -3.4824,  ..., -5.5273, -5.2148, -1.8252],
        [-3.0742, -3.0273, -4.4180,  ..., -3.5117, -2.4961, -1.3711],
        [-3.2949, -1.8252, -3.2637,  ..., -1.9668, -4.6680, -3.6230],
        ...,
        [-5.0781, -1.9854, -2.6738,  ..., -4.4219, -2.9082, -1.4541],
        [-5.2070, -4.1602, -5.4570,  ..., -6.8789, -3.3496, -2.9121],
        [-4.7227, -4.2695, -5.3477,  ..., -7.2383, -6.6602, -0.1431]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  3, 17,  0, 22,  9, 27,  4, 18,  3,  7,  4, 18, 27,  4, 17, 26,
         2, 27,  7, 27,  2, 17, 27, 27, 27, 14, 20,  7,  3, 27, 20,  8, 15, 10,
        18, 27,  6, 27, 12, 17, 17, 20, 27, 27, 27, 27, 23, 15, 18, 27, 27, 27,
        27,  3, 27,  4, 22, 27, 10,  0, 27, 27], device='cuda:0')
step: 522
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4790, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2211,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4087, -0.2974, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0991,  ...,  0.6562,  0.7080, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.9891e-04, -1.5914e-04, -9.1505e-04,  ...,  8.0824e-04,
          1.1816e-03,  3.4809e-05],
        [-4.5800e-04,  1.0242e-03, -2.2984e-04,  ..., -1.6222e-03,
          4.8935e-05,  4.8101e-05],
        [ 5.7220e-04,  1.6856e-04, -5.2309e-04,  ...,  5.9986e-04,
          1.0347e-03, -5.7125e-04],
        [ 3.3927e-04, -1.3292e-04, -2.0370e-03,  ...,  3.0994e-06,
          7.0214e-05, -7.9489e-04],
        [ 1.1301e-04,  5.0926e-04, -5.5742e-04,  ...,  2.5296e-04,
          2.8896e-04, -1.5497e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6609, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.7227, -2.6270, -4.1133,  ..., -4.2852, -3.3457, -2.0957],
        [-4.7578, -4.3359, -5.4609,  ..., -6.8516, -6.4922, -0.1175],
        [-5.8359, -2.8184, -2.9121,  ..., -2.7871, -5.2227, -0.9277],
        ...,
        [-4.5078, -5.1680, -8.4922,  ..., -6.5234, -6.5117, -6.4180],
        [-4.9219, -3.6562, -4.9844,  ..., -5.2344, -2.6562, -3.3594],
        [-2.8828, -2.8828, -4.7734,  ..., -6.8359, -5.2422, -3.5859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 27,  3,  1, 15, 27, 15,  7,  0, 24, 20, 26, 27, 18, 27, 27, 15, 27,
         1, 27, 22,  2, 17,  5, 18, 27, 27, 15, 18,  0,  9, 27, 27, 22, 27, 27,
        27, 15,  6,  1, 10, 27, 27,  2, 11,  8,  7,  7, 11,  6,  7, 20,  1, 14,
        26,  1, 27, 15, 18, 27,  6, 15, 13, 22], device='cuda:0')
step: 523
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4790, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2211,  0.7939,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4087, -0.2974, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0991,  ...,  0.6562,  0.7080, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.2195e-04,  2.7597e-05,  6.3837e-05,  ..., -1.1854e-03,
         -3.1447e-04, -7.0667e-04],
        [ 7.6485e-04,  5.0545e-05, -4.7255e-04,  ...,  2.6011e-04,
          3.9148e-04, -2.4557e-04],
        [ 8.2064e-04, -1.4448e-04, -1.4191e-03,  ...,  9.3031e-04,
          1.1845e-03, -2.6345e-04],
        [-7.2575e-04,  1.8668e-04,  1.4791e-03,  ...,  7.8535e-04,
          6.7139e-04,  6.8998e-04],
        [ 5.0068e-04,  5.3215e-04, -1.9073e-04,  ...,  8.6641e-04,
          6.4909e-05,  2.5320e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9863, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.8945, -3.4258, -2.8008,  ..., -1.5664, -5.0664, -2.7539],
        [-5.2461, -2.9160, -2.9785,  ..., -1.4473, -5.3555, -2.3848],
        [-5.8047, -3.8984, -4.2578,  ..., -5.2109, -6.5391, -0.5073],
        ...,
        [-3.9629, -4.4180, -8.6953,  ..., -7.7773, -6.7461, -5.9805],
        [-5.8086, -4.1211, -4.9492,  ..., -5.7617, -3.6660, -1.2607],
        [-1.8867, -3.4961, -5.2617,  ..., -5.6680, -5.3086, -0.9961]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([24, 12, 27,  4,  5, 15, 13, 27, 27, 17, 27, 20, 17,  1, 27,  3,  0,  9,
         3,  0, 19,  2,  7, 18,  4,  5,  4, 27, 18,  0,  1, 27, 27,  3, 15, 27,
        11,  6, 27, 15, 18, 27,  1, 10,  4, 10, 17,  3, 27, 17, 27, 27, 20, 26,
        11,  0,  5, 27, 27,  2,  3, 15,  7,  4], device='cuda:0')
step: 524
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4790, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4087, -0.2974, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0991,  ...,  0.6562,  0.7080, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.3466e-03,  9.5654e-04,  9.0933e-04,  ...,  2.6035e-04,
         -7.9346e-04,  6.6948e-04],
        [ 4.9829e-04,  4.1676e-04,  1.1444e-04,  ...,  4.5419e-04,
         -8.7261e-04,  2.2674e-04],
        [-8.2111e-04,  8.4019e-04,  9.6464e-04,  ...,  2.9354e-03,
         -4.2748e-04, -6.0463e-04],
        [ 2.4438e-04,  4.0174e-05,  5.4455e-04,  ..., -5.7697e-05,
          1.4365e-04,  2.9135e-04],
        [-2.7609e-04, -1.0958e-03,  4.6587e-04,  ..., -2.8086e-04,
         -4.7421e-04,  6.3086e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7321, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4570, -3.7227, -3.6758,  ..., -2.9570, -2.8945, -0.8169],
        [-4.7188, -3.5000, -4.0938,  ..., -6.5312, -6.6562, -0.4211],
        [-4.3945, -3.5820, -4.6133,  ..., -5.9727, -5.2383, -0.3167],
        ...,
        [-5.1562, -3.3125, -3.7031,  ..., -5.0000, -5.7656, -0.8911],
        [-5.9922, -3.3809, -2.3652,  ..., -1.9287, -5.6797, -2.3184],
        [-5.0938, -3.6074, -4.5625,  ..., -5.3125, -2.9980, -0.8110]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 27, 27, 20,  0,  4, 27, 27,  4,  2, 27, 17, 27,  4, 13, 27,  5, 27,
        27, 18,  1,  5, 15, 15, 27,  6, 18, 15, 25, 20, 27, 27, 15, 27, 22, 21,
        27,  3,  1,  1, 10, 15, 27,  0, 27, 20, 27,  5,  0, 15, 14,  8,  0,  6,
        10,  0, 18, 15, 27,  0, 27,  4, 27,  6], device='cuda:0')
step: 525
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4790, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4087, -0.2974, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0991,  ...,  0.6562,  0.7080, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.9339e-04, -2.1815e-04, -2.2995e-04,  ..., -5.6648e-04,
          1.6272e-05,  2.1505e-04],
        [-1.9312e-04,  1.3275e-03,  8.1730e-04,  ..., -1.2960e-03,
          1.9860e-04, -8.8549e-04],
        [-3.6955e-05,  1.1671e-04,  6.3610e-04,  ..., -1.1616e-03,
         -3.2473e-04,  4.2319e-06],
        [ 3.8815e-04, -5.0306e-04,  1.5497e-03,  ...,  2.1720e-04,
          6.3801e-04,  1.0544e-04],
        [ 2.7347e-04, -9.5546e-05,  3.6621e-04,  ..., -2.6274e-04,
          7.6532e-05,  1.6320e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6824, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0273, -4.8711, -9.1250,  ..., -7.9336, -7.2617, -5.3398],
        [-6.4883, -3.1445, -3.4258,  ..., -4.8477, -3.3477, -1.4414],
        [-1.2656, -3.4688, -4.9531,  ..., -4.9688, -2.6250, -1.7656],
        ...,
        [-5.7148, -3.0293, -3.6230,  ..., -2.2324, -5.3086, -1.6064],
        [-5.7305, -2.8574, -2.9980,  ..., -3.8730, -3.9805, -1.4502],
        [-3.2324, -3.1855, -4.6680,  ..., -4.3867, -4.6836, -0.7319]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15,  6, 27,  4, 20,  3, 27, 27,  3, 27, 27, 25, 22,  7, 18, 27,  5,  7,
        12, 10, 18, 18,  7, 27, 27, 27, 27,  7,  7, 18, 20, 27,  3, 16, 10, 27,
        27, 14, 15, 10, 25, 26,  2,  5, 15, 15,  7, 22, 20, 27, 15,  2, 27, 27,
         0,  0,  0,  3, 25, 20, 13, 27, 10, 27], device='cuda:0')
step: 526
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4790, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4084, -0.2974, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0991,  ...,  0.6562,  0.7080, -1.7988]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.7898e-04,  1.9634e-04,  9.7322e-04,  ...,  3.8075e-04,
         -7.3731e-05,  2.7037e-04],
        [-7.6950e-05, -9.1434e-05,  6.7186e-04,  ...,  1.1063e-03,
          3.4952e-04, -1.6713e-04],
        [-2.6488e-04, -2.5511e-05,  8.0252e-04,  ..., -4.1699e-04,
         -1.1330e-03,  4.3368e-04],
        [-2.2340e-04, -3.5691e-04,  4.0698e-04,  ...,  2.3186e-04,
          3.0565e-04,  2.4247e-04],
        [-3.3212e-04, -1.8537e-05,  3.6025e-04,  ..., -1.8001e-04,
         -3.5858e-04,  3.2520e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8086, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.7852, -3.6309, -4.5039,  ..., -5.4609, -3.8340, -0.9434],
        [-5.1406, -3.2051, -4.4844,  ..., -5.4375, -3.1582, -1.0332],
        [-4.8125, -4.6875, -5.4062,  ..., -6.0000, -6.7500, -4.2656],
        ...,
        [-5.7930, -4.0898, -4.9805,  ..., -4.5273, -5.8086, -0.3564],
        [-5.0039, -3.2520, -3.5332,  ..., -4.7227, -3.4238, -0.9556],
        [-7.0781, -4.8281, -5.4219,  ..., -7.7812, -0.1414, -4.1562]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  7,  8, 22,  3, 22, 20, 27, 27, 27,  4, 26, 27, 27, 17,  0,  0, 20,
         2,  0, 20, 27, 27,  0, 12, 15, 27, 27, 27,  7,  9, 20, 27,  3,  1, 18,
        13, 27,  4,  1,  6, 10, 27,  7,  0, 27,  0, 13,  9,  1, 10, 27, 22,  3,
        27, 27, 27, 17, 25, 27,  0, 27, 27, 26], device='cuda:0')
step: 527
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4790, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4084, -0.2974, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0991,  ...,  0.6562,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.7098e-04, -3.5465e-05, -3.6836e-04,  ...,  8.3542e-04,
         -6.3133e-04, -4.5419e-04],
        [-4.3344e-04, -1.2612e-04,  3.5763e-04,  ..., -9.1839e-04,
         -1.1892e-03, -1.9443e-04],
        [ 2.7537e-04,  7.1430e-04,  7.3338e-04,  ...,  8.4102e-05,
          3.6788e-04,  3.2187e-04],
        [ 4.9782e-04,  1.6248e-04, -3.6526e-04,  ..., -8.7023e-04,
         -1.2484e-03, -2.8992e-04],
        [ 2.9922e-04, -4.9543e-04, -2.5570e-05,  ..., -2.0051e-04,
         -4.5395e-04,  1.8001e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6760, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -5.4609,  -3.6953,  -4.1953,  ...,  -0.9302,  -2.7422,  -2.7266],
        [ -3.5996,  -3.2715,  -3.6621,  ...,  -3.6934,  -0.5059,  -2.3965],
        [ -1.0596,  -3.5273,  -5.1055,  ...,  -6.4961,  -4.3867,  -1.5908],
        ...,
        [ -7.5234,  -4.2578,  -3.8672,  ...,  -5.0859,  -6.7891,  -0.2568],
        [ -0.6309,  -3.6465,  -4.7852,  ...,  -5.2539,  -3.5215,  -2.6934],
        [ -7.1562,  -6.9375, -10.9531,  ...,  -8.0625,  -9.4062,  -8.9219]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25, 26,  0,  5,  4, 27, 18,  7, 15, 20,  2,  0,  7, 27, 27, 18, 15,  0,
        27, 27, 11,  4, 27,  7,  7,  7, 27, 15,  3, 27,  2, 27, 18, 18, 25, 27,
        15,  1, 27,  3, 26, 15, 27, 17,  4, 15,  7, 27, 27, 20,  0,  4, 27, 27,
         0,  6, 15, 15,  0, 26, 27,  2,  0, 15], device='cuda:0')
step: 528
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4790, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4084, -0.2974, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6562,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.9148e-04,  1.3037e-03,  8.1396e-04,  ..., -7.0620e-04,
         -3.3617e-04,  8.6451e-04],
        [ 1.6510e-04, -6.1333e-05, -4.1008e-05,  ...,  1.1438e-04,
          7.0000e-04,  6.4850e-04],
        [-3.5858e-04,  4.8733e-04,  2.0981e-04,  ..., -3.7491e-05,
         -4.4441e-04,  8.9025e-04],
        [-8.4114e-04,  1.8501e-04,  1.3266e-03,  ...,  8.4114e-04,
          5.4884e-04,  1.0624e-03],
        [ 1.9956e-04,  1.7393e-04,  3.1662e-04,  ...,  1.1277e-04,
         -1.7238e-04, -5.4777e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0961, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.7275, -3.4004, -6.0234,  ..., -5.4609, -3.9766, -3.0723],
        [-5.2461, -2.5742, -3.3574,  ..., -3.6836, -3.6699, -1.3408],
        [-1.7793, -4.2656, -6.5469,  ..., -6.1719, -4.8750, -4.5469],
        ...,
        [-4.5859, -3.9160, -4.0859,  ..., -4.1328, -5.0234, -0.6499],
        [-3.7090, -3.2402, -4.1641,  ..., -6.3203, -2.6621, -1.0684],
        [-2.1895, -4.0156, -5.0312,  ..., -4.6094, -5.5000, -1.5635]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20,  4, 15,  1, 10, 27,  1, 10, 15, 15, 11, 27, 27, 27,  1, 15, 18,  6,
         3,  5, 27, 27, 27, 23,  6,  9, 27,  0, 14, 17, 11, 20, 15,  4, 27, 10,
        27,  2, 18, 27, 27,  2, 27,  3, 15,  4, 10, 25, 25, 20, 27, 15, 25,  1,
        27, 27, 20, 27, 27, 10,  4, 27, 26,  3], device='cuda:0')
step: 529
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4084, -0.2974, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6562,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.1907e-04,  1.7381e-04,  8.8835e-04,  ...,  8.1205e-04,
         -9.5892e-04,  9.0790e-04],
        [-5.0640e-04,  1.3590e-04,  5.7936e-04,  ...,  2.4452e-03,
          5.5504e-04, -2.4724e-04],
        [-4.7755e-04, -3.0947e-04,  6.0678e-05,  ...,  1.2550e-03,
         -6.1870e-05, -1.7059e-04],
        [-1.4460e-04,  2.0695e-04,  2.4366e-04,  ...,  1.6558e-04,
          1.9896e-04, -1.0699e-04],
        [-1.4281e-04,  5.4121e-04, -2.9206e-06,  ...,  3.8528e-04,
         -1.6630e-05,  1.3947e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8392, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6289, -4.8164, -7.4727,  ..., -6.7852, -6.5352, -5.5195],
        [-3.4609, -2.9766, -3.8828,  ..., -4.7109, -4.0859, -0.9614],
        [-5.3477, -3.3184, -4.3164,  ..., -2.7246, -5.0977, -1.7861],
        ...,
        [-2.6367, -4.5430, -7.2617,  ..., -6.3398, -4.7930, -4.6055],
        [-1.2803, -3.2812, -4.9531,  ..., -7.3125, -5.1875, -2.5000],
        [-3.5566, -3.0254, -3.1660,  ..., -2.4160, -5.1211, -4.1836]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15,  4,  9, 27, 25, 17, 22,  3,  5, 27, 27, 15, 27,  4, 20, 17,  5, 27,
         2, 27, 10,  1, 27,  4, 20, 27,  0,  0, 15, 25,  6, 20, 27, 27, 15, 10,
        27,  4,  2, 21, 15, 27, 27,  3,  7, 27, 27, 25, 15, 27,  9,  3,  1, 22,
        18, 12,  2, 27, 27, 14,  2, 15, 18, 18], device='cuda:0')
step: 530
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4084, -0.2974, -1.7207,  ..., -0.6597,  0.6533,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6562,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.3372e-04, -1.2732e-04, -8.0967e-04,  ...,  7.1526e-07,
          8.9073e-04, -9.3758e-05],
        [-1.9097e-04, -4.2868e-04, -7.7677e-04,  ..., -1.4429e-03,
          1.0163e-04,  9.5463e-04],
        [ 4.0603e-04, -7.4577e-04, -7.1526e-04,  ..., -2.1148e-04,
          6.2752e-04,  6.0856e-05],
        [ 5.1403e-04,  1.5676e-05, -6.8521e-04,  ..., -2.2280e-04,
          5.7411e-04,  1.2970e-04],
        [ 1.2422e-04,  3.1376e-04, -5.5456e-04,  ..., -2.5153e-05,
          4.0245e-04, -1.6832e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9241, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.4805, -4.0898, -4.3867,  ..., -5.3555, -3.8066, -1.0107],
        [-3.1348, -3.7598, -5.7109,  ..., -3.5098, -3.8848, -4.2148],
        [-2.2070, -3.7695, -6.4883,  ..., -8.0000, -5.9883, -4.7695],
        ...,
        [-3.3477, -4.3945, -4.6602,  ..., -6.0352, -5.1758, -0.2382],
        [-3.6074, -2.2168, -3.6074,  ..., -2.1543, -4.4180, -1.9355],
        [-3.2422, -3.6797, -6.9609,  ..., -5.6328, -3.9453, -4.6328]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22, 21, 20,  4, 10,  4,  4,  3, 27, 10,  5,  4, 17,  0, 27,  9, 27,  8,
        10, 18,  4,  6,  4, 11, 15,  4, 27,  1, 27, 27,  7, 27, 21, 27, 10, 27,
        13, 26, 27, 12, 27,  6, 27,  3, 15, 22,  4,  8,  2, 20, 19,  4, 27, 17,
         0, 27,  2, 18,  4, 27, 24, 27,  1, 17], device='cuda:0')
step: 531
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4084, -0.2974, -1.7207,  ..., -0.6597,  0.6533,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6562,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.5558e-04,  1.2910e-04, -1.7285e-05,  ..., -6.8426e-04,
         -1.5497e-05,  7.5281e-05],
        [-6.0749e-04,  8.4114e-04,  8.6403e-04,  ..., -1.1194e-04,
         -8.0681e-04,  2.0504e-04],
        [-9.2387e-05, -7.6389e-04, -4.7493e-04,  ...,  4.6945e-04,
          5.6362e-04, -7.1430e-04],
        [ 1.8907e-04,  1.4138e-04,  2.2578e-04,  ..., -3.6454e-04,
         -2.8372e-04, -3.3307e-04],
        [ 2.6286e-05,  7.1669e-04, -2.2244e-04,  ...,  1.3292e-04,
          3.9876e-05,  7.4327e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9404, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.9961, -4.4336, -5.7305,  ..., -7.6836, -5.8711, -4.7148],
        [-1.8613, -3.9082, -4.0039,  ..., -3.0645, -4.6758, -2.9863],
        [-3.4434, -3.3027, -4.0195,  ..., -2.0371, -4.3008, -2.1465],
        ...,
        [-5.0781, -3.1699, -3.7480,  ..., -3.2324, -1.8105, -1.4668],
        [-6.6172, -5.7266, -6.1758,  ..., -7.9766, -6.9766, -0.1153],
        [-6.2070, -3.0352, -2.8164,  ..., -5.8320, -2.3320, -1.3008]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 18,  9,  3,  4,  2, 27, 23, 27,  3,  1, 15, 27,  5, 27, 27, 20, 15,
         3,  7, 18, 27, 27,  3,  1, 24, 27,  9, 27,  3, 22, 18,  0, 18,  4, 27,
         0, 27, 27, 26, 27,  6, 18, 18, 27,  1, 27, 27, 24,  3, 25, 26, 27, 27,
        11, 15, 27, 20, 27,  6,  1,  7,  3,  1], device='cuda:0')
step: 532
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4084, -0.2974, -1.7207,  ..., -0.6597,  0.6533,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6562,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.8590e-04, -6.2132e-04,  7.5865e-04,  ..., -8.6641e-04,
         -8.8406e-04, -4.0007e-04],
        [ 5.0354e-04, -4.2915e-05,  1.4057e-03,  ...,  7.4387e-05,
         -1.4725e-03,  4.5180e-04],
        [ 6.7902e-04, -1.9217e-04, -5.9032e-04,  ..., -1.4150e-04,
          3.7766e-04,  2.1100e-04],
        [-4.5085e-04, -3.8171e-04,  2.0199e-03,  ...,  7.9584e-04,
          4.5228e-04, -7.3576e-04],
        [-1.1802e-04,  8.6355e-04, -1.6093e-06,  ...,  7.1812e-04,
         -1.8549e-04, -3.2949e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9768, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.0000, -3.8438, -4.4219,  ..., -4.8125, -5.3281, -0.5156],
        [-4.6523, -4.0430, -4.3711,  ..., -6.3242, -4.9023, -0.3711],
        [-4.2109, -4.1641, -6.9141,  ..., -7.3984, -6.3984, -4.9609],
        ...,
        [-4.7344, -3.1094, -3.9219,  ..., -3.6406, -3.0312, -0.9526],
        [-5.8203, -3.7754, -4.4922,  ..., -6.8047, -4.8047, -0.3684],
        [-5.4727, -2.6602, -3.3633,  ..., -3.1445, -3.7383, -1.1924]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 9, 27, 15,  0, 27, 15, 18, 13, 27,  7, 27, 26, 15, 27, 17, 27, 13, 14,
         1, 27, 27, 27, 22, 24,  4, 24, 11,  0, 26, 27, 15, 27,  5, 18, 27, 27,
        27, 20, 10,  3, 10, 27, 27, 10, 23,  1, 17,  6,  0, 15, 27, 11, 27, 26,
        10, 20,  3, 27, 27, 27, 10, 27,  4, 27], device='cuda:0')
step: 533
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6147,  ...,  0.4792, -0.0808,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4084, -0.2974, -1.7207,  ..., -0.6597,  0.6533,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6562,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.8585e-04,  1.4076e-03,  7.9250e-04,  ..., -5.5504e-04,
          1.2326e-04, -1.6797e-04],
        [ 1.4067e-04,  1.8597e-05, -4.0650e-04,  ...,  3.0565e-04,
          9.1076e-04,  3.4523e-04],
        [-2.2936e-04,  6.1226e-04,  8.1301e-05,  ...,  1.4992e-03,
          1.2279e-05, -1.2703e-03],
        [ 2.4605e-04,  8.1658e-06, -1.0747e-04,  ...,  2.0051e-04,
          1.8179e-04, -6.0225e-04],
        [ 2.6274e-04,  5.6696e-04, -2.6751e-04,  ...,  3.1352e-04,
          3.4046e-04, -3.7146e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2179, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.0381, -3.1152, -5.5078,  ..., -6.2891, -3.5684, -1.8818],
        [-5.4922, -3.7109, -3.6484,  ..., -3.0859, -4.1172, -1.4297],
        [-5.4102, -4.1758, -4.4102,  ..., -3.4414, -3.9570, -3.3320],
        ...,
        [-4.4297, -3.8027, -4.7266,  ..., -5.9766, -4.9141, -0.7402],
        [-4.0234, -4.0547, -5.6484,  ..., -7.7109, -7.4453, -0.9907],
        [-5.1367, -3.3535, -1.7764,  ..., -2.8535, -4.9805, -1.3389]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 27,  9,  6, 10,  2, 26, 27, 27,  9,  0,  3, 27, 27, 25, 15, 15,  3,
         1,  9, 18, 10, 27, 18, 20,  0, 27, 26,  2, 22, 17,  2,  3, 22, 27,  0,
         0, 27, 11,  4, 15, 27, 24, 20, 27, 27, 26, 27,  7,  5,  0, 27, 27,  9,
         4,  8,  7,  7,  0, 12,  4,  4,  4,  2], device='cuda:0')
step: 534
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6147,  ...,  0.4792, -0.0808,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4084, -0.2971, -1.7207,  ..., -0.6597,  0.6533,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6562,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.6883e-04, -1.1501e-03,  9.2459e-04,  ...,  6.6662e-04,
         -4.6110e-04,  5.9223e-04],
        [ 3.8409e-04, -6.0320e-04,  8.5115e-04,  ...,  1.9894e-03,
         -1.1663e-03,  1.2851e-04],
        [ 7.3791e-05, -5.0306e-04, -9.5487e-05,  ...,  4.2152e-04,
         -3.0828e-04, -3.3712e-04],
        [ 1.5068e-03, -2.1291e-04, -1.4467e-03,  ..., -4.3702e-04,
          2.6965e-04, -5.8413e-04],
        [ 2.9254e-04, -2.0885e-04, -1.7059e-04,  ...,  1.4913e-04,
          4.6492e-05,  1.6093e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8435, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.4023, -3.7168, -4.0742,  ..., -5.7656, -4.5312, -1.2793],
        [-4.4609, -1.8184, -3.2090,  ..., -1.8496, -3.0215, -2.7402],
        [-5.5977, -3.5352, -3.4258,  ..., -3.6445, -6.1602, -1.3154],
        ...,
        [-6.1992, -3.9199, -4.2969,  ..., -5.3086, -3.1699, -1.2324],
        [-6.2266, -3.5215, -4.4766,  ..., -6.7266, -2.7090, -1.5059],
        [-6.1055, -4.0117, -4.4336,  ..., -5.0742, -5.9336, -0.2761]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 12, 27, 27, 27, 18,  1, 27,  5, 27,  6, 27, 27,  2,  3, 27, 27, 23,
        27,  4, 27, 18, 10, 27,  0, 18, 27,  4, 15,  1, 27, 27, 20, 15, 27, 10,
         1,  1, 10, 13, 27, 27, 22, 25, 20,  5,  1, 26, 26, 27, 22, 27, 27, 17,
        27, 27,  1, 19, 27, 27, 27, 27,  7,  3], device='cuda:0')
step: 535
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6147,  ...,  0.4792, -0.0808,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4084, -0.2971, -1.7207,  ..., -0.6597,  0.6533,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.7752e-04,  1.2255e-03,  3.9387e-04,  ...,  1.5497e-03,
         -3.1662e-04, -2.3437e-04],
        [-2.8753e-04,  2.9492e-04, -1.3580e-03,  ...,  1.6832e-04,
          9.2936e-04,  1.0991e-04],
        [ 3.7849e-05,  4.6873e-04, -2.3508e-04,  ...,  3.2425e-05,
         -4.0293e-04, -1.7929e-04],
        [ 5.2834e-04,  7.7677e-04,  6.3467e-04,  ...,  6.8808e-04,
         -1.3552e-03,  7.7963e-04],
        [-2.7871e-04, -4.4107e-06,  4.8733e-04,  ...,  6.4421e-04,
         -1.7393e-04,  5.4693e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2317, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.8164, -2.4570, -2.5527,  ..., -2.1777, -5.1133, -1.4111],
        [-6.2344, -4.3438, -4.8438,  ..., -6.3750, -5.5156, -0.2332],
        [-5.0781, -4.0781, -4.2812,  ..., -6.5000, -3.5312, -1.0010],
        ...,
        [-5.3320, -3.3477, -3.6602,  ..., -2.8477, -5.9258, -1.3643],
        [-5.1094, -4.6094, -5.0469,  ..., -5.1562, -6.8750, -0.4050],
        [-2.3301, -3.5488, -5.4219,  ..., -5.2500, -4.2188, -4.1250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([14, 27,  7,  7, 14, 10, 27, 18, 27, 25,  7, 20,  9, 22, 25, 27, 25, 27,
         9, 27, 18,  4, 27,  7, 13, 13, 13, 20, 20, 27, 27, 27, 27,  7, 25, 24,
        25, 27, 27, 14, 10, 27, 27,  3, 27,  7, 15, 18,  3, 10,  4, 27, 20,  5,
        27,  1, 27, 27,  1, 13, 27, 10, 27,  8], device='cuda:0')
step: 536
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6147,  ...,  0.4792, -0.0808,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4084, -0.2971, -1.7207,  ..., -0.6597,  0.6533,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.3709e-04,  1.8988e-03, -4.2057e-04,  ..., -9.9182e-04,
          7.2193e-04,  4.3225e-04],
        [-2.8753e-04, -9.2554e-04,  9.0897e-05,  ...,  2.0027e-03,
          7.6103e-04, -3.2783e-05],
        [-2.0564e-04,  5.8556e-04,  2.6965e-04,  ...,  2.0657e-03,
         -4.9067e-04, -1.0910e-03],
        [ 2.8515e-04, -2.2650e-04, -2.1286e-03,  ..., -2.5392e-05,
          5.2214e-04, -6.8712e-04],
        [ 2.0516e-04, -1.8144e-04, -2.4676e-04,  ..., -9.0539e-05,
          5.0211e-04,  2.8610e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0366, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.5137, -4.8867, -5.9023,  ..., -7.3242, -6.3242, -4.7930],
        [-5.4023, -3.9492, -3.7129,  ..., -4.0430, -5.1055, -0.4324],
        [-3.5801, -4.7812, -5.6250,  ..., -8.1250, -7.3750, -4.3906],
        ...,
        [-2.0723, -3.0566, -4.7773,  ..., -6.6523, -5.5117, -2.7910],
        [-1.4854, -4.0156, -5.2656,  ..., -6.1719, -2.0469, -2.5332],
        [-4.9453, -3.9141, -4.3828,  ..., -6.9297, -6.5234, -0.3054]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 22, 18, 26, 27,  3, 21, 27,  9, 27,  5, 27,  4,  7, 10,  8, 27, 26,
        20, 24, 27, 22, 27,  0,  0,  4, 26, 18,  7, 27,  0, 27,  2, 27,  3, 27,
         4,  7,  4, 27, 11, 27, 18, 27, 27, 13, 27,  0, 27, 19, 13, 10, 24,  4,
        27, 27, 14, 25,  0,  8, 27, 18, 27, 27], device='cuda:0')
step: 537
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6147,  ...,  0.4792, -0.0808,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2971, -1.7207,  ..., -0.6597,  0.6533,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.2178e-03,  9.2840e-04, -8.5735e-04,  ..., -5.2452e-04,
          5.9557e-04, -1.2755e-04],
        [-3.4118e-04,  2.8515e-04, -8.4591e-04,  ...,  1.1692e-03,
          1.8911e-03, -3.8409e-04],
        [ 5.3048e-06,  8.4209e-04,  4.1270e-04,  ..., -5.3024e-04,
         -8.5831e-04, -9.4056e-05],
        [ 1.1721e-03, -3.7241e-04, -1.9913e-03,  ..., -5.3108e-05,
         -6.7651e-05,  5.4979e-04],
        [ 4.1783e-05, -6.6137e-04, -2.8038e-04,  ...,  1.1468e-04,
         -4.4048e-05,  1.9276e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7428, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-7.1445, -4.0977, -4.2227,  ..., -5.2539, -3.5977, -0.5347],
        [-6.1328, -3.8809, -4.4258,  ..., -6.2539, -4.1445, -0.8184],
        [-5.1484, -2.6172, -2.9785,  ..., -4.6797, -2.4629, -1.0713],
        ...,
        [-3.7070, -4.6289, -5.1133,  ..., -6.1758, -5.5195, -0.1606],
        [-5.5000, -3.4375, -3.2969,  ..., -2.2500, -4.3750, -1.2344],
        [-3.6836, -3.4961, -4.0117,  ..., -3.9961, -4.2930, -0.5439]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  1, 24,  0, 27, 27,  4, 27,  6,  3, 27, 10, 27, 10,  0, 27, 10,
        11, 15, 27, 18,  0, 27, 15, 27,  0, 27, 12, 27,  4,  3, 27, 27, 27, 27,
        27,  0, 27, 27, 27,  5,  5, 15,  3, 27,  9, 22, 26, 14,  4, 14,  0,  4,
        27,  4, 15, 27,  5, 27, 18, 27, 25, 10], device='cuda:0')
step: 538
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6147,  ...,  0.4792, -0.0808,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2971, -1.7207,  ..., -0.6597,  0.6533,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0462e-03, -4.3058e-04, -1.5488e-03,  ...,  1.4365e-05,
         -1.3053e-05, -1.2207e-03],
        [ 3.3474e-04,  2.2793e-04,  2.6774e-04,  ..., -1.0842e-04,
         -2.5201e-04, -8.3780e-04],
        [ 9.8348e-05, -3.0065e-04, -2.9087e-04,  ...,  8.2350e-04,
          1.1468e-04, -6.4754e-04],
        [ 5.4359e-04,  4.1151e-04, -1.6413e-03,  ..., -7.1859e-04,
         -7.9346e-04,  2.2864e-04],
        [ 1.0937e-04,  1.9026e-04,  1.1420e-04,  ..., -5.5313e-05,
          2.3687e-04,  4.7183e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7720, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.8867, -4.6992, -4.8398,  ..., -3.5898, -6.8086, -3.4961],
        [-2.4238, -4.3320, -4.5820,  ..., -6.0820, -5.0977, -3.2363],
        [-6.2461, -3.6074, -3.6543,  ..., -4.0156, -5.0586, -0.7949],
        ...,
        [-6.0078, -3.4453, -3.8828,  ..., -4.1328, -5.4609, -1.2734],
        [-4.5469, -2.3438, -2.6719,  ..., -3.4375, -4.7344, -1.7656],
        [-6.1328, -4.0547, -4.5703,  ..., -5.1797, -4.8828, -0.3523]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 18, 10,  4,  0,  9, 27, 27,  1, 18,  4, 27, 27, 27, 27, 27,  1, 18,
        18,  4, 22, 10,  0, 27,  0, 13, 27,  4, 27, 27,  4, 27, 27, 27, 27,  2,
         2, 26, 27, 27, 18, 27, 18, 11,  3, 13, 27,  6, 27, 27, 12, 15, 16,  4,
         0, 20, 26,  0, 27,  3, 18, 10, 11, 27], device='cuda:0')
step: 539
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6147,  ...,  0.4792, -0.0808,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2971, -1.7207,  ..., -0.6597,  0.6533,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.2861e-04,  3.2401e-04,  1.7462e-03,  ...,  2.5535e-04,
         -6.6185e-04,  1.4572e-03],
        [ 1.6356e-04, -4.3344e-04,  8.3542e-04,  ...,  7.1585e-05,
         -1.0824e-03,  5.9748e-04],
        [-3.4237e-04, -4.2105e-04,  6.8188e-04,  ...,  4.5061e-04,
         -8.6880e-04, -3.9041e-05],
        [-5.3453e-04,  5.5194e-05,  1.4076e-03,  ...,  1.4484e-04,
          6.3896e-04,  8.3685e-04],
        [-3.4380e-04,  1.6665e-04,  6.6614e-04,  ..., -2.4176e-04,
         -1.1957e-04,  4.7040e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8375, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.4961, -3.1855, -2.7637,  ..., -4.1055, -4.2773, -1.4658],
        [-7.1250, -5.0625, -5.2500,  ..., -8.0469, -4.4844, -0.6260],
        [-4.8438, -3.6387, -3.5293,  ..., -3.7637, -4.8750, -0.9043],
        ...,
        [-4.3750, -4.0938, -4.7812,  ..., -6.6094, -6.1875, -0.1420],
        [-6.2070, -5.8477, -8.9219,  ..., -7.5508, -8.5156, -7.5195],
        [-1.2363, -3.0645, -5.4258,  ..., -7.1445, -4.6719, -2.1270]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 27, 22,  6, 27, 15, 25, 17, 27, 27, 27,  9, 18,  9,  6,  1, 20, 18,
        27, 17, 27, 11,  3,  3, 27, 27, 27, 27, 25, 15,  4, 22, 27,  1, 27, 27,
        15, 20, 27,  3, 15,  1, 27,  5, 27,  1, 18, 11, 27, 27, 27, 15, 27, 26,
         4,  1, 12, 17, 18, 27,  5, 27, 15,  0], device='cuda:0')
step: 540
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6147,  ...,  0.4792, -0.0808,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4084, -0.2971, -1.7207,  ..., -0.6597,  0.6533,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.3913e-04, -2.4796e-04, -1.4601e-03,  ..., -6.4468e-04,
          4.6945e-04, -5.3787e-04],
        [-5.8556e-04, -9.4235e-05, -3.3903e-04,  ..., -3.0088e-04,
          1.4009e-03, -3.2067e-04],
        [-1.2124e-04, -2.7037e-04, -3.8719e-04,  ...,  2.0003e-04,
          5.2214e-04, -3.6430e-04],
        [-9.9123e-05, -2.3663e-04,  6.1798e-04,  ...,  1.7726e-04,
          1.1806e-03,  3.2902e-04],
        [ 3.8314e-04, -2.2233e-05,  9.7334e-05,  ..., -1.0550e-05,
          5.4836e-04,  3.0017e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0649, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.2695, -2.9258, -4.9570,  ..., -7.1133, -4.7383, -2.7227],
        [-0.8760, -3.5469, -4.8125,  ..., -6.3906, -5.3125, -1.8916],
        [-4.6719, -3.0625, -3.6719,  ..., -5.7344, -4.3281, -0.4529],
        ...,
        [-4.4258, -3.4258, -5.4258,  ..., -6.5664, -7.2852, -1.3789],
        [-6.0703, -2.7402, -4.1953,  ..., -6.4922, -7.7422, -0.1945],
        [-3.6621, -3.4590, -2.9746,  ..., -1.5215, -3.5684, -1.8809]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  0, 27, 27, 27, 17, 27, 27, 27, 27, 27,  0, 25,  4, 27,  0,  5,  6,
        15,  2,  4, 15,  2,  7,  6, 14, 20, 17,  9, 27, 27,  1, 27,  1, 17, 27,
         7,  4, 26, 27, 10, 20, 24, 10,  6, 10, 18,  3, 27,  7, 27,  1, 27, 14,
        10, 17, 10, 15,  7,  9, 27,  0, 27, 25], device='cuda:0')
step: 541
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6147,  ...,  0.4792, -0.0808,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2971, -1.7207,  ..., -0.6597,  0.6533,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-3.4881e-04, -8.9765e-05,  1.0509e-03,  ...,  6.3992e-04,
         -4.4203e-04,  5.6219e-04],
        [-4.1842e-05, -2.6250e-04, -5.6624e-05,  ..., -1.2255e-03,
         -1.0376e-03,  1.0748e-03],
        [-3.8958e-04,  3.9291e-04,  7.1049e-05,  ..., -4.5300e-06,
         -7.4530e-04,  2.4271e-04],
        [ 4.5395e-04, -2.0707e-04,  4.1080e-04,  ..., -4.4644e-05,
         -3.7694e-04,  4.1938e-04],
        [-2.0027e-04,  6.5756e-04, -1.5736e-04,  ...,  4.1676e-04,
         -1.5473e-04,  1.2153e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8606, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.8867, -3.4355, -4.6523,  ..., -6.3398, -5.8242, -0.5122],
        [-5.7891, -4.2734, -4.9297,  ..., -5.5078, -5.8672, -0.2432],
        [-6.0000, -4.0156, -4.2031,  ..., -6.6094, -3.5625, -0.8594],
        ...,
        [-3.5234, -2.6328, -4.0859,  ..., -5.9297, -4.0078, -1.4141],
        [-3.0547, -3.2246, -3.3047,  ..., -3.5703, -5.4141, -0.8042],
        [-5.9062, -2.6562, -3.8281,  ..., -5.1875, -3.6719, -0.7344]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 10,  7, 15, 27,  4, 17, 27,  3,  0,  2, 20,  7, 27, 27, 27, 15,  0,
        27, 15, 17, 27,  4, 25,  4, 25,  1,  5, 10,  4, 27, 27, 27, 27, 18, 27,
         7, 27, 27, 27, 20, 27, 27, 25,  3,  4,  2, 25, 27,  0,  9,  1, 17, 22,
        27, 15,  7,  3, 15, 27, 10,  1,  4, 10], device='cuda:0')
step: 542
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6147,  ...,  0.4792, -0.0808,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2971, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.3041e-03,  3.1090e-03, -1.1902e-03,  ...,  1.1501e-03,
          1.5869e-03, -3.6740e-04],
        [-5.3167e-04, -1.2732e-03, -1.2836e-03,  ...,  7.4959e-04,
         -4.2200e-04,  1.5154e-03],
        [ 6.8963e-05, -4.2391e-04,  2.2650e-04,  ...,  1.2150e-03,
         -1.9908e-04,  6.0654e-04],
        [ 3.0403e-03, -2.7466e-04, -7.0839e-03,  ..., -3.4142e-03,
         -3.9196e-04, -1.4372e-03],
        [ 3.9792e-04, -5.4932e-04, -6.1274e-04,  ...,  8.3065e-04,
          3.4666e-04,  3.5572e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0672, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.6367, -2.8711, -2.6211,  ..., -3.1211, -3.3398, -1.1367],
        [-5.4336, -2.9961, -2.4023,  ..., -3.2148, -4.5898, -0.7925],
        [-5.0938, -3.3281, -4.4688,  ..., -6.4688, -4.9062, -0.4993],
        ...,
        [-6.2266, -3.0996, -3.0059,  ..., -5.1328, -5.1641, -1.1162],
        [-4.0430, -2.5762, -3.3418,  ..., -4.7500, -3.0449, -0.6538],
        [-1.8721, -2.7324, -4.1836,  ..., -4.0586, -3.8867, -1.2783]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0, 13, 27, 13, 27,  3, 27,  4, 15, 27, 17,  1, 27, 27,  1, 15, 27,
        18,  0, 20, 27,  1,  0, 20,  1, 27,  8,  9, 22, 15,  6, 27, 27, 11, 15,
        25, 18, 18,  5, 22, 27, 25, 24, 27, 20, 27,  0,  7, 27, 23, 27,  9, 27,
         8, 27, 19,  1, 27,  2, 27, 27, 27,  3], device='cuda:0')
step: 543
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6147,  ...,  0.4792, -0.0808,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2971, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.4121e-04, -7.8917e-05, -1.3285e-03,  ...,  3.8767e-04,
         -1.8895e-05, -8.7023e-04],
        [ 3.1328e-04, -8.2397e-04, -3.8600e-04,  ..., -4.3809e-05,
         -2.5921e-03, -4.0197e-04],
        [-1.1384e-05,  2.2328e-04, -8.2970e-05,  ...,  1.1158e-03,
         -3.3796e-05, -1.4038e-03],
        [-5.3787e-04, -1.8227e-04,  2.0142e-03,  ...,  1.2696e-05,
         -6.4564e-04, -3.3331e-04],
        [-9.8109e-05, -4.3201e-04,  7.3385e-04,  ...,  5.7316e-04,
         -4.9353e-04, -4.3154e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7607, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.4180, -3.1074, -3.5918,  ..., -3.0762, -5.2148, -1.0908],
        [-6.0859, -3.0547, -2.7109,  ..., -1.4131, -4.1641, -2.5078],
        [-4.9844, -3.1895, -3.7832,  ..., -5.5781, -3.6719, -0.9854],
        ...,
        [-5.5859, -2.6641, -2.7891,  ..., -3.5078, -4.5078, -1.3525],
        [-3.4023, -3.4492, -3.9336,  ..., -3.9648, -4.3711, -0.6826],
        [-5.1680, -3.3730, -3.8730,  ..., -5.4961, -4.4023, -0.4817]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 24, 10, 18, 27, 27, 15, 27, 17, 27, 27, 27, 22,  0,  3,  7,  1,  4,
         3, 10, 27, 22,  4, 27, 27,  3, 27,  3, 15, 27,  3,  4, 27, 12,  6, 27,
        18, 17, 27,  5, 27,  3, 10,  0, 15, 15,  4, 27, 27,  8,  0,  3,  8, 27,
        27, 26, 27, 27, 10, 15,  7,  3, 27,  4], device='cuda:0')
step: 544
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6147,  ...,  0.4792, -0.0808,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2971, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.0879e-04,  1.4114e-03,  7.3373e-05,  ..., -7.5245e-04,
          2.6035e-04, -1.1501e-03],
        [-2.8038e-04, -8.3923e-04, -8.7786e-04,  ..., -1.2851e-04,
          3.3069e-04,  7.4244e-04],
        [ 2.1338e-04,  1.2517e-05, -5.4312e-04,  ...,  7.6115e-05,
          3.6073e-04, -3.2973e-04],
        [ 7.7534e-04, -2.0444e-05, -2.7771e-03,  ..., -3.4392e-05,
          1.7452e-04,  1.6272e-04],
        [ 2.3830e-04,  2.2197e-04, -5.0449e-04,  ...,  1.0306e-04,
          3.1734e-04,  1.4305e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6907, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.8418, -3.3418, -4.5898,  ..., -6.5273, -4.4805, -0.4817],
        [-6.3945, -2.7383, -2.7695,  ..., -4.0820, -5.2070, -0.6133],
        [-3.0000, -3.6562, -4.1719,  ..., -7.1719, -6.1094, -2.3594],
        ...,
        [-5.3320, -3.3633, -3.1602,  ..., -1.9727, -3.6602, -1.7539],
        [-5.9688, -3.6719, -3.3594,  ..., -4.2656, -4.0000, -1.8584],
        [-5.0859, -2.8652, -2.6309,  ..., -1.6318, -3.2109, -1.7100]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22, 10,  8,  3, 27, 27, 27,  7, 27,  4, 27,  6, 20, 27, 27,  6, 18, 17,
        27, 27, 27, 15,  4,  7, 27,  0, 27, 10,  1, 26, 27, 15, 27, 27, 15, 27,
         2, 17, 10,  7, 11,  1,  0, 27, 17, 27, 18,  0, 27, 27, 27, 22,  5, 27,
        17, 14, 15,  7, 20, 27, 27,  3, 27, 24], device='cuda:0')
step: 545
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0808,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2971, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.2665e-03,  7.0000e-04, -9.9659e-04,  ..., -8.9109e-05,
          6.5756e-04, -8.4496e-04],
        [ 3.0088e-04, -8.7070e-04, -7.5579e-04,  ..., -3.1042e-04,
          3.3474e-04,  7.8869e-04],
        [ 6.5088e-04, -1.6987e-05, -2.6655e-04,  ..., -1.4009e-03,
          1.2636e-04,  2.0256e-03],
        [-6.9427e-04,  4.3225e-04, -1.8048e-04,  ..., -1.0651e-04,
         -3.9434e-04, -6.5279e-04],
        [ 2.6417e-04, -1.6975e-04, -5.2166e-04,  ...,  2.0480e-04,
          1.9717e-04, -2.2948e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7103, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.8203, -3.1191, -3.1816,  ..., -4.0742, -5.6680, -0.7603],
        [-3.0078, -3.3203, -5.6172,  ..., -7.1016, -4.1641, -3.9902],
        [-2.6055, -3.1055, -4.3867,  ..., -4.0117, -4.3242, -3.4629],
        ...,
        [-1.9805, -3.0586, -4.3711,  ..., -5.8867, -4.6055, -0.7153],
        [-2.9043, -4.4375, -6.9844,  ..., -7.3398, -5.8711, -5.0742],
        [-5.9141, -4.1484, -3.9141,  ..., -4.6953, -4.1484, -0.4146]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 15, 10,  4, 27,  9, 27,  0,  0, 15,  0, 27, 27, 14, 22,  0,  0,  7,
         1, 27, 21, 27,  2,  4,  7, 15, 27, 27, 10,  7,  7, 20,  1, 27,  1, 27,
        14, 27, 27,  0,  0, 27, 26, 27,  0,  0, 27,  8, 27,  1,  0, 27, 20, 15,
         9,  7, 22,  5, 27, 26, 17, 27, 15,  5], device='cuda:0')
step: 546
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0808,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2971, -1.7207,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.1975e-04,  7.0763e-04,  1.2612e-04,  ...,  5.4550e-04,
          3.3903e-04, -8.1348e-04],
        [ 6.7329e-04, -1.6594e-03, -2.7323e-04,  ...,  1.3514e-03,
         -1.1557e-04,  1.2341e-03],
        [-4.8542e-04,  7.5626e-04,  1.1885e-04,  ...,  1.8990e-04,
         -6.7663e-04,  3.4356e-04],
        [ 4.2057e-04, -2.7514e-04, -5.1355e-04,  ..., -3.7384e-04,
         -5.7518e-05,  3.8624e-05],
        [-1.8442e-04, -1.0509e-03,  1.7309e-04,  ...,  4.4048e-05,
         -4.9400e-04,  5.5647e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6775, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.4844, -4.0312, -4.4219,  ..., -6.7656, -4.3906, -1.1230],
        [-1.7383, -2.5020, -4.3789,  ..., -5.4883, -2.8164, -2.0488],
        [-3.4062, -4.0625, -5.1406,  ..., -7.0156, -5.7031, -0.4529],
        ...,
        [-6.3789, -2.2832, -2.3613,  ..., -4.3008, -4.3164, -1.2686],
        [-3.8281, -5.0469, -6.1562,  ..., -7.8438, -6.5781, -5.3906],
        [-4.6445, -3.1934, -3.6934,  ..., -6.0195, -5.0859, -0.4438]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0, 27, 27, 27,  9, 27, 27, 27, 27, 26, 18, 27, 15,  2, 27,  9,  3,
        18, 27,  7, 27, 27,  0, 27, 27, 27,  6, 15, 27,  2,  6,  0, 27, 24,  7,
         1, 27,  7, 10,  4,  7, 27,  0,  3,  4, 15,  9, 25, 27, 22,  5, 27,  4,
        20, 25, 27, 22, 27,  2, 27,  2, 18, 27], device='cuda:0')
step: 547
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0808,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2971, -1.7197,  ..., -0.6597,  0.6528,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.5954e-04,  3.5310e-04,  6.5517e-04,  ...,  9.6846e-04,
          1.5581e-04, -2.7156e-04],
        [-1.9991e-04, -2.5988e-05, -8.9931e-04,  ..., -2.1148e-04,
          8.9169e-04,  1.0605e-03],
        [-1.2153e-04,  1.3781e-03,  8.6975e-04,  ..., -6.8188e-04,
          4.6492e-05,  1.2932e-03],
        [ 4.4107e-04,  3.4690e-04,  4.8494e-04,  ...,  3.7050e-04,
         -5.5933e-04,  3.5357e-04],
        [ 1.5497e-05,  2.4188e-04, -1.6999e-04,  ...,  4.2963e-04,
          4.4966e-04, -1.8644e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7793, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.2764, -2.5898, -4.1523,  ..., -6.4805, -3.4805, -2.1348],
        [-5.7617, -3.0898, -3.5898,  ..., -5.0117, -4.3398, -0.5747],
        [-6.3438, -3.3105, -3.1855,  ..., -0.5605, -5.1406, -2.6387],
        ...,
        [-4.4375, -2.5488, -2.7832,  ..., -4.2031, -4.0938, -1.5332],
        [-4.1094, -3.6074, -5.0156,  ..., -6.7656, -5.4531, -1.0928],
        [-3.0215, -3.9121, -6.5195,  ..., -6.6133, -5.3789, -3.0684]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1,  3, 25, 27, 22,  3,  0, 27,  3,  8, 27,  8, 27, 27, 27, 27, 27,  0,
        10, 13,  6, 27, 27, 11, 27, 13, 27,  2, 11,  9,  3, 27,  0, 23,  4, 15,
         0, 15, 24, 18, 18, 27, 27, 27, 13, 12, 27,  7,  2, 27,  4, 14,  5, 22,
        18, 20,  6, 10, 27, 27, 27, 27, 27,  0], device='cuda:0')
step: 548
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0166e-03, -9.3555e-04, -1.2903e-03,  ...,  3.1686e-04,
         -2.7132e-04, -6.7616e-04],
        [-9.4652e-04,  2.2173e-04, -3.1281e-04,  ...,  2.3890e-04,
          5.7364e-04,  1.0920e-03],
        [-2.9922e-04, -3.8481e-04, -1.2579e-03,  ...,  1.8334e-04,
          2.5988e-04, -1.9407e-03],
        [ 1.6947e-03,  2.3961e-04, -2.7122e-03,  ..., -1.5745e-03,
         -1.2693e-03, -1.6479e-03],
        [ 1.9705e-04,  8.6963e-05, -7.5531e-04,  ...,  2.0695e-04,
          5.4836e-06, -3.1829e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8591, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.5059, -3.6152, -5.1172,  ..., -3.9746, -3.5371, -1.2725],
        [-4.3203, -4.4297, -8.3203,  ..., -7.9922, -6.8359, -6.2109],
        [-5.9609, -2.9609, -3.4160,  ..., -4.2422, -5.5859, -1.3213],
        ...,
        [-3.5781, -3.3594, -3.4375,  ..., -6.4531, -4.6875, -0.8130],
        [-2.9238, -3.6426, -4.8320,  ..., -6.4727, -5.1758, -0.5337],
        [-6.3047, -2.9590, -3.5059,  ..., -5.6328, -3.3340, -0.8970]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 15, 27,  0, 27,  0, 26, 13, 27,  4, 15, 27,  7, 27,  7, 27, 10, 27,
         0, 14, 15,  3,  0,  4, 15,  7, 13, 10, 24,  4,  2, 27, 27, 27, 17, 15,
        27, 12, 25, 18, 15, 27,  0, 15, 14,  4, 26, 20, 15,  8, 27,  5, 26, 22,
        18, 27,  0, 27,  0, 20,  6, 27, 27, 27], device='cuda:0')
step: 549
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4087, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2666],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.2131e-03,  8.1062e-04, -1.2035e-03,  ...,  8.7500e-04,
          1.1702e-03, -3.8433e-04],
        [ 1.2484e-03, -3.5133e-03, -9.3412e-04,  ..., -1.5306e-03,
         -1.2579e-03,  2.5864e-03],
        [ 7.4291e-04, -1.0548e-03, -6.2656e-04,  ..., -2.3136e-03,
         -1.0128e-03,  1.1053e-03],
        [-2.5511e-04, -2.6846e-04, -1.8387e-03,  ...,  4.1890e-04,
         -2.2483e-04, -4.8065e-04],
        [-1.3185e-04, -8.8167e-04, -6.8128e-05,  ..., -2.9016e-04,
         -1.3781e-04,  2.5964e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6601, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9609, -2.8359, -3.2891,  ..., -5.6797, -3.8027, -0.4912],
        [-3.9297, -3.9609, -4.7422,  ..., -6.6172, -7.2734, -0.3364],
        [-0.4226, -3.0938, -5.2500,  ..., -7.1562, -3.7500, -3.6895],
        ...,
        [-5.2930, -3.7930, -3.8711,  ..., -4.1836, -6.5117, -0.5107],
        [-6.6367, -3.2949, -3.5762,  ..., -5.4805, -4.9961, -0.7163],
        [-5.6797, -4.4453, -4.6328,  ..., -7.1484, -6.9922, -0.0867]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26,  4,  0, 20, 22, 27,  4, 27,  3, 27, 27,  5,  0, 18, 27, 27,  0, 27,
        15,  8, 27,  4, 27, 18,  7, 27, 27,  0,  0, 11, 27,  0, 27, 26,  0,  4,
         6, 11,  6,  2,  1, 27, 17,  3, 18, 25, 25,  0, 27,  9,  7, 27, 27, 26,
        27, 27, 17, 26, 27, 18,  4, 27,  1, 27], device='cuda:0')
step: 550
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.2953e-03,  9.2649e-04, -4.0627e-03,  ..., -2.7637e-03,
         -3.3402e-04,  1.2112e-03],
        [-6.8092e-03, -5.3406e-04,  3.3836e-03,  ...,  2.1324e-03,
          8.0795e-03, -2.1667e-03],
        [-1.6537e-03,  3.6836e-04,  1.9007e-03,  ..., -2.6836e-03,
          4.6301e-04,  3.6860e-04],
        [-2.4867e-04, -5.0831e-04, -3.1490e-03,  ..., -3.2663e-04,
          8.9216e-04, -2.5964e-04],
        [ 8.9216e-04, -3.1769e-05, -1.7047e-05,  ..., -1.3924e-04,
          3.7313e-05,  3.3259e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.4799, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.5156, -4.6406, -5.6562,  ..., -8.2500, -6.3438, -5.3750],
        [-5.7109, -3.3359, -4.0234,  ..., -5.4141, -3.7422, -0.8193],
        [-7.5352, -5.9570, -5.9844,  ..., -0.0485, -6.6406, -4.6602],
        ...,
        [-4.3242, -3.8867, -4.6992,  ..., -5.6992, -2.1211, -1.5420],
        [-4.4375, -3.7949, -5.1406,  ..., -5.8906, -5.2969, -4.7344],
        [-2.5996, -2.8340, -3.6152,  ..., -4.6133, -2.7559, -1.2559]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 10, 25, 27,  7,  0,  7, 15, 15, 22, 18, 15,  0,  4, 18, 25,  1, 15,
        27,  7, 27, 17, 27, 14,  4,  7, 21, 27, 20, 14, 18, 27, 27,  1, 15,  3,
         0, 27, 27, 15, 15, 27,  0, 22,  0, 15,  4,  7,  4, 27, 25, 27, 27, 27,
        27,  0,  0, 24, 27,  7, 10, 26, 20, 27], device='cuda:0')
step: 551
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.1233e-04,  1.0967e-03,  3.1304e-04,  ...,  8.2636e-04,
          7.2241e-04, -1.9860e-04],
        [-2.5845e-04, -8.7214e-04, -7.7486e-05,  ...,  7.3433e-05,
          1.7405e-04,  1.5998e-04],
        [ 1.3733e-04,  1.6880e-04, -1.5378e-04,  ..., -9.3412e-04,
         -2.1076e-04,  3.0732e-04],
        [ 1.1578e-03,  3.4690e-04, -2.6875e-03,  ..., -1.1873e-03,
         -1.2894e-03, -4.0579e-04],
        [-1.3280e-04, -5.4932e-04,  1.3995e-04,  ...,  2.5630e-05,
          3.3426e-04,  3.6383e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7402, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.4727, -3.9746, -4.1445,  ..., -5.1602, -6.4570, -0.1934],
        [-4.6406, -3.0781, -4.1250,  ..., -5.5938, -4.5938, -0.2351],
        [-4.5625, -3.6738, -4.6562,  ..., -6.3125, -4.5156, -0.3452],
        ...,
        [-4.0469, -2.7012, -3.6250,  ..., -4.7812, -3.9688, -0.6558],
        [-4.0352, -4.0352, -5.0977,  ..., -3.8633, -4.1445, -2.8164],
        [-4.3633, -2.5371, -3.0215,  ..., -5.0195, -3.0840, -1.0215]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2, 27, 27, 24,  4, 17, 15, 27, 27, 27,  4, 22, 17, 20, 27, 20,  3, 27,
        17, 14, 11, 17,  6,  4, 27, 25, 27, 18, 20, 18,  7,  1, 27,  2, 18,  0,
        22,  7, 17, 26,  3,  8, 27,  7, 27,  1,  0,  2, 27,  8, 27, 20,  1, 27,
         7,  2,  1, 24, 15, 27, 27, 27, 17, 27], device='cuda:0')
step: 552
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.0862e-04,  1.5984e-03,  1.2827e-03,  ..., -9.5320e-04,
         -8.0204e-04,  6.3038e-04],
        [-7.9870e-05,  4.3082e-04, -1.4186e-04,  ...,  2.5105e-04,
         -1.8954e-04,  3.1614e-04],
        [-4.2105e-04, -1.9646e-04,  1.0097e-04,  ...,  6.3944e-04,
         -5.3310e-04,  6.2943e-04],
        [ 4.8041e-05,  4.0722e-04,  9.4032e-04,  ...,  2.3234e-04,
          3.5429e-04,  1.5092e-04],
        [-2.1529e-04,  4.1699e-04,  4.5848e-04,  ...,  2.7776e-05,
         -4.5085e-04,  2.9111e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8357, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4453, -2.6348, -3.0723,  ..., -2.1660, -4.7266, -2.0410],
        [-5.5508, -2.8633, -2.3008,  ..., -4.8789, -5.2227, -0.8174],
        [-4.5859, -2.6504, -3.4473,  ..., -5.2266, -3.9941, -0.9463],
        ...,
        [-5.4883, -3.0352, -3.0352,  ..., -4.5977, -5.2383, -1.1602],
        [-2.1387, -3.1074, -4.0781,  ..., -5.7500, -4.0469, -0.7646],
        [-4.0391, -4.5859, -5.2422,  ..., -7.2266, -4.7422, -0.4922]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 9,  5, 10, 27, 27, 27, 27, 24, 11, 10,  4, 25, 27, 27, 14,  2,  4,  7,
        20, 25, 27,  0, 15,  3, 17,  4,  4, 27, 12, 23, 26, 27, 18,  9, 26, 27,
        18,  4, 12, 27,  7, 27, 26, 27, 20,  4, 10, 13, 27, 27, 25, 27, 27,  1,
         0, 11, 27, 11,  3, 18, 15, 10, 27, 27], device='cuda:0')
step: 553
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.9496e-04,  7.6628e-04, -1.3113e-06,  ..., -1.6785e-03,
          1.6809e-04,  2.3890e-04],
        [-1.4353e-04,  1.2124e-04,  1.2722e-03,  ...,  2.7962e-03,
          1.1034e-03, -1.0118e-03],
        [-5.7161e-05,  2.7359e-05,  1.0204e-04,  ...,  3.2568e-04,
          2.3675e-04, -7.6151e-04],
        [-3.0446e-04, -2.8062e-04,  1.5249e-03,  ...,  5.4121e-04,
         -8.1253e-04, -3.1471e-05],
        [-6.1214e-05,  5.9891e-04,  6.0499e-05,  ..., -8.8036e-05,
          2.7442e-04, -5.6684e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2142, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.3125, -3.6406, -4.1250,  ..., -5.2500, -6.4531, -0.3120],
        [-4.1211, -3.2441, -4.7305,  ..., -6.2148, -5.6055, -0.7139],
        [-3.4961, -3.4512, -3.4355,  ..., -3.1680, -5.2930, -1.8096],
        ...,
        [-5.1680, -3.0117, -3.2305,  ..., -4.4805, -5.4180, -0.7148],
        [-1.1562, -4.4062, -5.4219,  ..., -5.5000, -4.1875, -3.5000],
        [-5.5586, -3.7793, -4.2930,  ..., -6.2344, -3.2168, -1.8887]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25, 27, 10, 27,  2, 10, 27,  6, 18, 27, 17, 27, 27, 13,  3,  0,  4, 27,
         0, 10, 17, 22, 27, 25,  4, 27, 27, 18,  2, 20, 10, 27, 25, 22,  0, 22,
         7,  5, 27,  0, 15, 22, 27,  3,  2, 17, 15,  2,  1, 27,  0, 15, 22,  7,
         7, 27, 27, 27, 22, 27,  9, 25,  5,  6], device='cuda:0')
step: 554
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.2646e-03,  6.7425e-04,  5.9175e-04,  ...,  1.2121e-03,
         -5.6267e-04, -1.1677e-04],
        [-3.4738e-04,  2.4974e-05,  5.1355e-04,  ..., -2.3699e-04,
         -7.9346e-04,  9.5272e-04],
        [-4.7517e-04,  1.4124e-03,  1.3514e-03,  ...,  5.6028e-05,
          3.0851e-04,  9.2220e-04],
        [ 3.3331e-04,  3.3355e-04,  1.4877e-03,  ...,  6.4468e-04,
          4.2915e-04,  6.6185e-04],
        [-2.8110e-04,  4.1151e-04,  4.1914e-04,  ...,  2.2054e-04,
         -3.0422e-04,  1.9908e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9143, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.9805, -4.8086, -5.0586,  ..., -7.3711, -3.9336, -1.7305],
        [-2.3672, -3.4297, -4.1953,  ..., -4.4141, -4.0234, -0.5239],
        [-5.1953, -3.1621, -2.5684,  ..., -3.0078, -4.2266, -1.3193],
        ...,
        [-3.8633, -3.0039, -4.3320,  ..., -6.4570, -4.3008, -0.4094],
        [-3.2070, -2.0664, -1.9727,  ..., -4.0039, -2.4414, -2.2852],
        [-4.5859, -3.1621, -3.0059,  ..., -3.1309, -3.8184, -1.2100]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6,  1,  9,  9,  0, 11, 27, 14, 27,  3, 27, 20,  5,  0, 14, 14,  0,  7,
        27, 10, 10, 27,  0,  3,  9, 27, 22, 27, 10, 27, 27,  7,  7,  1,  4,  5,
         0, 27, 10,  7, 26, 27,  0,  7, 27, 15, 17, 15,  1, 27,  0, 20, 27, 17,
        27, 12, 27, 27, 27,  5,  0, 27, 27,  3], device='cuda:0')
step: 555
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.2439e-04,  8.9025e-04,  5.7364e-04,  ..., -9.7179e-04,
         -3.4630e-05, -4.8018e-04],
        [ 1.7958e-03,  3.3319e-05,  5.8746e-04,  ...,  2.8191e-03,
          6.8331e-04, -5.6314e-04],
        [ 7.3957e-04, -1.2422e-04, -1.8656e-04,  ..., -6.1083e-04,
         -7.1704e-05, -7.5936e-05],
        [-1.9360e-04,  1.4734e-04,  3.5214e-04,  ..., -3.6097e-04,
         -3.2425e-04,  5.5170e-04],
        [ 3.7694e-04, -3.6621e-04, -3.6597e-05,  ..., -3.6573e-04,
          2.7537e-05, -1.4830e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7532, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.7207, -3.3145, -5.0664,  ..., -6.5664, -3.6426, -2.1738],
        [-2.6914, -3.8789, -4.5820,  ..., -5.7383, -5.2539, -0.7222],
        [-3.1074, -2.3418, -2.6875,  ..., -3.5156, -3.5781, -1.6084],
        ...,
        [-4.7852, -3.9570, -4.0195,  ..., -4.7852, -4.7383, -0.3789],
        [-3.4707, -3.4707, -2.0645,  ..., -2.3613, -4.3633, -3.3301],
        [-3.4219, -2.6113, -2.8613,  ..., -3.0020, -2.5488, -1.9707]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 20, 27, 27, 27,  7, 27,  3,  4, 27, 10, 27, 15, 27, 26, 26, 25, 27,
         5,  3,  0, 10, 27, 18, 18, 13,  3, 27,  7, 22, 15,  4, 20, 27, 27, 27,
         0,  4, 27, 27,  1, 25, 27, 27,  0, 27, 22,  7,  2, 21, 24,  3, 27, 27,
        27, 27, 14,  9, 22, 27, 27, 27, 20, 27], device='cuda:0')
step: 556
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2208,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.8752e-04, -2.5043e-03, -1.9341e-03,  ..., -6.0678e-05,
         -1.1086e-04, -1.1282e-03],
        [-1.3280e-04,  5.0592e-04,  1.9979e-04,  ...,  5.4169e-04,
          7.9584e-04,  2.5105e-04],
        [ 1.7047e-04, -5.6088e-05, -3.5667e-04,  ..., -3.4213e-04,
          3.2091e-04, -6.1941e-04],
        [ 3.8767e-04,  4.8971e-04, -3.8910e-04,  ..., -7.7295e-04,
         -4.8399e-04,  3.8552e-04],
        [-6.5804e-05, -5.6207e-05, -4.8757e-04,  ...,  3.9291e-04,
          1.5008e-04,  5.9009e-06]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8231, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.3936, -3.2539, -5.0352,  ..., -6.5039, -5.8945, -2.3320],
        [-1.3076, -1.8535, -3.0879,  ..., -2.7441, -4.8242, -3.5566],
        [-2.7773, -3.2461, -4.0273,  ..., -4.8398, -3.7461, -0.6055],
        ...,
        [-6.1328, -3.5547, -3.1953,  ..., -4.5078, -4.3984, -0.7891],
        [-4.7539, -5.0664, -7.4414,  ..., -7.4102, -7.1602, -6.3320],
        [-3.5488, -2.8457, -3.5801,  ..., -4.8750, -3.5176, -0.5796]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10,  0,  1, 27, 27,  6, 13,  4, 20,  4, 27,  0, 20,  3,  4, 10, 27, 10,
        14, 15, 13, 27, 27, 17, 27,  9, 25, 27,  4, 27, 27, 11, 17, 27, 25, 27,
        27, 25, 27, 27, 27,  4, 27, 27,  0,  4, 26, 25,  4,  2,  1, 27, 26, 26,
        27, 27,  4,  6, 27, 27, 27, 27, 15,  1], device='cuda:0')
step: 557
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6558,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.4572e-03, -1.1606e-03,  8.1158e-04,  ..., -2.5368e-04,
         -5.4836e-05,  1.4381e-03],
        [-3.0541e-04,  1.4076e-03, -3.2187e-04,  ..., -1.4591e-03,
          6.7711e-04,  2.2449e-03],
        [ 8.0872e-04,  7.1669e-04, -2.7776e-04,  ..., -6.2227e-04,
          5.6028e-04,  2.2030e-04],
        [ 1.1711e-03,  2.0528e-04, -1.2350e-03,  ..., -3.1066e-04,
         -1.7571e-04,  4.5371e-04],
        [ 3.9625e-04,  6.5994e-04, -8.2111e-04,  ..., -5.5552e-05,
          3.3450e-04,  1.5473e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0276, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.1875, -4.8438, -4.9531,  ..., -7.5000, -4.8594, -2.2500],
        [-4.5156, -3.0137, -3.4043,  ..., -4.2812, -4.6719, -0.8735],
        [-4.7773, -3.4648, -3.8398,  ..., -4.5117, -5.0586, -0.8096],
        ...,
        [-3.8555, -3.1367, -3.4824,  ..., -4.3555, -3.3105, -0.7314],
        [-3.0508, -2.0664, -3.2539,  ..., -4.3164, -3.1445, -1.6123],
        [-0.3174, -4.1289, -6.8008,  ..., -8.3203, -5.8008, -3.5996]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7,  3, 27, 27, 27, 25, 11, 27, 27, 17, 27, 10,  5, 27, 22, 10, 20, 10,
         7, 27, 27, 27, 22,  4,  1,  3, 27, 18, 18,  6, 27, 18, 23, 18, 27, 27,
         2, 13, 27, 25, 11, 13, 27, 27, 10,  4, 27, 10, 27,  7,  0, 20, 27, 27,
        25, 27, 27,  6,  6,  0,  4, 10,  1,  0], device='cuda:0')
step: 558
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.1187e-05, -6.6376e-04, -2.0719e-04,  ...,  4.2748e-04,
          1.1981e-04, -6.1750e-05],
        [-1.5640e-03, -7.0524e-04, -7.8773e-04,  ..., -3.3360e-03,
         -2.2812e-03,  2.5082e-03],
        [ 1.2541e-04,  2.9802e-05, -2.9659e-04,  ...,  5.6171e-04,
          4.1318e-04, -1.7905e-04],
        [ 2.6059e-04,  2.3758e-04, -2.3985e-04,  ..., -9.6750e-04,
         -3.6621e-04, -8.9288e-05],
        [-3.3927e-04,  2.5249e-04,  6.5565e-06,  ..., -1.6057e-04,
         -1.5581e-04,  4.0245e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6904, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.3203, -4.4766, -4.5234,  ..., -5.3203, -6.1172, -0.1957],
        [-2.7715, -3.8496, -4.9102,  ..., -6.0820, -3.5527, -0.8018],
        [-5.1992, -3.0273, -2.8086,  ..., -4.8555, -4.9180, -0.6216],
        ...,
        [-1.6660, -3.0723, -5.4453,  ..., -6.0547, -2.8379, -2.6504],
        [-5.4648, -3.1680, -4.3711,  ..., -5.5273, -2.1055, -1.3555],
        [-4.3750, -3.4980, -3.8262,  ..., -5.1719, -3.3105, -0.7490]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  5, 20, 27, 27,  7, 27,  8, 27, 27, 27, 20, 27, 10, 27, 27,  0,
        27, 15, 27,  8, 22, 22,  8, 11, 27,  2,  9, 17,  7,  9, 18,  9, 27, 17,
        27, 22, 14, 20, 15, 27, 27, 18, 11,  4,  0,  7, 27, 18, 27, 18, 23, 27,
         7, 20, 27, 27, 15, 18, 27,  0,  7, 27], device='cuda:0')
step: 559
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.0783e-04,  5.0402e-04,  2.1324e-03,  ..., -1.0478e-04,
          3.0518e-04,  7.9966e-04],
        [-6.0737e-05,  4.6849e-04, -1.1473e-03,  ..., -1.8997e-03,
          1.3933e-03,  1.2712e-03],
        [ 1.4162e-04,  5.4789e-04,  8.2850e-06,  ..., -6.7091e-04,
          6.8092e-04,  3.8385e-04],
        [ 6.3598e-05,  5.9795e-04,  8.4877e-04,  ...,  2.9564e-05,
          1.5521e-04, -4.9734e-04],
        [-9.5308e-05, -7.0715e-04,  4.3654e-04,  ..., -1.6046e-04,
         -3.5930e-04,  9.3818e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6433, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.7461, -5.7305, -5.6055,  ..., -4.9961, -6.8555, -4.0898],
        [-4.4297, -3.4766, -4.4922,  ..., -3.0723, -4.8359, -0.4771],
        [-4.8750, -2.7031, -2.9219,  ..., -1.4062, -5.0469, -2.0156],
        ...,
        [-2.9609, -3.0840, -4.4453,  ..., -4.9141, -1.0068, -1.9756],
        [-4.6680, -2.8105, -3.3887,  ..., -2.9043, -4.7773, -1.4512],
        [-4.0859, -4.8672, -8.3516,  ..., -6.9766, -6.7578, -6.9141]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([11,  4,  9, 17, 22, 18,  1, 27, 17, 25, 27,  8, 15, 27, 27, 15, 27,  3,
         7, 25, 27,  1,  0,  7, 22, 17, 27,  0, 27, 21, 27, 27,  2,  0,  2, 25,
        27, 27,  1, 27, 27, 27, 27, 27,  7,  0, 27, 27, 15,  2,  0, 10, 27,  4,
        15, 17, 27,  1, 27, 20, 10, 26, 11, 15], device='cuda:0')
step: 560
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6030],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.2275e-04, -1.1177e-03,  1.7242e-03,  ...,  1.6689e-03,
         -6.4373e-04,  7.0477e-04],
        [-4.1485e-04,  9.4509e-04, -6.0511e-04,  ..., -4.9324e-03,
         -1.9150e-03,  9.7275e-05],
        [ 1.7858e-04,  9.8348e-05, -1.6022e-04,  ..., -5.8079e-04,
         -3.6645e-04,  6.3658e-04],
        [-1.3142e-03, -7.3957e-04,  2.2717e-03,  ...,  1.1206e-03,
         -1.0937e-04,  9.5654e-04],
        [-3.0041e-04, -3.6335e-04,  6.3896e-04,  ..., -3.5942e-05,
         -6.5708e-04, -2.3723e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9961, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.1152, -3.1777, -4.5234,  ..., -7.2266, -5.1914, -0.9438],
        [-3.7949, -3.1230, -4.1367,  ..., -6.0625, -3.3574, -1.0918],
        [-6.7188, -2.6094, -2.1562,  ..., -2.0938, -3.8906, -1.6094],
        ...,
        [-6.0273, -4.3711, -4.6367,  ..., -5.4336, -7.7305, -0.2766],
        [-3.2695, -4.0508, -4.4414,  ..., -5.4727, -3.2988, -1.9404],
        [-2.5410, -3.6328, -3.8828,  ..., -6.1172, -4.9609, -0.9775]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  1,  6, 27,  1, 27, 25, 27,  7,  5, 15,  0,  3, 27,  7, 22, 27,  2,
         0, 15, 23,  1, 14, 11, 27, 12, 27,  9, 20,  8, 25,  6, 27,  7, 27, 27,
         0,  4,  3, 27, 24, 18, 27,  4, 27,  4, 22,  1,  5, 14, 27, 27,  5, 27,
        17,  3, 24,  7,  5,  7, 27, 27, 27, 27], device='cuda:0')
step: 561
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.1572e-05, -2.1629e-03,  5.6219e-04,  ..., -1.1549e-03,
         -6.8998e-04, -1.0681e-03],
        [ 4.2129e-04, -4.6968e-05,  1.1320e-03,  ...,  3.3045e-04,
          2.9516e-04, -1.4048e-03],
        [ 1.2851e-04,  5.7554e-04,  8.6975e-04,  ..., -7.9393e-04,
         -1.2541e-04,  1.0824e-03],
        [-1.1146e-05, -1.6289e-03,  1.9426e-03,  ...,  9.5892e-04,
          1.8978e-03, -2.1248e-03],
        [ 3.6180e-05,  2.5606e-04, -2.4796e-04,  ...,  2.6727e-04,
         -4.4775e-04, -8.1587e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7135, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.8633, -4.0039, -4.3164,  ..., -5.0352, -4.0508, -0.6289],
        [-4.3242, -2.6816, -3.6816,  ..., -3.9473, -3.3223, -1.1348],
        [-4.8750, -4.5938, -5.4844,  ..., -7.7969, -7.6875, -0.0936],
        ...,
        [-4.6016, -4.6953, -5.2578,  ..., -5.6641, -5.3984, -0.1804],
        [-0.5254, -4.2734, -5.7422,  ..., -6.2578, -1.9629, -2.6973],
        [-3.2949, -2.6855, -3.5137,  ..., -4.3750, -4.2148, -0.5918]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27, 27, 18,  8, 27, 20, 27, 27, 25,  3,  4, 15, 27, 27, 27, 27,
         1, 18, 15,  0, 15,  0, 10, 27, 27,  7, 17,  0,  3, 24, 17,  3, 23, 17,
         4, 27, 25, 27,  6,  9, 27, 17, 27, 27, 10,  9, 10, 15, 27, 10, 27,  1,
        18,  7, 26,  1, 24, 11, 18, 27, 26, 27], device='cuda:0')
step: 562
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.9543e-04,  5.0163e-04, -5.4073e-04,  ...,  1.1492e-04,
          2.7370e-04,  1.0147e-03],
        [-4.1628e-04, -1.0147e-03, -6.7043e-04,  ...,  2.4772e-04,
         -2.1613e-04,  7.6342e-04],
        [-4.8637e-05, -9.2554e-04, -2.9421e-04,  ...,  5.1928e-04,
          3.9673e-04,  3.2842e-05],
        [ 1.0281e-03, -7.9441e-04, -7.8773e-04,  ..., -1.9884e-04,
         -7.1955e-04, -5.2738e-04],
        [ 2.7132e-04,  8.0156e-04, -4.3344e-04,  ...,  2.5320e-04,
          4.2677e-04,  3.5524e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2627, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.9961, -3.7480, -3.4199,  ..., -4.4648, -6.5742, -0.4817],
        [-1.5156, -3.9062, -5.5312,  ..., -6.3438, -5.6094, -0.7808],
        [-4.6016, -3.0703, -3.0078,  ..., -4.0234, -3.1641, -0.8208],
        ...,
        [-4.4180, -2.6055, -3.3242,  ..., -4.3711, -4.1055, -0.6064],
        [-0.5278, -4.0273, -5.6211,  ..., -6.4805, -2.4180, -2.8867],
        [-4.7305, -5.0273, -8.5625,  ..., -8.0781, -7.3711, -5.9023]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 26,  7,  1, 23, 27, 27, 18,  1, 27, 27,  6, 11,  5, 27, 17,  0,
         0,  2, 27, 27, 18, 15, 27, 27, 26, 27, 25, 15,  0, 18, 15,  3, 20, 27,
        22, 27,  7, 20, 12, 10, 27, 24, 27,  2, 10, 18,  1, 27,  1,  3, 19, 27,
        25, 27,  9, 20, 13, 15,  5, 11,  0, 15], device='cuda:0')
step: 563
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.0858e-04,  6.4516e-04,  8.3160e-04,  ..., -3.1519e-04,
         -1.8406e-04,  1.8156e-04],
        [ 1.3418e-03, -1.5841e-03,  5.5027e-04,  ...,  1.1665e-04,
         -1.3895e-03, -1.6475e-04],
        [ 6.2704e-05, -1.0414e-03,  8.7643e-04,  ...,  1.1814e-04,
         -1.3103e-03,  6.0034e-04],
        [-1.2426e-03, -2.8419e-04,  9.2125e-04,  ...,  3.7217e-04,
          3.1829e-04, -1.1474e-04],
        [-1.1659e-04,  1.2994e-04,  8.9169e-04,  ...,  1.5855e-05,
         -3.0017e-04,  5.9509e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0811, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4258, -5.1602, -3.9883,  ..., -4.7695, -6.2539, -0.1752],
        [-5.8906, -5.1719, -5.3594,  ..., -7.9961, -4.7500, -0.9355],
        [-3.6387, -4.4492, -3.6230,  ..., -4.5312, -6.0742, -1.8877],
        ...,
        [-2.9570, -3.2852, -4.0195,  ..., -4.3477, -6.1602, -0.7534],
        [-4.6133, -3.2246, -3.5215,  ..., -5.0820, -5.7070, -0.3804],
        [-4.3789, -4.0508, -4.5352,  ..., -5.9414, -5.4883, -0.4739]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25, 27,  8, 13,  1, 27, 17, 27, 22, 11, 27, 15, 15,  5, 25, 17,  0, 27,
        27, 10, 27, 27, 27, 27, 27,  1, 13, 20, 15, 22,  8, 27,  7, 16, 14, 22,
        22, 27,  4,  1, 27, 27, 27, 14, 27,  7, 26, 22,  4,  3,  6, 10, 27, 27,
        11, 17,  3,  3, 22, 27,  3, 10, 27, 27], device='cuda:0')
step: 564
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.3787e-04,  8.0729e-04, -4.9829e-04,  ..., -4.8423e-04,
         -3.6144e-04,  7.6675e-04],
        [ 5.2118e-04, -1.5342e-04,  1.6499e-04,  ...,  4.2076e-03,
          1.6031e-03, -1.1692e-03],
        [-2.5558e-04, -5.0926e-04,  3.2806e-04,  ...,  5.8270e-04,
          4.5228e-04,  3.6430e-04],
        [ 1.0529e-03, -5.8508e-04, -4.7636e-04,  ..., -5.4598e-04,
          1.8656e-05, -1.5831e-04],
        [ 3.3689e-04,  4.3440e-04, -1.2803e-04,  ...,  3.0339e-05,
          1.2541e-04,  8.1956e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1269, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2070, -4.5039, -4.8789,  ..., -5.7227, -4.6758, -0.1296],
        [-1.3457, -2.8457, -5.1406,  ..., -6.8633, -5.0977, -2.8457],
        [-5.3594, -3.5469, -3.8438,  ..., -4.7344, -5.2188, -0.6709],
        ...,
        [-5.5195, -2.9746, -2.9277,  ..., -2.1934, -4.6289, -1.4111],
        [-1.8369, -3.1660, -5.0391,  ..., -7.4297, -6.4766, -2.3691],
        [-6.0469, -3.8418, -4.1523,  ..., -4.9336, -6.1094, -0.2325]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26, 18, 10,  4, 26, 12,  0, 27, 18,  4, 10, 26, 27, 27,  8,  7, 27, 15,
        27, 27,  5,  5,  5, 14,  2, 27, 27,  2,  8, 11,  3,  4,  4, 25, 27,  0,
        18, 15, 14,  0,  1, 20, 18, 21,  2,  0,  0, 27,  7, 10, 27, 15, 20, 27,
        18, 12,  7,  1,  9, 10, 27, 24, 18, 27], device='cuda:0')
step: 565
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.0797e-04,  1.2226e-03, -3.8576e-04,  ...,  1.6012e-03,
          3.8862e-04,  8.2636e-04],
        [-1.6069e-04, -7.5388e-04, -1.3599e-03,  ...,  2.3155e-03,
          1.0014e-03,  8.4114e-04],
        [ 3.6860e-04, -1.0335e-04, -4.6229e-04,  ...,  8.9073e-04,
         -6.4373e-04,  9.7930e-05],
        [ 1.1311e-03,  9.7609e-04, -2.9716e-03,  ..., -7.4196e-04,
         -3.0041e-04,  2.3270e-04],
        [ 3.8564e-05,  1.0223e-03, -8.0299e-04,  ...,  8.9788e-04,
          5.2643e-04,  4.7231e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9235, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.6997, -3.6367, -5.2930,  ..., -6.2148, -4.9180, -1.2461],
        [-3.8711, -4.0742, -4.2773,  ..., -6.1055, -3.8555, -1.0430],
        [-1.5146, -2.9668, -5.5781,  ..., -5.0625, -3.7012, -1.8896],
        ...,
        [-4.3516, -3.5547, -2.6641,  ..., -3.7266, -5.0859, -1.6016],
        [-4.2500, -2.6562, -1.8281,  ..., -2.5781, -2.1719, -1.9541],
        [-4.7578, -3.8379, -4.5117,  ..., -5.7773, -5.0430, -0.2600]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  7,  0, 27, 27, 27,  5, 15, 27, 27, 20, 27, 10, 27,  0, 27, 27,  4,
        27, 20, 15,  0,  7, 27, 15, 27,  3,  9,  1, 22, 26, 22, 11,  5,  4,  0,
         0,  6,  0,  7, 20, 27,  3, 27, 27,  0, 17,  3, 26, 27, 27, 27, 27, 27,
         0, 18,  0,  6, 27, 17, 27, 27, 26, 27], device='cuda:0')
step: 566
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.7452e-04, -6.3419e-04, -2.6941e-04,  ...,  1.2751e-03,
          2.6524e-05,  6.3133e-04],
        [ 1.2338e-04,  1.2052e-04,  3.3665e-04,  ...,  6.0320e-04,
         -4.8232e-04,  1.9538e-04],
        [ 3.7980e-04,  1.1454e-03,  8.4352e-04,  ..., -1.3208e-03,
         -5.4359e-05,  1.0939e-03],
        [ 1.5771e-04,  2.1303e-04, -1.1559e-03,  ..., -5.3644e-04,
         -4.3440e-04,  6.0177e-04],
        [-6.7651e-05, -1.7178e-04, -9.6917e-05,  ...,  9.9659e-05,
          4.3702e-04,  3.7432e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7764, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.8223, -3.0723, -4.2734,  ..., -3.4160, -5.0742, -1.0410],
        [-5.9531, -4.5156, -4.7188,  ..., -6.0156, -3.1094, -1.0479],
        [-3.1074, -3.3262, -3.4355,  ..., -2.7949, -5.3086, -0.8877],
        ...,
        [-3.2578, -4.2578, -5.6484,  ..., -3.4922, -4.7578, -3.3359],
        [-0.0323, -6.2344, -8.1094,  ..., -9.3438, -6.3125, -6.1562],
        [-5.8281, -3.7969, -3.7031,  ..., -3.7812, -5.2500, -0.3591]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10,  7,  0,  3,  3, 27, 27,  7, 15,  4, 25, 27, 27,  7,  9,  8, 18, 27,
        27, 25, 15,  7, 15, 27, 27, 18,  4, 27, 17, 14, 27, 27,  4, 27,  3, 18,
        24, 27, 10, 18, 27, 27, 20,  3, 27,  4, 22,  0, 17, 27, 27, 10, 18, 24,
        20, 11,  4,  1, 27, 27, 27,  4,  0, 27], device='cuda:0')
step: 567
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.9363e-04, -1.3647e-03, -3.4571e-04,  ...,  1.5092e-04,
         -5.0604e-05, -1.4086e-03],
        [ 4.9324e-03,  2.4567e-03, -3.3646e-03,  ...,  3.7727e-03,
          1.0910e-03, -4.2801e-03],
        [ 9.9468e-04,  4.4560e-04, -1.1587e-03,  ...,  7.6532e-04,
          9.5034e-04, -1.1339e-03],
        [ 7.6294e-06, -1.0473e-04, -1.2474e-03,  ..., -2.1553e-04,
         -6.3372e-04, -3.4809e-05],
        [ 6.1274e-04,  4.8041e-05, -2.9230e-04,  ...,  8.7559e-05,
          1.1224e-04, -2.8849e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9178, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.7305, -4.2148, -4.6055,  ..., -6.4180, -3.8242, -2.2148],
        [-5.7070, -3.5820, -4.2695,  ..., -5.6602, -3.6133, -1.2383],
        [-1.8242, -2.5742, -2.8398,  ..., -2.3867, -5.0117, -1.6680],
        ...,
        [-0.6758, -3.1133, -5.8164,  ..., -6.4102, -4.2227, -2.4883],
        [-2.0273, -2.7148, -5.0586,  ..., -6.5742, -5.0898, -0.8096],
        [-3.9180, -2.6680, -2.7305,  ..., -2.9961, -4.0273, -1.3086]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  2,  6,  3,  4, 27, 27,  3, 27, 10, 10,  7, 10, 11, 20,  3,  8,  0,
        27, 27, 20, 27, 17, 27, 20, 26,  0, 26,  0, 18,  3, 27,  4,  0, 27, 27,
         1, 27, 15,  0, 15,  9, 27,  7,  1,  0,  3, 27, 10, 27, 23, 25,  0, 27,
        27, 17, 27,  1, 10, 10, 17, 27,  0, 26], device='cuda:0')
step: 568
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.3511e-04,  5.6934e-04,  1.3885e-03,  ..., -7.5054e-04,
         -2.6131e-04,  1.0786e-03],
        [-4.5347e-04, -3.9530e-04, -1.8609e-04,  ...,  1.1549e-03,
          8.2350e-04,  1.0834e-03],
        [-1.2076e-04,  6.2180e-04,  3.1185e-04,  ...,  6.7043e-04,
         -1.4186e-04,  3.0756e-05],
        [ 4.7112e-04, -8.4460e-05,  4.5657e-04,  ..., -1.1005e-03,
         -7.4100e-04, -4.3702e-04],
        [ 1.6069e-04,  2.4581e-04,  2.6894e-04,  ..., -1.0246e-04,
          1.5831e-04,  2.0397e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7173, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.1406, -5.0312, -6.0938,  ..., -6.9844, -6.5781, -0.1396],
        [-3.3652, -4.6641, -5.0039,  ..., -7.9883, -5.5859, -3.4434],
        [-4.3828, -3.4434, -3.3809,  ..., -4.5234, -3.7090, -0.6943],
        ...,
        [-5.1836, -4.3711, -5.1211,  ..., -6.6367, -3.0273, -1.1514],
        [-1.8320, -3.5508, -4.9102,  ..., -4.9883, -2.2070, -3.1914],
        [-4.8125, -3.6406, -3.6875,  ..., -5.5625, -3.7969, -0.5635]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  7, 27, 27, 15, 17, 27, 10, 27, 27,  6, 27, 27,  9,  1, 18, 20, 27,
        27, 27,  4, 15, 18,  7, 27, 15, 18, 17, 15, 15, 15, 27, 10, 27, 18, 20,
         1,  9,  3,  9, 15, 10, 27, 18,  0, 13,  9, 10,  5, 27, 27, 14, 27,  3,
        27, 12, 15,  2, 22,  5,  7,  7, 26, 20], device='cuda:0')
step: 569
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.9727e-04,  2.9602e-03, -8.6069e-04,  ..., -3.7909e-05,
          6.8474e-04, -1.0376e-03],
        [-1.4734e-03, -2.6703e-04, -7.2527e-04,  ..., -1.3268e-04,
          2.9862e-05,  2.5349e-03],
        [ 3.3069e-04, -3.7146e-04, -1.7035e-04,  ...,  2.4185e-03,
         -7.9775e-04, -8.1599e-05],
        [ 2.9030e-03,  5.5695e-04, -4.3983e-03,  ..., -2.1458e-03,
         -7.1239e-04, -1.6212e-04],
        [ 4.1938e-04,  2.4390e-04, -3.0804e-04,  ..., -8.2552e-05,
          6.7759e-04,  8.0395e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9646, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.0625, -4.2656, -4.7344,  ..., -6.8906, -5.4531, -1.0479],
        [-3.5703, -4.3984, -5.2422,  ..., -4.0703, -0.2883, -2.8516],
        [-5.5391, -3.6016, -4.0234,  ..., -5.8828, -3.1172, -1.3047],
        ...,
        [-4.7383, -3.0488, -3.0977,  ..., -3.5488, -4.4258, -1.1738],
        [-1.9043, -4.0586, -4.2930,  ..., -4.6211, -4.9805, -3.2637],
        [-3.2324, -3.4199, -4.7812,  ..., -4.8438, -3.2637, -2.4355]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 26, 10, 26,  4, 27,  4, 18, 10, 17, 22, 20, 27, 18, 15,  9, 25, 19,
         3, 20,  9, 22, 20, 18,  3, 22, 27,  1, 15,  7, 12, 27, 27,  0, 27, 27,
         6, 26,  7, 26, 27, 27, 27, 20,  4, 27, 22, 27,  5, 27, 15, 19,  7,  6,
        18, 10, 20, 27, 15, 11, 27,  2, 25, 23], device='cuda:0')
step: 570
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.2343e-04, -9.7847e-04,  4.7588e-04,  ..., -8.9884e-05,
         -2.2650e-06,  5.9509e-04],
        [-2.3842e-04, -4.3797e-04,  2.4021e-04,  ..., -5.5027e-04,
         -8.8978e-04,  1.1188e-04],
        [ 3.3331e-04, -3.0231e-04, -1.5950e-04,  ..., -1.1215e-03,
          2.1732e-04, -2.9254e-04],
        [-5.4646e-04,  1.0830e-04,  7.0155e-05,  ..., -3.3689e-04,
          4.4918e-04,  6.0463e-04],
        [ 3.0637e-04, -4.1342e-04,  6.0797e-05,  ..., -2.2900e-04,
         -9.4116e-05,  3.1376e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6476, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.8125, -3.5938, -3.6250,  ..., -4.0938, -4.1406, -0.4834],
        [-3.1660, -4.2266, -4.9141,  ..., -5.2422, -4.6016, -0.3220],
        [-3.4160, -3.7754, -4.0586,  ..., -3.4785, -4.0586, -0.6816],
        ...,
        [-4.9727, -3.3027, -4.6016,  ..., -6.0195, -2.5059, -1.2559],
        [-5.5352, -2.9727, -2.6758,  ..., -2.4883, -3.8008, -2.1133],
        [-5.1641, -4.7578, -4.9766,  ..., -5.0703, -6.0078, -4.3828]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 15, 20, 18, 27, 27,  1, 24, 27, 22, 22, 15, 27, 27,  7, 26, 22, 20,
        18,  0, 27, 27,  6,  3, 27,  7,  0, 27, 27, 17, 18,  3, 15, 15, 18, 10,
        11, 27,  7, 18, 21, 26, 18, 27,  6, 27, 15, 27, 18, 27, 27, 27, 27, 26,
        27, 27, 10,  2, 27, 26,  1, 27,  1, 20], device='cuda:0')
step: 571
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.6866e-04,  4.5853e-03,  1.5335e-03,  ..., -4.1389e-04,
         -2.0945e-04,  8.7070e-04],
        [ 1.0386e-03,  9.5367e-04,  1.0109e-04,  ..., -2.6016e-03,
         -2.4452e-03,  1.0204e-03],
        [ 3.6478e-04,  7.3099e-04,  7.0333e-06,  ...,  1.0538e-04,
         -3.8123e-04, -2.2066e-04],
        [-5.0068e-04,  8.1110e-04,  3.3641e-04,  ...,  8.0872e-04,
         -7.9155e-04, -8.5294e-05],
        [-3.4904e-04,  5.9509e-04,  2.3079e-04,  ...,  1.0008e-04,
          4.8399e-05, -2.2125e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6115, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.5234, -3.5391, -4.8828,  ..., -4.8516, -5.4453, -1.0547],
        [-4.3633, -2.9727, -4.0508,  ..., -4.6602, -2.0664, -1.3467],
        [-5.7500, -3.6406, -3.9062,  ..., -5.4062, -5.0938, -0.2961],
        ...,
        [-3.6367, -3.8555, -4.9492,  ..., -6.7148, -2.4023, -4.7148],
        [-1.1162, -3.4746, -5.2422,  ..., -5.0547, -4.1797, -1.8350],
        [-0.3162, -3.5508, -5.9102,  ..., -8.1406, -3.3789, -3.8633]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 26, 27, 26, 10, 15, 27, 15,  1, 27,  0, 11, 10, 27, 26, 27, 14, 17,
        15, 27, 27, 18, 27, 12,  0,  6, 11, 18, 27, 27, 27, 27, 11, 10,  7, 24,
        24, 27,  3, 27, 27, 15, 27, 27, 27, 27,  0,  0, 17, 18, 26, 11, 27,  2,
         2, 27, 27, 27, 26,  3,  8, 13,  0,  7], device='cuda:0')
step: 572
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.1225e-03,  1.1921e-03, -4.1068e-05,  ...,  9.3460e-04,
          6.4087e-04,  6.8998e-04],
        [-9.2554e-04,  1.0328e-03,  6.0415e-04,  ...,  3.4285e-04,
         -3.7956e-04,  1.0509e-03],
        [-8.6308e-05, -5.3644e-06, -3.1638e-04,  ...,  3.4547e-04,
         -7.3314e-05,  4.5705e-04],
        [ 1.0719e-03,  7.0381e-04, -1.2839e-04,  ...,  8.2111e-04,
         -1.3971e-03,  2.5344e-04],
        [ 4.3511e-06,  4.4286e-05, -2.5058e-04,  ...,  5.9032e-04,
          2.9254e-04,  3.9816e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8920, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.9238, -2.9395, -3.9707,  ..., -5.6758, -4.3320, -0.6738],
        [-5.5898, -3.9473, -4.6680,  ..., -4.8711, -5.5430, -0.6978],
        [-5.1680, -3.9648, -4.1055,  ..., -4.5742, -3.0117, -0.4810],
        ...,
        [-4.5273, -3.7617, -4.7305,  ..., -6.0273, -4.9648, -0.8706],
        [-3.0625, -4.2656, -5.5781,  ..., -6.7969, -6.6094, -0.9048],
        [-3.3848, -3.5566, -3.6973,  ..., -3.2598, -3.1816, -1.3223]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27, 11,  4, 27, 10, 10,  3,  6,  4, 27, 27, 17, 15, 27, 20, 20,
        27, 20, 21,  3, 25, 27, 27,  7, 27, 27, 27, 22,  4,  0, 13, 27,  3, 24,
        17,  5, 27, 27, 27, 27, 26, 25,  1, 16, 15,  4,  7, 27, 27,  1,  4,  5,
        27,  0,  1, 27, 20,  9, 27, 20, 27, 23], device='cuda:0')
step: 573
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1320e-03, -1.6642e-03, -1.5774e-03,  ..., -3.8087e-05,
          8.7881e-04, -1.6556e-03],
        [ 9.6941e-04,  4.1046e-03,  3.4733e-03,  ..., -2.8973e-03,
         -3.6888e-03, -3.8300e-03],
        [ 3.0923e-04,  2.9254e-04,  1.7738e-03,  ..., -1.7853e-03,
         -2.7275e-04,  1.7090e-03],
        [ 2.0409e-04, -8.5163e-04, -1.1492e-03,  ..., -5.9843e-04,
          9.9850e-04, -1.3294e-03],
        [-7.7486e-05, -1.2760e-03,  4.9448e-04,  ..., -4.9400e-04,
          3.5214e-04, -6.9046e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6489, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.1484, -4.2266, -7.0547,  ..., -6.3203, -6.0547, -5.4141],
        [-4.8711, -5.7148, -6.2461,  ..., -5.8398, -6.9492, -4.6680],
        [-6.1250, -3.8613, -4.0000,  ..., -5.5625, -4.6719, -0.3447],
        ...,
        [-4.2930, -2.6816, -3.5254,  ..., -4.4766, -2.2598, -1.7910],
        [-5.0000, -5.0781, -5.6719,  ..., -5.4727, -6.3125, -4.3594],
        [-5.4805, -2.3223, -2.7441,  ..., -2.8535, -3.6504, -1.6035]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15, 20, 27, 27,  5, 27,  0, 15, 13,  1, 27, 10,  4, 27, 27,  1, 27,  6,
         6, 15,  2, 20,  0, 27, 25, 25, 27, 27,  2, 25, 27,  7, 27, 27,  4, 27,
         3,  1,  4, 27, 17, 27, 27,  7, 27,  2, 20, 23,  4,  7,  4, 27, 18, 27,
         7,  3,  6, 27, 25,  1, 27,  6, 20,  2], device='cuda:0')
step: 574
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2971, -1.7197,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.9215e-05,  5.7507e-04, -3.8266e-04,  ...,  3.2067e-05,
         -9.0063e-05, -1.1559e-03],
        [-1.0532e-04,  3.7718e-04,  3.3474e-04,  ...,  5.1451e-04,
          1.5440e-03,  5.0640e-04],
        [-2.5368e-04,  1.7643e-05,  4.4537e-04,  ..., -2.5415e-04,
         -9.2840e-04, -9.2030e-04],
        [-1.4150e-04, -4.0531e-04,  1.0805e-03,  ...,  4.4441e-04,
          3.1662e-04,  2.0683e-05],
        [ 7.0751e-05, -2.4235e-04,  5.4598e-04,  ...,  9.2685e-05,
         -8.0526e-05,  9.3877e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8836, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.4629, -3.0098, -3.7754,  ..., -4.6680, -4.1328, -0.5098],
        [-1.6621, -4.0234, -6.8047,  ..., -5.1641, -4.5547, -4.0547],
        [-5.3711, -3.7285, -3.7129,  ..., -4.6055, -3.4785, -0.5107],
        ...,
        [-3.0273, -3.8711, -4.4961,  ..., -5.0898, -4.3086, -1.5273],
        [-4.3125, -3.8613, -3.7188,  ..., -3.5332, -4.5156, -0.6108],
        [-4.0664, -2.6602, -3.9414,  ..., -5.8320, -4.2383, -1.0342]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0,  4, 27, 25, 20,  4, 24, 18,  4, 15, 27,  9,  3,  2, 11, 27, 27,
        27, 10,  4,  7, 13,  9,  0,  0, 27, 21, 15, 27,  3,  1, 17,  3, 27, 27,
        14, 15, 27, 10, 27, 17,  7, 18, 17, 27,  8, 27, 27,  2, 15,  0, 17, 21,
         4,  9, 27, 10,  6, 10, 27, 17,  1,  4], device='cuda:0')
step: 575
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0809,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2971, -1.7197,  ..., -0.6592,  0.6533,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.8968e-04,  8.7738e-04,  1.3351e-05,  ...,  7.9155e-04,
          2.0289e-04,  1.0881e-03],
        [-2.8038e-04, -9.5177e-04, -3.6526e-04,  ...,  2.7237e-03,
          4.4823e-04,  1.5059e-03],
        [ 1.0338e-03, -6.3515e-04, -2.0485e-03,  ..., -2.7580e-03,
          4.2057e-04,  1.4648e-03],
        [ 1.3609e-03,  8.7595e-04, -2.8133e-03,  ..., -1.0223e-03,
         -1.9445e-03, -7.6580e-04],
        [ 5.3346e-05, -3.8147e-05, -1.5044e-04,  ...,  2.1422e-04,
          1.2177e-04,  1.4982e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8023, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.8516, -4.0703, -4.0391,  ..., -4.8984, -4.8672, -0.2258],
        [-4.6992, -3.5430, -4.1523,  ..., -5.3398, -5.4805, -0.4810],
        [-4.8516, -3.1348, -3.8848,  ..., -4.2734, -4.3398, -0.4785],
        ...,
        [-3.3184, -3.7402, -6.7891,  ..., -6.0547, -5.3047, -4.5078],
        [-6.8242, -4.4688, -5.2344,  ..., -7.4375, -6.8086, -0.1385],
        [-3.5547, -2.2891, -3.6953,  ..., -2.4297, -3.4609, -1.7109]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27, 26, 16, 25,  6, 18,  6, 27, 27, 10,  2, 27, 15, 22, 27, 11,
         1, 27,  0, 27,  2,  9, 11, 27, 15,  3,  7, 26, 27, 27, 27, 27, 15,  0,
        27, 18, 26, 27, 27, 20, 27, 17,  5, 17, 27, 11,  3, 17,  4, 27, 27, 27,
         3,  4, 24, 18, 27,  0, 18, 15, 27,  3], device='cuda:0')
step: 576
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2971, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.2105e-04,  8.8263e-04, -4.2820e-04,  ...,  4.3869e-04,
          7.4100e-04,  5.2404e-04],
        [-2.3317e-04, -7.6532e-05, -3.5119e-04,  ..., -3.1281e-04,
         -4.3583e-04, -1.2201e-04],
        [-1.4400e-04, -3.0375e-04, -1.1044e-03,  ...,  1.0452e-03,
          1.0529e-03, -9.4271e-04],
        [-4.2367e-04,  2.2793e-04, -1.0738e-03,  ..., -2.9945e-04,
         -3.2568e-04, -9.1314e-04],
        [-2.5487e-04,  5.7316e-04, -6.5863e-05,  ...,  2.6941e-04,
          3.3140e-04,  3.9101e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9353, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3906, -3.2793, -4.4375,  ..., -6.1094, -5.4531, -0.7646],
        [-3.9434, -2.0371, -2.0684,  ..., -3.1152, -4.0820, -1.9736],
        [-4.0039, -4.4883, -7.2070,  ..., -6.1289, -4.7695, -5.6914],
        ...,
        [-5.7852, -4.3945, -3.1133,  ..., -0.6597, -4.5352, -4.4258],
        [-1.0156, -3.5156, -5.2812,  ..., -6.8594, -5.0625, -2.1094],
        [-3.8574, -4.1562, -7.7188,  ..., -6.9688, -6.6250, -5.1094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4,  2, 15, 27,  4, 27,  0,  0, 15, 27, 26, 26,  4, 25,  5, 27, 14, 18,
        26,  7,  6, 20, 27,  4, 22, 27,  1, 27, 15,  6,  4, 20, 10, 27, 27,  3,
        11,  8, 10,  1,  3, 18, 26,  5, 15,  8,  9,  9, 27, 15, 27, 27, 17, 27,
         8, 13,  4, 22, 27, 27,  3, 25,  0, 15], device='cuda:0')
step: 577
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4792, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2971, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.2655e-03,  1.6108e-03, -1.3437e-03,  ...,  1.9760e-03,
          1.3332e-03,  2.3003e-03],
        [-5.0259e-04,  3.9148e-04,  2.8586e-04,  ...,  9.8896e-04,
         -1.8845e-03,  5.7268e-04],
        [ 8.1420e-05, -3.2246e-05,  5.6267e-04,  ...,  2.6917e-04,
         -1.3390e-03,  9.6464e-04],
        [ 1.0757e-03, -2.8086e-04, -1.7328e-03,  ...,  4.0245e-04,
         -2.5902e-03,  8.6832e-04],
        [ 6.4790e-05, -9.8038e-04, -1.2035e-03,  ...,  7.6294e-04,
         -3.7909e-05,  3.7980e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7875, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.6758, -3.7363, -4.0820,  ..., -4.3008, -4.3633, -0.6118],
        [-5.2109, -3.5996, -4.1641,  ..., -4.6641, -4.4453, -0.4600],
        [-0.6299, -3.6914, -5.9570,  ..., -7.2227, -4.7227, -1.8477],
        ...,
        [-2.4219, -3.1875, -4.2969,  ..., -5.4844, -4.6094, -1.8906],
        [-4.4180, -3.8105, -6.0430,  ..., -4.8711, -5.7148, -3.8867],
        [-7.2812, -4.2188, -3.2656,  ..., -3.9082, -4.0000, -3.5469]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  3,  0, 13, 24,  0, 27, 27, 27, 20, 27, 27, 11,  0, 27, 20,  0, 27,
        25, 10, 11,  6, 23, 18, 27,  1,  0, 27,  6, 27,  4, 20,  0,  4, 27,  4,
        27, 10,  2, 27,  1, 11, 27, 27,  6,  0, 26,  0, 18, 22, 25, 14, 27, 27,
        27, 10, 27, 11, 20, 27, 27, 17, 13, 14], device='cuda:0')
step: 578
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2388, -0.6147,  ...,  0.4792, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2971, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.1267e-04, -2.8152e-03, -1.0881e-03,  ...,  6.0415e-04,
         -4.8327e-04, -1.1673e-03],
        [ 5.1403e-04, -4.6921e-04,  3.5453e-04,  ..., -9.1696e-04,
         -6.3753e-04, -6.9284e-04],
        [-1.7297e-04, -1.6928e-04,  4.7374e-04,  ..., -1.7281e-03,
          6.2656e-04,  1.1425e-03],
        [-1.8806e-03, -2.1672e-04,  2.9063e-04,  ...,  3.8409e-04,
          1.7557e-03, -1.4114e-04],
        [-1.2064e-04,  3.4213e-05,  2.3615e-04,  ..., -4.1628e-04,
         -4.2677e-04, -4.2772e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8272, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.1113, -4.1758, -5.0664,  ..., -5.8945, -5.9883, -1.3623],
        [-5.7188, -2.8105, -3.7324,  ..., -4.7031, -3.2480, -0.6079],
        [-3.3359, -3.6152, -4.2578,  ..., -6.5078, -4.1484, -0.3977],
        ...,
        [-1.5293, -3.7012, -4.9648,  ..., -6.5273, -6.3594, -0.6235],
        [-5.0938, -2.9062, -3.5625,  ..., -2.5312, -4.7031, -1.3438],
        [-5.5352, -3.8320, -4.0352,  ..., -5.4258, -3.9258, -0.5347]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 22, 27, 27, 27, 27, 27, 27,  1, 20, 15, 27, 27, 27,  1, 27, 27, 27,
        18,  4, 27,  7,  6, 10, 20,  9,  1, 25, 27,  3, 27, 15, 13, 27, 17,  1,
        27,  4, 27,  5,  0, 27,  5, 27,  4, 27, 18, 18, 18, 17,  6, 10, 27,  8,
        18, 15,  6, 24,  4,  9, 25, 21, 10,  6], device='cuda:0')
step: 579
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2388, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2971, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0319e-03, -1.7891e-03, -1.0815e-03,  ..., -1.0777e-04,
         -4.9162e-04, -1.6804e-03],
        [ 2.0466e-03,  2.0332e-03,  1.3847e-03,  ...,  1.3895e-03,
          2.0428e-03, -3.2711e-03],
        [ 2.5082e-04,  1.6046e-04,  4.1556e-04,  ..., -8.4162e-04,
          1.0681e-03,  6.5994e-04],
        [ 3.5000e-04, -7.5936e-05,  3.1662e-03,  ...,  4.1413e-04,
         -1.2722e-03,  3.8481e-04],
        [ 7.0906e-04, -1.0264e-04,  1.6189e-04,  ..., -3.2330e-04,
          1.1134e-04, -1.9705e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6040, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.5742, -3.8535, -2.7441,  ..., -2.3066, -5.4961, -2.1660],
        [-2.8848, -3.4609, -2.6504,  ..., -2.9473, -6.3203, -1.7910],
        [-4.9844, -3.8301, -4.1875,  ..., -4.6562, -3.6719, -0.5010],
        ...,
        [-7.1719, -4.5000, -4.9375,  ..., -6.0625, -3.3594, -1.1875],
        [-5.8750, -2.7500, -2.7344,  ..., -2.0781, -4.7969, -1.3604],
        [-4.0039, -4.4414, -4.5508,  ..., -4.0664, -3.7070, -0.8477]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 18, 27, 27, 27, 10, 27, 18, 27,  2,  0, 27, 15, 24, 26, 18,  9,  0,
         0,  8,  4, 27,  6, 15,  1, 27, 13, 27, 27, 14, 20,  0, 27,  3, 27, 27,
         7, 27, 10, 27,  5, 27, 15,  0,  1, 27, 25, 18, 13, 27,  4,  1, 27,  0,
        27,  3, 13,  7, 27, 25, 25,  6, 25, 25], device='cuda:0')
step: 580
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2388, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2971, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.8419e-04, -7.5102e-04, -3.4750e-05,  ...,  2.4366e-04,
         -6.3658e-04,  9.1124e-04],
        [ 6.7711e-04, -3.2711e-04,  1.4191e-03,  ..., -2.8753e-04,
         -2.0351e-03,  2.6822e-05],
        [-4.6849e-05, -8.9455e-04,  3.5763e-04,  ..., -4.8208e-04,
         -4.1795e-04,  3.2282e-04],
        [-1.5678e-03, -8.6117e-04,  7.7152e-04,  ...,  1.0719e-03,
          6.6614e-04, -7.6437e-04],
        [-3.5429e-04, -5.5599e-04,  8.5020e-04,  ..., -1.5521e-04,
         -4.7541e-04,  7.2575e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1178, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9453, -2.1172, -3.1328,  ..., -2.6172, -3.2109, -2.2266],
        [-5.3203, -4.1641, -4.4141,  ..., -5.0078, -5.5703, -0.2581],
        [-4.7812, -3.3301, -3.6113,  ..., -3.1738, -4.9531, -0.8765],
        ...,
        [-6.5898, -4.4961, -4.4648,  ..., -5.6211, -3.6230, -1.5918],
        [-3.9590, -2.9902, -3.3027,  ..., -3.8496, -2.9434, -0.7095],
        [-5.2070, -5.6289, -8.5391,  ..., -6.5508, -6.5820, -6.1758]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 10,  5,  1,  6, 27,  0, 27, 13, 17, 27, 27, 27, 27,  0, 27,  8,  2,
        27, 13, 20, 27,  4,  3,  2, 18,  0, 27,  0, 27, 17, 27, 22, 19,  9,  1,
         0,  6, 27, 18, 15, 13, 20, 10, 27, 21, 27,  0, 27, 27, 27, 13,  4, 10,
         5,  9, 17, 27, 27, 20, 17,  7, 27, 22], device='cuda:0')
step: 581
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2388, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2971, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0006, -0.0018, -0.0012,  ..., -0.0026, -0.0002, -0.0012],
        [ 0.0007, -0.0008, -0.0006,  ..., -0.0004,  0.0006, -0.0005],
        [ 0.0006, -0.0005, -0.0004,  ..., -0.0010,  0.0009, -0.0005],
        [-0.0003, -0.0012,  0.0006,  ..., -0.0007,  0.0010, -0.0010],
        [ 0.0005, -0.0005,  0.0001,  ..., -0.0006,  0.0006, -0.0008]],
       device='cuda:0', dtype=torch.float16)
loss: tensor(1.7350, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.4395, -3.0938, -2.6426,  ..., -2.3906, -3.5312, -1.0322],
        [-1.9521, -3.9668, -4.2344,  ..., -5.9062, -2.6250, -1.2646],
        [-5.7344, -3.7363, -3.9707,  ..., -5.2656, -4.5000, -0.3447],
        ...,
        [-4.2539, -3.2520, -3.0508,  ..., -4.5820, -2.8008, -0.5962],
        [-5.9570, -3.4082, -4.3477,  ..., -4.5977, -4.9414, -0.8149],
        [-6.0625, -4.6875, -5.3438,  ..., -6.0781, -4.5625, -0.6099]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  2, 20, 19,  4, 27, 11,  0,  8,  0, 27, 25,  4, 27, 25, 18,  3,
        25, 19, 27, 14, 15, 15,  7,  3, 13,  4, 27, 18,  2, 18, 27, 27, 25, 18,
        24, 25, 27, 27, 25, 27,  0, 26, 27, 27,  3, 14, 17, 15, 18, 22, 26, 17,
        15, 27, 27, 15, 20,  9, 26, 27, 27,  7], device='cuda:0')
step: 582
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2388, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2971, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.2712e-03, -7.4720e-04,  1.7643e-04,  ..., -6.0272e-04,
          5.0497e-04, -7.2193e-04],
        [-6.2704e-05, -1.2913e-03, -6.1369e-04,  ..., -4.0283e-03,
         -2.4071e-03,  1.6909e-03],
        [-9.1076e-05, -1.0071e-03,  4.4799e-04,  ...,  1.6034e-04,
         -6.2275e-04,  5.7220e-04],
        [-1.7672e-03, -6.0129e-04,  5.2691e-05,  ...,  3.5954e-04,
          1.5202e-03, -5.6171e-04],
        [-2.7108e-04, -9.3746e-04,  7.9918e-04,  ..., -2.0981e-04,
         -6.8247e-05, -3.1924e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8122, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.5117, -3.4492, -4.5586,  ..., -4.4805, -2.2148, -3.1992],
        [-3.1230, -3.1074, -3.9043,  ..., -5.4961, -5.9336, -0.4348],
        [-4.1914, -5.1758, -8.2500,  ..., -7.5508, -6.4883, -5.2070],
        ...,
        [-5.2422, -3.7109, -4.6797,  ..., -7.0547, -5.8359, -0.2264],
        [-3.4902, -4.6484, -5.3516,  ..., -6.6953, -6.2578, -5.5078],
        [-5.6992, -4.0117, -4.3398,  ..., -5.4492, -2.7324, -2.2637]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26,  0, 15, 15,  1,  3, 18, 27, 11,  4, 20,  7, 13, 26,  0, 27, 15, 10,
         3, 26,  0, 27, 15,  3, 15, 27,  4, 27, 27, 18,  0,  3, 24, 15, 25, 15,
         0, 27,  4,  3,  4, 27, 11, 27, 27,  3, 27,  7,  0, 15,  3,  3, 11,  1,
        27, 22, 27, 10, 27, 15,  2, 27, 18,  7], device='cuda:0')
step: 583
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2388, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2971, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.2398e-04, -6.6280e-04, -1.0414e-03,  ...,  6.4754e-04,
         -1.2665e-03, -7.5102e-04],
        [-2.2721e-04,  2.3925e-04,  1.7691e-03,  ...,  4.9934e-03,
          8.5294e-05, -9.2506e-04],
        [-2.4021e-04, -1.7762e-04, -4.1175e-04,  ...,  1.6584e-03,
          5.2691e-05, -1.0548e-03],
        [ 1.2159e-05, -1.0687e-04, -5.3644e-04,  ...,  4.9496e-04,
          1.5736e-04, -2.7466e-04],
        [ 3.9434e-04,  8.9836e-04, -1.7905e-04,  ...,  7.6151e-04,
          1.7190e-04, -4.1127e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6214, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.5176, -3.4238, -4.6406,  ..., -4.2812, -2.0488, -0.7051],
        [-1.8721, -3.5449, -3.3574,  ..., -4.4023, -1.4502, -2.2949],
        [-4.4766, -2.7422, -3.0234,  ..., -3.6328, -1.7891, -1.5547],
        ...,
        [-5.6953, -4.8047, -5.0078,  ..., -7.1484, -4.4141, -1.2588],
        [-4.6133, -3.0371, -3.8965,  ..., -4.5234, -5.2422, -0.6152],
        [-3.5840, -4.7578, -5.9102,  ..., -7.7422, -5.9453, -5.9766]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27, 27, 27,  1, 25,  2,  7, 26,  4, 14, 20,  4, 15, 22, 11, 27,
        10, 18,  0, 27, 15, 25, 15, 27,  6,  5,  7, 25, 27,  3, 27, 24, 20,  5,
        27,  3, 15, 24,  0, 26, 27,  2, 20, 27, 27, 20,  4,  7,  1, 27, 27, 20,
        17, 27, 27, 26, 27, 18, 27, 27,  4, 17], device='cuda:0')
step: 584
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2388, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2971, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.5725e-04,  1.3971e-04,  9.3508e-04,  ...,  6.5899e-04,
          1.0643e-03,  1.0910e-03],
        [-2.3222e-04, -1.7583e-04, -3.5048e-04,  ..., -2.1191e-03,
         -6.8712e-04,  9.5701e-04],
        [-2.7061e-05, -3.8862e-05,  8.7857e-05,  ..., -5.3549e-04,
         -5.3883e-04,  1.0214e-03],
        [ 1.3599e-03,  4.1723e-04,  1.3936e-04,  ...,  5.2929e-04,
         -1.1101e-03,  2.6274e-04],
        [ 1.1021e-04,  3.5238e-04, -7.3290e-04,  ...,  5.3930e-04,
         -9.0778e-05, -8.4162e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7539, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.4138, -3.7422, -5.7734,  ..., -7.8672, -3.1016, -3.0859],
        [-0.8110, -3.0293, -4.8594,  ..., -6.1406, -2.4668, -4.0156],
        [-6.8516, -5.3984, -5.7266,  ..., -7.0391, -6.1484, -0.1033],
        ...,
        [-3.5781, -4.7188, -5.3125,  ..., -6.2500, -5.3125, -4.5469],
        [-4.0391, -4.2578, -6.9922,  ..., -5.2266, -4.3359, -5.1328],
        [-2.8906, -4.4219, -5.2344,  ..., -5.7969, -4.2812, -1.0312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  0,  4, 10, 18, 26, 27,  9, 27,  4,  0, 27, 11, 27, 17,  3,  3, 23,
        27, 10, 10,  0, 27,  4, 27,  3, 26, 18, 27, 18, 27, 27, 27,  2,  4, 20,
        27, 27,  4, 18, 27, 27, 27,  4, 27,  3, 27, 27,  0,  1,  9, 27,  8,  9,
         1, 27, 27, 27,  0,  4, 27, 18, 17, 23], device='cuda:0')
step: 585
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2388, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2971, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.9731e-04, -1.9016e-03, -1.7738e-04,  ..., -7.4911e-04,
         -1.1802e-04, -6.5994e-04],
        [ 2.6166e-05,  1.1711e-03,  1.0529e-03,  ...,  2.4242e-03,
          8.3923e-04, -9.9754e-04],
        [ 4.0436e-04,  9.5701e-04,  1.1282e-03,  ..., -9.7847e-04,
          1.2455e-03,  7.8821e-04],
        [ 1.3888e-05, -5.7840e-04,  5.2166e-04,  ...,  6.0701e-04,
          8.2159e-04, -7.9453e-05],
        [ 3.5501e-04,  7.0906e-04, -4.2057e-04,  ...,  3.3259e-04,
          2.2280e-04, -7.4005e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6631, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.3438, -3.6250, -4.7500,  ..., -5.2031, -2.7031, -0.5303],
        [-6.2930, -2.5449, -2.4199,  ..., -2.7012, -4.8555, -1.2314],
        [-6.3828, -4.8359, -5.1797,  ..., -6.0703, -5.9141, -0.2277],
        ...,
        [-1.2773, -3.6836, -5.3398,  ..., -5.3867, -3.0898, -3.7773],
        [-4.5195, -3.5195, -4.5820,  ..., -5.0039, -0.9404, -1.1436],
        [-3.4785, -2.8066, -3.6973,  ..., -3.9785, -5.1367, -0.6509]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 25, 27, 20, 27, 27, 15, 10,  0,  0, 27, 27,  0, 27, 27,  3, 26, 11,
         0, 27, 18,  7, 15, 27, 25,  4, 22, 17, 27, 27, 18, 25,  1,  6,  0, 27,
        27, 10, 27, 11,  0, 13,  7, 27,  1,  9, 26, 27, 27, 27, 27,  2,  4, 17,
        27,  8,  2, 27,  0, 27, 15, 17, 27,  1], device='cuda:0')
step: 586
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2388, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2971, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.0797e-04, -1.7281e-03, -3.0088e-04,  ...,  1.0920e-03,
         -7.5197e-04,  1.0576e-03],
        [-9.3699e-04, -4.4847e-04,  3.7432e-05,  ..., -3.7994e-03,
         -3.9215e-03,  2.6989e-04],
        [-4.0114e-05,  8.2350e-04,  1.1950e-03,  ..., -1.2960e-03,
         -9.1553e-05,  2.0218e-03],
        [-1.1563e-05, -7.0286e-04, -1.2407e-03,  ..., -1.2379e-03,
          4.8018e-04, -3.7074e-05],
        [ 2.8586e-04, -8.4352e-04, -4.6039e-04,  ..., -5.1498e-04,
         -4.3368e-04, -7.3719e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7760, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.4226, -2.6719, -5.2656,  ..., -7.7344, -2.8438, -3.6875],
        [-5.8281, -3.5605, -3.7969,  ..., -3.8418, -4.6250, -0.4050],
        [-1.8682, -3.5723, -4.6328,  ..., -7.3359, -3.8047, -0.9307],
        ...,
        [-3.1348, -3.5098, -3.7598,  ..., -4.7930, -2.2285, -2.4785],
        [-0.2444, -4.0430, -6.4648,  ..., -7.2930, -5.3359, -3.0879],
        [-1.6211, -3.0273, -4.4492,  ..., -4.8555, -3.7617, -1.6064]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26, 27,  4,  1,  0, 27,  1,  2, 15, 27, 27, 27, 26,  7, 17, 20, 27, 27,
         9,  0, 27,  3, 27, 27, 27,  3,  4,  4,  7,  4,  7, 27,  5, 10,  0, 22,
        27, 27,  7, 27, 25, 27, 27,  9, 27, 25, 27, 18, 27,  6,  3, 27, 27,  2,
        17, 27,  4, 27, 18, 27, 27,  8, 27,  0], device='cuda:0')
step: 587
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2388, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2971, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.3613e-03, -1.7490e-03,  2.8744e-03,  ...,  6.1989e-04,
         -4.0865e-04, -1.2245e-03],
        [ 2.2144e-03,  6.7425e-04, -3.3641e-04,  ..., -8.0872e-04,
         -3.1033e-03,  5.2738e-04],
        [ 1.2045e-03, -2.7990e-04, -1.9968e-04,  ...,  3.2723e-05,
         -1.7929e-04,  4.5300e-06],
        [-5.0211e-04,  2.6083e-04,  1.9522e-03,  ...,  4.2439e-05,
          2.1505e-04, -3.2783e-06],
        [-3.5310e-04, -7.6199e-04,  8.0538e-04,  ..., -2.1005e-04,
         -5.7030e-04, -1.9515e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.4645, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.7812, -3.8301, -4.6094,  ..., -4.2500, -4.5625, -0.5640],
        [-4.2422, -3.6152, -3.9453,  ..., -2.4434, -1.9756, -1.5381],
        [-5.3320, -4.3633, -4.9883,  ..., -6.1289, -3.2852, -0.8955],
        ...,
        [-6.1680, -3.9043, -3.6992,  ..., -3.8555, -4.9961, -0.4348],
        [-0.2345, -4.4531, -6.9219,  ..., -7.5312, -4.7500, -3.3125],
        [-4.3281, -4.4531, -6.2656,  ..., -4.0781, -3.6406, -3.8438]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27,  7,  1,  1, 17, 27,  3, 27, 18, 20, 27, 15, 27,  6,  0, 10,  4,
         0,  8, 15, 27, 11, 27,  7,  4, 27, 27, 20, 27,  4, 27, 27,  4,  8, 27,
        13, 27, 15, 17,  4,  1, 27, 27, 27,  9, 27,  3, 15, 27,  2, 27, 27, 18,
        27,  4, 10, 25,  9, 27,  0, 10,  0, 17], device='cuda:0')
step: 588
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2971, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0992,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.3580e-04, -9.4271e-04,  8.6403e-04,  ..., -6.7830e-05,
         -3.8910e-04, -6.2037e-04],
        [ 1.8921e-03,  2.4490e-03,  9.9945e-04,  ...,  3.0804e-03,
          2.8400e-03, -3.6926e-03],
        [ 3.2735e-04, -4.0388e-04,  5.8460e-04,  ..., -5.3835e-04,
          6.5804e-04,  9.5367e-06],
        [-1.4877e-03, -3.5381e-04,  4.5738e-03,  ...,  1.3256e-03,
          1.0967e-03,  1.7834e-03],
        [ 3.9697e-05,  3.1090e-04,  9.0122e-04,  ...,  4.2200e-04,
         -2.1088e-04,  3.7861e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0760, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.0195, -4.3164, -7.3945,  ..., -7.0820, -5.5977, -4.5977],
        [-5.9375, -3.6406, -3.8906,  ..., -4.1875, -4.7188, -0.3127],
        [-1.8105, -2.9668, -5.2773,  ..., -7.3906, -4.4805, -3.6074],
        ...,
        [-2.1895, -3.9395, -5.5977,  ..., -4.0195, -3.4238, -4.7070],
        [-5.6602, -4.7227, -2.9414,  ..., -0.8794, -4.4258, -4.1602],
        [-5.9648, -3.8867, -4.2773,  ..., -4.3398, -4.2773, -0.6216]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15,  4, 27,  4, 20,  2,  4, 25,  4, 27, 27,  7,  6,  7, 27,  3,  4,  4,
         9, 21, 12, 27,  5, 18, 27,  1,  3, 27, 27, 18, 27, 27, 27, 11,  6,  4,
        17, 27,  0,  6, 14, 20, 11,  9,  0,  8,  7,  3,  3,  3, 27,  1, 18,  5,
        18,  7,  9, 26, 18, 20, 27, 17, 27,  5], device='cuda:0')
step: 589
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2971, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.3227e-03, -4.7207e-04, -1.0109e-03,  ...,  4.4942e-04,
          2.5773e-04, -5.7316e-04],
        [ 2.3556e-04,  1.1234e-03,  1.5316e-03,  ..., -2.9373e-04,
         -3.6025e-04,  9.2173e-04],
        [ 4.8351e-04, -4.5872e-04, -2.4390e-04,  ...,  5.1439e-05,
          1.3571e-03,  5.0068e-06],
        [ 7.6294e-04,  4.3488e-04,  7.7009e-04,  ...,  3.3402e-04,
         -5.4884e-04,  1.1456e-04],
        [ 5.1689e-04,  4.0245e-04, -4.6587e-04,  ..., -8.1360e-05,
          1.4210e-04, -1.0878e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9178, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6953, -3.8516, -4.0391,  ..., -4.0547, -4.6641, -0.5850],
        [-5.0938, -4.8594, -5.6406,  ..., -5.6406, -0.1716, -3.7188],
        [-4.3203, -2.5059, -2.6777,  ..., -2.5840, -4.4453, -2.4902],
        ...,
        [-5.6562, -4.2969, -3.2832,  ..., -1.7510, -3.2656, -1.7510],
        [-5.5625, -3.0918, -3.3887,  ..., -4.2812, -2.2500, -0.7651],
        [-6.5664, -3.6465, -3.5684,  ..., -4.9102, -4.1133, -0.7554]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([11, 26, 12, 18,  1, 27, 17, 10, 10, 18, 17, 27, 20, 18,  4, 19, 14, 10,
        27,  7,  4,  1, 27, 27,  0, 27, 20,  1,  4,  3, 27, 10, 12, 10,  1, 27,
        12, 26,  6,  0,  4, 27, 11, 27, 27, 27, 20,  1, 26,  1, 27,  1, 24,  3,
        27, 18, 27, 27, 27, 14, 14, 10,  9,  6], device='cuda:0')
step: 590
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2047,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2969, -1.7197,  ..., -0.6592,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.6866e-04, -2.1019e-03, -6.1417e-04,  ...,  4.2605e-04,
          3.4547e-04,  5.5492e-05],
        [-7.0691e-05,  1.6546e-04,  1.6146e-03,  ..., -4.4823e-04,
         -2.0809e-03, -1.2970e-03],
        [-3.2187e-05, -6.7139e-04, -2.6274e-04,  ...,  4.3654e-04,
          5.5730e-05, -5.0735e-04],
        [-2.4176e-04,  3.9518e-05,  7.5150e-04,  ..., -1.0520e-04,
         -1.7262e-04,  1.3053e-04],
        [-4.2963e-04,  3.5024e-04, -1.4901e-04,  ..., -4.4489e-04,
         -3.5977e-04, -1.3578e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9585, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2305, -4.5742, -4.9336,  ..., -7.1211, -4.3086, -0.1992],
        [-0.5991, -4.4414, -5.6914,  ..., -6.1758, -6.3359, -2.3027],
        [-3.4102, -3.6133, -4.5508,  ..., -4.9883, -5.0352, -1.6758],
        ...,
        [-5.2344, -2.8750, -3.3906,  ..., -5.2969, -4.6562, -0.7510],
        [-2.7305, -5.0586, -6.6523,  ..., -8.0391, -5.6367, -5.3555],
        [-3.3809, -4.6797, -5.5547,  ..., -5.8516, -5.6953, -4.2578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0, 21, 27, 27, 26, 20,  1, 27, 17, 20, 27, 26, 17,  0, 15,  5, 17,
        26,  0, 27, 27,  7, 27, 27, 27, 27, 27,  9,  5,  0, 20, 27, 22, 24, 12,
        27, 27, 27, 27,  9,  4, 27, 25, 10, 27,  3, 10,  1, 20, 27,  1, 25,  8,
         1,  4, 17, 27, 25, 27,  7, 27, 18, 20], device='cuda:0')
step: 591
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2969, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.0633e-04, -7.2813e-04,  1.6909e-03,  ..., -7.8821e-04,
         -4.3964e-04,  1.0389e-04],
        [ 1.9693e-04,  3.4690e-04,  1.1082e-03,  ..., -1.1215e-03,
         -1.3571e-03, -3.1590e-04],
        [-3.6669e-04, -5.7888e-04,  6.8903e-05,  ...,  1.4246e-04,
         -5.7602e-04, -2.2697e-04],
        [-1.3399e-03, -5.6314e-04,  1.8711e-03,  ...,  1.1616e-03,
          1.0328e-03,  4.7517e-04],
        [-2.1124e-04, -4.3964e-04,  9.0694e-04,  ...,  1.3220e-04,
         -7.2145e-04,  2.7800e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7181, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9180, -3.9336, -5.3242,  ..., -6.4336, -4.7305, -1.0732],
        [-1.7969, -2.6406, -3.7969,  ..., -5.0781, -4.2969, -1.2188],
        [-6.6289, -6.0977, -9.9766,  ..., -7.2227, -8.2891, -7.5977],
        ...,
        [-5.9688, -4.2031, -4.4062,  ..., -8.1406, -5.4844, -0.5171],
        [-3.6953, -4.3984, -4.5547,  ..., -4.8828, -6.2266, -0.4302],
        [-5.8125, -3.7051, -4.4219,  ..., -5.4688, -3.3301, -1.2988]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 15, 13,  9,  7,  2, 10, 27, 27, 20, 27,  4, 27,  1,  1, 10, 27,
        27, 22,  0,  0,  1, 15, 27,  3,  4, 15, 27,  0, 27, 27,  8, 18, 26, 27,
        18, 18,  7, 10, 27, 22, 27,  4, 27, 27, 27, 27, 27, 17, 26, 20, 26, 27,
         2, 27, 26,  3,  6,  0, 27,  7,  5,  7], device='cuda:0')
step: 592
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2969, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.3170e-03, -1.0509e-03,  1.0366e-03,  ..., -9.2506e-04,
         -1.4830e-03, -5.3740e-04],
        [-8.6260e-04, -3.4392e-05, -6.8569e-04,  ..., -1.3809e-03,
          2.2030e-03,  1.5354e-04],
        [ 2.9898e-04, -1.8549e-04, -1.0204e-03,  ..., -1.0986e-03,
          3.6597e-04, -5.0783e-04],
        [-1.0198e-04, -4.3511e-04,  6.9427e-04,  ...,  5.2404e-04,
          1.3828e-05, -2.7895e-05],
        [ 4.0388e-04, -2.2125e-04,  3.7169e-04,  ..., -2.9874e-04,
         -6.9678e-05, -1.4639e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7638, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.7285, -4.7461, -6.1523,  ..., -8.7422, -6.5117, -6.1836],
        [-3.9277, -3.2559, -3.6934,  ..., -3.1152, -1.8496, -1.6143],
        [-5.7578, -3.2891, -4.0078,  ..., -4.3516, -3.3047, -0.7900],
        ...,
        [-0.4175, -3.5586, -6.1680,  ..., -7.7930, -5.1523, -2.1523],
        [-3.1914, -3.5508, -4.2383,  ..., -4.7852, -4.9102, -0.6133],
        [-1.9482, -3.3066, -5.6836,  ..., -7.3555, -5.3086, -1.6348]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18,  6, 27,  5,  1, 18, 17,  0,  2,  9, 26,  0, 20, 27, 14,  2, 10, 10,
        27, 27, 17,  0, 27, 13, 27, 27, 27, 27,  7, 20, 26, 26, 11,  4,  0, 27,
         4,  4, 10, 27, 10,  0,  3, 17, 15,  9, 22, 27, 18, 24, 27, 27, 20,  2,
        27, 20, 22, 27,  2, 25, 27,  0, 27,  0], device='cuda:0')
step: 593
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4094, -0.2969, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.3360e-05,  2.0027e-03, -4.6134e-04,  ...,  4.2772e-04,
         -1.9634e-04,  5.0783e-04],
        [ 1.4114e-04,  7.1621e-04,  2.4486e-04,  ..., -2.4929e-03,
         -1.5316e-03, -1.6565e-03],
        [-6.3086e-04,  1.0395e-03,  2.0199e-03,  ..., -2.1195e-04,
         -5.6839e-04, -2.0242e-04],
        [-4.3464e-04,  2.2602e-03,  7.5340e-04,  ..., -8.9550e-04,
         -1.5879e-03,  6.5517e-04],
        [ 4.8065e-04,  4.8757e-05, -2.2840e-04,  ..., -1.7977e-04,
          5.8460e-04,  1.0300e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8899, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.4866, -3.1426, -6.1602,  ..., -6.7539, -3.9238, -4.1406],
        [-5.9961, -4.1680, -4.8242,  ..., -6.3086, -2.8379, -0.6045],
        [-1.1523, -4.2773, -6.9180,  ..., -6.1836, -4.7930, -4.9336],
        ...,
        [-2.7266, -2.5859, -2.7734,  ..., -3.4297, -4.0234, -2.0547],
        [-4.4648, -4.1211, -5.8086,  ..., -7.6523, -5.1992, -0.5913],
        [-3.4902, -2.0059, -4.0977,  ..., -3.7246, -3.1465, -1.0674]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 27, 15, 18,  7,  1, 27, 27, 11,  3, 25,  3, 27, 27,  0,  0, 20,  4,
         5,  1, 27, 24, 24, 27,  4, 27, 10, 27, 27, 27, 26, 27, 10,  2, 10, 27,
        27, 27,  5, 27, 27,  1,  1,  2,  3,  0, 10, 15, 27,  1, 25, 25, 22,  1,
         7, 27, 27, 14,  4, 18, 27, 27,  4,  1], device='cuda:0')
step: 594
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2969, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.1567e-04, -5.9843e-04,  3.1614e-04,  ...,  4.5156e-04,
         -3.7956e-04, -2.1315e-04],
        [-4.6253e-05, -1.6165e-04,  5.1737e-04,  ...,  5.2261e-04,
         -9.7179e-04, -3.9816e-04],
        [-7.2241e-05,  2.8753e-04,  1.9526e-04,  ...,  1.7452e-04,
          2.6655e-04, -5.3763e-05],
        [-3.8457e-04, -1.7619e-04,  1.5440e-03,  ...,  1.0365e-04,
          5.3692e-04, -5.5981e-04],
        [ 1.7965e-04,  4.8280e-04,  2.9802e-04,  ...,  4.9925e-04,
         -7.7128e-05, -1.9872e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8025, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.0547, -4.0234, -4.8359,  ..., -6.8203, -7.9766, -0.4131],
        [-0.7007, -3.3730, -6.1992,  ..., -8.0781, -5.7773, -1.4502],
        [-6.1641, -4.1953, -2.3828,  ..., -0.7261, -5.0078, -4.1328],
        ...,
        [-4.1836, -2.1211, -3.0898,  ..., -3.3398, -4.5586, -0.8242],
        [-5.5859, -3.4922, -4.1172,  ..., -5.0234, -4.7734, -0.5083],
        [-5.2305, -6.0742, -9.9453,  ..., -8.1172, -7.9805, -6.6523]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 5,  0, 24,  9, 20, 25,  9,  4, 25, 27, 26, 27, 10, 27, 27,  0,  7, 26,
        27,  9,  1,  4,  2,  4, 27, 27, 27,  9,  5, 27,  1, 27, 27, 18,  3, 22,
         0, 26,  3, 17, 15,  1, 10, 20, 15, 27, 27, 11,  3, 18, 11, 20,  4,  6,
         4, 27, 20, 24, 27,  8, 27, 27, 27, 15], device='cuda:0')
step: 595
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4790, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2969, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.8673e-03,  2.4433e-03,  2.4498e-05,  ...,  4.8184e-04,
          1.3008e-03, -6.8092e-04],
        [ 2.1315e-04,  1.4009e-03, -4.2725e-04,  ..., -1.1617e-04,
          1.5221e-03, -7.4577e-04],
        [ 7.9250e-04, -8.6427e-05, -7.4291e-04,  ..., -4.0960e-04,
          6.7472e-04,  8.0585e-05],
        [ 1.4267e-03,  1.1530e-03, -1.7347e-03,  ...,  6.7329e-04,
         -1.1587e-03,  9.2268e-05],
        [ 4.9412e-05,  1.1511e-03, -1.0138e-03,  ...,  5.9891e-04,
          6.3753e-04,  5.4717e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8160, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.2168, -3.3730, -5.2812,  ..., -4.8906, -2.3730, -1.8428],
        [-1.7324, -3.2793, -4.1523,  ..., -4.1094, -3.1855, -1.1387],
        [-3.7734, -3.1484, -1.9453,  ..., -2.3359, -4.9297, -2.8516],
        ...,
        [-2.3730, -4.5586, -4.9805,  ..., -6.9492, -6.7305, -1.0439],
        [-5.3086, -3.0449, -3.1387,  ..., -5.6055, -3.5137, -0.7007],
        [-3.3789, -2.5195, -2.4414,  ..., -3.8164, -1.7686, -1.3789]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26,  9, 18, 27,  9, 15, 15, 27, 22, 27, 27, 27, 26, 27, 27, 18,  4, 10,
        27, 25, 27, 27, 14, 27,  6, 27, 10, 18, 27,  1, 10,  0, 27, 27,  0, 27,
        27, 22, 27, 18,  6, 27, 26, 14, 26,  0, 11, 27, 20,  4,  4, 15, 27, 12,
        21,  3, 20, 27, 27, 20, 11, 27, 10, 20], device='cuda:0')
step: 596
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2386, -0.6147,  ...,  0.4790, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2969, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1719,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.0446e-04, -2.0790e-03,  8.4043e-06,  ...,  1.2994e-04,
          2.8944e-04, -6.1512e-04],
        [ 1.9372e-05,  4.0054e-04,  1.7881e-05,  ..., -4.8280e-06,
          8.8215e-04, -9.8515e-04],
        [-5.1439e-05,  7.6103e-04,  1.0192e-04,  ..., -4.9210e-04,
          4.6873e-04, -1.1808e-04],
        [-7.0715e-04,  1.2422e-04,  2.5320e-04,  ...,  4.5466e-04,
          8.0061e-04,  2.0909e-04],
        [ 4.0698e-04, -3.0756e-04,  1.1277e-04,  ..., -7.4029e-05,
          7.8917e-05,  1.0109e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7828, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -5.2539,  -2.7090,  -3.4277,  ...,  -4.1133,  -3.4746,  -0.8960],
        [ -5.6289,  -3.8633,  -4.7852,  ...,  -5.3633,  -5.8164,  -0.1915],
        [ -4.1094,  -5.0938,  -8.4219,  ...,  -7.4062,  -7.0000,  -5.5156],
        ...,
        [ -5.6445,  -6.4102, -10.7969,  ...,  -8.6875,  -8.8438,  -7.0195],
        [ -4.9453,  -3.8359,  -3.9766,  ...,  -5.4141,  -2.4453,  -0.8052],
        [ -2.0723,  -3.1973,  -4.8203,  ...,  -5.4180,  -4.0898,  -1.8379]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 27, 15, 11,  7, 27, 27, 25,  7,  7, 15, 15, 18,  1, 27, 11, 27,  9,
         7,  2, 15, 27,  9,  4, 25, 13,  7, 10,  3, 27, 27, 20, 17,  9, 24, 27,
         3, 10,  4,  7, 27,  4, 27,  2, 27,  0, 27, 27,  4, 27,  2, 27, 25, 27,
        25,  4, 10, 27, 27, 24,  2, 15, 27,  3], device='cuda:0')
step: 597
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4790, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.1553e-04, -8.4734e-04,  2.8539e-04,  ...,  5.6887e-04,
          4.7207e-04, -5.6696e-04],
        [ 2.1648e-03, -3.5954e-04, -7.9966e-04,  ...,  4.7088e-04,
         -1.4000e-03,  6.3610e-04],
        [ 1.3399e-03, -7.6151e-04, -6.6376e-04,  ..., -6.9714e-04,
         -9.2268e-04,  4.1032e-04],
        [ 2.6345e-05, -6.9141e-05,  5.4121e-04,  ...,  1.8799e-04,
         -3.0780e-04, -6.9332e-04],
        [ 2.9421e-04, -3.7074e-05, -3.5048e-04,  ...,  1.8215e-04,
          8.2195e-05, -5.1594e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7581, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.4492, -4.1328, -4.8203,  ..., -6.4180, -3.8223, -0.3376],
        [-3.2324, -4.0156, -5.3398,  ..., -7.2148, -4.6367, -4.5898],
        [-4.5195, -2.2988, -3.9863,  ..., -5.9883, -4.8633, -0.8618],
        ...,
        [-5.7344, -5.4844, -6.1250,  ..., -6.3125, -4.9531, -4.7656],
        [-5.7344, -3.5332, -4.0000,  ..., -4.9688, -4.1562, -0.4233],
        [-5.3711, -3.7305, -4.2617,  ..., -5.3867, -3.2617, -0.9185]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 18,  4, 11, 10,  7, 27, 17, 13, 27,  5, 27, 27, 27, 10, 18,  4, 24,
        27,  8, 17, 18, 27,  1,  0, 22,  6, 15,  4,  1, 27, 27, 27, 27,  0, 27,
        15, 27, 27, 27, 27, 27, 27, 26, 24, 27,  6, 22, 27, 10,  1, 27,  0,  1,
        27, 24, 18,  3, 27, 27,  4, 20, 27, 27], device='cuda:0')
step: 598
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4790, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.8644e-04, -2.2101e-04, -3.1781e-04,  ...,  6.8998e-04,
          3.6764e-04, -5.4359e-04],
        [-6.5231e-04,  6.4230e-04, -2.2399e-04,  ...,  1.5249e-03,
          1.2636e-03, -7.8869e-04],
        [ 4.4608e-04,  2.2650e-05,  1.2732e-04,  ...,  7.8440e-04,
         -3.8528e-04, -4.7421e-04],
        [ 1.3685e-03, -5.1785e-04, -5.6648e-04,  ..., -9.8228e-05,
         -7.4911e-04, -5.0831e-04],
        [ 1.3614e-04, -6.2132e-04,  7.3910e-06,  ...,  4.9889e-05,
         -1.2410e-04, -5.3048e-06]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8363, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.2930, -3.4004, -5.4492,  ..., -4.6367, -4.0117, -4.5586],
        [-0.7290, -4.1211, -6.2305,  ..., -6.4961, -6.7148, -4.4805],
        [-2.0410, -2.6016, -3.5078,  ..., -4.7578, -3.1504, -1.5088],
        ...,
        [-3.7578, -3.1641, -2.6172,  ..., -2.8516, -3.8047, -2.0859],
        [-4.1484, -2.3066, -3.3848,  ..., -3.4629, -2.8223, -1.4785],
        [-5.3945, -3.5645, -3.3926,  ..., -3.9707, -4.0820, -1.5967]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17,  1,  2, 27, 25,  3, 27, 27,  4, 27, 14,  2, 10, 24, 27, 27,  0,  4,
        27,  0, 14,  1, 17, 27, 27, 17, 24,  6, 27,  4, 27, 26,  9, 23, 20,  7,
        27, 27,  6, 15, 27, 27, 18, 10,  0, 27,  3,  5, 27, 18, 27, 17, 15,  6,
        18, 27,  3, 20, 27,  3, 27, 14, 27, 27], device='cuda:0')
step: 599
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4790, -0.0810,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.3671e-04, -3.2806e-03,  6.6519e-04,  ..., -1.6994e-03,
          8.6606e-05, -9.6989e-04],
        [-8.9407e-04,  1.0729e-03, -9.5844e-04,  ..., -5.8022e-03,
         -9.0027e-04, -1.8787e-04],
        [-2.1327e-04,  6.7663e-04, -1.5831e-04,  ..., -1.0099e-03,
          1.3294e-03, -2.5320e-04],
        [-1.9627e-03, -7.4482e-04,  3.6793e-03,  ...,  1.9798e-03,
          1.5860e-03, -2.1720e-04],
        [-1.2326e-04,  3.9363e-04,  1.5688e-04,  ..., -5.1856e-05,
         -5.7077e-04, -1.0195e-03]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3122, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.0508, -3.1289, -3.5195,  ..., -3.9570, -4.7383, -0.3782],
        [-1.9365, -3.7188, -5.3281,  ..., -7.2344, -5.9219, -4.1562],
        [-5.3477, -3.5645, -3.6582,  ..., -5.3125, -2.5332, -1.3301],
        ...,
        [-5.5352, -3.4863, -4.0352,  ..., -4.7500, -4.4102, -1.6426],
        [-5.5312, -4.4844, -5.1250,  ..., -6.7969, -4.9062, -0.2029],
        [-1.2646, -2.8105, -4.4844,  ..., -4.8750, -3.2793, -2.0762]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([12, 18, 27, 27, 20, 27, 27,  2,  5, 27,  0, 27,  4, 10,  1, 13, 27,  0,
        27,  0,  0, 25,  3, 27, 22, 27,  6, 20, 26, 14, 27,  6, 27, 27, 10,  0,
        17,  4,  9, 18, 26,  7,  0,  7, 17, 17, 27,  5,  0, 27, 14, 10,  0, 27,
        18, 27,  3, 13,  3, 20,  0, 25,  4,  0], device='cuda:0')
step: 600
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.5306e-04,  9.5654e-04, -1.3456e-03,  ..., -2.1877e-03,
          2.1279e-05, -1.5469e-03],
        [ 2.1005e-04, -7.0286e-04, -5.8174e-04,  ...,  4.1485e-05,
          1.1539e-03,  6.8378e-04],
        [-2.0421e-04, -2.6655e-04,  2.1768e-04,  ..., -4.9877e-04,
         -2.9397e-04,  3.6359e-05],
        [-1.0943e-04, -6.1417e-04, -1.8668e-04,  ..., -6.7890e-05,
          5.2643e-04, -1.9932e-04],
        [ 1.5950e-04, -4.4274e-04,  5.9891e-04,  ..., -6.3324e-04,
          2.0301e-04,  2.3389e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7125, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.0879, -2.9316, -3.7910,  ..., -3.4785, -3.7754, -1.8994],
        [-5.8828, -2.7734, -3.1328,  ..., -4.0078, -0.7427, -1.8057],
        [-5.1484, -3.7578, -3.9297,  ..., -5.8672, -5.4609, -0.3208],
        ...,
        [-0.9180, -3.6523, -5.1992,  ..., -6.2930, -3.3867, -2.1992],
        [-1.5107, -2.8223, -4.9961,  ..., -7.1211, -4.3398, -2.8848],
        [-2.5117, -3.7930, -7.0430,  ..., -7.2617, -5.4648, -4.2461]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 26,  3, 27, 27, 27, 25, 20,  2, 27,  0, 27, 10, 10, 15, 27, 15, 27,
        13,  4, 27,  7, 27, 27,  9, 27, 15,  3, 27, 17, 27, 27, 17, 27, 27, 18,
        20, 11, 27,  4,  3, 27,  7,  2, 27,  1,  3, 27, 27, 27,  4,  7, 27, 20,
         2, 27, 27, 27,  1, 26,  1,  0,  0, 20], device='cuda:0')
step: 601
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.1178e-04, -1.2493e-03, -1.8358e-04,  ..., -2.9111e-04,
          8.6260e-04, -3.2425e-04],
        [-4.6194e-05, -1.4772e-03, -8.5926e-04,  ..., -3.7718e-04,
          3.3736e-04,  9.3222e-04],
        [ 3.4690e-05,  1.3590e-04,  8.5497e-04,  ..., -6.5565e-04,
         -8.7166e-04,  1.4801e-03],
        [ 1.3371e-03, -1.0514e-04, -1.5764e-03,  ..., -3.8218e-04,
         -4.6062e-04, -1.1311e-03],
        [ 9.0063e-05, -6.9094e-04, -2.1148e-04,  ..., -6.2227e-05,
          3.4976e-04,  1.3208e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8320, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-7.1133, -5.0820, -5.4102,  ..., -7.8008, -4.0195, -0.5513],
        [-7.3242, -4.5898, -4.4336,  ..., -6.4180, -6.8242, -0.1844],
        [-0.8643, -3.7383, -5.8477,  ..., -8.6016, -5.6133, -1.4736],
        ...,
        [-2.4121, -2.7871, -4.4414,  ..., -5.1445, -3.9121, -0.9585],
        [-5.9648, -3.7461, -4.0898,  ..., -4.9336, -5.7773, -0.6519],
        [-5.0156, -4.2344, -5.2969,  ..., -5.4062, -6.1406, -3.7344]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([26,  2,  4,  5, 27, 27, 27, 27,  4, 24, 11, 17,  0, 18, 27,  7,  1,  0,
        10, 27,  0,  6,  0,  4, 27,  2, 15,  3, 27, 18, 27, 17,  8, 21,  7, 27,
        15,  0, 27, 27, 27, 20, 25, 24,  0,  9,  7, 27, 27, 27, 14,  1, 27, 27,
        24,  8, 27, 25,  0, 15, 27,  4, 27, 20], device='cuda:0')
step: 602
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2385, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2209,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0080e-03,  1.2751e-03,  8.2827e-04,  ..., -4.2057e-04,
         -6.8760e-04,  2.3174e-04],
        [-5.9032e-04,  1.3266e-03,  7.8821e-04,  ..., -1.9836e-03,
         -1.6756e-03, -1.0967e-04],
        [-4.2582e-04,  1.9550e-03,  1.2369e-03,  ...,  4.4727e-04,
         -1.9181e-04,  8.6844e-05],
        [-4.8709e-04,  2.2829e-05,  5.5492e-05,  ...,  1.7273e-04,
         -3.2878e-04,  7.9155e-05],
        [-2.1768e-04, -8.0252e-04,  1.5616e-05,  ..., -1.8322e-04,
         -4.8304e-04,  3.6192e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.5549, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.7656, -4.4531, -5.5312,  ..., -6.2344, -6.0000, -4.3594],
        [-5.6953, -3.6797, -3.6016,  ..., -5.6016, -3.3359, -1.1006],
        [-5.5625, -3.4844, -3.0312,  ..., -4.6094, -4.0156, -0.8740],
        ...,
        [-5.1016, -2.3203, -2.6797,  ..., -3.9141, -3.6797, -0.9771],
        [-6.8320, -3.7363, -2.7051,  ..., -3.5801, -5.5469, -0.3926],
        [-1.5488, -2.6582, -4.0352,  ..., -3.3770, -1.6113, -2.8457]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 27,  6, 15,  9,  4, 26, 27, 27,  9, 27,  0, 10, 10,  0, 17, 27,  3,
         7, 26, 18, 15, 27, 17, 18,  4,  3, 27, 27, 15,  0, 18, 27, 10,  6,  3,
        27,  0, 25, 27, 27, 27,  7, 27, 27,  1,  0,  7,  1,  4, 17,  1, 22,  0,
        27, 27, 27, 18,  0, 27, 27,  2,  2, 13], device='cuda:0')
step: 603
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2385, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.2684e-04, -6.9046e-04, -1.7023e-03,  ...,  7.5245e-04,
          1.1683e-03, -4.6182e-04],
        [-2.7609e-04,  6.9809e-04,  2.7835e-05,  ..., -2.5630e-04,
         -9.2685e-05, -9.1648e-04],
        [ 8.9705e-05, -2.3746e-04, -4.7684e-04,  ...,  5.5981e-04,
         -3.1638e-04, -8.4496e-04],
        [ 4.0627e-04,  1.5914e-04, -2.1267e-03,  ..., -7.8535e-04,
         -2.1982e-04, -1.8082e-03],
        [ 1.5140e-04,  6.6423e-04, -3.1853e-04,  ...,  3.9434e-04,
          6.0511e-04, -3.5429e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6577, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.0176, -3.0645, -3.8613,  ..., -5.8281, -5.9844, -1.5479],
        [-5.3750, -3.0781, -2.1250,  ..., -2.2500, -5.1094, -1.0771],
        [-5.5703, -3.9746, -3.9434,  ..., -4.5859, -5.1641, -0.5840],
        ...,
        [-6.9922, -3.9316, -4.2578,  ..., -5.7734, -6.1172, -0.3684],
        [-5.1445, -3.5977, -4.3164,  ..., -5.9727, -3.6465, -0.9111],
        [-4.7344, -3.1582, -4.2188,  ..., -6.8750, -6.0469, -0.4392]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 8, 25, 27,  7, 25,  6, 27,  3, 20, 15,  1,  0, 18, 17,  3,  5,  9,  3,
        20, 20, 27, 27, 27,  7, 26, 13, 27, 10, 18,  4, 27, 27,  3, 18, 27, 15,
        27,  3, 15, 27, 27, 17, 27, 27, 20, 14,  5, 27,  4, 27,  1,  2, 27, 15,
        27, 27,  9, 18,  0, 18, 10, 10, 27, 27], device='cuda:0')
step: 604
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2385, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.2030e-05,  1.1387e-03, -1.2350e-03,  ..., -6.5899e-04,
          6.1035e-04, -1.0853e-03],
        [-1.7095e-04,  1.0192e-04, -8.8120e-04,  ...,  3.5667e-04,
          1.1015e-03, -3.0684e-04],
        [-2.6083e-04, -3.3832e-04, -4.7565e-04,  ...,  1.0805e-03,
          7.4148e-04,  4.5085e-04],
        [ 2.5153e-05,  1.2338e-04, -2.2697e-03,  ..., -7.2336e-04,
          8.5831e-05, -8.2016e-04],
        [-2.7871e-04, -4.2105e-04, -8.2552e-05,  ..., -8.0466e-06,
          4.0126e-04,  2.4343e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9937, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.7812, -2.1074, -2.7480,  ..., -3.7637, -4.1719, -1.2178],
        [-2.5176, -2.8926, -5.8320,  ..., -5.7500, -3.8613, -3.2363],
        [-2.8496, -3.2715, -4.3047,  ..., -5.2109, -4.5234, -0.7871],
        ...,
        [-3.4453, -3.5078, -5.3516,  ..., -6.6797, -6.2891, -1.1328],
        [-3.1309, -3.6621, -4.8945,  ..., -6.1914, -4.3477, -4.3633],
        [-3.5430, -3.6992, -3.4961,  ..., -4.6211, -4.1836, -2.1992]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 15, 27, 25, 12, 18, 27,  4, 27, 27,  1,  4, 27, 27, 27, 15, 20,  0,
        10, 25,  8, 20,  7,  6,  3, 27, 12, 27,  1, 27,  8, 27,  7, 15,  7, 27,
         7, 27, 20,  0, 26,  6,  1, 25, 27,  0, 10, 27,  3, 25, 24, 27,  3,  1,
        10,  7,  5, 26,  0, 10,  1,  0, 18,  2], device='cuda:0')
step: 605
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2385, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0496e-04,  1.7281e-03,  1.5759e-04,  ...,  1.1244e-03,
          3.5143e-04, -2.3830e-04],
        [ 3.1781e-04,  1.7834e-04,  3.9291e-04,  ...,  7.8249e-04,
          3.3236e-04, -5.2691e-04],
        [ 1.6987e-04, -3.4213e-04,  1.7977e-04,  ..., -1.9693e-04,
         -1.8253e-03,  4.3702e-04],
        [-4.3273e-04,  1.7023e-04,  1.1320e-03,  ...,  7.7438e-04,
         -7.6294e-04,  7.5150e-04],
        [-2.3162e-04, -5.0020e-04,  1.9789e-04,  ..., -6.2287e-05,
         -7.3528e-04,  5.5885e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.5425, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.5293, -5.1406, -5.7500,  ..., -7.5469, -5.7969, -0.2179],
        [-0.9468, -3.4609, -5.0234,  ..., -6.5859, -6.5859, -3.5410],
        [-4.2695, -3.0645, -4.2227,  ..., -6.5508, -6.4570, -0.5957],
        ...,
        [-6.8945, -4.1133, -4.2539,  ..., -5.9258, -3.2559, -0.8960],
        [-5.6875, -4.9336, -4.7500,  ..., -4.8750, -5.3594, -3.6230],
        [-2.9453, -3.3672, -3.7109,  ..., -3.6172, -5.1953, -0.5850]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 20, 27, 27, 27, 18, 14,  4,  4,  0, 10,  3,  6, 17,  7,  2, 27, 27,
         2,  1, 27, 10, 24, 25, 25, 10, 18,  6, 27, 27, 27, 27, 27, 27, 27, 27,
         5, 25, 22,  3,  7, 14,  7, 27,  9, 27,  2, 27, 27, 15, 27, 20, 27, 15,
        20, 15, 27, 26, 27, 27, 27, 27, 10, 27], device='cuda:0')
step: 606
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2385, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.0800e-04,  4.2367e-04, -1.9431e-04,  ..., -9.4223e-04,
         -2.3460e-04, -1.0128e-03],
        [-4.4775e-04,  7.8559e-05, -3.4118e-04,  ...,  1.5345e-03,
          1.7481e-03, -6.2752e-04],
        [-2.3139e-04,  3.8528e-04,  1.4114e-04,  ..., -4.7863e-05,
         -8.0252e-04, -5.6458e-04],
        [ 1.3323e-03,  4.2200e-04, -5.3310e-04,  ..., -2.9862e-05,
          1.6260e-04, -2.2078e-04],
        [-9.1016e-05, -4.9210e-04,  3.1590e-05,  ..., -4.5717e-05,
         -2.7180e-04,  5.2452e-06]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7908, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.6211, -4.4805, -4.8711,  ..., -6.0742, -4.4805, -0.2468],
        [-5.1523, -3.5918, -3.9043,  ..., -4.0273, -4.0938, -1.0605],
        [-6.4883, -4.5039, -3.7402,  ..., -5.9570, -3.7402, -0.4583],
        ...,
        [-8.0234, -4.8477, -5.2227,  ..., -8.5391, -6.3164, -0.0671],
        [-6.1719, -4.1562, -3.6895,  ..., -5.3594, -5.9062, -0.6577],
        [-2.0508, -3.1289, -3.5352,  ..., -3.8789, -3.6602, -1.7393]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 10,  6, 27, 20,  7, 18,  0, 10, 27, 27, 25, 27,  4, 17,  5, 27, 27,
         4,  0, 27, 15,  0,  1,  0, 27, 25,  7,  6, 27, 27, 27, 27, 15, 25, 15,
         1, 27, 27, 27,  3,  7, 20,  6,  8,  6, 27, 12,  0, 27, 17, 17, 10, 18,
        25, 27,  7,  4, 22, 15, 17, 27, 27, 15], device='cuda:0')
step: 607
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2385, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.3091e-04,  1.9550e-03,  6.8617e-04,  ...,  3.8719e-04,
          1.3208e-04,  6.2180e-04],
        [-1.6451e-05, -1.2007e-03, -8.1444e-04,  ..., -1.9188e-03,
         -7.3385e-04,  1.6479e-03],
        [-2.7716e-05,  1.2541e-04,  1.2541e-03,  ...,  3.3140e-04,
         -1.6651e-03,  5.6458e-04],
        [-8.0824e-04,  1.4400e-04, -8.3864e-05,  ...,  7.3528e-04,
          1.1339e-03,  1.1711e-03],
        [-9.0718e-05,  7.6056e-05,  3.9387e-04,  ..., -5.4836e-05,
         -1.1253e-04,  3.6001e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.3896, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.9707, -2.9082, -3.9707,  ..., -5.2656, -3.4863, -2.0332],
        [-5.6445, -3.1289, -3.2051,  ..., -2.4414, -5.1602, -2.0664],
        [-1.6221, -3.0273, -4.9492,  ..., -6.7148, -6.2930, -2.6367],
        ...,
        [-4.9805, -3.2930, -3.7773,  ..., -5.6836, -1.9492, -0.7622],
        [-5.3789, -4.3320, -4.7695,  ..., -5.6602, -5.9727, -0.1279],
        [-2.8477, -3.0352, -3.5664,  ..., -5.9883, -3.9414, -0.4258]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 14, 18, 27, 27, 27, 27, 27, 27, 11, 27, 12, 27, 27, 17, 17, 27,  3,
        27,  3, 10,  4, 27, 27, 27,  7,  2, 10, 27, 27, 15, 17, 27, 27, 27, 25,
         2, 27,  7, 18, 27,  7, 27,  0, 25, 26,  4, 27, 15, 17, 27, 27,  4, 27,
        27,  9, 27, 27, 17, 27, 27,  1, 25, 27], device='cuda:0')
step: 608
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2385, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.4564e-04,  2.1706e-03,  7.6485e-04,  ...,  1.4758e-04,
         -5.9748e-04,  2.8682e-04],
        [-3.5763e-04, -7.1406e-05,  8.8215e-06,  ...,  3.3069e-04,
         -6.9618e-05,  3.9387e-04],
        [-5.5432e-05,  1.4901e-04,  6.7902e-04,  ..., -2.4295e-04,
         -7.7581e-04,  5.5313e-04],
        [ 2.3580e-04,  7.7724e-04,  5.3978e-04,  ...,  3.6907e-04,
         -4.4966e-04,  4.9019e-04],
        [-1.5116e-04, -2.3818e-04, -2.2745e-04,  ...,  1.6797e-04,
         -2.7752e-04,  1.2875e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2925, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.9766, -4.5703, -4.5078,  ..., -6.6797, -6.2109, -1.8525],
        [-3.3105, -2.2969, -3.4844,  ..., -5.0469, -1.8271, -1.5615],
        [-3.2676, -3.4707, -4.6875,  ..., -6.7031, -4.1094, -0.7202],
        ...,
        [-4.7695, -3.4902, -4.0820,  ..., -3.5996, -4.4570, -1.8018],
        [-6.1094, -3.9219, -4.3438,  ..., -6.0156, -4.9844, -0.2659],
        [-4.1328, -3.5410, -3.8223,  ..., -5.6367, -5.3398, -0.5098]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20, 17, 27, 25, 15, 27,  7, 10, 10,  0, 27,  0,  4, 15, 17, 15, 27, 27,
         7,  2, 26,  1,  3, 17,  4, 18, 27, 27, 22,  4,  6,  4,  1,  9, 27, 17,
        18, 27, 26, 27, 15,  0, 27,  6,  1, 18, 27, 20,  3, 14, 11, 27, 14,  1,
         3, 10, 27,  5, 27,  4, 20,  9, 27, 22], device='cuda:0')
step: 609
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2385, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.6941e-04, -7.0858e-04,  3.1948e-05,  ..., -7.9107e-04,
          6.6948e-04, -1.0099e-03],
        [ 9.7847e-04,  2.8276e-04,  4.8351e-04,  ..., -3.7842e-03,
         -3.0422e-03, -5.8365e-04],
        [-1.3518e-04, -6.2275e-04, -8.5735e-04,  ...,  1.4095e-03,
          1.4858e-03, -2.2292e-04],
        [-3.6550e-04, -1.3323e-03, -1.0080e-03,  ...,  8.6498e-04,
          1.1396e-03, -1.1501e-03],
        [ 8.8155e-05, -3.2139e-04,  2.5773e-04,  ...,  7.5102e-06,
          5.8532e-05, -1.8966e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8361, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.7852, -2.5508, -3.0020,  ..., -2.3164, -4.2227, -1.5498],
        [-3.0527, -3.8027, -5.1445,  ..., -6.7383, -2.3184, -1.8809],
        [-4.9805, -4.1328, -4.4023,  ..., -6.1523, -4.2148, -0.4785],
        ...,
        [-5.1680, -2.3711, -2.9961,  ..., -3.5430, -5.7617, -1.6211],
        [-1.5977, -3.8477, -4.7070,  ..., -7.1602, -5.2852, -3.3320],
        [-6.0664, -3.7383, -3.6445,  ..., -4.3008, -5.4570, -0.8623]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 27, 27, 27, 27, 10, 20, 22, 10, 20,  4, 18,  5, 27, 27, 22, 27, 27,
         6, 27,  3, 15,  3,  0,  4, 27, 25, 17, 27, 27, 25, 27, 27,  0, 20, 10,
         6, 18, 27, 10, 27, 10,  9, 27, 27, 27,  1, 27, 15,  9, 25, 23, 27, 25,
        26,  6, 27, 18, 15, 13, 27, 11, 18, 22], device='cuda:0')
step: 610
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2385, -0.6147,  ...,  0.4790, -0.0812,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0691e-03,  1.2369e-03,  2.2984e-03,  ...,  3.8147e-04,
         -1.8907e-04,  1.6356e-03],
        [-1.6582e-04,  1.1349e-03,  1.3037e-03,  ...,  1.8682e-03,
          6.3610e-04, -2.2531e-04],
        [-6.5625e-05,  5.9175e-04,  6.5517e-04,  ...,  1.8907e-04,
         -2.4414e-04,  3.9291e-04],
        [-3.3092e-04,  4.7624e-05,  1.0262e-03,  ...,  9.9754e-04,
          7.7105e-04,  4.3869e-04],
        [ 7.9453e-05,  6.4564e-04,  2.8634e-04,  ...,  2.6131e-04,
          1.2565e-04,  1.1492e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8753, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.9258, -3.9434, -3.4902,  ..., -3.8633, -5.7227, -0.8799],
        [-4.1328, -4.8828, -8.6797,  ..., -7.5234, -6.6953, -5.3047],
        [-6.1680, -4.5273, -4.9336,  ..., -6.6523, -3.6660, -0.7134],
        ...,
        [-4.2227, -3.9121, -4.2539,  ..., -5.2227, -5.2070, -0.5215],
        [-3.0527, -3.7246, -4.5352,  ..., -3.6465, -5.0664, -0.6309],
        [-2.9082, -4.3125, -4.8164,  ..., -7.0195, -6.4727, -0.4553]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 15,  7, 27,  0, 27, 27,  4,  1,  2,  1, 15, 27,  4,  4,  0,  0,  4,
        13,  0,  1, 27, 25, 22, 27, 27, 27, 25, 27,  1, 27, 18, 20, 10,  7, 13,
        27, 27, 26, 13,  6, 27, 27, 27,  1,  8, 15, 27, 20, 15,  2, 13,  6, 27,
        18, 27, 20, 18,  6,  8, 27, 27, 27, 27], device='cuda:0')
step: 611
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2385, -0.6147,  ...,  0.4790, -0.0812,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.6016e-03, -1.6279e-03,  2.3193e-03,  ..., -1.2608e-03,
         -1.7872e-03, -3.2043e-04],
        [ 5.7793e-04, -4.3678e-04, -2.9659e-04,  ..., -1.3008e-03,
         -1.0532e-04, -1.2493e-03],
        [-4.2892e-04, -3.0458e-05,  1.3018e-03,  ..., -2.8920e-04,
         -4.4298e-04, -1.2779e-04],
        [-2.2640e-03, -4.7016e-04,  1.1578e-03,  ..., -6.4087e-04,
          2.5768e-03,  2.5702e-04],
        [-2.3699e-04, -8.7976e-04,  1.5430e-03,  ..., -8.1062e-04,
         -4.4680e-04, -8.2874e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6237, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.9961, -2.5117, -3.5273,  ..., -4.5117, -2.3867, -1.0898],
        [-0.8901, -3.4531, -4.8906,  ..., -7.3438, -3.4688, -1.6396],
        [-5.0586, -3.5586, -3.5098,  ..., -4.4492, -5.1992, -0.4639],
        ...,
        [-3.2305, -4.3086, -4.4023,  ..., -5.7148, -6.8398, -0.3088],
        [-3.0371, -3.8496, -4.5039,  ..., -5.6289, -5.0664, -0.5532],
        [-4.9336, -3.8867, -3.5605,  ..., -4.6680, -2.7324, -0.4968]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([11, 18, 27, 18, 27,  8,  1, 27, 27, 17, 27,  1, 22, 27, 27, 15,  1,  9,
        22, 27,  5,  6,  2,  1,  3, 15, 18,  1, 27,  8, 10, 13, 27, 27,  2, 24,
         9, 27, 27,  0, 25, 27, 27,  6,  0, 26, 27, 27, 25, 27, 27,  1,  1,  0,
        14, 27, 27, 20, 11, 27, 25, 27,  4, 27], device='cuda:0')
step: 612
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2385, -0.6147,  ...,  0.4790, -0.0812,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0993,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.5163e-04,  5.5504e-04,  3.1948e-04,  ...,  1.0948e-03,
          4.8995e-05,  4.3988e-05],
        [-2.8276e-04,  2.4009e-04,  1.7309e-04,  ...,  1.5841e-03,
          9.5415e-04, -2.6226e-04],
        [-2.5654e-04,  3.3998e-04, -1.4126e-04,  ...,  9.1553e-04,
          7.0906e-04, -4.9353e-04],
        [ 2.2173e-04,  6.1083e-04, -3.1805e-04,  ..., -1.7333e-04,
          3.7360e-04,  6.3944e-04],
        [-1.1957e-04,  3.5262e-04, -1.7643e-04,  ...,  4.9543e-04,
          1.5283e-04, -5.8472e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.4395, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3008, -2.4902, -3.2090,  ..., -4.2852, -4.4883, -0.5361],
        [-3.6191, -3.0098, -3.5566,  ..., -4.6328, -4.0391, -0.4153],
        [-4.1016, -3.7754, -4.6484,  ..., -5.6797, -3.5723, -0.7749],
        ...,
        [-4.6719, -3.1387, -3.1250,  ..., -3.6406, -5.2188, -0.6709],
        [-6.0312, -4.3594, -4.7188,  ..., -6.5312, -4.4219, -1.2812],
        [-4.5859, -4.2422, -4.6172,  ..., -5.0391, -6.8047, -0.2101]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 22, 27, 27, 15,  6, 27, 15, 18, 18, 27,  7, 27, 25, 27, 27, 17,
        11,  3, 27, 10,  7, 10, 26, 17, 18,  9, 20, 25, 27,  4, 27, 27, 15, 24,
        14,  3, 27, 17, 27, 27, 27, 18, 10,  2, 15,  7, 27, 18,  1, 27, 27,  1,
        27, 25, 15,  9, 27, 27, 18, 25, 27, 27], device='cuda:0')
step: 613
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7666, -0.2385, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.6818e-04,  8.1158e-04,  9.2268e-04,  ...,  5.3978e-04,
          2.0385e-04,  6.5184e-04],
        [ 1.0908e-05, -3.4308e-04,  3.7026e-04,  ...,  7.2861e-04,
         -9.8228e-05, -1.1206e-05],
        [ 1.0711e-04, -5.6744e-04,  2.2233e-05,  ...,  6.8665e-04,
          1.3995e-04, -5.6076e-04],
        [-4.3535e-04,  4.3225e-04,  4.3869e-04,  ...,  1.1688e-04,
          4.7660e-04,  9.1195e-05],
        [-3.7813e-04,  3.7432e-04,  4.6134e-05,  ..., -6.6757e-05,
          1.9479e-04,  1.0026e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9789, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.2246, -1.9736, -1.6934,  ..., -2.7090, -2.4121, -3.0996],
        [-3.8164, -2.8633, -2.8320,  ..., -4.8945, -4.3945, -0.7544],
        [-5.9766, -3.7871, -3.9609,  ..., -5.4297, -5.3984, -0.2412],
        ...,
        [-3.0039, -2.9414, -3.7383,  ..., -3.1289, -3.9102, -0.8159],
        [-5.4492, -3.6543, -4.0938,  ..., -5.1094, -4.7188, -0.6235],
        [-3.6094, -2.4219, -3.9844,  ..., -4.7969, -1.4844, -1.1875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([14,  9, 27, 27, 26,  2,  0, 22, 27, 25, 15, 27, 27,  7, 27,  0, 27, 17,
        24, 27,  0, 15,  4, 27,  2,  6,  9, 27, 27, 26,  1, 27, 27,  7,  4,  3,
        25, 20, 12, 12, 10,  3, 13, 24,  4,  4, 27, 18, 15, 10,  5,  8, 13, 27,
        18,  7,  1, 27, 11, 27, 17, 27, 27, 17], device='cuda:0')
step: 614
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.7588e-04, -1.1597e-03, -1.9054e-03,  ...,  3.3200e-05,
          3.9434e-04, -1.2102e-03],
        [ 2.9325e-05,  6.1798e-04, -2.3961e-05,  ..., -1.7967e-03,
         -4.6825e-04, -4.6706e-04],
        [ 6.4909e-05,  1.5330e-04,  3.8385e-04,  ...,  4.5776e-04,
         -5.0831e-04, -7.2896e-05],
        [ 7.2861e-04, -9.1970e-05, -5.8174e-04,  ..., -4.1723e-04,
          1.8716e-04,  8.5235e-06],
        [ 2.4605e-04, -2.7084e-04, -5.4300e-05,  ..., -6.5863e-05,
         -2.5797e-04,  1.6129e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9205, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6016, -3.2266, -3.7578,  ..., -4.3359, -4.2109, -0.3523],
        [-1.4951, -2.5254, -5.1055,  ..., -5.1992, -4.1523, -2.3086],
        [-4.0352, -3.6426, -3.5645,  ..., -4.1914, -2.2207, -1.0332],
        ...,
        [-3.2988, -2.4082, -3.0332,  ..., -2.5488, -2.0645, -2.5957],
        [-5.5859, -2.9609, -3.4902,  ..., -3.2090, -3.4902, -0.6626],
        [-5.2461, -2.5898, -2.6504,  ..., -3.3223, -4.6055, -1.7139]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 17, 20,  2,  0, 11, 25,  1, 27, 27, 25,  3, 27, 27,  7, 15, 27,  5,
        27,  4,  4, 27, 22, 27, 18,  6, 27,  1,  7, 20, 20, 27,  3, 27,  3, 22,
        27, 13, 27,  4,  0, 27,  4, 27, 20,  0, 27,  3, 26, 27,  6,  2,  1, 13,
        18,  8, 27,  4,  0, 27,  0, 13, 27,  3], device='cuda:0')
step: 615
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4790, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.6001e-04,  9.0981e-04, -9.0933e-04,  ..., -4.7684e-07,
         -2.5773e-04, -2.2650e-04],
        [-2.0468e-04,  6.0940e-04, -5.5492e-05,  ...,  1.3790e-03,
          5.3596e-04,  3.5691e-04],
        [ 4.1699e-04,  1.1625e-03,  1.0376e-03,  ..., -9.6607e-04,
          1.7142e-04,  1.7948e-03],
        [ 7.9203e-04,  3.5954e-04,  2.1935e-04,  ...,  1.0884e-04,
         -9.9564e-04, -1.1623e-05],
        [ 4.1628e-04,  9.9480e-05, -5.1308e-04,  ...,  3.0231e-04,
          7.4685e-05,  2.4307e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0313, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3477, -2.1758, -2.5664,  ..., -4.1133, -2.9414, -1.0654],
        [-2.8789, -2.4570, -2.8477,  ..., -1.9414, -5.1445, -1.4580],
        [-2.9668, -4.7461, -6.0117,  ..., -8.2812, -6.1055, -4.7930],
        ...,
        [-2.6445, -3.7676, -5.3320,  ..., -6.7070, -5.5820, -0.8623],
        [-2.9082, -2.2520, -2.8613,  ..., -3.7988, -1.4092, -2.2207],
        [-5.3359, -3.0547, -3.0703,  ..., -1.1787, -5.1172, -2.2090]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([14, 11, 18, 27, 27, 27,  3,  4, 27,  7, 27, 27, 11,  3, 27,  9, 27, 15,
         7,  0, 10,  4, 10, 20, 10, 26,  6, 27,  7, 22, 12,  3, 27,  3, 27, 15,
         3,  0, 27, 10, 22,  0, 10, 27,  0,  0,  4, 18, 25, 20, 10, 27, 22, 17,
         0, 25,  3,  4, 15, 22,  0,  4, 26,  9], device='cuda:0')
step: 616
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1244e-03, -1.5318e-05,  1.9531e-03,  ...,  5.1498e-04,
         -1.6756e-03,  1.4572e-03],
        [ 7.9584e-04, -6.8378e-04,  8.5020e-04,  ...,  7.2956e-04,
         -7.5626e-04,  2.6155e-04],
        [ 4.9114e-04, -4.5061e-04,  1.7655e-04,  ..., -6.1703e-04,
         -3.4499e-04,  1.1387e-03],
        [-9.9277e-04, -5.3263e-04,  1.5802e-03,  ...,  8.3590e-04,
         -4.8161e-05,  1.4849e-03],
        [-1.3113e-06, -6.2585e-06,  4.5252e-04,  ...,  3.2520e-04,
         -8.3160e-04,  4.6420e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6217, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.2656, -3.2344, -3.7188,  ..., -4.1250, -6.2188, -0.6099],
        [-5.1719, -2.4668, -2.4355,  ..., -2.6074, -4.4375, -1.2168],
        [-5.4219, -2.8125, -2.8906,  ..., -0.6406, -4.9688, -3.3594],
        ...,
        [-9.3047, -7.0859, -6.8203,  ..., -6.0391, -8.7266, -8.7891],
        [-6.6758, -4.8164, -5.5195,  ..., -7.9258, -5.0352, -0.1746],
        [-0.9941, -3.8066, -6.4297,  ..., -5.6641, -6.6328, -2.7129]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 11, 11,  3, 27, 27,  8, 27, 27,  3,  1, 15, 15,  7, 27,  1,  5, 27,
         0, 22, 10,  5, 27, 27, 27, 27, 27, 27,  6,  6, 26, 15, 11, 27, 27, 27,
        27,  6, 27,  7,  7,  5,  2,  0, 15, 27, 27,  1, 27, 15, 27, 15, 18, 27,
         2, 22, 18, 17,  1, 27,  5,  3, 27,  0], device='cuda:0')
step: 617
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.9274e-06, -1.1187e-03, -1.2083e-03,  ..., -7.6675e-04,
          1.7428e-04, -1.9875e-03],
        [ 2.6464e-04, -1.0948e-03, -1.2732e-03,  ...,  5.8508e-04,
          1.2474e-03,  8.3685e-04],
        [ 2.7370e-04,  5.2261e-04, -1.9503e-04,  ..., -7.5483e-04,
          7.1430e-04,  5.1785e-04],
        [-4.3917e-04, -6.6185e-04,  9.4461e-04,  ..., -4.3249e-04,
          7.2289e-04, -1.3518e-04],
        [ 1.0401e-04, -2.9135e-04, -3.4785e-04,  ...,  1.1927e-04,
         -9.9361e-05, -2.3007e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7181, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.6187, -3.2129, -5.6328,  ..., -7.5078, -4.1797, -3.1973],
        [-0.6641, -3.2266, -5.0234,  ..., -7.2578, -5.1641, -1.4141],
        [-2.8535, -2.6992, -3.2285,  ..., -5.1836, -4.2305, -1.2764],
        ...,
        [-4.8281, -3.1270, -2.7520,  ..., -3.0645, -2.9707, -1.8613],
        [-4.2148, -3.2129, -3.6348,  ..., -4.4961, -4.3086, -0.4475],
        [-2.2793, -3.2637, -5.9648,  ..., -5.5898, -5.7305, -4.6367]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 27, 27, 27, 27,  0,  5, 14,  8,  0, 27, 15, 25, 11, 27,  9,  3, 22,
        27, 15, 15,  8, 15,  8, 24, 17, 17,  4,  0,  6, 13, 18, 27,  1, 26, 27,
        15, 18,  1, 27,  4, 14,  1, 15,  9,  7, 27,  7, 27,  0, 17,  4,  0, 24,
         4,  4,  0, 14, 14,  4, 27, 27, 27, 15], device='cuda:0')
step: 618
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.7874e-04,  1.2283e-03,  8.9455e-04,  ..., -1.1082e-03,
         -1.1134e-04, -2.3603e-04],
        [ 2.0957e-04,  3.9887e-04,  4.1485e-05,  ...,  2.3308e-03,
          2.2793e-03, -1.0300e-03],
        [ 3.2902e-04,  5.6219e-04, -5.3692e-04,  ..., -1.5717e-03,
          5.0545e-04,  3.7003e-04],
        [ 1.3542e-03,  6.5041e-04, -7.0667e-04,  ..., -1.2803e-04,
         -1.7090e-03, -5.7697e-04],
        [ 1.2350e-04,  1.5378e-05, -5.3072e-04,  ...,  1.1832e-04,
         -3.8791e-04, -1.4424e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9946, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.8379, -3.2910, -4.5234,  ..., -3.7285, -4.6328, -0.8687],
        [-6.5703, -4.8203, -4.4766,  ..., -6.1797, -2.8652, -1.5381],
        [-1.8535, -2.3691, -3.1816,  ..., -2.9629, -2.2910, -2.2754],
        ...,
        [-3.4512, -2.4648, -3.5273,  ..., -3.6211, -2.5273, -1.2939],
        [-3.4805, -3.0586, -4.6367,  ..., -5.9805, -5.5117, -0.2620],
        [-4.8438, -2.4375, -3.0625,  ..., -4.0625, -3.1406, -1.1406]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20, 27,  3,  3,  0, 14,  4, 27,  1, 27, 18,  0, 10,  2, 27, 27, 27,  1,
        24, 20,  8, 24, 18, 27, 14, 17, 15,  7, 27,  2,  7, 27, 20, 18,  4, 18,
         6, 27,  3,  4, 12, 26, 15,  2, 17,  8,  7,  4, 27, 26, 22, 27, 20, 17,
         7, 26, 25, 19, 13, 20,  3,  2, 27, 10], device='cuda:0')
step: 619
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4092, -0.2971, -1.7197,  ..., -0.6597,  0.6538,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.2561e-04, -1.1044e-03, -7.0190e-04,  ..., -1.5974e-04,
          6.4611e-05, -1.4811e-03],
        [-3.8862e-04, -1.7738e-03, -8.6689e-04,  ...,  1.3571e-03,
          1.5259e-03,  1.9798e-03],
        [ 1.5140e-04, -6.2037e-04, -3.7885e-04,  ...,  1.9336e-04,
          1.2140e-03,  1.8048e-04],
        [-1.2417e-03, -6.8045e-04,  1.2846e-03,  ...,  1.3914e-03,
          1.1406e-03,  1.9050e-04],
        [ 2.1839e-04,  4.9734e-04,  8.0228e-05,  ...,  5.2977e-04,
         -1.8656e-04, -1.6999e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8559, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.0615, -2.7793, -4.5000,  ..., -4.9531, -3.2793, -1.7021],
        [-4.6445, -3.0664, -2.5957,  ..., -3.6289, -6.2695, -1.2842],
        [-1.4453, -2.5703, -3.5234,  ..., -2.4453, -1.3516, -2.8984],
        ...,
        [-5.0312, -3.2051, -3.7676,  ..., -4.6250, -1.7041, -1.3135],
        [-3.5605, -2.6699, -3.5605,  ..., -4.2344, -2.4980, -1.0137],
        [-1.6846, -3.0742, -5.1992,  ..., -6.4648, -4.5898, -1.5283]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 25,  3,  7, 27, 27, 17,  7,  1, 26, 17,  1,  6, 17,  2, 27,  9,  4,
        27,  0, 15, 27, 27,  3, 13, 27, 27, 27,  0,  7, 26,  4,  7,  8,  4, 17,
        27,  4, 22, 27,  2, 27,  2, 18, 26,  3, 17, 27, 15,  4, 27, 21, 27, 26,
        27,  5, 27, 27, 17, 27, 25,  7, 27, 27], device='cuda:0')
step: 620
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7080, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.7619e-05,  1.0586e-03,  8.9645e-05,  ...,  6.1572e-05,
          3.4237e-04, -4.0650e-04],
        [-9.9897e-05, -2.4009e-04, -9.1934e-04,  ..., -8.5831e-04,
          4.6313e-05,  3.7098e-04],
        [-6.7115e-05, -8.9467e-05,  2.5511e-04,  ...,  1.6475e-04,
         -6.7568e-04, -2.1815e-04],
        [-9.7752e-04, -4.2462e-04,  8.2159e-04,  ...,  3.3879e-04,
          7.6723e-04,  7.3099e-04],
        [ 4.3869e-05, -3.5715e-04,  2.7299e-05,  ..., -2.3222e-04,
         -3.1114e-04,  4.2737e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6557, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.6387, -3.7168, -4.8750,  ..., -5.2969, -3.0293, -1.2012],
        [-4.0352, -3.2539, -2.8789,  ..., -2.2852, -4.5977, -1.2705],
        [-5.9531, -3.7363, -4.7969,  ..., -6.3906, -3.4219, -1.7979],
        ...,
        [-6.1172, -3.3184, -3.6309,  ..., -3.9277, -5.3008, -0.3655],
        [-6.2891, -4.2422, -4.6797,  ..., -5.4922, -3.3672, -1.4775],
        [-4.4297, -4.2891, -5.2578,  ..., -5.1797, -3.6172, -4.2578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 10, 27, 27, 27,  0, 27, 27,  8,  3, 27,  0,  1, 10, 27, 11, 27,  2,
        27, 27, 14,  5, 27,  6, 14,  3, 17,  0, 17, 13, 27,  7, 15, 25, 18, 17,
        20, 27, 27, 11, 22,  4,  7, 27, 27, 27,  0,  0,  7, 27, 26, 27, 14, 18,
         6,  0, 27,  0, 27,  9, 18,  3,  7, 17], device='cuda:0')
step: 621
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.2384e-04, -5.8031e-04,  2.0623e-04,  ..., -2.0385e-04,
         -4.6670e-05,  7.6914e-04],
        [ 3.4690e-05,  7.6199e-04,  1.6785e-03,  ...,  2.4259e-04,
         -1.5850e-03, -2.1553e-03],
        [-1.5497e-06, -2.7943e-04, -4.4489e-04,  ..., -5.6839e-04,
          5.9557e-04, -6.7139e-04],
        [-1.3075e-03, -5.4896e-05,  5.5933e-04,  ..., -8.7118e-04,
          1.2617e-03, -1.2517e-04],
        [-3.3975e-06, -4.9353e-04,  4.9686e-04,  ..., -8.6975e-04,
         -1.2398e-05, -3.3689e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8704, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3633, -5.3633, -6.4102,  ..., -8.6719, -6.9883, -7.0664],
        [-5.1797, -2.0234, -2.8203,  ..., -3.1484, -4.1953, -2.3672],
        [-5.3242, -2.0898, -2.9199,  ..., -3.5742, -5.0586, -1.0439],
        ...,
        [-6.7695, -4.7383, -5.2539,  ..., -6.4883, -3.5195, -2.2383],
        [-6.8906, -3.0957, -2.8613,  ..., -2.0488, -3.4238, -2.6270],
        [-5.0977, -2.3770, -2.6758,  ..., -3.7832, -4.4570, -0.6123]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 10, 27, 27, 27, 27, 27, 27, 27, 11, 11, 25, 27, 17,  1, 15,  5,  0,
         7,  1, 13,  7, 20, 27, 27,  0,  1, 13, 27,  3, 18, 27, 21, 27,  7,  7,
         3,  0,  5,  6, 20, 13,  0, 20, 27, 17, 26,  3, 27, 27, 27, 15, 25, 27,
         5, 12, 27, 27,  2, 22, 27,  7, 14,  2], device='cuda:0')
step: 622
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.7057e-04, -8.8501e-04, -4.1556e-04,  ..., -1.0071e-03,
         -1.5020e-04,  6.9571e-04],
        [-5.0831e-04,  3.0303e-04,  9.9182e-04,  ...,  8.0490e-04,
         -3.1853e-04, -5.0640e-04],
        [ 6.8426e-05, -9.5034e-04,  1.8525e-04,  ..., -4.3631e-05,
          3.7384e-04,  2.1768e-04],
        [ 4.3929e-05,  9.2506e-05,  7.4100e-04,  ...,  1.0514e-04,
          1.0929e-03,  3.4428e-04],
        [-1.3566e-04,  2.2674e-04,  2.7132e-04,  ..., -4.1819e-04,
          2.9373e-04, -2.6226e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6384, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.7520, -1.9395, -2.8145,  ..., -4.2695, -2.8301, -0.9551],
        [-3.8086, -1.9014, -2.6523,  ..., -2.8242, -3.9648, -1.5420],
        [-4.8125, -2.7324, -3.3750,  ..., -0.8901, -4.5781, -3.6562],
        ...,
        [-3.8809, -3.8340, -3.8184,  ..., -4.5547, -4.5820, -0.5684],
        [-4.9883, -4.3320, -5.1758,  ..., -4.4258, -4.4883, -0.3315],
        [-0.9814, -2.2305, -4.6680,  ..., -7.1055, -4.3086, -1.7158]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3,  2,  1, 25,  9, 27, 27, 15,  9, 27, 27,  7, 18, 27, 15, 22, 18, 27,
        15, 20, 27, 20,  8, 18, 10, 18,  1, 25, 27, 15, 27,  4, 27, 27,  0, 11,
        18, 10, 27, 27,  4,  2, 18, 17, 27, 17, 27, 27, 27, 11, 27,  4, 15, 20,
        12,  4, 27,  0, 15, 12, 27, 27, 27,  0], device='cuda:0')
step: 623
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.9087e-04,  3.7730e-05,  2.8062e-04,  ..., -7.3314e-05,
         -5.2118e-04,  1.7881e-04],
        [-8.7023e-06,  2.2900e-04,  3.4404e-04,  ...,  1.2150e-03,
          8.4114e-04, -1.0386e-03],
        [ 9.4533e-05, -3.8767e-04, -6.6662e-04,  ..., -2.2960e-04,
          7.2956e-04, -1.2560e-03],
        [-6.7616e-04,  1.9479e-04,  8.0538e-04,  ...,  4.6372e-05,
          2.0671e-04,  7.5769e-04],
        [-2.3532e-04, -1.0878e-04,  3.6001e-04,  ..., -1.3411e-04,
          4.8697e-05, -1.4651e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7597, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.3711, -3.9805, -3.9492,  ..., -4.5586, -2.2930, -2.7930],
        [-4.7930, -2.9512, -3.6230,  ..., -4.6055, -5.6992, -0.6226],
        [-3.8066, -3.3398, -4.0273,  ..., -3.9336, -3.7441, -0.3704],
        ...,
        [-5.8086, -2.5117, -2.1387,  ..., -2.8398, -3.3105, -1.3252],
        [-2.2520, -3.3613, -5.1758,  ..., -5.8789, -5.3789, -5.4570],
        [-3.4434, -4.4258, -8.1484,  ..., -6.4570, -6.7539, -5.5195]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6,  5,  4, 27, 27,  7, 10,  0, 27, 27,  0, 27,  4, 25, 25, 27, 24, 27,
         0, 20, 20,  3,  9, 27, 27, 13,  3,  3, 25,  1, 22, 15, 27, 11, 27, 27,
        27, 18, 13, 16, 20, 22, 27, 27, 27,  1, 15, 27, 27, 19,  0, 27,  0, 11,
        27,  3, 27, 27, 27, 27, 15,  2, 18, 15], device='cuda:0')
step: 624
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.2500e-04, -2.1400e-03,  8.7261e-04,  ..., -5.0259e-04,
         -1.6451e-04, -4.8447e-04],
        [ 7.2432e-04,  1.1215e-03, -1.0318e-04,  ..., -2.6131e-03,
         -4.3726e-04, -1.0509e-03],
        [ 1.3256e-03,  2.5392e-05, -1.6880e-03,  ..., -5.6028e-06,
          1.2512e-03,  8.0395e-04],
        [-1.8692e-04, -5.7268e-04,  1.9646e-03,  ..., -5.0926e-04,
         -2.5845e-04, -1.1883e-03],
        [ 3.8576e-04, -5.8270e-04,  6.3992e-04,  ...,  1.8334e-04,
         -3.0231e-04, -3.3832e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7246, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.0273, -2.5586, -3.1055,  ..., -4.0117, -3.6211, -1.0576],
        [-0.4014, -4.0273, -5.8867,  ..., -7.2461, -5.0117, -3.4629],
        [-4.5625, -2.8125, -2.8750,  ..., -2.4375, -2.0625, -1.5000],
        ...,
        [-2.7793, -1.8574, -2.2012,  ..., -2.5449, -1.7793, -2.6855],
        [-4.5859, -2.3984, -2.1953,  ..., -1.9141, -3.2891, -1.8213],
        [-0.7578, -3.2578, -5.0547,  ..., -6.6172, -4.9609, -1.5234]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0,  5, 25, 26,  9,  3, 10, 20, 27, 27, 10, 27, 27, 27,  2, 10,  8,
        27, 27, 10, 18, 15, 18, 27,  0,  0,  3, 24,  0,  2, 15, 26, 25,  6,  0,
         4, 27, 12, 18, 10, 27, 20, 27, 27,  6,  0, 27, 27,  7, 18, 27, 27, 20,
         3, 10, 15, 19, 24, 27, 10, 14,  3,  0], device='cuda:0')
step: 625
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.9472e-05, -4.0579e-04,  2.5511e-04,  ...,  5.2214e-05,
          1.2703e-03, -9.6893e-04],
        [-5.1880e-04, -4.2915e-04,  5.3549e-04,  ..., -1.4343e-03,
         -6.3229e-04,  6.1512e-04],
        [ 1.2636e-04, -4.5061e-04, -2.3866e-04,  ...,  1.5402e-04,
          2.0981e-04, -3.6120e-04],
        [ 1.1435e-03, -5.1117e-04, -7.3099e-04,  ..., -1.7715e-04,
         -9.3126e-04, -5.2834e-04],
        [-1.1146e-04,  1.9789e-04, -2.1100e-04,  ..., -1.0151e-04,
          3.0398e-04, -2.3460e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8946, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.3340, -1.4268, -2.5215,  ..., -2.3809, -2.4590, -2.1777],
        [-6.0781, -2.7051, -1.9072,  ..., -1.1572, -5.0781, -3.1426],
        [-4.5938, -1.7354, -2.7676,  ..., -1.8770, -3.8457, -2.7832],
        ...,
        [-5.0820, -3.1602, -2.1914,  ..., -0.8794, -4.6602, -2.5352],
        [-0.8472, -2.2070, -4.3008,  ..., -5.7227, -2.8633, -3.1758],
        [-4.1250, -2.6699, -3.4824,  ..., -2.1699, -4.2148, -1.7168]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2, 25,  1,  4, 10, 27, 27, 20,  5,  3,  2, 11,  9, 27, 13, 25,  3, 18,
        25, 22,  4, 18, 27,  6, 15, 27, 27, 27, 15, 10, 11, 27,  3, 25, 15,  1,
         3, 26, 27, 27, 25,  1,  0, 15, 25,  6, 27, 17,  4, 27, 11,  1, 27,  6,
         0,  0, 14, 18, 22, 20, 27, 27,  0,  9], device='cuda:0')
step: 626
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 8.6689e-04, -4.6730e-04, -1.9665e-03,  ...,  1.6031e-03,
          5.3406e-04, -8.2445e-04],
        [ 1.1325e-06, -8.9931e-04,  2.5094e-05,  ..., -8.3160e-04,
         -1.0738e-03,  8.6355e-04],
        [ 8.3983e-05, -8.0538e-04, -9.0742e-04,  ...,  1.9431e-04,
         -3.3379e-05, -1.0128e-03],
        [-2.9874e-04, -3.0613e-04, -8.6117e-04,  ..., -5.4216e-04,
         -6.1131e-04, -1.4830e-04],
        [-1.6356e-04,  4.1270e-04, -4.2558e-04,  ..., -3.6430e-04,
          1.4210e-04,  6.7294e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6008, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.1211, -2.8848, -3.0723,  ..., -3.2285, -5.2148, -1.0107],
        [-5.4883, -4.7070, -5.2227,  ..., -5.5039, -7.3164, -4.4258],
        [-6.4922, -4.3672, -4.3984,  ..., -5.3203, -3.5859, -1.5391],
        ...,
        [-6.5664, -3.5820, -2.5059,  ..., -1.4268, -4.4727, -1.5986],
        [-5.0586, -2.9336, -2.8398,  ..., -5.9180, -6.3711, -2.6680],
        [-3.8301, -3.5176, -2.9707,  ..., -4.1289, -5.5977, -0.7524]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([22, 20,  7, 27, 27, 14, 27, 27, 27, 27, 27, 27, 18, 27, 27, 15, 10, 27,
        27,  7,  8, 27,  6, 27, 27, 20, 10, 18, 11, 20,  4, 27, 15, 27, 27, 27,
        25,  4,  1,  3, 27, 18, 27,  2, 27, 26, 27, 26, 17, 27, 17, 11, 27,  4,
         3, 27, 27, 15, 20, 27, 15, 25, 20, 27], device='cuda:0')
step: 627
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.2512e-05, -5.5504e-04, -6.4182e-04,  ...,  1.0242e-03,
          4.3154e-04, -5.3585e-05],
        [ 1.9848e-05,  1.5364e-03, -1.2279e-04,  ..., -3.9215e-03,
         -1.9779e-03,  3.1900e-04],
        [ 4.4560e-04,  8.3828e-04, -5.3692e-04,  ..., -3.8052e-04,
          9.1124e-04, -9.8038e-04],
        [-5.5981e-04, -3.0279e-04, -7.1621e-04,  ..., -1.8525e-04,
          2.6822e-06, -1.6832e-04],
        [ 1.9884e-04,  3.3426e-04, -5.9271e-04,  ...,  2.2817e-04,
          2.0170e-04, -6.8569e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.5004, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -5.0508,  -3.8008,  -3.3477,  ...,  -5.5039,  -5.2383,  -0.4250],
        [ -7.1680,  -6.7930, -10.4219,  ...,  -7.2500,  -9.2344,  -9.4062],
        [ -4.4570,  -2.5371,  -3.0684,  ...,  -4.4102,  -4.3477,  -0.7080],
        ...,
        [ -4.5938,  -3.6113,  -4.2344,  ...,  -6.4844,  -4.8281,  -0.5479],
        [ -5.2422,  -2.1016,  -1.8203,  ...,  -2.1484,  -6.8828,  -2.5859],
        [ -4.4531,  -4.2188,  -5.4375,  ...,  -4.8906,  -3.9355,  -4.2500]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 15,  2, 15,  7,  5,  1, 17, 17,  4, 18,  3, 27, 10,  0, 10,  2, 10,
        24,  9, 27,  9, 15, 27, 27, 18, 27,  3,  0, 18, 27, 13, 18,  0, 27, 27,
         3,  4, 27,  1,  0,  5, 15, 27,  5, 18, 27, 27, 26, 27, 13, 17, 18, 27,
         0, 27, 27, 24,  1, 27,  5, 27,  2, 17], device='cuda:0')
step: 628
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.6963e-04, -4.1938e-04, -5.5265e-04,  ...,  6.0844e-04,
         -6.1333e-05, -9.3794e-04],
        [-6.1095e-05,  1.2932e-03,  8.2397e-04,  ...,  3.7026e-04,
         -7.6830e-05, -1.1139e-03],
        [-1.3888e-04, -1.8680e-04, -1.0347e-03,  ..., -1.3638e-04,
         -2.8789e-05, -6.0177e-04],
        [-2.2197e-04,  3.3379e-04, -1.1498e-04,  ...,  2.1052e-04,
         -2.8920e-04,  3.8743e-06],
        [-1.1837e-04,  3.3879e-04, -1.5700e-04,  ...,  8.4686e-04,
         -9.4414e-05,  1.6057e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8335, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.3633, -2.6914, -4.6758,  ..., -6.8633, -4.0195, -3.2695],
        [-5.2969, -5.3594, -8.7031,  ..., -6.5156, -8.0781, -7.2969],
        [-5.0703, -3.2090, -3.4121,  ..., -5.1602, -3.6152, -1.2100],
        ...,
        [-2.4629, -3.9805, -4.9336,  ..., -5.9961, -6.3242, -4.0586],
        [-4.9961, -3.6035, -3.6660,  ..., -4.2617, -2.5723, -0.9946],
        [-5.2461, -3.6211, -3.6523,  ..., -3.6523, -3.8867, -0.8096]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 15,  3,  3,  1, 27,  3, 26, 27,  1, 27, 27, 27,  0,  5,  1,  4,  6,
        10, 15,  2, 19, 25, 27, 27, 27,  2,  4,  0, 24, 15, 27,  5, 27, 27, 18,
        27,  0, 27, 27, 24, 27,  9, 14, 27,  3, 27, 27,  0,  4, 27,  0, 18, 27,
         9,  2, 27,  0,  7, 18,  0, 20, 27,  3], device='cuda:0')
step: 629
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2385, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.5891e-04, -3.2253e-03, -1.0357e-03,  ...,  3.0065e-04,
         -2.0742e-04, -2.6011e-04],
        [-5.6922e-05,  2.3341e-04, -4.8351e-04,  ...,  1.4191e-03,
          7.7343e-04, -1.0357e-03],
        [ 3.6335e-04, -1.1005e-03, -1.4248e-03,  ...,  1.3752e-03,
          1.9093e-03, -9.4318e-04],
        [-7.9250e-04, -1.9133e-04,  6.1321e-04,  ...,  1.8239e-04,
          6.2227e-04,  7.3862e-04],
        [ 4.4894e-04,  1.1015e-03, -7.5722e-04,  ...,  3.3593e-04,
          8.0347e-04, -3.9387e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6797, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.5039, -2.6445, -3.7852,  ..., -4.5820, -5.4414, -0.4890],
        [-5.2969, -3.4043, -4.0586,  ..., -3.8887, -5.2344, -0.9668],
        [-2.5449, -3.8730, -5.1836,  ..., -7.6836, -3.5762, -2.7324],
        ...,
        [-2.6055, -2.7305, -3.4023,  ..., -3.9180, -3.4023, -3.0117],
        [-7.4102, -4.3320, -4.4570,  ..., -6.5977, -6.9258, -0.1927],
        [-4.1992, -2.7305, -2.5117,  ..., -2.3086, -5.0586, -1.2305]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 13,  4, 22, 27, 27,  0, 27, 27, 18,  9, 27, 27, 20, 27,  3,  4,
         7, 27,  3, 12,  2, 27,  0,  2,  0, 10,  2,  4,  2, 24,  8, 27, 27, 27,
        27, 27, 27,  4, 27, 27, 27,  7, 15,  4, 17, 27, 17, 20, 15, 18,  3, 27,
        27, 27,  4, 15,  2, 20, 27, 13,  4, 22], device='cuda:0')
step: 630
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.5061e-05, -7.6294e-04, -4.5586e-04,  ...,  5.0163e-04,
          5.2118e-04, -6.0511e-04],
        [ 4.7255e-04,  7.3290e-04, -2.2554e-04,  ..., -2.8458e-03,
         -9.6846e-04,  3.5501e-04],
        [ 5.2309e-04,  1.8537e-04, -1.4675e-04,  ...,  3.9673e-04,
          1.2770e-03, -8.1587e-04],
        [ 3.8326e-05,  2.3723e-04, -9.6798e-04,  ...,  4.2617e-05,
         -1.4257e-04,  5.5552e-04],
        [ 2.6774e-04, -1.2040e-04, -1.2302e-04,  ..., -4.9472e-06,
          3.5763e-04, -2.0504e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9146, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2227, -4.9570, -4.8789,  ..., -7.0820, -7.0352, -0.1125],
        [-1.4922, -3.4766, -4.1328,  ..., -4.8203, -4.3984, -0.8198],
        [-5.5000, -2.2832, -3.0781,  ..., -4.0781, -4.2812, -1.5010],
        ...,
        [-4.6211, -4.3398, -4.1836,  ..., -5.0273, -5.0117, -0.2295],
        [-4.7188, -4.3438, -4.9219,  ..., -6.4219, -3.5488, -0.8760],
        [-1.5586, -4.4492, -5.0898,  ..., -4.2617, -4.5586, -3.5273]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 27,  1,  0,  0, 27,  4, 26,  6, 10, 27,  4,  0,  0, 27, 12, 12, 27,
        27, 15,  1, 19,  5, 27, 18,  0,  6,  0, 27, 15,  0, 27,  4,  1,  4, 27,
        17, 17,  4, 25, 27, 27,  4, 27, 27,  2, 24, 10, 27, 10, 27,  1,  6,  6,
         0, 17, 27, 15, 27, 27, 11,  3,  7, 21], device='cuda:0')
step: 631
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.6928e-04, -1.2074e-03,  3.7956e-04,  ...,  7.2241e-04,
         -6.1083e-04, -2.1851e-04],
        [ 4.2915e-04, -6.6185e-04,  5.6267e-04,  ...,  1.7576e-03,
         -5.2834e-04,  1.2445e-04],
        [-1.8024e-04, -2.7609e-04, -1.9848e-04,  ...,  3.8671e-04,
          6.5267e-05,  4.0245e-04],
        [-1.1597e-03, -2.6417e-04,  5.9462e-04,  ..., -3.3140e-05,
          3.3331e-04,  5.8222e-04],
        [-1.4925e-04, -1.9252e-04,  6.2275e-04,  ...,  3.9816e-04,
         -2.2519e-04,  5.0116e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9440, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.2520, -4.1289, -6.8281,  ..., -7.0000, -6.0039, -3.8770],
        [-5.1328, -3.0234, -3.3359,  ..., -2.5234, -4.4609, -1.1494],
        [-4.1992, -3.4961, -3.9336,  ..., -4.4961, -6.6836, -0.4646],
        ...,
        [-3.7871, -4.2422, -5.3984,  ..., -7.5078, -6.6953, -4.6172],
        [-0.2070, -4.4883, -5.4102,  ..., -6.6758, -3.8633, -3.5977],
        [-5.4688, -4.6875, -5.3594,  ..., -6.0312, -6.7930, -3.6074]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15,  1, 27,  1, 18,  6, 15, 27, 27, 27, 10,  3, 14, 27, 27, 23, 17, 17,
        18, 27,  0, 11, 27, 10,  1, 27, 10, 15,  1, 18, 27, 10, 19, 27, 20, 18,
        10, 15, 27, 27, 27, 14, 14, 10, 27,  0, 27,  9,  0,  2,  4, 19,  1,  7,
        15, 10, 26, 26, 20, 15, 27, 18,  0, 20], device='cuda:0')
step: 632
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2971, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.5776e-04, -1.9197e-03, -8.1778e-04,  ...,  4.4608e-04,
          9.4175e-04, -4.2629e-04],
        [-1.6034e-04,  6.7139e-04,  6.3801e-04,  ..., -6.2704e-04,
         -1.1683e-03, -8.8978e-04],
        [-6.9618e-05, -5.9414e-04, -3.2830e-04,  ...,  3.5667e-04,
          1.2219e-04, -5.0735e-04],
        [-1.2565e-04,  2.2018e-04,  7.4863e-05,  ...,  1.5199e-05,
         -1.7405e-04, -4.5991e-04],
        [ 5.9605e-08,  2.4962e-04, -2.8968e-04,  ...,  1.7929e-04,
          5.4121e-04,  1.2875e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1055, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.1602, -3.6758, -3.7539,  ..., -5.0977, -6.6914, -0.3015],
        [-4.8828, -2.5684, -2.6172,  ..., -2.2578, -4.5859, -1.8662],
        [-6.3242, -4.9961, -2.8223,  ..., -0.9326, -6.5742, -4.7930],
        ...,
        [-3.3281, -2.9844, -3.7969,  ..., -7.3125, -5.8281, -0.3740],
        [-5.8281, -6.0625, -8.6875,  ..., -7.0000, -8.3438, -6.5000],
        [-0.5205, -3.8789, -5.3633,  ..., -7.0352, -4.8320, -2.8965]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4,  3, 24, 27, 10, 17, 24,  7, 27, 27, 14, 24, 24, 15,  0, 27,  4,  4,
         4, 12,  0, 27, 27, 27,  2, 11,  1,  7, 27,  1, 27, 27, 11,  4, 26,  5,
         7, 27, 15, 27, 25,  4, 17, 10, 27, 18,  3, 22,  0, 10,  7,  1, 27, 15,
         0,  6,  2,  9, 11,  5, 27, 27, 15,  0], device='cuda:0')
step: 633
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5127],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.1263e-03, -9.4175e-06,  3.7003e-04,  ..., -8.3447e-04,
         -2.4533e-04,  2.9266e-05],
        [ 3.4332e-04, -1.4734e-03, -1.3371e-03,  ...,  8.2207e-04,
          4.4703e-06,  1.8482e-03],
        [ 1.4563e-03, -1.9207e-03, -2.9907e-03,  ...,  1.2617e-03,
          1.1702e-03, -1.4651e-04],
        [ 8.6927e-04, -8.5926e-04,  1.6031e-03,  ...,  1.4496e-04,
         -8.3268e-05, -3.5715e-04],
        [ 3.8052e-04,  3.7134e-05, -5.9903e-05,  ...,  2.2960e-04,
         -2.3425e-04,  3.2949e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8024, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.4102, -2.5664, -2.6133,  ..., -2.6758, -5.0820, -1.5508],
        [-5.4805, -2.5723, -2.9473,  ..., -4.6836, -3.4785, -1.0264],
        [-6.5859, -3.8340, -3.2559,  ..., -5.0547, -3.7402, -0.6626],
        ...,
        [-4.5391, -3.6504, -2.9629,  ..., -2.9160, -3.8066, -0.8374],
        [-5.8008, -5.5508, -5.5039,  ..., -7.7852, -7.9570, -0.0363],
        [-6.3086, -2.1660, -2.0723,  ..., -2.1504, -3.2441, -2.5566]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([11, 10,  6, 27, 10, 27, 27,  9, 18,  4, 27, 19,  6,  5, 27, 27, 17, 27,
        27, 26, 10,  3, 17, 14, 27, 20, 27,  4, 27,  0, 27, 15, 27,  4,  7, 27,
        18,  6, 20, 10, 27, 27, 26,  6, 27, 15, 27, 27, 10,  5, 27,  2, 27, 14,
        27,  0,  1,  5,  7, 11,  7, 27, 27,  3], device='cuda:0')
step: 634
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5127],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4089, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.0511e-04,  4.4537e-04,  3.2520e-04,  ..., -7.2384e-04,
         -8.8263e-04,  1.9093e-03],
        [ 3.5703e-05,  3.7372e-05, -1.4257e-04,  ...,  1.2541e-03,
          5.6362e-04, -3.2544e-05],
        [ 1.2505e-04,  1.9431e-04, -2.2244e-04,  ...,  5.0306e-05,
          6.9320e-05,  5.1117e-04],
        [-2.8944e-04, -2.5225e-04,  1.0576e-03,  ...,  8.2016e-04,
          4.7922e-04,  2.2817e-04],
        [ 1.8120e-04,  5.7101e-05,  5.2357e-04,  ..., -3.7408e-04,
         -2.4259e-04,  5.3930e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7665, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.6016, -4.9727, -5.9414,  ..., -9.1484, -7.8633, -6.0039],
        [-7.0508, -4.6289, -5.0820,  ..., -7.7070, -5.4570, -0.2537],
        [-2.5977, -2.5820, -4.4102,  ..., -5.5820, -4.4883, -0.8633],
        ...,
        [-3.1895, -2.4707, -3.2832,  ..., -4.8750, -5.0938, -1.2822],
        [-7.7656, -5.0898, -4.5938,  ..., -7.2500, -3.8574, -2.5293],
        [-1.0703, -2.3828, -3.8359,  ..., -6.3516, -2.6016, -1.9922]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18,  7,  1, 27, 11, 27,  4,  0, 27, 27, 10, 27,  1,  1,  2, 27, 27, 27,
         0, 27, 15,  5, 27, 27,  3,  6, 27, 27, 27, 27,  1, 27, 27,  0,  3,  4,
        15, 25,  1, 27, 18,  2,  0,  3, 27, 27, 17, 14,  8, 27, 23, 18, 10, 22,
         0, 15,  4, 20, 25, 27,  4, 27,  6, 26], device='cuda:0')
step: 635
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5127],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.5297e-03,  3.7994e-03,  1.1902e-03,  ..., -8.9705e-05,
         -8.2111e-04,  1.8196e-03],
        [-6.6757e-04,  1.6756e-03,  9.1171e-04,  ..., -1.7662e-03,
         -3.1891e-03, -2.4548e-03],
        [-2.0242e-04,  4.0197e-04, -4.8280e-06,  ...,  3.1328e-04,
          6.3896e-04, -4.3583e-04],
        [-1.1826e-03,  3.2377e-04,  1.8322e-04,  ..., -4.8733e-04,
          1.1951e-04,  5.6028e-04],
        [-2.5845e-04, -3.9279e-05,  7.3290e-04,  ...,  3.0351e-04,
         -3.3832e-04,  4.4823e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9449, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.8838, -4.0234, -5.7422,  ..., -8.6797, -5.0078, -4.2891],
        [-5.6328, -3.2715, -3.4434,  ..., -4.9609, -5.5547, -0.4287],
        [-3.8867, -3.7773, -4.0586,  ..., -6.8086, -8.1406, -0.4968],
        ...,
        [-4.2500, -4.0156, -5.0312,  ..., -7.3281, -6.9844, -0.5156],
        [-2.1738, -3.5020, -3.4551,  ..., -5.2500, -4.5820, -0.6895],
        [-4.1992, -4.8867, -6.2617,  ..., -9.2578, -7.1836, -6.0898]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 10, 27, 15, 15, 10,  0, 27, 27, 27,  4,  5,  4, 10,  3,  4, 18, 15,
        27,  3, 18,  3,  9, 18, 27, 15,  3,  2, 14,  5,  4, 18, 27, 27, 14, 27,
        27, 18, 15, 10,  3,  6, 27,  3, 27,  7, 27, 19,  6, 27, 27, 27,  9, 22,
        27,  1, 27, 19, 27, 10, 27, 27, 27,  0], device='cuda:0')
step: 636
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5127],
        [ 1.2500, -1.8926, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.7514e-04, -2.7752e-04, -1.2326e-04,  ...,  1.2970e-04,
         -7.8726e-04,  1.9789e-04],
        [-9.5797e-04,  4.0388e-04,  8.7166e-04,  ..., -1.3475e-03,
         -1.7843e-03, -3.2711e-04],
        [ 1.3018e-04,  7.7391e-04,  9.7156e-05,  ...,  1.0568e-04,
         -7.5698e-06, -8.2016e-04],
        [ 1.0014e-05,  5.2023e-04,  7.2956e-04,  ..., -1.5068e-04,
         -3.4356e-04,  9.2983e-04],
        [-1.3351e-04,  1.0471e-03, -4.9257e-04,  ...,  2.8181e-04,
         -8.7976e-05, -5.6744e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7496, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4609, -2.2402, -2.5371,  ..., -5.1797, -3.0527, -1.1006],
        [-4.0469, -2.9531, -3.9531,  ..., -5.4531, -5.2031, -0.6250],
        [-5.7031, -3.2500, -3.0000,  ..., -4.5000, -5.7188, -0.4536],
        ...,
        [-2.0488, -2.7363, -3.1582,  ..., -4.2070, -3.3457, -1.6895],
        [-0.4424, -3.8633, -5.4414,  ..., -6.0664, -2.8789, -2.8320],
        [-5.2812, -3.5176, -3.7656,  ..., -5.1250, -7.7031, -0.3606]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  4, 27, 27, 27, 27,  7,  0, 27, 15,  0, 15,  0,  7,  3, 27, 14,  0,
        17, 10, 10,  6, 27, 14,  4, 25,  1, 10,  4,  9,  0, 26, 20,  9, 15, 15,
        27, 27, 10, 25, 10, 27, 27, 27, 10, 27, 18,  7, 27, 27, 20, 15, 17, 20,
         5,  6, 14,  7, 27, 15, 11,  6, 13,  2], device='cuda:0')
step: 637
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5127],
        [ 1.2500, -1.8936, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9092, -0.2211,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.3804e-04,  1.0862e-03,  4.2152e-04,  ...,  4.1127e-05,
         -1.8620e-04,  6.8569e-04],
        [ 4.5240e-05,  1.5488e-03,  1.4725e-03,  ..., -1.2331e-03,
         -1.4439e-03, -1.6546e-03],
        [-5.0783e-04, -8.4996e-05,  1.1158e-03,  ..., -6.7139e-04,
         -1.2436e-03,  5.8699e-04],
        [-3.0613e-04,  1.1563e-05,  4.0936e-04,  ..., -7.1228e-05,
          1.0002e-04,  5.6267e-04],
        [-1.4031e-04, -2.9755e-04,  7.3373e-05,  ..., -1.1283e-04,
         -1.5235e-04,  2.4486e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.2262, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.4551, -3.8145, -4.5977,  ..., -3.1133, -4.4883, -1.8311],
        [-5.1914, -2.5508, -1.9404,  ..., -3.4883, -2.8789, -2.4082],
        [-7.7031, -4.6875, -4.8125,  ..., -7.9844, -4.7344, -1.0176],
        ...,
        [-5.8047, -3.2871, -2.8809,  ..., -5.9141, -4.4922, -0.6152],
        [-3.4961, -2.5898, -2.6211,  ..., -2.8398, -5.8711, -1.8867],
        [-4.9102, -3.5352, -3.5996,  ..., -6.0664, -7.2227, -0.4109]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17,  6,  6,  2,  1, 17, 27,  2, 10, 22,  3, 27, 27, 13,  2, 25, 22,  5,
         4, 27,  2, 18, 27, 27,  9, 17, 27, 27, 27, 27, 27,  6, 27, 15, 20,  7,
        27,  1,  1, 27,  4,  6,  9,  2, 23, 10,  1,  3, 15, 27,  0,  7, 13,  1,
        10,  3, 27, 27, 27,  0, 27, 27, 25,  4], device='cuda:0')
step: 638
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5127],
        [ 1.2500, -1.8936, -0.2048,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9092, -0.2211,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.8133e-04,  2.3499e-03, -1.1520e-03,  ..., -1.7881e-03,
          2.3055e-04, -4.4131e-04],
        [ 1.2989e-03,  1.2913e-03, -6.8951e-04,  ...,  1.1454e-03,
          3.0041e-03,  1.1930e-03],
        [ 4.0960e-04, -1.2624e-04, -9.1410e-04,  ...,  2.6631e-04,
         -5.8031e-04, -7.3338e-04],
        [ 1.5516e-03,  3.5405e-04, -6.0225e-04,  ..., -3.7289e-04,
          6.1131e-04, -6.3276e-04],
        [ 9.0599e-05,  3.5858e-04, -3.8218e-04,  ...,  5.0426e-05,
          5.6803e-05,  4.2295e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8300, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.2185, -4.3125, -6.5938,  ..., -8.4219, -6.1250, -2.7031],
        [-5.1836, -3.4180, -3.6035,  ..., -6.0742, -6.2148, -0.3384],
        [-3.3008, -3.1445, -2.5039,  ..., -3.8164, -3.6914, -1.2705],
        ...,
        [-1.8828, -3.2266, -4.2422,  ..., -6.3828, -5.1797, -1.6328],
        [-4.2305, -3.4980, -2.6855,  ..., -4.3555, -5.7773, -1.0762],
        [-1.8154, -3.3164, -4.7227,  ..., -7.0195, -4.5039, -2.9570]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0,  3,  8,  7, 27, 27, 10, 27, 27,  3,  4, 27,  0, 24, 22, 27, 17,  0,
        10, 27,  6,  4, 10, 13, 25,  6, 15, 14, 27,  9, 27, 27, 26, 12, 11, 11,
         5, 11,  0, 27, 27, 27, 27, 26, 27, 27,  0, 27, 25,  3, 15, 27, 18, 20,
        10,  6, 27, 10, 22, 27,  0, 17, 27, 18], device='cuda:0')
step: 639
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8936, -0.2050,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9092, -0.2211,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6553,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.5006e-04, -5.3120e-04, -1.6708e-03,  ..., -8.4400e-04,
          8.2397e-04, -1.8463e-03],
        [ 2.2829e-04,  1.3423e-04, -2.3353e-04,  ...,  1.3390e-03,
          1.9350e-03, -1.2827e-04],
        [ 2.1458e-06,  7.1526e-04, -6.1178e-04,  ...,  3.4809e-04,
          3.4356e-04, -1.4997e-04],
        [ 1.2722e-03,  2.8396e-04, -8.8692e-04,  ..., -6.2346e-05,
          2.1470e-04, -1.2560e-03],
        [ 5.8460e-04,  7.2432e-04, -5.6553e-04,  ...,  2.8563e-04,
          6.1417e-04, -3.7193e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.5857, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4531, -3.8125, -5.5312,  ..., -8.2812, -7.4062, -1.3584],
        [-4.6016, -2.7266, -3.4766,  ..., -5.3203, -5.5547, -1.0703],
        [-4.5625, -3.1738, -3.7363,  ..., -5.6250, -5.7031, -0.5322],
        ...,
        [-5.5664, -5.6914, -8.7812,  ..., -8.4375, -8.2656, -6.3945],
        [-5.5117, -3.3242, -4.4805,  ..., -6.9492, -7.3555, -0.4961],
        [-5.1016, -3.1172, -3.7422,  ..., -4.7891, -5.4609, -1.0381]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 27, 27,  5, 15, 27, 15, 15, 11,  6, 27, 13, 11, 27, 27,  0,  0, 18,
         0, 26, 25,  4, 26,  1, 27, 27, 27, 27, 26, 25, 15,  3, 14, 27, 27, 27,
        27,  0, 27, 27,  1, 27, 13, 26, 26, 18, 27, 27, 18,  6, 26,  4,  7, 27,
        27, 18, 27, 12, 10, 15,  6, 15, 27,  0], device='cuda:0')
step: 640
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0810,  1.5117],
        [ 1.2500, -1.8936, -0.2050,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9092, -0.2211,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.1049e-04,  1.5011e-03,  4.2510e-04,  ...,  7.4506e-06,
          6.3467e-04,  3.8910e-04],
        [ 4.1628e-04, -5.3310e-04,  9.9277e-04,  ..., -1.9817e-03,
         -1.8520e-03,  8.4066e-04],
        [-2.7966e-04,  5.1737e-04,  1.7910e-03,  ..., -2.2316e-03,
         -1.6880e-03,  2.7828e-03],
        [-9.3746e-04, -2.5320e-04,  8.6784e-04,  ...,  1.9014e-04,
          3.6645e-04, -2.9230e-04],
        [-4.3654e-04, -9.6989e-04,  1.8024e-04,  ..., -1.2922e-04,
         -3.2127e-05, -1.9860e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9505, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.1133, -3.4863, -3.0801,  ..., -4.9570, -7.2539, -0.7524],
        [-6.4297, -2.4766, -2.5566,  ..., -3.6797, -4.0703, -1.1953],
        [-5.0664, -3.6465, -4.5195,  ..., -6.3945, -6.4727, -0.4739],
        ...,
        [-3.6016, -3.4922, -4.0391,  ..., -3.5859, -5.2578, -0.8989],
        [-5.1328, -4.0234, -4.2578,  ..., -5.6328, -5.8516, -0.4138],
        [-3.5703, -3.1348, -2.2754,  ..., -2.5098, -5.2734, -1.8057]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 27, 27, 27, 13, 18,  5, 26,  1,  0,  4,  9,  7, 20, 10, 27, 27, 20,
        22,  6, 22, 27, 15, 21, 10, 17,  0, 27, 27,  2, 27, 10, 27, 17, 27,  6,
        27, 20, 25, 27, 18, 15,  0,  8, 27, 15,  0,  6, 11, 27, 10, 10, 27, 27,
        10, 14, 27, 24, 27, 24,  8, 24, 27,  8], device='cuda:0')
step: 641
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0810,  1.5117],
        [ 1.2500, -1.8936, -0.2050,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9092, -0.2211,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.1338e-05, -2.9221e-03,  5.8842e-04,  ...,  2.4891e-04,
          1.0939e-03, -5.3167e-04],
        [-2.7001e-05,  2.1410e-04, -5.6744e-04,  ...,  9.1743e-04,
          1.7319e-03, -7.6056e-04],
        [-7.0286e-04,  1.5326e-03, -6.1417e-04,  ..., -1.5378e-05,
          7.7152e-04, -5.0592e-04],
        [ 1.0014e-03, -4.2033e-04, -4.2701e-04,  ...,  6.8569e-04,
          4.7445e-04, -9.7656e-04],
        [-7.3969e-05, -1.6367e-04,  1.8978e-04,  ...,  3.0351e-04,
          1.4627e-04, -2.3580e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7188, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.4697, -2.9863, -4.5469,  ..., -5.9062, -5.2031, -2.3301],
        [-5.9102, -3.9727, -3.1758,  ..., -4.2383, -5.9258, -1.5508],
        [-5.8359, -3.0723, -3.1816,  ..., -6.1172, -6.5703, -0.3372],
        ...,
        [-5.6914, -3.3809, -2.4902,  ..., -4.6758, -2.5996, -0.7407],
        [-5.2188, -3.4082, -3.5020,  ..., -4.8633, -6.7656, -0.4233],
        [-4.9414, -3.7402, -4.5508,  ..., -4.0664, -5.1133, -3.2090]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3, 10, 27,  4,  7, 27,  2, 10,  2, 26,  7, 21, 15, 27,  0, 27, 22, 11,
        27, 27,  3,  3, 20, 27, 20, 27, 27, 27, 10,  4,  7, 27, 27,  4, 27,  0,
        14, 18, 27, 15,  4,  4, 27, 27, 27,  0, 11,  7,  2, 27, 18,  2, 15, 27,
        27, 27, 27, 15,  3, 10, 17,  7, 27, 20], device='cuda:0')
step: 642
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5117],
        [ 1.2500, -1.8936, -0.2050,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9092, -0.2211,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.6655e-04, -5.4979e-04, -6.8569e-04,  ...,  1.3375e-04,
         -5.2404e-04,  1.5807e-04],
        [-4.4513e-04, -1.6575e-03, -9.1171e-04,  ..., -4.1237e-03,
         -2.7485e-03,  1.9665e-03],
        [ 3.0518e-04, -1.0147e-03,  1.0264e-04,  ...,  1.4305e-04,
         -7.4291e-04,  4.8685e-04],
        [ 9.9182e-04, -7.1049e-05, -2.2697e-04,  ..., -3.8815e-04,
         -2.0421e-04, -7.7200e-04],
        [ 1.8919e-04, -7.9966e-04,  5.0783e-04,  ..., -2.0289e-04,
         -5.5909e-05, -5.0831e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0356, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.9688, -3.4531, -3.5312,  ..., -4.8906, -4.0625, -0.8125],
        [-5.5781, -4.4062, -5.8281,  ..., -7.7031, -8.5625, -0.6250],
        [-5.8047, -3.7891, -3.7266,  ..., -5.3203, -3.7891, -1.3203],
        ...,
        [-2.0117, -4.7461, -6.7305,  ..., -5.6523, -5.6680, -4.1055],
        [-8.3750, -6.0000, -5.9219,  ..., -7.5312, -9.6562, -0.2023],
        [-5.6875, -3.3594, -2.3281,  ..., -2.7520, -4.3438, -1.7354]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 27, 27, 16, 14,  6, 22, 10, 10,  0,  5, 27, 22, 27, 27, 15, 27, 20,
         5,  2, 10,  1, 25, 27, 27,  4, 25, 18, 27,  0,  3,  2,  7, 27, 27, 27,
        27, 14, 24, 27, 27, 27,  1, 25, 27,  4, 27, 27,  1, 18, 27, 19, 27, 27,
        26,  4, 18, 27, 27,  8, 15, 17, 27,  0], device='cuda:0')
step: 643
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5127],
        [ 1.2500, -1.8936, -0.2050,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9092, -0.2211,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.4105e-03, -1.2999e-03,  1.2374e-04,  ...,  5.8174e-04,
          5.6076e-04,  1.0794e-04],
        [-5.8770e-05,  1.7729e-03, -1.8930e-04,  ..., -9.0313e-04,
          9.6369e-04, -1.0309e-03],
        [-5.0449e-04,  1.9016e-03,  1.2226e-03,  ..., -5.1594e-04,
         -5.2309e-04,  5.9509e-04],
        [-4.1771e-04, -5.1212e-04,  3.5262e-04,  ...,  2.4354e-04,
          1.0748e-03,  4.3201e-04],
        [ 1.7941e-05, -8.3065e-04,  1.1597e-03,  ..., -2.2149e-04,
          8.4639e-05,  4.3941e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6431, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.0273, -3.7773, -4.1992,  ..., -6.2148, -3.8555, -1.6211],
        [-4.1953, -3.9941, -4.1016,  ..., -5.9766, -9.2266, -0.3376],
        [-1.5020, -3.7207, -5.2695,  ..., -6.7695, -3.8926, -3.4395],
        ...,
        [-1.3486, -3.6777, -5.5352,  ..., -8.4766, -5.6289, -1.0049],
        [-7.0547, -5.0234, -4.7734,  ..., -5.3047, -6.3203, -0.1945],
        [-5.6016, -2.6641, -1.9766,  ..., -2.6953, -5.7578, -2.5078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27,  3,  9, 20, 18, 27,  8,  0, 15, 15, 15, 15,  5, 22,  3, 21,
        27, 15,  0,  7, 23, 27, 27,  3, 27,  7, 21, 27, 27, 18,  3, 27, 10,  1,
        13, 15,  4,  4, 18, 27,  4, 10,  1, 15, 27, 27, 27,  0, 27, 10, 18, 27,
        27,  4, 27,  3, 27, 25, 27,  0,  5,  3], device='cuda:0')
step: 644
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5127],
        [ 1.2500, -1.8936, -0.2050,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9092, -0.2212,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6602,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.9848e-04, -4.2152e-03, -2.7714e-03,  ...,  1.0080e-03,
          7.9095e-05, -1.9112e-03],
        [ 8.6164e-04,  4.0674e-04, -1.6375e-03,  ..., -1.6556e-03,
         -3.3331e-04,  2.9087e-04],
        [ 5.8222e-04,  3.9387e-04, -3.6478e-04,  ..., -5.9032e-04,
         -5.4646e-04,  4.9114e-04],
        [ 4.4537e-04,  6.5625e-05, -2.6894e-04,  ..., -6.0129e-04,
         -5.8460e-04,  3.2949e-04],
        [-1.1343e-04, -4.4250e-04, -2.7657e-05,  ...,  2.2936e-04,
         -1.2153e-04,  3.4904e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0521, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.8750, -3.7344, -4.0938,  ..., -4.0312, -5.4844, -0.4377],
        [-4.2109, -4.3203, -6.9766,  ..., -5.6016, -5.8516, -6.1172],
        [-3.7754, -3.2285, -3.3379,  ..., -4.0430, -3.9473, -0.9478],
        ...,
        [-5.5859, -2.8672, -2.2734,  ..., -3.8359, -5.7266, -1.9922],
        [-5.3242, -3.0430, -3.1055,  ..., -5.7930, -6.1211, -0.6064],
        [-5.8594, -3.7832, -3.5176,  ..., -5.7383, -5.8906, -1.1895]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 20,  4, 27, 10, 17, 12,  1, 27,  7,  4,  9, 12, 25, 27,  2, 18, 27,
        10, 27,  7, 27, 15,  3, 27,  4, 18,  4, 15, 26, 27,  0,  4, 27,  1, 20,
        13, 27, 27, 15, 27, 17,  9, 25,  4,  4,  1,  1, 27,  1, 27, 27, 15,  3,
         7, 13, 27, 18,  1, 20, 27,  3, 27,  2], device='cuda:0')
step: 645
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5127],
        [ 1.2500, -1.8936, -0.2050,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9092, -0.2212,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6602,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.9536e-04, -6.5374e-04, -4.3416e-04,  ...,  2.4366e-04,
          1.2755e-05,  5.1689e-04],
        [ 7.0572e-04,  4.0352e-05,  5.2452e-04,  ..., -1.3514e-03,
         -2.1172e-03,  1.2201e-04],
        [ 1.6701e-04, -3.5787e-04,  1.1092e-04,  ...,  3.5906e-04,
          8.2684e-04,  3.2353e-04],
        [ 9.5308e-05, -4.8661e-04,  1.5993e-03,  ...,  6.0511e-04,
          2.8110e-04,  4.3726e-04],
        [ 1.9407e-04, -6.4659e-04,  9.0885e-04,  ..., -8.0466e-05,
          3.6955e-06,  3.9530e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7678, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-7.7109, -4.7422, -3.9590,  ..., -6.1641, -2.7871, -3.6934],
        [-3.1953, -4.5078, -5.9297,  ..., -7.7578, -5.9297, -5.5234],
        [-5.0117, -4.0898, -3.7305,  ..., -4.8711, -6.0117, -0.7456],
        ...,
        [-6.5234, -2.5215, -2.0527,  ..., -1.8652, -6.0078, -3.9277],
        [-6.0234, -2.5547, -0.5073,  ..., -5.9297, -5.0547, -3.8516],
        [-6.2188, -4.1406, -3.8125,  ..., -5.1406, -3.2969, -1.4678]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 6, 18, 27, 19, 20, 12, 22, 26, 27, 20,  0, 17, 15, 27,  3, 22, 27, 15,
        27, 25, 18, 27, 10,  7, 27, 27, 27, 26, 27, 18,  3, 24,  6, 18, 18,  5,
         1,  0, 27, 27, 27, 27, 15, 10,  4, 18,  0, 27,  0,  6, 27, 15,  5, 18,
         0, 27,  2, 26, 27, 18, 22,  3,  2,  7], device='cuda:0')
step: 646
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5127],
        [ 1.2500, -1.8936, -0.2050,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9092, -0.2212,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6602,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-4.8351e-04, -1.8811e-04, -2.2507e-03,  ..., -1.2188e-03,
          1.3962e-03, -1.5087e-03],
        [-7.8354e-03, -4.4403e-03,  5.9967e-03,  ..., -3.3112e-03,
         -3.0661e-04,  6.3896e-03],
        [-1.8616e-03, -4.8804e-04,  1.8768e-03,  ..., -6.9737e-05,
         -1.0843e-03,  2.0771e-03],
        [-1.0687e-04, -9.0063e-05, -1.6413e-03,  ...,  4.8232e-04,
          1.2660e-04, -1.0271e-03],
        [-8.2111e-04, -1.3952e-03,  3.7241e-04,  ...,  1.4305e-06,
         -2.9945e-04,  5.0354e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8444, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.9258, -3.8145, -4.5508,  ..., -6.0977, -6.4414, -0.6436],
        [-4.6641, -2.9609, -3.1016,  ..., -4.2891, -3.5078, -0.7578],
        [-6.7734, -2.7598, -2.0566,  ..., -3.1348, -2.4629, -1.4316],
        ...,
        [-6.4570, -3.4883, -3.5332,  ..., -5.4414, -4.1758, -0.7212],
        [-4.9414, -2.7695, -3.3633,  ..., -3.8945, -4.2852, -0.5352],
        [-2.9043, -3.3887, -5.6367,  ..., -4.8398, -3.8574, -2.0605]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4,  9, 22,  1, 27, 11,  4,  2, 27, 27,  4, 27, 27,  7, 27, 18, 15, 27,
         0, 27,  0, 27,  5, 27,  1,  1,  7, 27,  7, 16, 27, 26, 27, 27, 18, 15,
        27,  0,  6, 27, 27, 27, 27, 18,  7,  0,  7, 22, 18,  3, 25,  0, 10,  6,
         7, 14,  1,  1,  0, 27, 22,  6,  6, 17], device='cuda:0')
step: 647
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5127],
        [ 1.2500, -1.8936, -0.2050,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9092, -0.2212,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6602,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.4009e-04,  1.0109e-03,  7.1669e-04,  ...,  1.1435e-03,
          5.9271e-04, -2.3746e-04],
        [-5.8889e-05, -2.4843e-04,  4.7588e-04,  ..., -3.9387e-04,
         -1.0252e-03,  5.1022e-04],
        [-1.3411e-04,  1.2422e-04,  5.8842e-04,  ..., -9.2745e-05,
         -1.1053e-03,  7.0691e-05],
        [-2.5153e-04, -1.1367e-04,  6.5136e-04,  ...,  9.8419e-04,
          6.0081e-04,  1.3494e-03],
        [-1.5903e-04, -2.8133e-04,  1.3757e-04,  ...,  3.5048e-05,
         -4.4584e-04,  9.7930e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8979, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.3633, -4.9102, -3.0020,  ..., -1.2207, -6.7695, -4.2070],
        [-4.1094, -4.3750, -7.3086,  ..., -3.5449, -3.9199, -3.6543],
        [-3.8750, -2.5938, -2.8281,  ..., -4.8594, -5.7969, -1.0312],
        ...,
        [-4.1172, -4.5391, -4.6484,  ..., -5.9922, -7.0234, -0.1313],
        [-2.2168, -2.2793, -3.1699,  ..., -2.9199, -3.5137, -1.2158],
        [-5.9219, -4.1562, -4.3281,  ..., -5.6875, -6.6094, -0.1710]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([25, 27,  2, 12, 18, 18, 27,  5, 20, 27, 20, 22,  4,  9, 18,  2,  4, 20,
        18, 26,  4, 15,  5, 22, 27, 27, 27, 27,  4, 27, 15, 27, 10, 27, 15, 15,
         4, 14, 24, 27, 27, 10, 27, 27,  9, 27, 25, 27, 27,  1,  1, 15, 27, 10,
         7, 10,  9, 10, 27, 23,  1, 27,  2,  2], device='cuda:0')
step: 648
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2384, -0.6147,  ...,  0.4792, -0.0811,  1.5127],
        [ 1.2500, -1.8936, -0.2050,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6602,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0994,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.1673e-03, -2.5177e-04, -1.2360e-03,  ...,  5.6934e-04,
          4.7302e-04,  5.2392e-05],
        [-1.6212e-04, -9.2936e-04, -2.3055e-04,  ..., -6.5756e-04,
         -1.0662e-03,  7.0333e-04],
        [-3.5882e-05,  8.7202e-05, -1.7262e-04,  ...,  9.6655e-04,
         -1.4699e-04,  7.0870e-05],
        [ 1.1864e-03,  4.8590e-04, -9.1028e-04,  ..., -1.0214e-03,
         -3.8719e-04, -4.9543e-04],
        [ 2.8229e-04,  2.2650e-04, -4.7159e-04,  ...,  3.4857e-04,
          2.9182e-04, -1.3947e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7988, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.5312, -2.8613, -1.9395,  ..., -3.5645, -4.8164, -1.8916],
        [-6.4805, -4.2305, -4.1367,  ..., -4.8086, -5.0117, -0.4014],
        [-6.3398, -2.8555, -2.4961,  ..., -2.4180, -4.6523, -2.9023],
        ...,
        [-6.2773, -4.1367, -3.6055,  ..., -4.9961, -5.3242, -0.1827],
        [-5.0234, -3.9297, -3.9297,  ..., -4.3047, -5.4297, -0.4287],
        [-3.3730, -1.7168, -2.4199,  ..., -3.0000, -3.6699, -2.2168]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 3,  9, 27, 25, 18, 27,  2,  0, 18, 27,  0, 15,  9, 22, 27, 20,  6,  9,
        13, 20, 20, 27, 27,  4,  9, 27,  5, 27, 11, 24, 17,  4, 17, 18, 27,  4,
         0,  1, 27, 26, 27,  1, 11,  3,  0, 27, 27, 10, 15, 27,  4, 27,  0, 27,
         1,  4, 15, 15, 10, 27, 12,  2, 27,  1], device='cuda:0')
step: 649
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6147,  ...,  0.4792, -0.0811,  1.5127],
        [ 1.2500, -1.8936, -0.2050,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6602,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.2582e-04, -5.7983e-04, -2.8062e-04,  ...,  9.3937e-04,
          3.4738e-04, -5.4240e-06],
        [ 7.7009e-04, -7.1478e-04,  1.3924e-04,  ...,  1.0738e-03,
         -1.0414e-03,  6.2466e-04],
        [ 6.4802e-04, -1.5593e-03, -5.6791e-04,  ...,  1.3580e-03,
          2.9039e-04, -6.2132e-04],
        [ 5.5194e-05,  1.0169e-04, -3.3784e-04,  ..., -2.7752e-04,
         -4.1366e-04, -9.5129e-04],
        [ 4.1366e-04, -7.4387e-05, -1.6379e-04,  ...,  4.9162e-04,
          1.8764e-04,  9.4175e-06]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6274, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3711, -2.7129, -3.1660,  ..., -4.2617, -3.2910, -0.5728],
        [-0.5439, -4.1055, -5.6211,  ..., -7.0273, -4.0430, -3.5273],
        [-6.2656, -4.1875, -3.2949,  ..., -2.7168, -4.3281, -3.3887],
        ...,
        [-4.6602, -5.1445, -5.9883,  ..., -7.4883, -7.1602, -0.1453],
        [-5.3242, -2.7754, -0.4946,  ..., -4.5898, -4.9180, -2.2598],
        [-5.6055, -4.1836, -4.2930,  ..., -6.9180, -3.8418, -0.4197]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([12, 18, 11,  1, 27, 27,  1,  6, 26,  6,  8, 15,  0, 18, 27,  7, 11, 24,
        27,  0, 13, 27, 27, 16,  0, 15,  0,  1,  4, 27, 16, 27, 27, 27,  4, 26,
        27, 27, 15, 18,  6, 27, 27, 15, 24, 27, 27, 27, 27, 27, 27,  4, 27, 27,
        24, 27,  2, 15,  1, 27, 27, 27,  6, 27], device='cuda:0')
step: 650
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6143,  ...,  0.4792, -0.0812,  1.5127],
        [ 1.2500, -1.8936, -0.2050,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6602,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.4935e-03, -5.5933e-04, -9.0599e-04,  ...,  1.0002e-04,
          2.1851e-04, -1.2589e-03],
        [ 6.0380e-05,  6.7711e-04, -2.4366e-04,  ...,  1.0796e-03,
          3.4308e-04,  4.7731e-04],
        [ 3.7575e-04,  5.7554e-04,  2.2876e-04,  ...,  5.9605e-05,
          1.5724e-04,  4.9782e-04],
        [ 7.9918e-04, -1.5533e-04, -1.1082e-03,  ..., -1.0437e-04,
         -1.0929e-03, -5.0449e-04],
        [ 6.2752e-04, -2.4056e-04, -8.5592e-05,  ...,  1.0700e-03,
          9.0742e-04, -9.9182e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.5715, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.4668, -3.1074, -4.6680,  ..., -6.1523, -3.0293, -0.6699],
        [-5.9414, -2.9746, -2.8965,  ..., -3.0820, -5.3789, -0.9736],
        [-4.1328, -4.1797, -4.6484,  ..., -5.7578, -6.1797, -0.3501],
        ...,
        [-3.9082, -3.6406, -6.0312,  ..., -3.9531, -3.0488, -4.4219],
        [-3.9863, -3.7500, -4.0938,  ..., -5.7656, -3.1426, -0.8760],
        [-5.8320, -3.4082, -2.2207,  ..., -1.1904, -3.3457, -3.4883]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 25, 27,  0, 27, 27, 27, 27,  4, 18, 17, 27, 27, 27, 17, 27, 24,  6,
         0, 20, 17, 27,  8, 27, 27, 15,  0, 20, 27, 20, 27, 27,  4, 27,  0,  1,
        18, 27,  2, 14,  3, 15, 20,  9, 10,  2, 27, 15, 27, 27,  0, 20, 26,  6,
        27, 27, 10, 27,  2, 25, 27, 17, 26, 24], device='cuda:0')
step: 651
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6143,  ...,  0.4792, -0.0812,  1.5127],
        [ 1.2500, -1.8936, -0.2050,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6602,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.2854e-04, -8.8453e-04,  4.1175e-04,  ...,  1.7691e-04,
         -5.0116e-04, -1.1101e-03],
        [-9.9063e-05, -1.3256e-04, -1.7345e-05,  ..., -1.8549e-04,
         -7.0930e-05, -3.7956e-04],
        [-5.1737e-04,  2.5034e-04,  8.0585e-04,  ..., -6.7759e-04,
         -1.3411e-05,  8.8310e-04],
        [ 2.9373e-04, -7.3671e-05,  5.3596e-04,  ..., -8.7857e-05,
         -5.1403e-04, -2.1911e-04],
        [ 8.8871e-05, -4.0436e-04,  1.8251e-04,  ..., -3.4904e-04,
         -2.0313e-04, -4.1962e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7614, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-0.4182, -3.7461, -6.7617,  ..., -8.2891, -6.6211, -2.2617],
        [-4.6328, -3.9746, -4.2734,  ..., -4.8828, -1.8965, -1.9756],
        [-5.2617, -3.4316, -3.2441,  ..., -4.4648, -5.9648, -0.9946],
        ...,
        [-7.7812, -3.5469, -2.7812,  ..., -3.3418, -3.4531, -3.1094],
        [-7.0859, -3.3672, -2.9922,  ..., -4.2422, -4.4297, -0.9771],
        [-1.6797, -3.3672, -3.9141,  ..., -4.0078, -3.5234, -1.2578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 26, 27, 22,  6, 27, 20,  2,  1, 20, 24, 15, 26, 20,  3, 27, 27, 27,
        27,  3, 20, 17, 11, 27, 18,  7, 27, 27, 26, 11, 25, 27, 20,  4,  3,  4,
         3, 26, 18, 27, 25, 27, 15,  4,  3,  3, 27,  5, 27, 18, 18, 20, 20,  6,
        27, 27,  0, 25, 27, 13,  1, 14, 19, 10], device='cuda:0')
step: 652
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6143,  ...,  0.4792, -0.0812,  1.5127],
        [ 1.2500, -1.8936, -0.2051,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6602,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.7745e-04,  5.3501e-04,  3.6526e-04,  ...,  5.4240e-06,
         -1.5144e-03, -1.1892e-03],
        [ 7.8964e-04, -7.6103e-04, -9.6321e-05,  ...,  2.0084e-03,
          4.4584e-05, -4.0674e-04],
        [-1.0329e-04, -7.9298e-04,  6.2704e-04,  ...,  4.4370e-04,
         -4.7255e-04, -3.8552e-04],
        [-1.2732e-03,  6.7651e-05, -1.6665e-04,  ..., -1.2608e-03,
          7.3051e-04, -1.6046e-04],
        [-2.6560e-04,  2.1458e-06,  2.7657e-04,  ...,  1.6189e-04,
         -2.8658e-04, -6.1178e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.5052, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.0430, -3.9336, -3.6055,  ..., -5.6055, -5.1523, -0.4175],
        [-6.5586, -4.5898, -5.0273,  ..., -7.3398, -4.0586, -1.0127],
        [-6.8516, -4.7422, -5.1328,  ..., -7.6172, -7.1641, -0.0557],
        ...,
        [-4.3477, -5.7852, -9.2500,  ..., -7.0820, -7.6133, -6.8164],
        [-3.5703, -3.7734, -4.7266,  ..., -5.0234, -7.3984, -1.6953],
        [-7.1289, -4.6445, -3.9746,  ..., -0.3494, -6.3633, -3.3184]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 27, 27, 27,  2,  0,  4,  7, 27, 18, 22, 27,  8, 27,  7, 11,  4,  0,
        27, 15, 27,  4, 27, 27, 25,  2,  6, 13, 27,  0,  1, 27, 27,  6,  5,  4,
        13, 15, 27, 27, 27, 18, 10, 18,  0, 26, 10,  4, 10, 27, 27, 10,  7,  7,
        27,  4, 12, 27, 20,  4,  0, 15,  5, 24], device='cuda:0')
step: 653
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6143,  ...,  0.4792, -0.0812,  1.5127],
        [ 1.2500, -1.8936, -0.2051,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6602,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.8091e-03, -8.6288e-03, -3.8338e-03,  ...,  2.4433e-03,
          6.6328e-04, -3.7956e-03],
        [ 8.2541e-04, -3.8218e-04,  2.3079e-04,  ..., -1.8644e-04,
         -8.8358e-04, -3.2115e-04],
        [ 1.5092e-04, -2.8372e-04,  2.4009e-04,  ...,  1.1826e-03,
          1.8144e-04, -1.2169e-03],
        [ 1.9312e-04, -4.2653e-04, -6.0976e-05,  ..., -1.7118e-04,
         -6.3753e-04,  2.8133e-04],
        [ 5.3787e-04, -9.0837e-04, -2.9159e-04,  ...,  4.5681e-04,
         -3.0470e-04, -1.6212e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9570, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.9883, -2.0508, -2.4414,  ..., -3.1777, -4.7695, -1.6143],
        [-5.1953, -2.8184, -5.0547,  ..., -6.6953, -5.4453, -0.3342],
        [-5.7266, -5.8047, -9.1641,  ..., -7.5078, -8.0078, -7.1953],
        ...,
        [-3.7500, -5.5469, -8.1094,  ..., -6.2812, -6.6406, -5.3438],
        [-3.4395, -2.1582, -2.7051,  ..., -3.8613, -4.2695, -1.0488],
        [-5.9844, -3.1855, -3.4668,  ..., -4.2969, -3.9219, -0.8584]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([11, 27, 15,  4,  7, 27,  6, 10, 11, 27, 18,  3,  4,  1,  5, 27,  8,  1,
        27,  7, 25, 26,  3, 14, 27,  0, 13, 27, 27, 19,  4, 27,  1,  7,  8,  3,
         4, 14, 27,  1, 19, 27, 14,  1, 17, 27, 27, 27, 27,  0, 27,  3,  7, 26,
         7,  7, 15, 27, 27, 27, 27, 15, 10,  6], device='cuda:0')
step: 654
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8936, -0.2051,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6602,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.2166e-04,  1.3885e-03, -8.2493e-04,  ...,  2.2745e-04,
         -3.9506e-04,  4.6110e-04],
        [-3.2902e-05, -1.8167e-03,  7.8678e-04,  ..., -1.1702e-03,
         -3.2139e-03,  1.1806e-03],
        [-2.0206e-04, -8.4209e-04,  2.7561e-04,  ...,  5.9271e-04,
          2.1625e-04, -1.4806e-04],
        [ 2.8968e-04,  2.2030e-04,  9.4557e-04,  ...,  3.5238e-04,
          6.3002e-05,  3.4523e-04],
        [ 5.9986e-04, -2.4736e-05,  3.7313e-04,  ..., -1.7250e-04,
          2.6941e-04,  1.3781e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6862, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -0.9204,  -3.9512,  -5.2031,  ...,  -5.7031,  -4.4062,  -2.4668],
        [ -3.3438,  -3.4062,  -4.4219,  ...,  -5.2188,  -3.7812,  -0.6567],
        [ -6.8711,  -7.2617, -10.7656,  ...,  -8.0938,  -9.2969,  -8.5938],
        ...,
        [ -5.3828,  -3.6641,  -3.9141,  ...,  -4.1484,  -5.1953,  -0.3833],
        [ -4.9961,  -3.0273,  -3.4180,  ...,  -4.6211,  -4.4023,  -0.6206],
        [ -5.6016,  -4.1484,  -4.0547,  ...,  -6.9141,  -4.8672,  -0.1175]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 0, 26, 15, 10,  0, 22, 27,  7,  6,  8, 27,  3, 18, 27, 10, 17, 27, 27,
        27,  5, 27,  5, 20,  3, 10, 27, 27,  1, 27, 27, 27, 27, 17, 27,  4, 15,
         1, 27,  0,  4,  7, 27,  0,  2, 27, 27, 10, 27, 27, 27, 20, 27, 12,  7,
        15, 27, 13,  1, 15, 25, 27, 17,  2, 14], device='cuda:0')
step: 655
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2383, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8936, -0.2051,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6602,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.6093e-05, -3.9673e-04, -9.1171e-04,  ..., -2.3079e-04,
          6.4564e-04, -3.9744e-04],
        [ 1.1075e-04, -9.1219e-04, -4.1175e-04,  ..., -4.6182e-04,
         -5.4359e-04,  4.9019e-04],
        [-2.0003e-04,  5.4169e-04, -1.8907e-04,  ..., -5.4598e-04,
         -1.0508e-04,  4.3130e-04],
        [-1.3161e-04, -2.4080e-04, -2.8658e-04,  ..., -3.7718e-04,
          3.2854e-04, -3.1710e-04],
        [-9.4652e-05, -3.5167e-04, -2.0933e-04,  ..., -4.0483e-04,
         -6.9916e-05, -2.4080e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8269, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-1.6045, -2.8398, -3.8711,  ..., -3.4180, -4.1836, -2.8086],
        [-5.7578, -3.1797, -2.4609,  ..., -3.9609, -3.8203, -0.7109],
        [-6.1953, -4.3359, -3.9766,  ..., -7.4609, -5.0859, -0.6484],
        ...,
        [-4.4102, -1.9111, -2.3652,  ..., -3.1465, -3.2871, -2.5996],
        [-3.3965, -2.8965, -2.8965,  ..., -2.9434, -4.2852, -0.6299],
        [-3.7383, -3.4414, -3.1602,  ..., -3.2227, -5.0977, -2.2852]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([17, 27, 27, 25, 27, 26,  7, 15, 10, 15,  1, 25, 27, 26, 27,  0, 17, 10,
        25, 10,  8, 18, 18, 13, 27,  0, 12, 17, 27, 27,  2, 27,  0, 27, 27, 22,
        10, 27, 27,  1, 27, 22, 27,  4,  0,  4, 14, 15,  4, 22, 15, 17, 15, 27,
        27, 27, 18, 26,  0, 20, 18,  1, 27,  8], device='cuda:0')
step: 656
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8936, -0.2051,  ..., -2.3828,  0.6050, -0.8252],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6602,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-9.0551e-04,  4.1604e-04,  5.1355e-04,  ...,  9.8515e-04,
         -5.5313e-04, -3.7527e-04],
        [ 4.9686e-04,  3.6061e-05, -1.0955e-04,  ..., -5.5695e-04,
         -1.8864e-03,  1.0748e-03],
        [ 5.8651e-04, -1.0681e-03, -2.0733e-03,  ...,  3.2163e-04,
         -6.7759e-04, -1.4820e-03],
        [ 1.7583e-04,  4.9472e-06,  5.4932e-04,  ..., -2.4509e-04,
         -9.5749e-04, -4.7946e-04],
        [ 1.3947e-05,  9.9719e-05,  1.8930e-04,  ..., -6.7890e-05,
         -2.9993e-04,  7.2241e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9176, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.0293, -2.4043, -3.3418,  ..., -5.3711, -2.8887, -2.2461],
        [-5.5195, -4.5977, -4.6914,  ..., -7.4414, -5.0195, -0.1927],
        [-1.8613, -2.9551, -4.2344,  ..., -4.3750, -3.0020, -1.3770],
        ...,
        [-4.3750, -3.8125, -3.9062,  ..., -3.3594, -4.0312, -1.1084],
        [-7.3086, -4.2305, -3.4199,  ..., -4.3242, -5.0898, -0.3882],
        [-5.2969, -3.4668, -3.8730,  ..., -5.0430, -6.1875, -0.2325]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2, 27, 27,  3, 15, 26,  0, 17, 27, 20, 22, 27, 27,  6, 25,  3, 24, 17,
         7, 27, 20, 27, 27,  2, 14, 15, 27, 26, 22, 27,  0, 27, 20,  7, 22,  6,
         3,  0,  1, 27, 27, 20,  8, 17, 18, 27, 17, 15, 27, 10,  6,  5, 27,  0,
         9,  3,  7,  7, 27,  9, 18, 27,  2, 27], device='cuda:0')
step: 657
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2051,  ..., -2.3828,  0.6050, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3486, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6602,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.2193e-04, -4.3464e-04, -2.5249e-04,  ..., -1.0662e-03,
          4.5109e-04, -4.6444e-04],
        [-3.1185e-04,  4.1342e-04, -2.8110e-04,  ...,  1.3709e-04,
          1.0843e-03, -5.8508e-04],
        [-1.4424e-05, -4.9734e-04, -1.1939e-04,  ...,  1.8930e-04,
          1.0145e-04, -3.2854e-04],
        [ 1.2455e-03,  1.7059e-04, -1.5497e-03,  ..., -2.7966e-04,
          5.5552e-04, -3.4428e-04],
        [ 3.4976e-04,  1.7285e-05, -1.0699e-04,  ...,  9.5129e-05,
          1.8537e-04,  1.2350e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8353, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.7656, -4.6055, -3.3574,  ..., -2.0762, -5.6250, -0.4043],
        [-3.9746, -2.1309, -2.7559,  ..., -2.3184, -5.2695, -2.3027],
        [-6.7617, -4.9961, -4.8867,  ..., -6.3398, -3.1211, -1.4492],
        ...,
        [-6.2812, -5.3125, -5.7969,  ..., -5.3125, -7.0938, -4.9062],
        [-5.0938, -2.3301, -3.3594,  ..., -3.3438, -3.6113, -1.2979],
        [-3.8027, -1.3184, -2.5059,  ..., -3.5371, -4.0547, -1.4590]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  2, 27, 18, 15, 15, 26, 27, 27,  1, 15, 11, 27, 27,  4, 20, 22,  2,
        27, 15, 27,  4,  0, 27, 27, 15,  3, 26, 27,  6,  4, 27, 25, 25, 25, 27,
        27, 27,  3, 27,  9,  0, 15, 20, 18, 22, 20, 27, 12, 27, 27, 15,  6, 27,
         7, 25, 27, 20, 27,  5, 10, 20, 27, 27], device='cuda:0')
step: 658
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2051,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6602,  0.6533,  1.2676],
        [-1.1729,  2.2305, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.1689e-04,  6.7520e-04,  3.5524e-04,  ..., -9.3842e-04,
          3.5763e-07, -9.5558e-04],
        [-1.8978e-04,  1.0929e-03,  3.4618e-04,  ...,  2.5902e-03,
          1.5850e-03, -1.4172e-03],
        [ 3.1686e-04,  4.7445e-04,  3.3236e-04,  ...,  6.5470e-04,
         -5.4657e-05, -1.0147e-03],
        [ 2.4014e-03,  1.1563e-05, -1.6870e-03,  ..., -5.3263e-04,
         -1.7095e-04,  1.2481e-04],
        [ 3.1662e-04, -3.8576e-04, -8.0466e-06,  ...,  7.1406e-05,
          5.1451e-04,  2.8849e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8097, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.8281, -3.8145, -3.8145,  ..., -3.4707, -5.2500, -2.1895],
        [-3.6465, -3.4570, -5.0977,  ..., -7.0039, -3.4434, -1.0361],
        [-1.1074, -3.3574, -4.8125,  ..., -5.0938, -4.7344, -1.0762],
        ...,
        [-6.2188, -3.7949, -3.7324,  ..., -4.7617, -3.1543, -1.4824],
        [-6.0039, -6.9258, -9.9609,  ..., -8.0859, -8.6016, -7.1289],
        [-3.2676, -4.2227, -6.0664,  ..., -5.2852, -4.1289, -0.6904]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([11,  6, 27,  4, 18, 18, 25,  1, 18, 18,  4, 27,  0, 27,  4, 19,  8,  1,
        17, 27,  6,  0, 20, 27, 27, 27, 27, 27, 25, 27,  6, 15,  3, 25, 27, 27,
        25, 18, 27,  3,  0,  1, 27,  7, 27, 27, 22,  1, 27, 11,  0, 27, 27, 27,
         5,  0, 27,  1, 18,  4, 11,  6, 15,  4], device='cuda:0')
step: 659
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2051,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2305, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-8.8358e-04, -1.7605e-03,  2.4490e-03,  ...,  8.1444e-04,
         -4.0245e-04, -5.5790e-05],
        [-1.9240e-04, -8.6737e-04, -1.4126e-05,  ..., -8.5545e-04,
         -2.0361e-04,  3.9864e-04],
        [-7.9274e-05,  9.8467e-05,  7.9930e-05,  ...,  6.7806e-04,
         -1.7881e-04,  2.6798e-04],
        [-2.5864e-03,  5.0783e-04,  1.8454e-03,  ...,  6.3229e-04,
          1.6804e-03,  1.0004e-03],
        [-4.1866e-04,  5.4181e-05,  1.0557e-03,  ...,  6.9916e-05,
         -5.4598e-04,  4.4656e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.3582, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4141, -3.6816, -3.2285,  ..., -4.2891, -5.1484, -2.9160],
        [-4.4531, -2.9551, -3.2520,  ..., -2.0332, -2.4395, -1.3145],
        [-6.6484, -2.9766, -3.7129,  ..., -5.4453, -3.0391, -0.6025],
        ...,
        [-2.6504, -3.3359, -4.8672,  ..., -7.1172, -6.1641, -0.6807],
        [-3.9746, -3.2402, -4.2266,  ..., -5.8516, -4.2891, -0.4124],
        [-3.6289, -3.9883, -5.0664,  ..., -4.0508, -4.4727, -1.4727]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 8,  3, 27, 23, 19, 15, 10, 10,  5, 12,  2, 20,  7, 10, 22, 22, 27, 17,
        25, 26,  7,  3, 27, 27, 18, 10, 21, 25,  3,  3,  6,  3,  5, 25,  7, 27,
        15, 22, 18, 25, 27, 27, 10, 27, 27, 19,  2, 10,  4,  1, 27, 27, 27, 27,
        27, 27,  3,  1, 27, 27, 25,  0, 27, 17], device='cuda:0')
step: 660
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2051,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4087, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2305, -0.0996,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 7.2002e-04,  1.9860e-04, -1.5748e-04,  ...,  2.9731e-04,
         -4.1986e-04,  8.4460e-05],
        [-1.6737e-04, -4.4632e-04,  6.3705e-04,  ...,  1.7614e-03,
          3.0279e-04,  2.1160e-05],
        [ 4.2129e-04, -1.3876e-03, -6.2656e-04,  ..., -1.1641e-04,
          8.5402e-04,  1.5450e-04],
        [ 9.0742e-04, -2.3639e-04, -3.4213e-04,  ..., -7.1001e-04,
          3.2425e-05, -4.3750e-04],
        [ 2.1231e-04,  1.4992e-03, -4.7088e-04,  ...,  6.4909e-05,
          5.4646e-04, -1.1271e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7740, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.4766, -3.8535, -4.4766,  ..., -5.7578, -6.9453, -0.2588],
        [-2.3867, -4.9023, -7.8398,  ..., -5.2617, -3.7148, -3.7617],
        [-4.3242, -1.6211, -3.5898,  ..., -3.7930, -3.5273, -1.4023],
        ...,
        [-5.6133, -5.0977, -5.6602,  ..., -4.6602, -5.2852, -0.1431],
        [-7.5508, -5.9102, -3.7852,  ..., -1.7373, -6.6289, -4.4414],
        [-4.5781, -3.5156, -3.9375,  ..., -4.3438, -3.7500, -0.5312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 15, 27, 17,  3,  7,  9,  8, 10,  2,  8,  4, 27, 26, 18,  3, 27,  7,
         9, 27,  0,  0,  4, 27, 25, 26, 23, 27, 15, 26,  0, 17, 27, 15, 22, 27,
        25, 27, 27, 27,  4, 27, 17, 27, 27, 10, 20, 27,  0, 10, 18,  4, 17, 24,
        27, 27, 25, 27,  6, 11, 15, 27, 25,  3], device='cuda:0')
step: 661
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2051,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2305, -0.0996,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.9448e-04,  5.6791e-04,  6.6328e-04,  ...,  9.6226e-04,
          3.6192e-04,  8.7023e-04],
        [ 2.5845e-04, -6.9380e-04,  3.1638e-04,  ..., -9.9182e-04,
         -1.9608e-03,  1.0395e-03],
        [ 2.0719e-04, -5.0449e-04,  2.9588e-04,  ...,  3.5477e-04,
          5.2631e-05,  5.5552e-04],
        [ 4.2558e-05, -1.8501e-04,  7.9632e-04,  ...,  5.5122e-04,
         -8.7595e-04, -9.7656e-04],
        [ 3.5954e-04,  1.6212e-04, -2.0671e-04,  ...,  5.0128e-05,
          5.5850e-05, -2.6369e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1303, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2500, -2.3262, -2.4355,  ..., -2.8105, -3.5605, -1.0605],
        [-3.0781, -1.4990, -2.7480,  ..., -3.7969, -2.2637, -1.3428],
        [-5.1758, -4.5156, -5.8750,  ..., -3.8770, -3.9395, -3.5176],
        ...,
        [-4.2461, -4.2461, -4.8867,  ..., -5.7305, -6.1992, -0.1670],
        [-5.1133, -2.2402, -2.7246,  ..., -3.3496, -4.3164, -1.3330],
        [-3.0840, -2.4746, -3.7246,  ..., -5.3359, -5.1797, -0.3818]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 11, 22, 27,  7, 27,  9, 19, 27, 27, 14, 27, 18, 10,  4, 27, 13, 25,
        27,  6, 19, 27,  1, 27, 27,  0,  3,  8, 27,  5, 15, 15, 13,  0, 27, 27,
        25, 19,  9, 27,  1,  6,  6,  5, 22, 27,  7,  1,  3,  2,  2, 10, 15,  0,
         7, 27, 25,  0, 27, 17,  0, 27, 22, 27], device='cuda:0')
step: 662
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2051,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2212,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2305, -0.0996,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.3821e-04,  3.5822e-05,  1.5364e-03,  ...,  6.3229e-04,
         -2.4438e-05, -4.9305e-04],
        [-1.3125e-04,  1.2312e-03,  5.4741e-04,  ...,  2.6369e-04,
          1.2398e-05, -1.1091e-03],
        [ 2.2888e-04, -4.8208e-04, -2.9802e-04,  ..., -9.5367e-07,
         -3.9077e-04,  2.4438e-06],
        [ 6.6328e-04,  3.5405e-05,  1.3762e-03,  ..., -1.4567e-04,
         -9.6130e-04,  8.4639e-05],
        [ 1.2398e-04,  9.2649e-04,  6.2048e-05,  ...,  1.9050e-04,
         -2.6107e-04, -2.3186e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.9264, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.9141, -4.8047, -4.0391,  ..., -4.0547, -6.1016, -0.1809],
        [-5.3516, -3.4434, -2.7559,  ..., -1.3496, -5.1602, -2.9902],
        [-5.6094, -3.9863, -4.5938,  ..., -6.9375, -5.3438, -0.1572],
        ...,
        [-3.6211, -1.7607, -2.4004,  ..., -5.4180, -3.2285, -0.7446],
        [-5.6133, -2.5371, -2.7871,  ..., -1.8174, -2.3496, -2.5059],
        [-2.8672, -4.2578, -5.7109,  ..., -6.3516, -5.6172, -5.3203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 27, 27,  4, 27, 27, 27,  9,  2,  5, 26, 27,  5, 27, 27,  3, 27,  4,
         7, 27, 27,  0, 27,  0,  0, 27, 20, 18, 12,  6,  0, 10, 18, 14, 27,  8,
         1, 27, 27, 27, 27, 18, 15, 27,  4,  6,  7, 11,  0, 27, 27,  3, 15, 10,
        27, 18, 27, 15, 10, 22, 15, 27, 11, 18], device='cuda:0')
step: 663
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2051,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2305, -0.0996,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.0974e-04,  6.4516e-04,  1.1702e-03,  ..., -1.3418e-03,
         -7.4387e-04,  7.8821e-04],
        [-5.7077e-04,  1.3752e-03, -7.4565e-05,  ..., -1.9944e-04,
          1.4849e-03, -9.4128e-04],
        [-6.8903e-04,  6.3610e-04,  2.7919e-04,  ...,  1.3180e-03,
          6.4278e-04,  1.2255e-04],
        [-3.0231e-04, -5.0664e-05,  1.0281e-03,  ...,  6.6805e-04,
          5.7364e-04,  4.4894e-04],
        [-4.8637e-05,  5.8270e-04, -2.7776e-04,  ..., -1.0729e-04,
         -1.1086e-04, -9.8884e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6881, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.0039, -3.5508, -4.4727,  ..., -5.8633, -5.9102, -0.1915],
        [-5.8086, -3.2598, -3.9941,  ..., -5.9180, -2.7910, -1.0264],
        [-1.1777, -2.4121, -4.1016,  ..., -4.5859, -3.1621, -1.2881],
        ...,
        [-3.9746, -3.9590, -4.3633,  ..., -4.6289, -3.5215, -1.9424],
        [-4.5117, -3.9512, -4.8242,  ..., -5.9180, -6.2461, -0.3254],
        [-5.4883, -3.0801, -3.0801,  ..., -3.1582, -4.7344, -0.8301]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  6,  2, 25,  4,  3, 22, 18, 27,  0,  4,  0, 27, 27,  7, 18, 20, 15,
         6, 27, 27, 17, 27,  0,  4, 27, 27,  1, 27,  1, 27,  6, 22, 15, 17,  4,
         9,  2, 18, 24, 27, 15,  9, 17, 17,  7,  1, 27, 11, 27, 10, 27, 26,  4,
        14, 27, 15, 27,  4, 22, 27, 20,  4, 27], device='cuda:0')
step: 664
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2051,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2305, -0.0996,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.6219e-04,  9.4700e-04,  4.0746e-04,  ..., -1.3649e-05,
         -3.2997e-04,  6.1607e-04],
        [-2.2278e-03,  2.7275e-04,  1.9855e-03,  ..., -4.5776e-04,
         -6.6757e-06,  1.5497e-03],
        [-2.6178e-04,  2.3365e-04,  1.7381e-04,  ..., -2.1636e-04,
          6.2525e-05, -1.6320e-04],
        [-9.8133e-04,  1.2875e-04,  6.3801e-04,  ..., -2.2709e-05,
         -1.0139e-04,  1.1377e-03],
        [-9.7394e-05, -4.3273e-05,  8.8811e-05,  ..., -3.0231e-04,
         -3.2926e-04,  2.9802e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.4952, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-7.4141, -5.5703, -5.9766,  ..., -8.6641, -5.6484, -1.7734],
        [-3.7852, -5.5977, -8.5078,  ..., -8.0391, -7.0352, -7.0352],
        [-4.8594, -3.0000, -4.1562,  ..., -5.4531, -3.9531, -0.4834],
        ...,
        [-4.8867, -4.0273, -4.8086,  ..., -5.3242, -5.9180, -0.4500],
        [-6.1641, -3.0703, -3.5234,  ..., -3.5078, -4.2109, -1.6650],
        [-5.3984, -3.2559, -3.7734,  ..., -4.3672, -5.4297, -0.6475]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 15, 22, 27, 15,  0, 13,  1,  0,  4, 27, 27, 18,  4, 10,  7, 13, 27,
        24, 23, 27, 10, 27, 27, 18, 18,  0,  0, 27, 27, 27, 27, 27, 27, 23, 27,
        27, 27, 26, 27, 27, 27, 17, 10,  4, 27,  1, 27, 27, 20, 27,  2, 27, 15,
        17,  2, 20, 27,  3, 15, 13, 20, 27, 27], device='cuda:0')
step: 665
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2382, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2051,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7197,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2305, -0.0996,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.1577e-04, -2.1858e-03,  9.6226e-04,  ...,  8.3590e-04,
         -5.0247e-05, -2.8729e-05],
        [-2.5392e-04,  1.1921e-03, -1.0926e-04,  ..., -2.1801e-03,
         -1.2529e-04,  1.8656e-05],
        [ 6.0558e-05,  2.6679e-04,  2.2054e-04,  ...,  4.6110e-04,
          7.1001e-04, -3.6049e-04],
        [ 1.0185e-03, -1.7953e-04, -8.6546e-05,  ...,  3.6120e-04,
         -2.2566e-04,  1.1551e-04],
        [ 1.3781e-04, -1.4114e-04,  1.4853e-04,  ...,  4.8137e-04,
          1.4615e-04, -3.2687e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.1201, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.1992, -4.2930, -4.7305,  ..., -5.5898, -6.2930, -0.4348],
        [-6.2344, -3.0293, -2.4668,  ..., -1.4053, -3.8730, -1.4521],
        [-2.3184, -3.0840, -4.7422,  ..., -7.1172, -4.9609, -2.5371],
        ...,
        [-5.8633, -2.8027, -2.2852,  ..., -2.4414, -4.4102, -2.2227],
        [-5.5703, -3.2402, -3.6641,  ..., -4.5703, -2.5234, -1.3975],
        [-5.9766, -5.0078, -4.8047,  ..., -6.6328, -4.0391, -1.4131]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 24,  4, 18, 12,  4, 15, 20,  3,  3, 27,  9,  2,  4, 10, 22, 18, 27,
        27,  1, 25,  2,  3, 15,  4,  3, 17, 26, 18,  7, 27,  2, 18, 25, 17,  4,
         5, 17,  7, 17, 27, 17, 15, 10, 22, 18, 11,  0, 27,  9, 15, 25, 25, 27,
        22, 14, 27, 27, 27, 20, 15, 12,  6,  7], device='cuda:0')
step: 666
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2305, -0.0996,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.9612e-04, -8.5235e-05, -2.5249e-04,  ..., -8.6927e-04,
          3.7026e-04,  3.1948e-04],
        [ 4.6563e-04, -7.1764e-04, -1.3566e-04,  ...,  1.4067e-04,
         -3.3426e-04, -8.9407e-05],
        [ 1.7166e-04, -1.8616e-03, -1.1654e-03,  ...,  1.1749e-03,
          8.0395e-04, -6.8617e-04],
        [-8.8692e-05,  1.3804e-04, -3.3855e-04,  ...,  8.0466e-06,
          4.5872e-04, -5.6744e-04],
        [ 4.2439e-04,  1.0900e-03, -6.5851e-04,  ..., -3.8648e-04,
          3.2091e-04,  1.6427e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.4900, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.3086, -3.8691, -4.0586,  ..., -4.8242, -3.8691, -1.4014],
        [-3.5391, -3.0547, -4.6172,  ..., -4.6797, -4.9297, -0.4302],
        [-6.2539, -3.6914, -3.4102,  ..., -5.3789, -4.8633, -0.4575],
        ...,
        [-5.6641, -2.8652, -3.5527,  ..., -3.3340, -4.1641, -0.4746],
        [-4.9609, -2.1934, -3.0996,  ..., -3.9590, -3.3652, -0.9751],
        [-5.9805, -3.2129, -4.1523,  ..., -5.9805, -3.3691, -0.5728]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  0,  4,  4, 15, 27, 27,  1,  4, 27, 27, 27, 27, 10, 27, 25, 27,  3,
        27, 27, 20, 27,  9, 15, 27, 26, 27, 10, 25, 15, 27, 27, 27, 27, 27, 27,
         1, 27, 10, 27, 27, 10,  3, 27, 20, 27,  4,  6, 27,  5, 27,  4, 10, 27,
        10, 25, 27,  0, 27, 27,  7, 27, 13, 27], device='cuda:0')
step: 667
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2305, -0.0996,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.9475e-04,  1.3804e-04,  7.9775e-04,  ...,  5.4741e-04,
         -4.9543e-04,  6.2227e-05],
        [ 2.1219e-04,  1.0419e-04,  6.1274e-04,  ..., -1.1740e-03,
         -1.2293e-03, -6.0177e-04],
        [ 1.6379e-04, -2.9111e-04,  1.7285e-06,  ..., -4.9257e-04,
          1.5640e-04, -8.0943e-05],
        [-8.6021e-04,  1.9717e-04,  1.0872e-03,  ...,  5.7125e-04,
          4.6825e-04,  5.4598e-04],
        [ 9.4771e-06, -1.5879e-04,  5.8937e-04,  ...,  2.6512e-04,
         -1.3793e-04,  1.8334e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.7366, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.7188, -3.8301, -4.5938,  ..., -6.3281, -5.2969, -0.1578],
        [-3.4082, -2.7227, -4.6602,  ..., -5.5508, -5.3008, -1.0186],
        [-5.4922, -3.8047, -4.3359,  ..., -5.9922, -6.6172, -0.1340],
        ...,
        [-4.9453, -3.3496, -3.6309,  ..., -5.1641, -6.6016, -0.4905],
        [-4.7031, -3.7656, -4.5938,  ..., -6.0156, -6.3750, -0.2500],
        [-4.5156, -4.7812, -6.1250,  ..., -8.3594, -7.4062, -5.4219]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27,  3, 27,  0,  2,  1, 11, 27, 27, 27,  2,  7, 27, 27, 10, 27, 15, 27,
        15,  9,  4, 27,  4,  9,  0, 27, 27, 10, 17, 27,  0,  6, 27, 27, 26, 27,
        17, 27,  1, 27, 10,  6, 22,  6,  3, 15, 27, 27,  9, 15, 27, 27,  7,  3,
         3,  4, 27, 27, 13, 25,  4, 27,  0, 18], device='cuda:0')
step: 668
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0996,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 6.0272e-04,  9.9659e-05,  8.6427e-05,  ...,  9.6798e-04,
         -7.5960e-04, -1.9181e-04],
        [ 4.3344e-04, -7.8344e-04,  7.9155e-04,  ...,  2.1667e-03,
         -3.7837e-04,  2.5272e-04],
        [ 5.1308e-04,  7.5102e-06,  1.9169e-04,  ...,  2.6703e-04,
         -6.9439e-05,  7.4244e-04],
        [ 1.7452e-04,  7.2622e-04,  1.8859e-04,  ..., -1.0414e-03,
         -7.4530e-04, -2.6083e-04],
        [ 4.3797e-04,  6.9284e-04, -4.3297e-04,  ...,  4.3058e-04,
          2.3055e-04, -2.6155e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.5225, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.6016, -5.2734, -6.5078,  ..., -8.1797, -3.3516, -2.7266],
        [-4.1758, -2.7852, -3.9570,  ..., -4.9102, -3.2852, -0.3789],
        [-5.0977, -5.2383, -7.2070,  ..., -6.0820, -5.7695, -4.4414],
        ...,
        [-6.0898, -2.2793, -2.6230,  ..., -1.4043, -4.6836, -2.0137],
        [-2.0625, -2.8906, -4.7188,  ..., -5.0625, -0.9219, -2.5469],
        [-3.3301, -2.8301, -3.7363,  ..., -4.3633, -3.1426, -1.7207]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 7, 27, 17,  9, 13, 27,  0,  1, 27, 20, 27,  1, 27, 27,  0,  4, 22,  4,
        20, 27, 17,  3, 10, 15, 27, 20, 20, 13,  6, 20, 27, 27,  9, 26,  7, 20,
        27, 26, 20,  2, 27,  7,  0,  3, 10,  8, 26, 18, 27, 27, 27, 27,  7,  1,
        27,  4,  0, 27,  2, 25, 27, 14, 26, 18], device='cuda:0')
step: 669
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0996,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 5.6601e-04, -7.8201e-05, -9.1124e-04,  ...,  8.3447e-04,
          1.0376e-03, -3.5334e-04],
        [ 2.1338e-05, -6.7949e-04, -1.9288e-04,  ..., -3.7384e-04,
         -6.5994e-04,  8.7118e-04],
        [ 4.0865e-04, -1.1320e-03, -7.8106e-04,  ..., -1.5450e-04,
         -2.5749e-04,  1.9765e-04],
        [ 9.8610e-04,  1.0049e-04, -5.5647e-04,  ..., -2.1863e-04,
         -3.2663e-05, -9.0504e-04],
        [ 9.1136e-05,  3.0732e-04, -5.7650e-04,  ..., -7.6246e-04,
         -5.0640e-04, -5.1260e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.5153, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.3086, -3.5723, -3.7285,  ..., -3.7129, -5.5078, -0.7754],
        [-3.5566, -4.9180, -6.4180,  ..., -8.1484, -6.8555, -6.4805],
        [-6.4805, -4.6055, -4.9648,  ..., -6.7617, -3.8242, -0.8237],
        ...,
        [-1.2803, -3.0449, -6.0938,  ..., -6.9375, -5.3594, -1.0918],
        [-5.8789, -2.5820, -3.0195,  ..., -4.5039, -5.1133, -0.5195],
        [-6.5820, -4.5195, -3.5645,  ..., -0.8306, -5.7383, -3.2832]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 18,  6, 15,  4, 27, 27, 14,  1,  1, 15, 15, 18, 27,  4, 27, 27, 27,
        20, 27, 20, 18,  0,  0,  4, 24, 17,  0, 15,  7, 27, 11,  7, 10, 27, 27,
        14, 27, 13, 27, 27, 17,  0, 27,  5, 15, 13, 27, 18, 15,  3,  5, 26, 18,
        15, 17, 27, 27, 27, 27, 27,  0, 27, 24], device='cuda:0')
step: 670
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0996,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.5361e-04, -5.6458e-04,  3.6144e-04,  ..., -9.5272e-04,
          5.5790e-05, -4.2939e-04],
        [-2.7537e-04,  1.2102e-03, -1.8787e-04,  ..., -2.0027e-03,
          3.0828e-04, -7.3814e-04],
        [-1.2946e-04,  9.7942e-04,  1.1148e-03,  ..., -6.2418e-04,
          8.6427e-06,  9.6607e-04],
        [ 4.1723e-07, -1.5783e-04,  5.2154e-05,  ..., -2.6321e-04,
         -2.2590e-05,  1.0800e-04],
        [-2.6488e-04, -3.1042e-04, -1.9217e-04,  ..., -7.8678e-06,
         -1.0622e-04, -3.4571e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.4830, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.7539, -4.9727, -6.2852,  ..., -4.2227, -5.8164, -3.5801],
        [-5.2812, -2.9863, -3.1426,  ..., -4.1289, -5.3125, -0.7832],
        [-3.9414, -1.6279, -3.3008,  ..., -3.6914, -2.5195, -1.4248],
        ...,
        [-2.6094, -3.0781, -3.7500,  ..., -4.9375, -1.4688, -1.6719],
        [-6.0000, -3.8750, -4.0625,  ..., -4.0625, -5.0312, -0.5156],
        [-6.3945, -4.0039, -4.4102,  ..., -5.5820, -2.9102, -1.3320]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([15, 27,  1, 27, 27, 27,  0, 27,  4, 22, 20,  4, 27, 27, 22,  1, 27,  4,
         1,  4, 27, 18, 27, 27,  7,  0, 18,  2, 27, 27, 27, 17,  0, 27, 27, 27,
        24,  0,  4, 26, 27,  0,  7, 27,  4,  0, 18, 27, 27,  5, 15, 18, 27, 27,
        18, 24, 27, 18,  0, 25, 27,  7, 27,  1], device='cuda:0')
step: 671
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0996,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 9.1434e-05, -1.0042e-03, -7.5293e-04,  ..., -3.2806e-04,
          5.9938e-04, -8.3828e-04],
        [-1.0520e-04,  1.9989e-03,  2.7227e-04,  ..., -1.0061e-03,
         -2.7275e-04, -1.0290e-03],
        [-1.0979e-04,  1.1950e-03,  6.9904e-04,  ...,  9.1314e-05,
          2.8968e-05,  1.8466e-04],
        [-5.9366e-04,  6.9237e-04, -9.9850e-04,  ..., -2.9802e-04,
          3.2067e-04, -1.2130e-04],
        [-6.5148e-05, -4.2820e-04,  3.4571e-06,  ..., -1.9813e-04,
         -3.8147e-06, -3.0398e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0758, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.9219, -3.5156, -3.6406,  ..., -4.3125, -5.2500, -0.7500],
        [-2.7871, -3.2246, -4.9141,  ..., -5.5547, -5.5078, -1.1621],
        [-2.8535, -5.0273, -8.5859,  ..., -6.0898, -5.8867, -5.0430],
        ...,
        [-6.2070, -2.7090, -2.6152,  ..., -4.1445, -3.7402, -2.6758],
        [-2.1621, -3.8184, -5.0977,  ..., -5.3008, -4.8008, -3.1465],
        [-6.1211, -3.1680, -3.6211,  ..., -4.3867, -5.4961, -0.6362]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([10, 17, 15,  5, 27, 26,  4, 17,  0,  5, 18, 27,  6, 13, 18,  3,  3, 26,
        27,  1, 15, 27,  4, 27, 17,  4, 27, 27, 27, 13, 27, 27, 27,  0,  7,  5,
        18, 27, 10,  1,  7, 25, 10, 15,  3, 20,  2,  4, 25, 27, 27, 14, 20, 24,
        20, 20, 26,  3, 20, 24, 27, 19, 18, 27], device='cuda:0')
step: 672
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.5517e-04, -3.5238e-04, -3.4332e-04,  ..., -1.9512e-03,
         -2.5368e-04,  1.3638e-04],
        [ 3.7503e-04,  8.3923e-05,  6.4087e-04,  ..., -1.3628e-03,
         -1.2617e-03, -1.7321e-04],
        [ 1.1843e-04, -6.8521e-04,  2.7752e-04,  ...,  3.9577e-04,
         -3.6383e-04,  1.5283e-04],
        [ 6.1464e-04, -8.5449e-04, -3.2139e-04,  ...,  3.3164e-04,
          7.7486e-05, -1.8024e-03],
        [ 5.4169e-04, -5.7364e-04,  4.1389e-04,  ..., -2.3460e-04,
          2.2173e-04,  2.8133e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8274, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.2402, -1.6465, -3.7559,  ..., -3.3496, -3.6309, -1.6621],
        [-0.0851, -4.6953, -6.5391,  ..., -8.9297, -4.7109, -4.8203],
        [-4.9648, -4.0469, -5.3125,  ..., -6.5742, -2.2949, -1.0918],
        ...,
        [-5.4219, -3.4043, -4.0469,  ..., -4.3750, -5.0625, -0.4985],
        [-4.6758, -3.4238, -4.8320,  ..., -6.0820, -5.2383, -0.2837],
        [-4.6055, -3.7012, -4.3594,  ..., -4.3750, -5.7656, -0.7324]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1,  0, 22, 27,  1, 18, 26, 27, 27, 17, 10, 18, 20,  7,  3, 22, 27, 27,
        18, 22,  7, 27, 15,  4, 27, 26, 10, 21, 27, 25,  0,  6, 27, 15, 18,  7,
        27, 27,  6, 15, 15, 10,  3,  4, 27, 17, 27, 17, 13, 10,  0, 18,  4, 10,
        26,  4, 10,  0,  3, 27,  7, 27, 27,  3], device='cuda:0')
step: 673
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0176e-03,  9.0122e-04,  1.2455e-03,  ..., -1.6010e-04,
         -2.8205e-04,  8.5306e-04],
        [ 2.8658e-04, -1.4296e-03, -9.6989e-04,  ...,  1.0071e-03,
          4.8113e-04,  5.5408e-04],
        [-2.0480e-04,  9.3412e-04,  5.6648e-04,  ...,  1.1182e-04,
          1.1778e-03,  1.4467e-03],
        [ 6.0940e-04, -2.0075e-04, -2.3746e-04,  ...,  3.0637e-04,
          7.5197e-04, -2.9540e-04],
        [ 4.4405e-05, -4.6825e-04,  4.4703e-04,  ...,  2.6035e-04,
         -6.8247e-05,  5.9795e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8083, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.7500, -3.3125, -3.6719,  ..., -3.2812, -5.5781, -1.1250],
        [-3.0527, -4.2383, -4.8516,  ..., -6.0039, -5.1289, -4.8672],
        [-5.3945, -5.2383, -6.0195,  ..., -6.1914, -7.3008, -4.7383],
        ...,
        [-5.1406, -4.1719, -4.7812,  ..., -4.6562, -4.6250, -0.4834],
        [-4.1172, -3.5703, -4.3047,  ..., -5.4453, -4.7422, -0.6475],
        [-2.0684, -1.8350, -3.5371,  ..., -3.2891, -3.4121, -2.0078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([18, 18, 20, 17,  3, 13,  3,  0,  5, 25, 26, 27, 17, 10, 20,  4, 27,  1,
        27, 15, 27, 27, 21, 25, 27,  1, 17,  6,  3, 24, 15, 27, 18, 27, 18,  3,
         0, 26, 17,  3, 27,  7, 20,  0, 13, 27,  7, 23,  0, 20,  1, 27, 27, 15,
        27,  8, 27,  7, 18, 27, 18, 10,  3, 11], device='cuda:0')
step: 674
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2500, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.3714e-03,  4.5824e-04, -6.4135e-05,  ..., -5.9843e-04,
          5.5790e-04, -4.4155e-04],
        [ 3.1590e-05, -2.7108e-04, -1.1787e-03,  ..., -3.9053e-04,
          1.4887e-03, -5.6207e-05],
        [-3.5262e-04, -5.6267e-04,  4.1533e-04,  ..., -1.9228e-04,
         -5.2834e-04,  1.9240e-04],
        [-8.5592e-04, -2.6274e-04,  9.6846e-04,  ...,  8.0299e-04,
          6.3562e-04,  1.7166e-04],
        [-3.8505e-04, -3.5691e-04,  3.9196e-04,  ..., -1.5426e-04,
         -5.1498e-04,  9.9242e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.5260, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.0508, -3.7539, -6.0195,  ..., -7.5508, -7.4883, -0.4890],
        [-3.7715, -3.8496, -5.0039,  ..., -6.0664, -6.7852, -0.7710],
        [-5.3984, -3.7734, -5.3359,  ..., -5.2734, -5.5234, -0.5703],
        ...,
        [-7.6289, -3.8184, -4.2539,  ..., -6.2070, -3.5527, -0.5059],
        [-7.3008, -5.3164, -4.7852,  ..., -6.4570, -3.8184, -1.2246],
        [-3.8613, -3.3613, -4.3945,  ..., -4.6445, -5.4414, -0.7993]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 4, 27, 20,  3, 10,  6, 27, 26,  3, 27,  7,  3, 27,  9, 27,  4, 27, 27,
        13,  7, 27,  4, 27, 17,  4, 27, 27, 27, 18, 27, 18,  3,  4, 14, 27,  0,
        12, 27, 27,  4,  0, 27,  8, 20, 18, 27,  9, 15,  8, 27, 24,  4, 14, 22,
         4,  0, 18, 27, 10, 26, 15, 27, 27, 27], device='cuda:0')
step: 675
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2510, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.9564e-04, -5.2691e-04, -1.1959e-03,  ...,  5.6410e-04,
          6.0463e-04,  1.4734e-04],
        [-6.2227e-04,  1.2674e-03, -1.0729e-03,  ...,  3.8075e-04,
          9.2125e-04, -1.1015e-03],
        [ 5.8770e-05,  4.5967e-04, -5.3263e-04,  ...,  1.1749e-03,
          3.4952e-04, -4.5133e-04],
        [ 1.5640e-03, -1.0509e-03, -2.6588e-03,  ..., -6.4325e-04,
         -9.7990e-05, -2.6493e-03],
        [ 5.3120e-04, -9.6464e-04,  6.1214e-05,  ..., -3.2187e-05,
          2.3007e-04,  2.6774e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.5964, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.3047, -3.9922, -5.4453,  ..., -6.6172, -5.7266, -0.5869],
        [-1.2461, -2.5742, -4.6836,  ..., -6.9180, -4.2148, -2.2617],
        [-5.2148, -3.9473, -4.8711,  ..., -6.6367, -5.1055, -0.2910],
        ...,
        [-3.9746, -4.1016, -3.2891,  ..., -0.8042, -4.9141, -3.2578],
        [-5.5195, -2.8652, -3.1152,  ..., -2.6934, -4.4414, -1.3486],
        [-5.8086, -3.8418, -4.2930,  ..., -4.8242, -4.1367, -0.3564]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 17, 27, 18, 27, 17, 27, 27, 17,  6,  4,  5, 10, 20,  8,  7, 27, 27,
        27, 20, 27, 13,  7, 27, 25, 26, 11, 20, 27, 21,  0, 20,  8,  1, 21, 11,
        27, 27, 27, 27, 15,  4, 26, 27, 27, 10, 25,  1, 27,  0,  0,  3, 27, 27,
         6, 27, 27, 27, 15, 27,  0,  8,  9, 27], device='cuda:0')
step: 676
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2510, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0996,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-2.1255e-04, -3.2616e-04,  7.9513e-05,  ...,  5.8603e-04,
         -2.1064e-04, -9.4604e-04],
        [-4.9257e-04,  7.5161e-05, -2.0218e-03,  ..., -5.8460e-04,
          1.8053e-03,  9.4318e-04],
        [-6.2180e-04,  4.5252e-04,  1.4198e-04,  ...,  8.0466e-06,
         -4.1556e-04, -1.7405e-04],
        [-1.7428e-04, -5.0604e-05, -1.3390e-03,  ..., -7.0810e-05,
          1.6284e-04, -3.3498e-05],
        [-4.1389e-04, -2.8372e-05,  1.6034e-04,  ...,  2.0730e-04,
          2.0218e-04, -2.1756e-05]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8051, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-2.9902, -3.1777, -5.1016,  ..., -5.2422, -3.1309, -3.9121],
        [-2.7656, -3.5938, -5.3906,  ..., -4.5156, -5.2031, -2.1719],
        [-5.1094, -3.8906, -4.3125,  ..., -5.1562, -5.7812, -0.9834],
        ...,
        [-3.8027, -3.0527, -4.6758,  ..., -7.3789, -5.6758, -1.5059],
        [-4.7109, -4.2422, -4.5234,  ..., -6.2422, -5.3359, -0.4460],
        [-5.8945, -4.2695, -4.9102,  ..., -6.9414, -2.8008, -2.0020]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1,  4, 27,  1, 15, 27,  4, 27, 27,  6, 27,  2,  9,  3,  3, 11,  4, 15,
         6, 27, 27, 27, 27, 10, 10, 26, 27, 27, 27, 27, 26, 15, 27, 14,  5, 27,
        17,  3, 10,  4, 27,  3,  7, 27, 27, 27, 20, 14, 20,  0,  1,  1,  1, 22,
         6,  3, 15, 27, 15, 10, 27,  0,  4,  7], device='cuda:0')
step: 677
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2510, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0996,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.1623e-04, -1.4038e-03,  4.2844e-04,  ...,  2.8896e-04,
         -7.2956e-04,  6.8843e-05],
        [-5.0068e-04,  7.2384e-04,  8.5831e-04,  ..., -3.9768e-04,
         -1.0490e-04, -1.6570e-05],
        [-5.7697e-04,  5.2500e-04,  3.5667e-04,  ..., -3.1948e-05,
         -8.3351e-04, -2.8563e-04],
        [-9.4032e-04,  4.2963e-04,  1.5745e-03,  ...,  4.4537e-04,
         -4.4298e-04,  1.0586e-03],
        [-6.1369e-04,  7.7295e-04, -5.0306e-04,  ...,  5.8889e-05,
         -3.6073e-04, -7.6675e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.3011, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[ -5.1758,  -4.8008,  -6.7070,  -4.8633,  -3.4727,  -3.7383, -10.4531,
          -7.5664,  -4.7070,  -7.8477,  -7.8477,  -7.8477,  -5.5352,  -2.6289,
          -9.4062,  -1.8467,  -9.1562,  -1.9561,  -4.3633,  -7.7227,  -0.7529,
          -7.0352,  -4.5352,  -4.9102,  -6.3633,  -5.3945,  -5.4883,  -3.7852],
        [ -3.5020,  -4.6758,  -4.8945,  -6.3477,  -3.4863,  -5.0664, -11.3203,
          -6.3320,  -0.6904,  -6.5508,  -6.5508,  -6.5508,  -5.5039,  -4.5039,
          -9.6250,  -6.6133,  -9.0938,  -3.4707,  -1.0811,  -9.9688,  -5.0820,
          -8.0781,  -5.7539,  -7.7852,  -7.9727,  -7.2539,  -6.9102,  -4.9102],
        [ -5.2227,  -4.4375,  -5.4570,  -4.3945,  -2.5020,  -5.0820,  -6.0039,
          -5.5977,  -6.2227,  -5.4258,  -5.4258,  -5.4258,  -6.4258,  -6.9844,
          -7.1094,  -7.0312, -10.6094,  -6.5039,  -6.5312,  -8.2969,  -4.9258,
          -6.8008,  -3.9238,  -8.0938,  -6.1250,  -6.9219,  -5.8164,  -0.2052],
        [ -0.4175,  -3.1055,  -4.7617,  -5.0742,  -3.5898,  -5.0742,  -6.9180,
          -5.7773,  -5.5898,  -6.5273,  -6.5273,  -6.5273,  -5.4023,  -3.8398,
          -6.5742,  -5.5430, -10.8359,  -3.7773,  -4.2773,  -7.7305,  -4.4805,
          -6.0430,  -3.4961,  -5.9805,  -7.1523,  -5.9180,  -2.8555,  -2.8242],
        [ -5.2148,  -3.1836,  -3.9180,  -3.3867,  -2.3555,  -4.4805,  -3.6816,
          -5.0898,  -6.2305,  -3.4961,  -3.4961,  -3.4961,  -5.7148,  -7.7148,
          -5.2773,  -7.6211,  -8.6953,  -6.8242,  -6.5430,  -6.6836,  -6.0430,
          -6.5898,  -3.9336,  -7.5742,  -5.3555,  -5.7930,  -5.3398,  -0.4795],
        [ -2.9434,  -3.0371,  -4.0508,  -5.3945,  -2.6602,  -3.7695,  -8.0547,
          -5.4102,  -1.2236,  -5.5977,  -5.5977,  -5.5977,  -4.4414,  -3.8027,
          -7.4570,  -4.9414,  -7.7695,  -2.5371,  -3.8965,  -7.8633,  -1.8330,
          -6.3945,  -2.7559,  -5.5508,  -6.1602,  -5.5977,  -5.0664,  -2.3965],
        [ -1.3955,  -2.7715,  -4.6133,  -5.5195,  -3.4570,  -4.0039,  -8.6172,
          -6.8008,  -3.2246,  -5.8477,  -5.8477,  -5.8477,  -4.4102,  -4.8633,
          -6.5352,  -6.5039,  -8.6328,  -1.4893,  -1.4736,  -7.6758,  -6.1758,
          -5.0352,  -5.4414,  -7.8320,  -6.1133,  -6.6758,  -4.4414,  -2.6621],
        [ -2.5020,  -4.2383,  -5.1289,  -5.5820,  -2.0020,  -5.1602,  -9.8906,
          -3.7207,  -1.1738,  -6.3633,  -6.3633,  -6.3633,  -5.6133,  -3.2676,
          -9.5781,  -4.7695,  -9.2188,  -4.2070,  -1.1904,  -9.3906,  -3.6738,
          -7.5664,  -5.5977,  -7.9414,  -6.8320,  -7.7695,  -5.5195,  -4.6602],
        [ -2.8477,  -3.4414,  -4.4570,  -3.9727,  -1.4883,  -4.3008,  -6.2070,
          -4.7539,  -5.2695,  -5.8477,  -5.8477,  -5.8477,  -5.2227,  -4.1445,
          -6.3164,  -6.1445,  -9.6094,  -4.6133,  -5.6914,  -7.7539,  -2.9102,
          -5.9258,  -2.4102,  -5.6602,  -6.0977,  -5.7383,  -3.4883,  -0.9414],
        [ -2.4062,  -3.4531,  -4.6875,  -3.4375,  -3.3418,  -3.7500,  -7.5781,
          -6.3438,  -5.9375,  -5.4688,  -5.4688,  -5.4688,  -4.8438,  -3.2500,
          -7.2188,  -2.5312,  -8.7656,  -1.9678,  -3.8438,  -6.4062,  -1.5303,
          -4.6875,  -3.6875,  -3.9219,  -5.6406,  -3.9844,  -3.6250,  -1.8896],
        [ -3.5684,  -4.0391,  -6.7266,  -4.8945,  -3.1152,  -3.5059,  -8.7109,
          -6.0195,  -6.7578,  -7.3984,  -7.3984,  -7.3984,  -5.8828,  -5.0039,
          -9.7734,  -0.3970,  -9.6172,  -3.4590,  -4.2070,  -8.2422,  -2.7402,
          -6.5195,  -3.8652,  -4.0391,  -5.7539,  -5.9258,  -4.5391,  -4.1641],
        [ -3.1426,  -4.1094,  -6.2344,  -4.6875,  -5.1406,  -3.5020,  -9.0312,
          -8.8906,  -6.2812,  -8.5469,  -8.5469,  -8.5469,  -5.6406,  -4.9844,
         -11.0312,  -0.2825,  -9.3438,  -2.9082,  -4.8125,  -7.0781,  -4.8594,
          -6.0469,  -4.4062,  -5.6406,  -5.6562,  -4.6406,  -5.0000,  -4.0781],
        [ -5.5117,  -3.5742,  -4.2773,  -4.1992,  -1.0439,  -4.5117,  -6.7461,
          -6.0742,  -4.8242,  -5.1367,  -5.1367,  -5.1367,  -5.2773,  -4.3242,
          -7.0273,  -6.3711,  -8.5781,  -4.4336,  -4.9023,  -6.7305,  -3.6387,
          -5.5742,  -3.6074,  -6.5117,  -6.3711,  -5.7461,  -5.6211,  -0.8252],
        [ -3.9668,  -2.7324,  -3.5918,  -3.5137,  -2.4043,  -4.5430,  -2.8418,
          -1.9355,  -4.9961,  -3.8418,  -3.8418,  -3.8418,  -5.0898,  -3.4199,
          -4.4805,  -5.9648,  -6.9648,  -4.9023,  -5.2305,  -5.5742,  -4.7773,
          -5.7148,  -3.0137,  -6.5586,  -5.7305,  -4.2773,  -1.8877,  -1.7002],
        [ -3.3125,  -4.1562,  -5.7344,  -5.6719,  -3.4062,  -2.1250,  -9.3438,
          -8.1406,  -4.5312,  -8.2812,  -8.2812,  -8.2812,  -4.3438,  -4.7969,
          -7.4375,  -3.9062,  -9.0156,  -2.1719,  -2.7969,  -7.1250,  -1.6406,
          -5.8750,  -4.0781,  -5.6562,  -5.9844,  -4.7188,  -7.3750,  -1.1094],
        [ -2.3984,  -2.4766,  -4.1484,  -3.5703,  -2.0859,  -4.0547,  -8.1484,
          -4.1641,  -4.9453,  -5.7891,  -5.7891,  -5.7891,  -4.4141,  -2.8984,
          -5.4609,  -4.0859,  -9.3359,  -3.8516,  -3.4297,  -8.1484,  -1.5381,
          -4.6484,  -3.2422,  -5.9609,  -7.3516,  -6.1172,  -3.3828,  -1.8193],
        [ -2.7793,  -3.6719,  -5.1562,  -4.5000,  -0.9048,  -4.1875,  -7.4688,
          -4.7188,  -4.6875,  -5.3125,  -5.3125,  -5.3125,  -5.1406,  -4.7812,
          -8.4531,  -5.2500,  -9.7188,  -4.2344,  -4.7969,  -7.7188,  -3.5312,
          -5.0781,  -3.0000,  -6.1875,  -7.0938,  -6.6094,  -5.4062,  -1.1865],
        [ -0.7227,  -3.4102,  -5.6758,  -6.3320,  -3.3633,  -4.8633,  -9.3828,
          -6.9570,  -3.7383,  -6.0352,  -6.0352,  -6.0352,  -4.9570,  -4.2227,
          -7.7852,  -6.9258, -11.3359,  -2.5352,  -1.3164,  -9.4453,  -6.9727,
          -6.4414,  -5.5508,  -8.0547,  -7.5039,  -7.2539,  -5.1133,  -4.0352]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([20, 18, 27,  0, 27,  8, 27, 18, 27, 17, 15, 15, 27,  7,  5,  4, 20, 18],
       device='cuda:0')
step: 678
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2510, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0996,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.2722e-03, -8.5831e-04, -1.7939e-03,  ..., -1.1330e-03,
         -4.0078e-04, -2.4300e-03],
        [ 4.6825e-04,  1.8299e-05,  1.6272e-04,  ..., -1.0204e-03,
          7.1239e-04, -2.6226e-04],
        [ 7.8058e-04, -8.4162e-04, -6.4707e-04,  ...,  1.0653e-03,
         -7.2622e-04, -6.1369e-04],
        [-1.3638e-03,  4.0352e-05,  1.7905e-04,  ..., -1.9073e-05,
          6.9714e-04,  5.6982e-04],
        [-3.5930e-04, -1.0033e-03,  2.5511e-04,  ...,  6.6614e-04,
         -7.0524e-04, -6.1464e-04]], device='cuda:0', dtype=torch.float16)
Epoch 0, average train loss: 2.4193431319825254, time 587.46s
Epoch 0, average test loss: 1.78737911196316
acc: 0.49741983044600074 
F1: 0.2684729147011003 
precision: 0.3773695139537598 
recall: 0.26133870231944734
lowest_loss updated

================ Epoch 1 ================
learning rate: 9.499263622974963e-05
loss: tensor(1.9483, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-5.2891, -2.9453, -4.2422,  ..., -5.5234, -3.9609, -0.5869],
        [-2.1406, -4.5469, -5.0938,  ..., -6.7969, -6.2656, -4.5469],
        [-4.6484, -2.2246, -2.1484,  ..., -1.7256, -6.9922, -4.1016],
        ...,
        [-5.6992, -4.0742, -4.5117,  ..., -5.1055, -7.5898, -0.4021],
        [-5.6445, -4.7539, -5.8008,  ..., -7.5039, -6.5820, -0.8486],
        [-5.4805, -2.5293, -3.1387,  ..., -3.7480, -2.7793, -1.4355]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 1, 27, 25,  9,  1,  7, 17, 27, 27, 18, 15,  5,  4,  9, 27,  1,  0, 27,
        26,  5, 27, 11,  9,  9,  3, 11,  2,  1, 27,  8, 27, 15, 26,  7, 10, 27,
        27,  7, 27, 27, 26, 27,  1, 14, 15, 25,  9, 20, 27, 27,  1, 27,  9,  4,
         2, 27,  1, 27,  4, 27, 27, 27, 27, 25], device='cuda:0')
step: 0
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2510, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.6757e-06, -6.6233e-04, -1.8716e-04,  ...,  1.1301e-03,
         -1.0699e-04, -1.1635e-04],
        [ 1.8978e-04, -2.8670e-05,  4.4632e-04,  ...,  7.6675e-04,
         -1.0271e-03, -6.4516e-04],
        [ 1.4138e-04,  7.3791e-05,  4.9353e-04,  ..., -1.1086e-04,
         -1.0595e-03, -4.5896e-06],
        [-9.1505e-04, -1.1861e-04,  8.0681e-04,  ...,  5.4455e-04,
         -1.3947e-04,  3.1948e-04],
        [-2.6965e-04,  1.2851e-04, -1.6522e-04,  ...,  2.9922e-05,
         -2.5344e-04, -4.9877e-04]], device='cuda:0', dtype=torch.float16)
Progress 0.15%, loss: 1.9483163356781006, time 1.85s
loss: tensor(1.7319, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.5117, -3.4648, -3.3711,  ..., -3.8555, -5.3555, -0.8716],
        [-0.3848, -4.1016, -7.2578,  ..., -7.7109, -5.2734, -3.4160],
        [-4.3242, -3.3086, -3.2148,  ..., -4.1211, -2.6836, -2.6367],
        ...,
        [-4.7031, -3.5312, -4.3594,  ..., -6.5938, -5.3750, -0.4521],
        [-4.9648, -3.1543, -3.8262,  ..., -4.8555, -2.3398, -1.5127],
        [-1.3789, -3.6602, -4.5352,  ..., -4.2070, -5.0508, -2.9883]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2,  0, 19, 27, 27, 22, 27, 15,  3,  1, 15, 15, 27, 27,  3,  4, 15, 10,
        18,  4, 27, 27,  4,  9, 27, 11, 10,  4,  3, 27, 27,  3,  5, 27, 27, 27,
         5, 27,  0, 27,  5, 15,  1, 25,  9, 27, 10, 27,  6, 27, 22, 27,  7,  7,
        22, 27, 27,  7, 27, 26, 20, 27,  6, 15], device='cuda:0')
step: 1
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0812,  1.5127],
        [ 1.2510, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-7.7581e-04,  6.4850e-05,  2.0676e-03,  ...,  3.5644e-04,
         -2.4021e-04, -5.8293e-05],
        [ 8.9836e-04, -4.7708e-04,  6.0034e-04,  ..., -1.2970e-04,
         -7.8011e-04, -8.0967e-04],
        [ 2.7466e-04, -1.8215e-04, -8.5402e-04,  ...,  2.6155e-04,
          9.6273e-04, -4.1080e-04],
        [-3.2616e-04,  3.0994e-04,  2.5787e-03,  ...,  1.2636e-03,
         -2.1648e-04,  7.5245e-04],
        [ 6.7174e-05,  1.6174e-03, -5.9891e-04,  ...,  1.4734e-04,
          4.3154e-04, -5.9938e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6675, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-3.5039, -3.5801, -5.4727,  ..., -6.2383, -4.5820, -2.3770],
        [-0.6660, -3.7910, -6.1055,  ..., -7.2148, -5.2617, -4.0273],
        [-1.2383, -3.5195, -5.5352,  ..., -7.0977, -5.1914, -3.8164],
        ...,
        [-3.4375, -2.8906, -4.1250,  ..., -5.4062, -3.2969, -1.0781],
        [-5.5039, -4.0000, -4.1406,  ..., -6.1602, -6.7344, -0.2052],
        [-3.8691, -2.8223, -4.0703,  ..., -4.8984, -3.7129, -0.8682]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 8, 18,  0,  8,  2, 27, 26, 10, 17, 27,  3, 15,  5, 27,  4,  3, 13, 22,
         0, 15,  1, 18, 11, 25, 27, 15,  7, 12, 13,  1, 22, 27, 27, 10, 18, 27,
         0, 27, 27, 18,  0, 18, 27, 18, 27, 18, 27,  0, 27, 27,  5, 19, 27, 27,
        27, 27, 24, 15,  1, 20, 27,  1,  5,  1], device='cuda:0')
step: 2
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0811,  1.5127],
        [ 1.2510, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0252e-04, -1.1606e-03,  4.6754e-04,  ..., -1.9350e-03,
          2.4557e-04, -1.8835e-04],
        [ 7.9966e-04,  1.6203e-03, -7.5817e-04,  ...,  1.0519e-03,
          1.1072e-03, -9.9373e-04],
        [ 3.6180e-05, -1.6510e-05, -8.2207e-04,  ..., -4.1103e-04,
          1.1373e-04, -4.9257e-04],
        [-1.4114e-03, -3.3569e-04,  3.7766e-03,  ...,  1.1024e-03,
          1.9312e-04, -6.7234e-04],
        [ 1.6928e-04,  8.7023e-04, -5.2869e-05,  ...,  1.1683e-05,
          3.4630e-05, -5.3263e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.8019, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.6289, -4.4102, -4.8164,  ..., -5.5820, -3.2676, -1.1738],
        [-3.3047, -2.9609, -4.3516,  ..., -6.0547, -3.5078, -1.2266],
        [-5.7266, -4.4922, -4.4922,  ..., -4.6016, -4.5703, -0.8218],
        ...,
        [-1.8125, -2.2500, -4.3750,  ..., -3.6094, -3.8125, -3.5156],
        [-4.7852, -2.5977, -3.0820,  ..., -3.8320, -1.7061, -2.4727],
        [-6.5508, -3.6113, -4.2383,  ..., -5.2227, -2.5020, -1.5645]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([27, 18, 27, 20,  1, 20, 13,  0,  4, 10, 15, 20,  1,  6, 25, 27, 18,  2,
         4,  2,  9,  1, 15, 27, 27, 27,  4,  0, 27, 27,  0,  4, 27,  9,  4,  0,
        27,  0,  9, 11, 27, 22, 27,  1, 25,  7,  2, 27, 18, 27,  4, 27,  2, 18,
         2, 15, 27, 18, 27, 27,  3,  0, 12,  7], device='cuda:0')
step: 3
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0811,  1.5127],
        [ 1.2510, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.4172e-03,  1.5459e-03, -1.3981e-03,  ..., -4.3106e-04,
         -8.8787e-04, -1.1129e-03],
        [ 1.3590e-04,  8.9943e-05, -3.3796e-05,  ...,  2.6016e-03,
          2.9778e-04, -3.4070e-04],
        [ 1.0240e-04, -8.8453e-05,  2.0802e-05,  ...,  1.7071e-03,
          3.6335e-04, -1.6296e-04],
        [ 1.2636e-03,  4.0293e-04, -8.9025e-04,  ..., -1.0848e-04,
         -1.1244e-03, -1.7986e-03],
        [ 2.7561e-04, -8.0407e-05,  3.2902e-04,  ...,  3.0994e-04,
          3.4165e-04, -1.9205e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(1.6863, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-4.1992, -1.7764, -2.5723,  ..., -3.1992, -2.5254, -2.2441],
        [-4.9336, -3.5566, -3.6035,  ..., -4.3984, -5.1680, -0.6499],
        [-5.3516, -3.4609, -4.1641,  ..., -5.3672, -6.1484, -0.6016],
        ...,
        [-3.7734, -3.6797, -5.3828,  ..., -6.2422, -1.7578, -0.5703],
        [-6.0625, -3.3594, -3.8262,  ..., -6.0469, -3.3750, -1.1396],
        [-3.2266, -4.3828, -5.8672,  ..., -8.6797, -6.2266, -5.0859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([11, 11, 20, 11, 27, 17, 15, 27, 20, 27, 20,  0, 27, 15, 24, 20, 27, 26,
         3,  7,  0, 27, 25, 21, 18, 10, 27, 27, 20, 20, 27, 27, 15, 27, 27, 25,
        25, 27,  1, 26,  4, 27, 27, 22,  3, 10, 11, 25, 22, 25,  0, 15,  3, 10,
        27,  0,  0,  4,  3, 13, 27, 27,  7, 18], device='cuda:0')
step: 4
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0811,  1.5127],
        [ 1.2510, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6597,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-6.4993e-04,  7.0381e-04,  6.6614e-04,  ...,  3.7849e-05,
         -9.2268e-04, -4.2415e-04],
        [ 9.1374e-05, -7.0190e-04,  5.2261e-04,  ...,  4.0054e-05,
         -1.9968e-05,  4.2605e-04],
        [-5.6982e-04,  1.3361e-03,  7.9012e-04,  ..., -4.5490e-04,
         -5.4026e-04,  7.3004e-04],
        [-1.5583e-03,  2.7800e-04,  1.5438e-05,  ..., -3.3379e-04,
          2.7108e-04,  7.6199e-04],
        [-6.8998e-04, -5.3549e-04,  3.7026e-04,  ...,  4.5359e-05,
         -7.9584e-04, -4.0531e-04]], device='cuda:0', dtype=torch.float16)
loss: tensor(2.0132, device='cuda:0', grad_fn=<DivBackward0>)
===============logits===============
tensor([[-6.4961, -5.0742, -5.1680,  ..., -6.9492, -3.8867, -1.4180],
        [-4.4414, -2.8320, -3.8945,  ..., -3.9414, -5.5195, -1.1133],
        [-5.4844, -4.2656, -5.3438,  ..., -7.4531, -3.9375, -1.1260],
        ...,
        [-5.3984, -4.4453, -4.9453,  ..., -5.5234, -1.8193, -1.8818],
        [-6.6719, -5.6406, -5.8594,  ..., -7.6406, -4.5000, -1.4824],
        [-3.9570, -3.0684, -3.8633,  ..., -5.2070, -2.8027, -1.4424]],
       device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)
===============label===============
tensor([ 2, 11, 27,  7, 27,  4, 22, 18, 27,  6, 12, 24, 27,  1, 27,  7,  7, 27,
        11,  0, 27, 27, 27,  3, 27, 27,  0, 27, 27,  3, 15, 17,  7,  0, 22, 26,
         6,  2,  0, 10, 22, 27,  0, 22,  1,  1, 15, 27, 27, 27,  1, 27, 27, 27,
        10, 10, 15,  0,  3, 27,  0, 26,  7, 27], device='cuda:0')
step: 5
prompt_model.template.soft_embedding.weight
=============prompt.data 1===========
tensor([[ 0.8643, -0.2338,  2.2871,  ...,  1.0430, -1.2549, -1.9160],
        [-1.7656, -0.2380, -0.6143,  ...,  0.4790, -0.0811,  1.5127],
        [ 1.2510, -1.8926, -0.2052,  ..., -2.3828,  0.6055, -0.8257],
        [ 1.9102, -0.2211,  0.7944,  ..., -1.9053, -1.3477, -0.6035],
        [-0.4089, -0.2969, -1.7207,  ..., -0.6602,  0.6533,  1.2676],
        [-1.1729,  2.2285, -0.0995,  ...,  0.6548,  0.7085, -1.7998]],
       device='cuda:0', dtype=torch.float16)
=============prompt.grad 1===========
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan],
        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',
       dtype=torch.float16)
